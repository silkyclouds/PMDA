#!/usr/bin/env python3
# -*- coding: utf-8 -*-
from __future__ import annotations

# --- GUI fallback stub for display_popup ---------------------------------------
try:
    import gui
except ModuleNotFoundError:
    # Fallback GUI stub if real gui module is missing
    class gui:
        @staticmethod
        def display_popup(message: str):
            # In headless mode, log the popup message as an error
            logging.error("[GUI POPUP] %s", message)

# Maximum consecutive artists with no valid files before aborting scan
NO_FILE_THRESHOLD = 10
# Global counter for consecutive no-file artists
no_file_streak_global = 0
# Track whether the no‚Äëfiles popup has been shown to avoid duplicates
popup_displayed = False

"""
v0.7.5
- Improvement of the detection for albums with "no name" 
"""

import base64
import ast
import html
import json
import os
import shutil
import uuid
import filecmp
import errno
import sqlite3
import subprocess
import threading
import time
import atexit
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path
from typing import NamedTuple, List, Dict, Optional, Tuple
from urllib.parse import quote, quote_plus

import logging
import re
import socket
import struct
import hashlib
import math
import unicodedata
import xml.etree.ElementTree as ET

import requests
import musicbrainzngs

# MusicBrainz user agent will be configured after config is loaded
# (see _configure_musicbrainz_useragent function)
# Set rate limiting: 1 request per second (MusicBrainz limit)
musicbrainzngs.set_rate_limit(limit_or_interval=1.0, new_requests=1)
# Reduce noise from musicbrainzngs XML parser (e.g. "in <ws2:release-group>, uncaught attribute type-id")
logging.getLogger("musicbrainzngs").setLevel(logging.WARNING)
from openai import OpenAI
try:
    import anthropic
except ImportError:
    anthropic = None
try:
    # google-generativeai is deprecated; use google-genai (import path: google.genai).
    from google import genai
except ImportError:
    genai = None

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ANSI colours for prettier logs ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
ANSI_RESET   = "\033[0m"
ANSI_BOLD    = "\033[1m"
ANSI_DIM     = "\033[2m"
ANSI_GREEN   = "\033[92m"
ANSI_YELLOW  = "\033[93m"
ANSI_CYAN    = "\033[96m"
ANSI_RED     = "\033[91m"
ANSI_MAGENTA = "\033[95m"
ANSI_BLUE    = "\033[94m"

def log_header(title: str) -> None:
    """Print a bold cyan header like `----- TITLE -----`."""
    logging.info("\n" + colour(f"----- {title.upper()} -----", ANSI_BOLD + ANSI_CYAN))

def colour(txt: str, code: str) -> str:
    """Wrap *txt* in an ANSI colour code unless NO_COLOR env var is set."""
    if os.getenv("NO_COLOR"):
        return txt
    return f"{code}{txt}{ANSI_RESET}"

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Robust cross‚Äëdevice move helper ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def safe_move(src: str, dst: str):
    """
    Move *src* ‚Üí *dst* de fa√ßon robuste, y compris entre volumes (EXDEV).
    - Try atomic same‚Äëdevice rename first (os.replace).
    - On EXDEV or other rename failures, copy (file or tree) then remove source with retries.
    - Handles ENOTEMPTY/EBUSY during rmtree on slow NAS/SMB by retrying.
    """
    src_path = Path(src)
    dst_path = Path(dst)
    dst_path.parent.mkdir(parents=True, exist_ok=True)

    # 1) Fast path: atomic rename when on same device
    try:
        os.replace(src, dst)
        return
    except OSError as exc:
        if exc.errno != errno.EXDEV:
            logging.warning("safe_move(): os.replace failed (%s) ‚Äì falling back to copy", exc)
        # continue to copy fallback

    # 2) Choose a non‚Äëclobbering destination (in case of leftovers)
    final_dst = dst_path
    if final_dst.exists():
        base = final_dst.name
        parent = final_dst.parent
        n = 1
        while (parent / f"{base} ({n})").exists():
            n += 1
        final_dst = parent / f"{base} ({n})"
        logging.warning("safe_move(): destination exists, using %s", final_dst)

    # 3) Copy (dir or single file)
    try:
        if src_path.is_dir():
            shutil.copytree(src_path, final_dst, dirs_exist_ok=False)
        else:
            shutil.copy2(src_path, final_dst)
    except Exception as copy_err:
        logging.error("safe_move(): copy failed %s ‚Üí %s ‚Äì %s", src_path, final_dst, copy_err)
        raise

    # 4) Remove source with retries (tolerate ENOTEMPTY/EBUSY on NAS)
    for attempt in range(5):
        try:
            if src_path.is_dir():
                shutil.rmtree(src_path)
            else:
                try:
                    src_path.unlink()
                except FileNotFoundError:
                    pass
            break
        except OSError as e:
            if e.errno in (errno.ENOTEMPTY, errno.EBUSY):
                logging.warning("safe_move(): rmtree(%s) failed (%s) ‚Äì retry %d/5", src, e, attempt + 1)
                time.sleep(1.5)
                continue
            raise
    else:
        logging.warning("safe_move(): forcing removal of residual files in %s", src)
        shutil.rmtree(src_path, ignore_errors=True)

    # 5) Final safety net
    if os.path.exists(src):
        shutil.rmtree(src, ignore_errors=True)

from queue import SimpleQueue, Queue
import sys
import random

try:
    import psycopg
except ImportError:
    psycopg = None
try:
    import redis as redis_lib
except ImportError:
    redis_lib = None
try:
    from watchdog.events import FileSystemEventHandler
    from watchdog.observers import Observer
except ImportError:
    FileSystemEventHandler = None  # type: ignore[assignment]
    Observer = None  # type: ignore[assignment]


from flask import Flask, request, jsonify, send_from_directory, redirect, Response, send_file

app = Flask(__name__)

# Path to integrated frontend build (self-hosted: one container = backend + UI)
_FRONTEND_DIST = os.path.join(os.path.dirname(os.path.abspath(__file__)), "frontend", "dist")
_HAS_STATIC_UI = os.path.isdir(_FRONTEND_DIST)

# CORS: allow frontend (e.g. dev server on port 3000 or 8080) to call the API
@app.after_request
def _cors_headers(response):
    response.headers["Access-Control-Allow-Origin"] = "*"
    response.headers["Access-Control-Allow-Methods"] = "GET, POST, PUT, OPTIONS"
    response.headers["Access-Control-Allow-Headers"] = "Content-Type"
    return response

# Ensure LOG_LEVEL exists for initial logging setup (effective level from SQLite applied later via merged)
LOG_LEVEL = "INFO"

# (8) Logging setup (must happen BEFORE any log statements elsewhere) ---------
_level_num = getattr(logging, LOG_LEVEL, logging.INFO)

logging.basicConfig(
    level=_level_num,
    format="%(asctime)s ‚îÇ %(levelname)s ‚îÇ %(threadName)s ‚îÇ %(message)s",
    datefmt="%H:%M:%S",
    force=True,
    handlers=[logging.StreamHandler(sys.stdout)],
)


PMDA_LOG_VERBOSE = bool(os.getenv("PMDA_LOG_VERBOSE"))


class ColourFormatter(logging.Formatter):
    """Add rich ANSI colours to logs so scanning the console is actually pleasant."""

    LEVEL_COLOURS = {
        logging.DEBUG: ANSI_CYAN,
        logging.INFO: ANSI_GREEN,
        logging.WARNING: ANSI_YELLOW,
        logging.ERROR: ANSI_RED,
        logging.CRITICAL: ANSI_RED + ANSI_BOLD,
    }

    def format(self, record):
        if not os.getenv("NO_COLOR"):
            # Colour the level name
            colour_code = self.LEVEL_COLOURS.get(record.levelno)
            if colour_code:
                record.levelname = f"{colour_code}{record.levelname}{ANSI_RESET}"

            # Colour thread names so concurrent workers are easier to follow
            thread = record.threadName
            try:
                if "background_scan" in thread:
                    thread_colour = ANSI_GREEN
                elif "process_request_thread" in thread:
                    thread_colour = ANSI_CYAN
                elif "ThreadPoolExecutor" in thread:
                    # Stable colour per executor thread
                    palette = [ANSI_BLUE, ANSI_MAGENTA, ANSI_YELLOW, ANSI_GREEN, ANSI_CYAN]
                    idx = abs(hash(thread)) % len(palette)
                    thread_colour = palette[idx]
                else:
                    thread_colour = ANSI_DIM
                record.threadName = f"{thread_colour}{thread}{ANSI_RESET}"
            except Exception:
                record.threadName = f"{ANSI_DIM}{thread}{ANSI_RESET}"

            # Colour‚Äëcode domains by [TAG] prefix
            msg = record.getMessage()
            coloured_msg = msg
            try:
                if msg.startswith("[MB]"):
                    coloured_msg = colour(msg, ANSI_CYAN)
                elif msg.startswith("[AI]"):
                    coloured_msg = colour(msg, ANSI_MAGENTA)
                elif msg.startswith("[DUPES]"):
                    coloured_msg = colour(msg, ANSI_BLUE)
                elif msg.startswith("[LIVE]"):
                    coloured_msg = colour(msg, ANSI_YELLOW)
                elif msg.startswith("[PATH]"):
                    coloured_msg = colour(msg, ANSI_GREEN)
                elif msg.startswith("[SCAN]"):
                    coloured_msg = colour(msg, ANSI_GREEN + ANSI_BOLD)
                elif msg.startswith("[CFG]"):
                    coloured_msg = colour(msg, ANSI_MAGENTA)
                elif msg.startswith("[COV]"):
                    coloured_msg = colour(msg, ANSI_GREEN)
                elif msg.startswith("[ART]"):
                    coloured_msg = colour(msg, ANSI_BLUE)
                elif msg.startswith("[TAG]"):
                    coloured_msg = colour(msg, ANSI_CYAN)
                # Override the formatted message and clear args so logging
                # does not attempt %-interpolation a second time.
                record.msg = coloured_msg
                record.args = ()
            except Exception:
                # If anything goes wrong while colourising, fall back to plain message
                record.msg = msg

        return super().format(record)


_root_logger = logging.getLogger()
for _handler in _root_logger.handlers:
    _handler.setFormatter(
        ColourFormatter(
            "%(asctime)s ‚îÇ %(levelname)s ‚îÇ %(threadName)s ‚îÇ %(message)s",
            datefmt="%H:%M:%S",
        )
    )


def _log(domain: str, level: int, msg: str, *args, **kwargs) -> None:
    """Internal helper to log with a [TAG] prefix and domain-aware colouring."""
    prefix = f"[{domain}] "
    logging.log(level, prefix + msg, *args, **kwargs)


def log_scan(msg: str, *args, **kwargs) -> None:
    _log("SCAN", logging.INFO, msg, *args, **kwargs)


def log_mb(msg: str, *args, **kwargs) -> None:
    _log("MB", logging.INFO, msg, *args, **kwargs)


def log_ai(msg: str, *args, **kwargs) -> None:
    _log("AI", logging.INFO, msg, *args, **kwargs)


def log_dupes(msg: str, *args, **kwargs) -> None:
    _log("DUPES", logging.INFO, msg, *args, **kwargs)


def log_live(msg: str, *args, **kwargs) -> None:
    _log("LIVE", logging.INFO, msg, *args, **kwargs)


def log_path(msg: str, *args, **kwargs) -> None:
    _log("PATH", logging.INFO, msg, *args, **kwargs)


def log_cfg(msg: str, *args, **kwargs) -> None:
    _log("CFG", logging.INFO, msg, *args, **kwargs)


def log_cov(msg: str, *args, **kwargs) -> None:
    """Logging helper for cover artwork operations."""
    _log("COV", logging.INFO, msg, *args, **kwargs)


def log_art(msg: str, *args, **kwargs) -> None:
    """Logging helper for artist-image operations."""
    _log("ART", logging.INFO, msg, *args, **kwargs)


def log_tag(msg: str, *args, **kwargs) -> None:
    """Logging helper for tag read/write operations."""
    _log("TAG", logging.INFO, msg, *args, **kwargs)

# Progress header filter: displays current/total progress as [cur/total X.X%]
PROGRESS_STATE = {"total": 0, "current": 0}

# Suppress verbose internal debug from OpenAI, HTTP libraries and werkzeug
# We do NOT want per-request 200 logs in INFO ‚Äì only surfacing errors/warnings.
logging.getLogger("openai").setLevel(logging.WARNING)
logging.getLogger("openai.api_requestor").setLevel(logging.WARNING)
logging.getLogger("openai._base_client").setLevel(logging.WARNING)
logging.getLogger("httpx").setLevel(logging.WARNING)
logging.getLogger("httpcore").setLevel(logging.WARNING)
logging.getLogger("werkzeug").setLevel(logging.WARNING)
logging.getLogger("urllib3").setLevel(logging.WARNING)


# High-frequency polling routes: do not log each request (only real work / errors matter)
_QUIET_REQUEST_PATHS = (
    "/api/progress",
    "/api/incomplete-albums/scan/progress",
    "/api/dedupe",
    "/api/duplicates",
    "/api/library/improve-all/progress",
    "/api/lidarr/add-incomplete-albums/progress",
)


class _QuietPollingFilter(logging.Filter):
    """Drop request log lines for high-frequency polling routes to keep logs readable."""
    def filter(self, record):
        try:
            msg = record.getMessage()
            for path in _QUIET_REQUEST_PATHS:
                if path in msg:
                    return False
        except Exception:
            pass
        return True


werk_logger = logging.getLogger("werkzeug")
werk_logger.setLevel(logging.WARNING)
werk_logger.addFilter(_QuietPollingFilter())

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ FFmpeg sanity-check ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Central store for worker exceptions
worker_errors = SimpleQueue()

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ AUTO‚ÄìPURGE "INVALID" EDITIONS ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def _purge_invalid_edition(edition: dict):
    """
    Instantly move technically‚Äëinvalid rips (0‚Äëbyte folder or no media‚Äëinfo) to
    /dupes and wipe their Plex metadata so they never show up as duplicates.

    This runs during the *scan* phase, therefore it must be completely
    exception‚Äësafe and thread‚Äësafe.
    """
    try:
        src_folder = Path(edition["folder"])
        if not src_folder.exists():
            return                                                    # already gone

        # Destination under /dupes, keep relative path when possible
        base_dst = build_dupe_destination(src_folder)
        dst = base_dst
        counter = 1
        while dst.exists():                                            # avoid clashes
            dst = base_dst.parent / f"{base_dst.name} ({counter})"
            counter += 1
        dst.parent.mkdir(parents=True, exist_ok=True)

        # Move (or copy‚Äëthen‚Äëdelete) the folder ----------------------
        try:
            safe_move(str(src_folder), str(dst))
        except Exception as move_err:
            logging.warning("Auto‚Äëpurge: moving %s ‚Üí %s failed ‚Äì %s",
                            src_folder, dst, move_err)
            return

        size_mb = folder_size(dst) // (1024 * 1024)
        increment_stat("removed_dupes", 1)
        increment_stat("space_saved", size_mb)

        # Tech‚Äëdata are irrelevant (all zero), but we still log them
        notify_discord(
            f"üóëÔ∏è  Auto‚Äëpurged invalid rip for **{edition['artist']} ‚Äì "
            f"{edition['title_raw']}** ({size_mb}‚ÄØMB moved to /dupes)"
        )

        # Kill Plex metadata so the ghost album disappears
        try:
            plex_api(f"/library/metadata/{edition['album_id']}/trash", method="PUT")
            time.sleep(0.3)
            plex_api(f"/library/metadata/{edition['album_id']}", method="DELETE")
            # Refresh artist view & empty trash
            art_enc = quote_plus(edition['artist'])
            letter  = quote_plus(edition['artist'][0].upper())
            plex_api(f"/library/sections/{SECTION_ID}/refresh"
                     f"?path=/music/matched/{letter}/{art_enc}", method="GET")
            plex_api(f"/library/sections/{SECTION_ID}/emptyTrash", method="PUT")
        except Exception as e:
            logging.debug("Plex cleanup for invalid edition failed: %s", e)

    except Exception as exc:
        logging.warning("Auto‚Äëpurge of invalid edition failed: %s", exc)


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ FFmpeg sanity-check ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def _check_ffmpeg():
    """
    Log the location of ffmpeg/ffprobe or warn clearly if they are missing.
    Runs once at startup.
    """
    log_header("ffmpeg")
    missing = []
    for tool in ("ffmpeg", "ffprobe"):
        path = shutil.which(tool)
        if path:
            logging.info("%s detected at %s", tool, path)
        else:
            missing.append(tool)
    if missing:
        logging.warning(
            "‚ö†Ô∏è  %s not found in PATH ‚Äì bit‚Äërate, sample‚Äërate and bit‚Äëdepth will be 0",
            ", ".join(missing),
        )

_check_ffmpeg()

# --- Scan control flags (global) ---------------------------------
scan_should_stop = threading.Event()
scan_is_paused   = threading.Event()

#
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ CONFIGURATION LOADING ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
"""
Configuration is stored in SQLite (state.db, settings table) only. No config.json is used.
* _get() reads from SQLite first, then falls back to hardcoded defaults.
* Web UI and API save settings to SQLite via api_config_put().
"""

import filecmp

# Helper parsers --------------------------------------------------------------
def _parse_bool(val: str | bool) -> bool:
    """Return *True* for typical truthy strings / bools and *False* otherwise."""
    if isinstance(val, bool):
        return val
    val_normalized = str(val).strip().lower()
    return val_normalized in {"1", "true", "yes", "on"}

# Helper for falsy logic, if needed later
def _is_false(val: str | bool) -> bool:
    """Return *True* for typical falsy strings / bools and *False* otherwise."""
    if isinstance(val, bool):
        return not val
    val_normalized = str(val).strip().lower()
    return val_normalized in {"0", "false", "no", "off"}

def _parse_int(val, default: int | None = None) -> int | None:
    """Return *int* or *default* on failure / None."""
    try:
        return int(val)
    except (TypeError, ValueError):
        return default

def _parse_path_map(val) -> dict[str, str]:
    """
    Accept either a dict, a JSON string or a CSV string of ``SRC:DEST`` pairs.
    Every key/val is coerced to *str*.
    """
    if isinstance(val, dict):
        return {str(k): str(v) for k, v in val.items()}
    if not val:
        return {}
    s = str(val).strip()
    if s.startswith("{"):
        try:
            data = json.loads(s)
            return {str(k): str(v) for k, v in data.items()}
        except json.JSONDecodeError as e:
            logging.warning("Failed to decode PATH_MAP JSON from env ‚Äì %s", e)
            return {}
    mapping: dict[str, str] = {}
    for pair in s.split(","):
        if ":" in pair:
            src, dst = pair.split(":", 1)
            mapping[src.strip()] = dst.strip()
    return mapping

# Determine runtime config dir -------------------------------------------------
BASE_DIR   = Path(__file__).parent
CONFIG_DIR = Path(os.getenv("PMDA_CONFIG_DIR", BASE_DIR))
CONFIG_DIR.mkdir(parents=True, exist_ok=True)
STATE_DB_FILE = CONFIG_DIR / "state.db"
SETTINGS_DB_FILE = CONFIG_DIR / "settings.db"
DROP_ALBUMS_BASE = CONFIG_DIR / "drop_albums"
DROP_MAX_FILES = 50
DROP_MAX_BYTES = 500 * 1024 * 1024  # 500 MB

# Files-mode all-in-one data services (embedded in container entrypoint)
PMDA_PG_HOST = (os.getenv("PMDA_PG_HOST", "127.0.0.1") or "127.0.0.1").strip()
PMDA_PG_PORT = int((os.getenv("PMDA_PG_PORT", "5432") or "5432").strip())
PMDA_PG_DB = (os.getenv("PMDA_PG_DB", "pmda") or "pmda").strip()
PMDA_PG_USER = (os.getenv("PMDA_PG_USER", "pmda") or "pmda").strip()
PMDA_PG_PASSWORD = os.getenv("PMDA_PG_PASSWORD", "pmda") or "pmda"
PMDA_REDIS_HOST = (os.getenv("PMDA_REDIS_HOST", "127.0.0.1") or "127.0.0.1").strip()
PMDA_REDIS_PORT = int((os.getenv("PMDA_REDIS_PORT", "6379") or "6379").strip())
PMDA_REDIS_DB = int((os.getenv("PMDA_REDIS_DB", "0") or "0").strip())
PMDA_REDIS_PASSWORD = os.getenv("PMDA_REDIS_PASSWORD", "") or ""
FILES_CACHE_PREFIX = "pmda:files:v2:"
PMDA_MEDIA_CACHE_ROOT = (os.getenv("PMDA_MEDIA_CACHE_ROOT", "") or "").strip()
PMDA_FILES_WATCHER_ENABLED = _parse_bool(os.getenv("PMDA_FILES_WATCHER_ENABLED", "true"))
PMDA_FILES_WATCHER_LOG_COOLDOWN_SEC = float(os.getenv("PMDA_FILES_WATCHER_LOG_COOLDOWN_SEC", "10") or "10")
PMDA_AUTO_CHANGED_ONLY_SCAN = _parse_bool(os.getenv("PMDA_AUTO_CHANGED_ONLY_SCAN", "true"))
PMDA_AUTO_CHANGED_ONLY_SCAN_DEBOUNCE_SEC = float(os.getenv("PMDA_AUTO_CHANGED_ONLY_SCAN_DEBOUNCE_SEC", "60") or "60")
PMDA_AUTO_CHANGED_ONLY_SCAN_COOLDOWN_SEC = float(os.getenv("PMDA_AUTO_CHANGED_ONLY_SCAN_COOLDOWN_SEC", "300") or "300")
PMDA_AUTO_CHANGED_ONLY_SCAN_MIN_PENDING = int(os.getenv("PMDA_AUTO_CHANGED_ONLY_SCAN_MIN_PENDING", "1") or "1")
PMDA_CACHE_TELEMETRY_TTL_SEC = float(os.getenv("PMDA_CACHE_TELEMETRY_TTL_SEC", "15") or "15")
PMDA_CACHE_TELEMETRY_MAX_WALK_FILES = int(os.getenv("PMDA_CACHE_TELEMETRY_MAX_WALK_FILES", "400000") or "400000")
PMDA_CACHE_TELEMETRY_MAX_REDIS_SCAN_KEYS = int(os.getenv("PMDA_CACHE_TELEMETRY_MAX_REDIS_SCAN_KEYS", "200000") or "200000")
RECO_EMBED_DIM = 64
RECO_EMBED_SOURCE = "pmda_hash_v1"


def _get_from_sqlite(key: str, default=None):
    """Read a single config value from SQLite settings table (used before merged exists)."""
    try:
        if SETTINGS_DB_FILE.exists():
            con = sqlite3.connect(str(SETTINGS_DB_FILE), timeout=5)
            cur = con.cursor()
            cur.execute("SELECT value FROM settings WHERE key = ?", (key,))
            row = cur.fetchone()
            con.close()
            if row and row[0]:
                return row[0]
    except Exception:
        pass
    return default


# Location of baked‚Äëin template for AI prompt (shipped inside the image)
DEFAULT_PROMPT_PATH  = BASE_DIR / "ai_prompt.txt"
AI_PROMPT_FILE = CONFIG_DIR / "ai_prompt.txt"

# Ensure ai_prompt.txt exists -------------------------------------------
if not AI_PROMPT_FILE.exists():
    logging.info("ai_prompt.txt not found ‚Äî default prompt created")
    shutil.copyfile(DEFAULT_PROMPT_PATH, AI_PROMPT_FILE)

# ‚îÄ‚îÄ‚îÄ Auto‚Äëgenerate PATH_MAP from Plex at *every* startup ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
#
# Design goal: avoid requiring users to manually configure PATH_MAP (Plex path ‚Üí host path).
# Flow:
#   1) Discovery: Plex API returns all <Location> paths for the music section(s).
#      We get the exact paths Plex uses (e.g. /music/matched, /music/unmatched).
#   2) Merge: If the user provided a broader PATH_MAP in SQLite (e.g. /music ‚Üí /music/‚Ä¶),
#      we apply it by prefix; otherwise we keep Plex path = container path (so Docker
#      volume binds must match: -v /host/Music_matched:/music/matched).
#   3) Cross-check: For each binding we sample CROSSCHECK_SAMPLES tracks from the DB,
#      resolve their paths via PATH_MAP, and verify the files exist on disk. If they
#      don‚Äôt, we try to find the correct host root (sibling dirs or rglob) and patch
#      PATH_MAP in memory and SQLite. So even when paths differ (e.g. Plex says /music/unmatched
#      but on host it‚Äôs /music/Music_dump), we auto-correct instead of failing.
# Result: users only need to mount volumes; PMDA discovers Plex paths and validates
# (and repairs) bindings so the same paths work for scan/dedupe.

def _discover_path_map(plex_host: str, plex_token: str, section_id: int) -> dict[str, str]:
    """
    Query Plex for all <Location> paths belonging to *section_id* and return
    a mapping of {container_path: container_path}.  This is run at each
    startup so that changes in the Plex UI (adding/removing folders) are
    picked up automatically.

    A hard failure (network/XML/bad token/empty list) is surfaced so that
    users notice mis‚Äëconfiguration early.
    """
    logging.debug(
        "PATH_MAP discovery: requesting %s (filter section=%s)",
        plex_host.rstrip('/') + "/library/sections",
        section_id
    )
    url = f"{plex_host.rstrip('/')}/library/sections"
    resp = requests.get(url, headers={"X-Plex-Token": plex_token}, timeout=10)
    logging.debug("PATH_MAP discovery: HTTP %s ‚Äì %d bytes", resp.status_code, len(resp.content))
    if logging.getLogger().isEnabledFor(logging.DEBUG):
        logging.debug("PATH_MAP discovery response (first 500 chars): %s", resp.text[:500])
    resp.raise_for_status()

    try:
        root = ET.fromstring(resp.text)
    except ET.ParseError as e:
        raise RuntimeError(f"Invalid XML returned by Plex: {e}") from None

    seen: set[str] = set()
    for directory in root.iter("Directory"):
        if directory.attrib.get("key") != str(section_id):
            continue
        for loc in directory.iter("Location"):
            path = (loc.attrib.get("path") or "").strip()
            if not path:
                continue
            # Support path with comma- or semicolon-separated entries (some Plex versions)
            for single in re.split(r"[,;]", path):
                single = single.strip()
                if single:
                    seen.add(single)
        # Do not break: some Plex versions return one Directory per Location; collect all.
    locations = list(seen)
    logging.debug("PATH_MAP discovery: parsed %d <Location> paths -> %s", len(locations), locations)
    if not locations:
        raise RuntimeError("No <Location> elements found for this section")

    logging.info("PATH_MAP discovery successful ‚Äì %d paths found", len(locations))
    return {p: p for p in locations}

# Always attempt discovery ‚Äì even when PATH_MAP already exists ‚Äì so the file
# stays in sync with the Plex configuration. If PLEX_HOST or PLEX_TOKEN is
# missing, we start in "wizard" (unconfigured) mode and skip discovery.
SECTION_IDS: list[int] = []
SECTION_NAMES: dict[int, str] = {}

# Load SECTION_IDS from SQLite first so user's saved library selection is never overwritten by auto-detect
try:
    cfg_db_path = SETTINGS_DB_FILE
    if cfg_db_path.exists():
        con = sqlite3.connect(str(cfg_db_path), timeout=5)
        cur = con.cursor()
        cur.execute("SELECT value FROM settings WHERE key = ?", ("SECTION_IDS",))
        row = cur.fetchone()
        con.close()
        if row and row[0]:
            raw = str(row[0]).strip()
            if raw:
                if raw.startswith("["):
                    SECTION_IDS = [int(x) for x in json.loads(raw)]
                else:
                    SECTION_IDS = [int(x.strip()) for x in raw.split(",") if x.strip()]
                logging.info("Loaded SECTION_IDS from SQLite at startup (saved selection): %s", SECTION_IDS)
except Exception as e:
    logging.debug("Could not load SECTION_IDS from SQLite at startup: %s", e)

try:
    plex_host = (_get_from_sqlite("PLEX_HOST", "") or "").strip() if isinstance(_get_from_sqlite("PLEX_HOST", ""), str) else ""
    plex_token = (_get_from_sqlite("PLEX_TOKEN", "") or "").strip() if isinstance(_get_from_sqlite("PLEX_TOKEN", ""), str) else ""
    # Require a URL-like host so we never call Plex API with empty/invalid base
    if not plex_host or not plex_token or not str(plex_host).strip().startswith(("http://", "https://")):
        logging.info("PLEX_HOST or PLEX_TOKEN missing or invalid ‚Äì starting in unconfigured (wizard) mode")
    else:
        # SECTION_IDS already loaded from SQLite above; do not use env/config
        raw_sections = None

        # Treat an empty string or whitespace‚Äëonly value as ‚Äúnot provided‚Äù
        logging.debug("SECTION_IDS from SQLite: %r", SECTION_IDS)

        # Only auto-detect if we don't already have SECTION_IDS from SQLite (user's saved selection)
        if not raw_sections:
            if not SECTION_IDS:
                try:
                    resp = requests.get(f"{plex_host.rstrip('/')}/library/sections", headers={"X-Plex-Token": plex_token}, timeout=10)
                    root = ET.fromstring(resp.text)
                    SECTION_IDS = [int(d.attrib['key']) for d in root.iter("Directory") if d.attrib.get('type') == 'artist']
                    logging.info("Auto-detected SECTION_IDS from Plex: %s", SECTION_IDS)
                except Exception as e:
                    logging.error("Failed to auto-detect SECTION_IDS: %s", e)
                    SECTION_IDS = []
            else:
                logging.info("Using SECTION_IDS from SQLite (saved selection): %s", SECTION_IDS)
        if SECTION_IDS:
            try:
                resp = requests.get(f"{plex_host.rstrip('/')}/library/sections", headers={"X-Plex-Token": plex_token}, timeout=10)
                root = ET.fromstring(resp.text)
                SECTION_NAMES.update({int(directory.attrib['key']): directory.attrib.get('title', '<unknown>') for directory in root.iter('Directory')})
            except Exception:
                pass
            if SECTION_NAMES:
                log_header("libraries")
                for sid in SECTION_IDS:
                    name = SECTION_NAMES.get(sid, "<unknown>")
                    logging.info("  %s (ID %d)", name, sid)
            auto_map = {}
            for sid in SECTION_IDS:
                part = _discover_path_map(plex_host, plex_token, sid)
                auto_map.update(part)
            log_header("path_map discovery")
            logging.info("Auto‚Äëgenerated raw PATH_MAP from Plex: %s", auto_map)
            raw_env_map = _parse_path_map(_get_from_sqlite("PATH_MAP") or {})
            logging.info("Raw PATH_MAP from SQLite: %s", raw_env_map)
            merged_map = {}
            for cont_path, cont_val in auto_map.items():
                mapped = False
                for prefix, host_base in sorted(raw_env_map.items(), key=lambda item: len(item[0]), reverse=True):
                    if cont_path.startswith(prefix):
                        suffix = cont_path[len(prefix):].lstrip("/")
                        merged_map[cont_path] = os.path.join(host_base, suffix)
                        mapped = True
                        break
                if not mapped:
                    merged_map[cont_path] = cont_val
            logging.info("Merged PATH_MAP for startup: %s", merged_map)
            log_header("volume bindings (plex ‚Üí pmda ‚Üí host)")
            logging.info("%-40s | %-30s | %s", "PLEX_PATH", "PMDA_PATH", "HOST_PATH")
            for plex_path, host_path in merged_map.items():
                logging.info("%-40s | %-30s | %s", plex_path, host_path, host_path)
            # Persist PATH_MAP to settings.db (single source of truth)
            try:
                init_settings_db()
                con = sqlite3.connect(str(SETTINGS_DB_FILE), timeout=5)
                con.execute("INSERT OR REPLACE INTO settings(key, value) VALUES('PATH_MAP', ?)", (json.dumps(merged_map),))
                con.commit()
                con.close()
            except Exception as e:
                logging.debug("Could not persist PATH_MAP to settings.db at discovery: %s", e)
            logging.info("Auto-generated/updated PATH_MAP from Plex (saved to settings.db)")
except Exception as e:
    logging.warning("‚ö†Ô∏è  Failed to auto‚Äëgenerate PATH_MAP ‚Äì %s", e)
    SECTION_IDS = []
    SECTION_NAMES = {}

# (4) Merge with environment variables ----------------------------------------
ENV_SOURCES: dict[str, str] = {}

def _get(key: str, *, default=None, cast=lambda x: x):
    """Return the merged value and remember where it came from.
    Priority: SQLite > env > default.
    """
    sqlite_val = None
    sqlite_has_row = False
    try:
        if SETTINGS_DB_FILE.exists():
            con = sqlite3.connect(str(SETTINGS_DB_FILE), timeout=5)
            cur = con.cursor()
            cur.execute("SELECT value FROM settings WHERE key = ?", (key,))
            row = cur.fetchone()
            con.close()
            if row is not None:
                sqlite_has_row = True
                sqlite_val = row[0]
    except Exception:
        pass

    if sqlite_has_row:
        ENV_SOURCES[key] = "sqlite"
        raw = sqlite_val
        if isinstance(raw, str) and raw.strip() == "" and default is not None:
            ENV_SOURCES[key] = "default"
            raw = default
        else:
            # Backward-compatible "perf default" migration:
            # some older builds persisted defaults into SQLite. When that happens, we treat legacy defaults
            # as if they were unset so the newer recommended defaults apply without forcing a manual reset.
            # Users can still override by setting any other value than the legacy default.
            try:
                raw_str = str(raw).strip()
            except Exception:
                raw_str = ""
            if key == "MB_SEARCH_ALBUM_TIMEOUT_SEC" and raw_str == "20" and default is not None:
                ENV_SOURCES[key] = "default"
                return cast(default)
            if key == "MB_TRACKLIST_FETCH_LIMIT" and raw_str == "2" and default is not None:
                ENV_SOURCES[key] = "default"
                return cast(default)
            return cast(raw)
    env_val = os.getenv(key)
    if env_val is not None:
        raw = env_val
        if isinstance(raw, str) and raw.strip() == "" and default is not None:
            ENV_SOURCES[key] = "default"
            return cast(default)
        ENV_SOURCES[key] = "env"
        return cast(raw)
    ENV_SOURCES[key] = "default"
    return cast(default)


# Old default that included MusicBrainz IDs (caused all albums to show as "incomplete" if no MB tags)
_REQUIRED_TAGS_OLD_DEFAULT_SET = {"artist", "album", "date", "musicbrainz_release_group_id", "musicbrainz_artist_id"}


def _parse_required_tags(val):
    """Return REQUIRED_TAGS as a list of strings. Handles JSON array string from DB, comma-separated string, or list.
    Migrates old default (with musicbrainz_release_group_id, musicbrainz_artist_id) to new default (artist, album, genre, year)."""
    # New default: focus on artist/album plus high-level album metadata (genre + year).
    # "date" is no longer required; year-only is sufficient for completeness.
    default = ["artist", "album", "genre", "year", "tracks"]
    if val is None:
        return default
    if isinstance(val, list):
        out = [str(t).strip().lower() for t in val if str(t).strip()]
    else:
        s = str(val).strip()
        if not s:
            return default
        if s.startswith("["):
            try:
                parsed = json.loads(s)
                if isinstance(parsed, list):
                    out = [str(t).strip().lower() for t in parsed if str(t).strip()]
                else:
                    out = []
            except (json.JSONDecodeError, TypeError):
                out = [t.strip().lower() for t in s.split(",") if t.strip()]
        else:
            out = [t.strip().lower() for t in s.split(",") if t.strip()]
    if not out:
        return default
    # Migrate old default so all albums are not reported as incomplete
    if set(out) == _REQUIRED_TAGS_OLD_DEFAULT_SET:
        return default
    return out


def _check_required_tags(meta: dict, required_tags: list, edition: dict | None = None) -> list:
    """
    Return the list of required tag names that are missing.
    Uses REQUIRED_TAGS from Settings as the single source of truth.
    meta: tags from first audio file (lowercase keys from ffprobe).
    edition: optional edition dict with 'tracks' (Track NamedTuples or dicts with title, idx/index).
    """
    # Map config tag name (lowercase) -> meta keys to check (ffprobe returns lowercase)
    TAG_META_KEYS = {
        "artist": ["artist", "albumartist"],
        "album": ["album"],
        "date": ["date", "originaldate"],
        "year": ["year", "date"],
        "genre": ["genre"],
        "musicbrainz_release_group_id": ["musicbrainz_releasegroupid", "musicbrainz_release_group_id"],
        "musicbrainz_artist_id": ["musicbrainz_artistid", "musicbrainz_albumartistid", "musicbrainz_artist_id"],
    }
    missing = []
    for tag in required_tags:
        key = (tag or "").strip().lower()
        if not key:
            continue
        if key == "tracks":
            # Require every track to have a non-empty title and a valid index (from Plex/edition)
            if not edition:
                missing.append(tag)
                continue
            tracks = edition.get("tracks") or []
            if not tracks:
                missing.append(tag)
                continue
            def _track_ok(t):
                if isinstance(t, dict):
                    title = t.get("title")
                    idx = t.get("idx")
                    if idx is None:
                        idx = t.get("index")
                    if idx is None:
                        idx = t.get("track_num")
                else:
                    title = getattr(t, "title", None)
                    idx = getattr(t, "idx", None)
                    if idx is None:
                        idx = getattr(t, "index", None)
                return bool(title and str(title).strip()) and idx is not None
            if not all(_track_ok(t) for t in tracks):
                missing.append(tag)
            continue
        meta_keys = TAG_META_KEYS.get(key, [key])
        if not any((meta or {}).get(k) for k in meta_keys):
            missing.append(tag)
    return missing


def _parse_skip_folders(val):
    """Return SKIP_FOLDERS as a list of path strings. Handles JSON array from DB (or corrupted double-encoded),
    comma-separated string, or list. Drops any element that looks like JSON (e.g. '[]') so corrupted values become []."""
    if val is None:
        return []
    if isinstance(val, list):
        raw = [str(p).strip() for p in val if str(p).strip()]
    else:
        s = str(val).strip()
        if not s:
            return []
        if s.startswith("["):
            try:
                parsed = json.loads(s)
                if isinstance(parsed, list):
                    raw = [str(p).strip() for p in parsed if str(p).strip()]
                else:
                    raw = []
            except (json.JSONDecodeError, TypeError):
                raw = [p.strip() for p in s.split(",") if p.strip()]
        else:
            raw = [p.strip() for p in s.split(",") if p.strip()]
    # Drop corrupted entries: paths must not look like JSON (e.g. "[]", '["[]"]')
    return [p for p in raw if p and not p.startswith("[")]


def _parse_files_roots(val) -> list[str]:
    """
    Return FILES_ROOTS as a normalized list of directory strings.
    Handles:
    - list/tuple values
    - CSV strings
    - JSON array strings
    - corrupted double-encoded JSON strings (e.g. "[\"[\\\"/music\\\"]\"]")
    """
    out: list[str] = []
    queue: list = [val]
    seen: set[str] = set()

    while queue:
        item = queue.pop(0)
        if item is None:
            continue
        if isinstance(item, (list, tuple, set)):
            queue.extend(list(item))
            continue

        if isinstance(item, str):
            s = item.strip()
            if not s:
                continue

            # Try JSON decode first (works for arrays and quoted strings).
            if s[0] in {'[', '"'}:
                try:
                    parsed = json.loads(s)
                except (json.JSONDecodeError, TypeError):
                    parsed = None
                if parsed is not None and parsed is not item:
                    queue.append(parsed)
                    continue

            # CSV fallback for plain strings.
            if "," in s:
                parts = [p.strip() for p in s.split(",") if p.strip()]
                if len(parts) > 1:
                    queue.extend(parts)
                    continue

            if s and s not in seen and not s.startswith("["):
                seen.add(s)
                out.append(s)
            continue

        s = str(item).strip()
        if s and s not in seen and not s.startswith("["):
            seen.add(s)
            out.append(s)

    return out


def _parse_format_preference_early(val):
    """Parse FORMAT_PREFERENCE for use in merged (before _parse_format_preference is defined)."""
    _default = ["dsf", "aif", "aiff", "wav", "flac", "m4a", "mp4", "m4b", "m4p", "aifc", "ogg", "opus", "mp3", "wma"]
    if val is None or (isinstance(val, str) and not val.strip()):
        return _default
    if isinstance(val, list):
        return val
    if isinstance(val, str):
        s = val.strip()
        if s.startswith("["):
            try:
                parsed = json.loads(s)
                if isinstance(parsed, list):
                    return parsed
            except (json.JSONDecodeError, TypeError):
                pass
            return _default
        parts = [x.strip() for x in s.split(",") if x.strip()]
        return parts if parts else _default
    return _default


# ‚îÄ‚îÄ‚îÄ Plex DB auto-discovery from base path (one-shot, persisted to SQLite) ‚îÄ‚îÄ‚îÄ
PLEX_DB_FILENAME = "com.plexapp.plugins.library.db"
# Known relative paths under Plex config root (Linux/Docker/macOS, then Windows-style)
_PLEX_DB_RELATIVE_PATHS = [
    "Library/Application Support/Plex Media Server/Plug-in Support/Databases",
    os.path.join("Plex Media Server", "Plug-in Support", "Databases"),
]


def _resolve_plex_db_from_base(base_path: str) -> str | None:
    """
    Find the directory containing com.plexapp.plugins.library.db under base_path.
    Tries known relative paths first, then recursive search (max depth 10).
    Returns the directory path (str) or None if not found.
    """
    base = Path(base_path).resolve()
    if not base.exists() or not base.is_dir():
        return None
    # Step 1: known relative paths
    for rel in _PLEX_DB_RELATIVE_PATHS:
        candidate = base / rel
        if (candidate / PLEX_DB_FILENAME).exists():
            return str(candidate)
    # Step 2: recursive search with max depth 10
    max_depth = 10
    base_str = str(base)
    for root, _dirs, files in os.walk(base, topdown=True):
        try:
            depth = len(Path(root).relative_to(base).parts) if root != base_str else 0
        except ValueError:
            continue
        if depth > max_depth:
            _dirs.clear()
            continue
        if PLEX_DB_FILENAME in files:
            return root
    return None


def _ensure_plex_db_path_resolved() -> str | None:
    """
    If PLEX_DB_PATH is already in SQLite and the DB file exists, return it.
    Otherwise resolve from PLEX_BASE_PATH (or /database), persist to SQLite if found, and return.
    Called before building merged so _get("PLEX_DB_PATH") can read the persisted value.
    """
    db_path = _get_from_sqlite("PLEX_DB_PATH")
    if db_path and (Path(db_path) / PLEX_DB_FILENAME).exists():
        return str(db_path).strip()
    # Prefer explicit base from settings; otherwise, if /plex exists (new recommended bind),
    # start discovery from there; fall back to /database for older setups.
    base = (_get_from_sqlite("PLEX_BASE_PATH") or "").strip()
    if not base:
        if Path("/plex").exists():
            base = "/plex"
        else:
            base = "/database"
    resolved = _resolve_plex_db_from_base(base)
    if resolved:
        try:
            init_settings_db()
            con = sqlite3.connect(str(SETTINGS_DB_FILE), timeout=5)
            con.execute("INSERT OR REPLACE INTO settings(key, value) VALUES(?, ?)", ("PLEX_DB_PATH", resolved))
            con.commit()
            con.close()
            logging.info("Plex DB discovered at %s (saved to settings.db)", resolved)
        except Exception as e:
            logging.debug("Could not persist PLEX_DB_PATH to settings.db: %s", e)
        return resolved
    return None


# PLEX_DB_PATH defaults to /database in container; no SystemExit if unconfigured.
merged = {
    "PLEX_DB_PATH":   (_ensure_plex_db_path_resolved() or _get("PLEX_DB_PATH",   default="/database", cast=str)),
    "PLEX_HOST":      _get("PLEX_HOST",      default="",                                cast=str),
    "PLEX_TOKEN":     _get("PLEX_TOKEN",     default="",                                cast=str),
    "SECTION_ID": SECTION_IDS[0] if SECTION_IDS else 0,
    "SCAN_THREADS":   _get("SCAN_THREADS",   default=os.cpu_count() or 4,               cast=_parse_int),
    "PATH_MAP":       _parse_path_map(_get("PATH_MAP", default={})),
    "DUPE_ROOT":      _get("DUPE_ROOT", default="/dupes", cast=str),
    "LOG_LEVEL":      _get("LOG_LEVEL",      default="INFO").upper(),
    "AI_PROVIDER": _get("AI_PROVIDER", default="openai", cast=str),
    "OPENAI_API_KEY": _get("OPENAI_API_KEY", default="",                                cast=str),
    "OPENAI_MODEL":   _get("OPENAI_MODEL",   default="gpt-4",                           cast=str),
    "ANTHROPIC_API_KEY": _get("ANTHROPIC_API_KEY", default="", cast=str),
    "GOOGLE_API_KEY": _get("GOOGLE_API_KEY", default="", cast=str),
    "OLLAMA_URL": _get("OLLAMA_URL", default="http://localhost:11434", cast=str),
    "DISCORD_WEBHOOK": _get("DISCORD_WEBHOOK", default="", cast=str),
    "USE_MUSICBRAINZ": _get("USE_MUSICBRAINZ", default=True, cast=_parse_bool),
    "MUSICBRAINZ_EMAIL": _get("MUSICBRAINZ_EMAIL", default="pmda@example.com", cast=str),
    "MB_QUEUE_ENABLED": _get("MB_QUEUE_ENABLED", default=True, cast=_parse_bool),
    "MB_RETRY_NOT_FOUND": _get("MB_RETRY_NOT_FOUND", default=False, cast=_parse_bool),
    # Time budget (seconds) per album for MusicBrainz search flow before we fall back faster to other providers.
    "MB_SEARCH_ALBUM_TIMEOUT_SEC": _get(
        "MB_SEARCH_ALBUM_TIMEOUT_SEC",
        # Default reduced to keep scans fast on large libraries; strict provider fallbacks cover the hard cases.
        default=12,
        cast=lambda x: max(10, min(300, int(x) if x is not None and str(x).strip().isdigit() else 12)),
    ),
    # Maximum number of MB candidates for which we fetch full details per album.
    "MB_CANDIDATE_FETCH_LIMIT": _get(
        "MB_CANDIDATE_FETCH_LIMIT",
        default=4,
        cast=lambda x: max(1, min(20, int(x) if x is not None and str(x).strip().isdigit() else 4)),
    ),
    # Fetch recording-level track titles only for first N candidates (expensive endpoint).
    "MB_TRACKLIST_FETCH_LIMIT": _get(
        "MB_TRACKLIST_FETCH_LIMIT",
        # Default reduced: this endpoint is expensive and can dominate scan time when many albums are ambiguous.
        default=1,
        cast=lambda x: max(0, min(20, int(x) if x is not None and str(x).strip().isdigit() else 1)),
    ),
    # If true, once MB budget is exceeded or MB has no candidates, we prioritize provider fallback and skip web+AI MBID hunt.
    "MB_FAST_FALLBACK_MODE": _get("MB_FAST_FALLBACK_MODE", default=True, cast=_parse_bool),
    # Provider identity arbitration (Discogs/Last.fm/Bandcamp when MB is unavailable).
    "PROVIDER_IDENTITY_STRICT": _get("PROVIDER_IDENTITY_STRICT", default=True, cast=_parse_bool),
    "PROVIDER_IDENTITY_USE_AI": _get("PROVIDER_IDENTITY_USE_AI", default=True, cast=_parse_bool),
    "PROVIDER_IDENTITY_MIN_SCORE": _get(
        "PROVIDER_IDENTITY_MIN_SCORE",
        default=0.72,
        cast=lambda x: float(x) if x is not None and str(x).replace(".", "", 1).replace("-", "", 1).isdigit() else 0.72,
    ),
    "PROVIDER_IDENTITY_SCORE_MARGIN": _get(
        "PROVIDER_IDENTITY_SCORE_MARGIN",
        default=0.08,
        cast=lambda x: float(x) if x is not None and str(x).replace(".", "", 1).replace("-", "", 1).isdigit() else 0.08,
    ),
    # Provider lookup cache TTLs (seconds).
    "PROVIDER_CACHE_FOUND_TTL_SEC": _get(
        "PROVIDER_CACHE_FOUND_TTL_SEC",
        default=60 * 60 * 24 * 30,
        cast=lambda x: max(60, min(60 * 60 * 24 * 365, int(x) if x is not None and str(x).strip().isdigit() else 60 * 60 * 24 * 30)),
    ),
    "PROVIDER_CACHE_NOT_FOUND_TTL_SEC": _get(
        "PROVIDER_CACHE_NOT_FOUND_TTL_SEC",
        default=60 * 60 * 8,
        cast=lambda x: max(60, min(60 * 60 * 24 * 30, int(x) if x is not None and str(x).strip().isdigit() else 60 * 60 * 8)),
    ),
    "PROVIDER_CACHE_ERROR_TTL_SEC": _get(
        "PROVIDER_CACHE_ERROR_TTL_SEC",
        default=60 * 20,
        cast=lambda x: max(30, min(60 * 60 * 24, int(x) if x is not None and str(x).strip().isdigit() else 60 * 20)),
    ),
    # When true, ignore cached MusicBrainz results and stored MBID tags for album‚ÜíMBID lookup.
    # This forces a full lookup on every scan and is intended for advanced testing/debugging only.
    "MB_DISABLE_CACHE": _get("MB_DISABLE_CACHE", default=False, cast=_parse_bool),
    # When true, ignore *all* caches during scans (audio format cache and MusicBrainz cache).
    # This forces the scan to re-run the full analysis and metadata flow even when cache entries exist.
    "SCAN_DISABLE_CACHE": _get("SCAN_DISABLE_CACHE", default=False, cast=_parse_bool),
    "LIDARR_URL": _get("LIDARR_URL", default="", cast=str),
    "LIDARR_API_KEY": _get("LIDARR_API_KEY", default="", cast=str),
    "AUTOBRR_URL": _get("AUTOBRR_URL", default="", cast=str),
    "AUTOBRR_API_KEY": _get("AUTOBRR_API_KEY", default="", cast=str),
    "AUTO_FIX_BROKEN_ALBUMS": _get("AUTO_FIX_BROKEN_ALBUMS", default=False, cast=_parse_bool),
    "BROKEN_ALBUM_CONSECUTIVE_THRESHOLD": _get("BROKEN_ALBUM_CONSECUTIVE_THRESHOLD", default=2, cast=int),
    "BROKEN_ALBUM_PERCENTAGE_THRESHOLD": _get("BROKEN_ALBUM_PERCENTAGE_THRESHOLD", default=0.20, cast=float),
    # Files-first default required tags: artist/album/genre/year + track numbers/titles.
    "REQUIRED_TAGS": _get("REQUIRED_TAGS", default="artist,album,genre,year,tracks", cast=_parse_required_tags),
    "SKIP_FOLDERS": _get("SKIP_FOLDERS", default="", cast=_parse_skip_folders),
    "AI_BATCH_SIZE": _get("AI_BATCH_SIZE", default=10, cast=int),
    "FFPROBE_POOL_SIZE": _get("FFPROBE_POOL_SIZE", default=8, cast=int),
    "AUTO_MOVE_DUPES": _get("AUTO_MOVE_DUPES", default=False, cast=_parse_bool),
    "AUTO_EXPORT_LIBRARY": _get("AUTO_EXPORT_LIBRARY", default=False, cast=_parse_bool),
    # End-to-end pipeline steps (all configurable from Settings)
    "PIPELINE_ENABLE_MATCH_FIX": _get("PIPELINE_ENABLE_MATCH_FIX", default=True, cast=_parse_bool),
    "PIPELINE_ENABLE_DEDUPE": _get("PIPELINE_ENABLE_DEDUPE", default=True, cast=_parse_bool),
    "PIPELINE_ENABLE_INCOMPLETE_MOVE": _get("PIPELINE_ENABLE_INCOMPLETE_MOVE", default=True, cast=_parse_bool),
    "PIPELINE_ENABLE_EXPORT": _get("PIPELINE_ENABLE_EXPORT", default=False, cast=_parse_bool),
    "PIPELINE_ENABLE_PLAYER_SYNC": _get("PIPELINE_ENABLE_PLAYER_SYNC", default=False, cast=_parse_bool),
    "PIPELINE_PLAYER_TARGET": _get("PIPELINE_PLAYER_TARGET", default="none", cast=str),
    # External media player integrations (scan trigger only)
    "JELLYFIN_URL": _get("JELLYFIN_URL", default="", cast=str),
    "JELLYFIN_API_KEY": _get("JELLYFIN_API_KEY", default="", cast=str),
    "NAVIDROME_URL": _get("NAVIDROME_URL", default="", cast=str),
    "NAVIDROME_USERNAME": _get("NAVIDROME_USERNAME", default="", cast=str),
    "NAVIDROME_PASSWORD": _get("NAVIDROME_PASSWORD", default="", cast=str),
    "NAVIDROME_API_KEY": _get("NAVIDROME_API_KEY", default="", cast=str),
    "USE_AI_FOR_MB_MATCH": _get("USE_AI_FOR_MB_MATCH", default=True, cast=_parse_bool),
    "USE_AI_FOR_MB_VERIFY": _get("USE_AI_FOR_MB_VERIFY", default=True, cast=_parse_bool),
    "USE_AI_VISION_FOR_COVER": _get("USE_AI_VISION_FOR_COVER", default=True, cast=_parse_bool),
    "AI_CONFIDENCE_MIN": _get("AI_CONFIDENCE_MIN", default=50, cast=lambda x: int(x) if x is not None and str(x).strip().isdigit() else 50),
    "OPENAI_VISION_MODEL": _get("OPENAI_VISION_MODEL", default="", cast=str),
    "USE_AI_VISION_BEFORE_COVER_INJECT": _get("USE_AI_VISION_BEFORE_COVER_INJECT", default=True, cast=_parse_bool),
    # AI cost controls for MusicBrainz "verify" prompt size.
    "AI_MB_VERIFY_MAX_CANDIDATES": _get("AI_MB_VERIFY_MAX_CANDIDATES", default=8, cast=lambda x: max(1, min(20, int(x) if x is not None and str(x).strip().isdigit() else 8))),
    "AI_MB_VERIFY_LOCAL_TRACK_PREVIEW": _get("AI_MB_VERIFY_LOCAL_TRACK_PREVIEW", default=12, cast=lambda x: max(0, min(60, int(x) if x is not None and str(x).strip().isdigit() else 12))),
    "AI_MB_VERIFY_MB_TRACK_PREVIEW": _get("AI_MB_VERIFY_MB_TRACK_PREVIEW", default=5, cast=lambda x: max(0, min(30, int(x) if x is not None and str(x).strip().isdigit() else 5))),
    "BACKUP_BEFORE_FIX": _get("BACKUP_BEFORE_FIX", default=False, cast=_parse_bool),
    "MAGIC_MODE": _get("MAGIC_MODE", default=False, cast=_parse_bool),
    "IMPROVE_ALL_WORKERS": _get("IMPROVE_ALL_WORKERS", default=1, cast=lambda x: max(1, min(8, int(x) if x is not None and str(x).strip().isdigit() else 1))),
    "USE_ACOUSTID": _get("USE_ACOUSTID", default=True, cast=_parse_bool),
    "ACOUSTID_API_KEY": _get("ACOUSTID_API_KEY", default="", cast=str),
    "USE_ACOUSTID_WHEN_TAGGED": _get("USE_ACOUSTID_WHEN_TAGGED", default="false", cast=_parse_bool),
    "USE_WEB_SEARCH_FOR_MB": _get("USE_WEB_SEARCH_FOR_MB", default=True, cast=_parse_bool),
    "SERPER_API_KEY": _get("SERPER_API_KEY", default="", cast=str),
    "CROSS_LIBRARY_DEDUPE": _get("CROSS_LIBRARY_DEDUPE", default="true", cast=_parse_bool),
    "CROSSCHECK_SAMPLES": _get("CROSSCHECK_SAMPLES", default=20, cast=lambda x: int(x) if x is not None and str(x).strip().isdigit() else 20),
    "LOG_FILE": _get("LOG_FILE", default="", cast=str) or str(CONFIG_DIR / "pmda.log"),
    "OPENAI_MODEL_FALLBACKS": _get("OPENAI_MODEL_FALLBACKS", default="", cast=str),
    "DISABLE_PATH_CROSSCHECK": _get("DISABLE_PATH_CROSSCHECK", default="false", cast=_parse_bool),
    "FORMAT_PREFERENCE": _get("FORMAT_PREFERENCE", default=None, cast=lambda v: _parse_format_preference_early(v)),
    "USE_DISCOGS": _get("USE_DISCOGS", default=True, cast=_parse_bool),
    "DISCOGS_USER_TOKEN": _get("DISCOGS_USER_TOKEN", default="", cast=str),
    "USE_LASTFM": _get("USE_LASTFM", default=True, cast=_parse_bool),
    "LASTFM_API_KEY": _get("LASTFM_API_KEY", default="", cast=str),
    "LASTFM_API_SECRET": _get("LASTFM_API_SECRET", default="", cast=str),
    # Extra artist image providers (optional)
    "FANART_API_KEY": _get("FANART_API_KEY", default="", cast=str),
    "THEAUDIODB_API_KEY": _get("THEAUDIODB_API_KEY", default="", cast=str),
    "USE_BANDCAMP": _get("USE_BANDCAMP", default=True, cast=_parse_bool),
    # Artist / metadata modes
    "ARTIST_CREDIT_MODE": _get("ARTIST_CREDIT_MODE", default="picard_like_default", cast=str),
    "LIVE_DEDUPE_MODE": _get("LIVE_DEDUPE_MODE", default="safe", cast=str),
    "SKIP_MB_FOR_LIVE_ALBUMS": _get("SKIP_MB_FOR_LIVE_ALBUMS", default="true", cast=_parse_bool),
    "TRACKLIST_MATCH_MIN": _get("TRACKLIST_MATCH_MIN", default="0.9", cast=lambda x: float(x) if x is not None and str(x).replace(".", "", 1).replace("-", "", 1).isdigit() else 0.9),
    "LIVE_ALBUMS_MB_STRICT": _get("LIVE_ALBUMS_MB_STRICT", default="false", cast=_parse_bool),
    # Improve-all reprocess policy
    "REPROCESS_INCOMPLETE_ALBUMS": _get("REPROCESS_INCOMPLETE_ALBUMS", default="true", cast=_parse_bool),
    # Library backend & file-library settings
    "LIBRARY_MODE": _get("LIBRARY_MODE", default="files", cast=str),
    "FILES_ROOTS": _get("FILES_ROOTS", default="", cast=_parse_files_roots),
    "EXPORT_ROOT": _get("EXPORT_ROOT", default="", cast=str),
    "EXPORT_NAMING_TEMPLATE": _get("EXPORT_NAMING_TEMPLATE", default="", cast=str),
    "EXPORT_LINK_STRATEGY": _get("EXPORT_LINK_STRATEGY", default="hardlink", cast=str),
    "MEDIA_CACHE_ROOT": _get("MEDIA_CACHE_ROOT", default=PMDA_MEDIA_CACHE_ROOT or str(CONFIG_DIR / "media_cache"), cast=str),
}
# PATH_MAP and all config from _get() (SQLite only > default)

SKIP_FOLDERS: list[str] = merged["SKIP_FOLDERS"]
LIBRARY_MODE: str = str(merged.get("LIBRARY_MODE", "files") or "files").strip().lower()
FILES_ROOTS: list[str] = merged.get("FILES_ROOTS", []) or []
EXPORT_ROOT: str = str(merged.get("EXPORT_ROOT", "") or "").strip()
EXPORT_NAMING_TEMPLATE: str = str(merged.get("EXPORT_NAMING_TEMPLATE", "") or "").strip()
EXPORT_LINK_STRATEGY: str = str(merged.get("EXPORT_LINK_STRATEGY", "hardlink") or "hardlink").strip().lower()
if EXPORT_LINK_STRATEGY not in {"hardlink", "symlink", "copy", "move"}:
    EXPORT_LINK_STRATEGY = "hardlink"
MEDIA_CACHE_ROOT: str = str(merged.get("MEDIA_CACHE_ROOT", PMDA_MEDIA_CACHE_ROOT or str(CONFIG_DIR / "media_cache")) or str(CONFIG_DIR / "media_cache")).strip()
USE_MUSICBRAINZ: bool = bool(merged["USE_MUSICBRAINZ"])
MUSICBRAINZ_EMAIL: str = merged.get("MUSICBRAINZ_EMAIL", "pmda@example.com")
MB_QUEUE_ENABLED: bool = bool(merged.get("MB_QUEUE_ENABLED", True))
MB_RETRY_NOT_FOUND: bool = bool(merged.get("MB_RETRY_NOT_FOUND", False))
MB_SEARCH_ALBUM_TIMEOUT_SEC: int = int(merged.get("MB_SEARCH_ALBUM_TIMEOUT_SEC", 20))
MB_CANDIDATE_FETCH_LIMIT: int = int(merged.get("MB_CANDIDATE_FETCH_LIMIT", 4))
MB_TRACKLIST_FETCH_LIMIT: int = int(merged.get("MB_TRACKLIST_FETCH_LIMIT", 2))
MB_FAST_FALLBACK_MODE: bool = bool(merged.get("MB_FAST_FALLBACK_MODE", True))
PROVIDER_IDENTITY_STRICT: bool = bool(merged.get("PROVIDER_IDENTITY_STRICT", True))
PROVIDER_IDENTITY_USE_AI: bool = bool(merged.get("PROVIDER_IDENTITY_USE_AI", True))
PROVIDER_IDENTITY_MIN_SCORE: float = float(merged.get("PROVIDER_IDENTITY_MIN_SCORE", 0.72))
PROVIDER_IDENTITY_SCORE_MARGIN: float = float(merged.get("PROVIDER_IDENTITY_SCORE_MARGIN", 0.08))
PROVIDER_CACHE_FOUND_TTL_SEC: int = int(merged.get("PROVIDER_CACHE_FOUND_TTL_SEC", 60 * 60 * 24 * 30))
PROVIDER_CACHE_NOT_FOUND_TTL_SEC: int = int(merged.get("PROVIDER_CACHE_NOT_FOUND_TTL_SEC", 60 * 60 * 8))
PROVIDER_CACHE_ERROR_TTL_SEC: int = int(merged.get("PROVIDER_CACHE_ERROR_TTL_SEC", 60 * 20))
# Global flags controlling cache usage during scans
SCAN_DISABLE_CACHE: bool = bool(merged.get("SCAN_DISABLE_CACHE", False))
MB_DISABLE_CACHE: bool = bool(merged.get("MB_DISABLE_CACHE", False) or SCAN_DISABLE_CACHE)

# Configure MusicBrainz User-Agent with user's email (if provided)
def _configure_musicbrainz_useragent():
    """Configure MusicBrainz User-Agent with the user's email."""
    email = MUSICBRAINZ_EMAIL.strip() if MUSICBRAINZ_EMAIL else "pmda@example.com"
    musicbrainzngs.set_useragent(
        "PMDA",               # application name
        "0.6.6",              # application version (sync with header)
        email                 # contact / support email
    )
    logging.debug("MusicBrainz User-Agent configured with email: %s", email)

# Configure User-Agent now that config is loaded
_configure_musicbrainz_useragent()
LIDARR_URL: str = merged.get("LIDARR_URL", "")
LIDARR_API_KEY: str = merged.get("LIDARR_API_KEY", "")
AUTOBRR_URL: str = merged.get("AUTOBRR_URL", "")
AUTOBRR_API_KEY: str = merged.get("AUTOBRR_API_KEY", "")
AUTO_FIX_BROKEN_ALBUMS: bool = bool(merged.get("AUTO_FIX_BROKEN_ALBUMS", False))
BROKEN_ALBUM_CONSECUTIVE_THRESHOLD: int = int(merged.get("BROKEN_ALBUM_CONSECUTIVE_THRESHOLD", 2))
BROKEN_ALBUM_PERCENTAGE_THRESHOLD: float = float(merged.get("BROKEN_ALBUM_PERCENTAGE_THRESHOLD", 0.20))
# Default required tags now focus on artist/album + genre/year + tracks.
REQUIRED_TAGS: list[str] = merged.get("REQUIRED_TAGS", ["artist", "album", "genre", "year", "tracks"])
AUTO_MOVE_DUPES: bool = bool(merged["AUTO_MOVE_DUPES"])
AUTO_EXPORT_LIBRARY: bool = bool(merged.get("AUTO_EXPORT_LIBRARY", False))
PIPELINE_ENABLE_MATCH_FIX: bool = bool(merged.get("PIPELINE_ENABLE_MATCH_FIX", True))
PIPELINE_ENABLE_DEDUPE: bool = bool(merged.get("PIPELINE_ENABLE_DEDUPE", True))
PIPELINE_ENABLE_INCOMPLETE_MOVE: bool = bool(merged.get("PIPELINE_ENABLE_INCOMPLETE_MOVE", True))
PIPELINE_ENABLE_EXPORT: bool = bool(merged.get("PIPELINE_ENABLE_EXPORT", False))
PIPELINE_ENABLE_PLAYER_SYNC: bool = bool(merged.get("PIPELINE_ENABLE_PLAYER_SYNC", False))
PIPELINE_PLAYER_TARGET: str = str(merged.get("PIPELINE_PLAYER_TARGET", "none") or "none").strip().lower()
if PIPELINE_PLAYER_TARGET not in {"none", "plex", "jellyfin", "navidrome"}:
    PIPELINE_PLAYER_TARGET = "none"

JELLYFIN_URL: str = str(merged.get("JELLYFIN_URL", "") or "").strip()
JELLYFIN_API_KEY: str = str(merged.get("JELLYFIN_API_KEY", "") or "").strip()
NAVIDROME_URL: str = str(merged.get("NAVIDROME_URL", "") or "").strip()
NAVIDROME_USERNAME: str = str(merged.get("NAVIDROME_USERNAME", "") or "").strip()
NAVIDROME_PASSWORD: str = str(merged.get("NAVIDROME_PASSWORD", "") or "").strip()
NAVIDROME_API_KEY: str = str(merged.get("NAVIDROME_API_KEY", "") or "").strip()
USE_AI_FOR_MB_MATCH: bool = bool(merged.get("USE_AI_FOR_MB_MATCH", True))
USE_AI_FOR_MB_VERIFY: bool = bool(merged.get("USE_AI_FOR_MB_VERIFY", True))
USE_AI_VISION_FOR_COVER: bool = bool(merged.get("USE_AI_VISION_FOR_COVER", True))
AI_CONFIDENCE_MIN: int = int(merged.get("AI_CONFIDENCE_MIN", 50))
OPENAI_VISION_MODEL: str = str(merged.get("OPENAI_VISION_MODEL", "") or "").strip()
USE_AI_VISION_BEFORE_COVER_INJECT: bool = bool(merged.get("USE_AI_VISION_BEFORE_COVER_INJECT", True))
# MusicBrainz AI verify prompt caps (input-token cost control).
AI_MB_VERIFY_MAX_CANDIDATES: int = int(merged.get("AI_MB_VERIFY_MAX_CANDIDATES", 8) or 8)
AI_MB_VERIFY_LOCAL_TRACK_PREVIEW: int = int(merged.get("AI_MB_VERIFY_LOCAL_TRACK_PREVIEW", 12) or 12)
AI_MB_VERIFY_MB_TRACK_PREVIEW: int = int(merged.get("AI_MB_VERIFY_MB_TRACK_PREVIEW", 5) or 5)
BACKUP_BEFORE_FIX: bool = bool(merged.get("BACKUP_BEFORE_FIX", False))
MAGIC_MODE: bool = bool(merged.get("MAGIC_MODE", False))
USE_ACOUSTID: bool = bool(merged.get("USE_ACOUSTID", True))
ACOUSTID_API_KEY: str = str(merged.get("ACOUSTID_API_KEY", "") or "").strip()
USE_ACOUSTID_WHEN_TAGGED: bool = bool(merged.get("USE_ACOUSTID_WHEN_TAGGED", False))
USE_WEB_SEARCH_FOR_MB: bool = bool(merged.get("USE_WEB_SEARCH_FOR_MB", True))
SERPER_API_KEY: str = str(merged.get("SERPER_API_KEY", "") or "").strip()
AI_BATCH_SIZE: int = int(merged.get("AI_BATCH_SIZE", 10))
FFPROBE_POOL_SIZE: int = int(merged.get("FFPROBE_POOL_SIZE", 8))
REPROCESS_INCOMPLETE_ALBUMS: bool = bool(merged.get("REPROCESS_INCOMPLETE_ALBUMS", True))
IMPROVE_ALL_WORKERS: int = int(merged.get("IMPROVE_ALL_WORKERS", 1))
# Cross-library dedupe configuration (from SQLite only)
CROSS_LIBRARY_DEDUPE = merged["CROSS_LIBRARY_DEDUPE"]

# Number of sample tracks per Plex mount to verify (from SQLite only)
CROSSCHECK_SAMPLES = merged["CROSSCHECK_SAMPLES"]

# Skip PATH cross-check at startup when set (from SQLite only)
DISABLE_PATH_CROSSCHECK = merged["DISABLE_PATH_CROSSCHECK"]

# Metadata fallback providers (Improve Album)
USE_DISCOGS: bool = bool(merged.get("USE_DISCOGS", True))
DISCOGS_USER_TOKEN: str = str(merged.get("DISCOGS_USER_TOKEN", "") or "")
USE_LASTFM: bool = bool(merged.get("USE_LASTFM", True))
LASTFM_API_KEY: str = str(merged.get("LASTFM_API_KEY", "") or "")
LASTFM_API_SECRET: str = str(merged.get("LASTFM_API_SECRET", "") or "")
FANART_API_KEY: str = str(merged.get("FANART_API_KEY", "") or "")
THEAUDIODB_API_KEY: str = str(merged.get("THEAUDIODB_API_KEY", "") or "")
USE_BANDCAMP: bool = bool(merged.get("USE_BANDCAMP", True))
ARTIST_CREDIT_MODE: str = str(merged.get("ARTIST_CREDIT_MODE", "picard_like_default") or "picard_like_default").strip().lower()
LIVE_DEDUPE_MODE: str = str(merged.get("LIVE_DEDUPE_MODE", "safe") or "safe").strip().lower()


def _apply_forced_runtime_defaults():
    """
    Keep UX simple without silently re-enabling costly features.

    We still enforce Files mode in this build, but we do NOT force-enable AI/Vision/Web-search
    toggles: those are controlled by settings.db + runtime config, and the pipeline itself
    is responsible for only invoking AI when it is truly ambiguous.
    """
    global LIBRARY_MODE
    LIBRARY_MODE = "files"
    merged["LIBRARY_MODE"] = "files"
    # MusicBrainz tuning is intentionally *not* forced here: the user may set
    # MB_SEARCH_ALBUM_TIMEOUT_SEC / MB_CANDIDATE_FETCH_LIMIT / MB_TRACKLIST_FETCH_LIMIT
    # from Settings (SQLite) and scans should respect those values.


_apply_forced_runtime_defaults()
SKIP_MB_FOR_LIVE_ALBUMS: bool = bool(merged.get("SKIP_MB_FOR_LIVE_ALBUMS", True))
TRACKLIST_MATCH_MIN: float = float(merged.get("TRACKLIST_MATCH_MIN", 0.8))
LIVE_ALBUMS_MB_STRICT: bool = bool(merged.get("LIVE_ALBUMS_MB_STRICT", False))

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Fixed container constants ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# DB filename is always fixed under the Plex DB folder
PLEX_DB_FILE = str(Path(merged["PLEX_DB_PATH"]) / "com.plexapp.plugins.library.db")
PLEX_DB_EXISTS = Path(PLEX_DB_FILE).exists()
# Fully configured only when we have host, token, sections, and readable DB
PLEX_CONFIGURED = bool(
    merged["PLEX_HOST"] and merged["PLEX_TOKEN"] and SECTION_IDS and PLEX_DB_EXISTS
)
if not PLEX_CONFIGURED:
    logging.info(
        "Starting in unconfigured (wizard) mode ‚Äì configure Plex and mount the database at /database in Settings."
    )
# Duplicates root defaults to /dupes but can be overridden from settings.
DUPE_ROOT = Path(str(merged.get("DUPE_ROOT", "/dupes") or "").strip() or "/dupes")
# WebUI always listens on container port 5005 inside the container
WEBUI_PORT = 5005


def _paths_rw_status() -> dict:
    """
    Check that music paths (PATH_MAP values, or /music when PATH_MAP empty) and DUPE_ROOT are readable and writable.
    Uses the same logic as _container_mounts_status() so Settings "Path access" matches the welcome modal.
    Returns dict with music_rw, dupes_rw (bool) and optional messages for UI.
    """
    path_map = getattr(sys.modules[__name__], "PATH_MAP", {})
    dupe_root = getattr(sys.modules[__name__], "DUPE_ROOT", Path("/dupes"))
    music_rw = True
    dupes_rw = dupe_root.exists() and os.access(dupe_root, os.R_OK) and os.access(dupe_root, os.W_OK)
    if path_map:
        for dest in path_map.values():
            p = Path(dest)
            if not p.exists():
                logging.debug("_paths_rw_status: music path %s does not exist", p)
                music_rw = False
                break
            if not os.access(p, os.R_OK) or not os.access(p, os.W_OK):
                logging.debug("_paths_rw_status: music path %s not R+W (R=%s W=%s)", p, os.access(p, os.R_OK), os.access(p, os.W_OK))
                music_rw = False
                break
    else:
        # Same fallback as _container_mounts_status(): check /music when no PATH_MAP yet
        default_music = Path("/music")
        music_rw = default_music.exists() and os.access(default_music, os.R_OK) and os.access(default_music, os.W_OK)
        if not music_rw:
            logging.debug("_paths_rw_status: default /music exists=%s R=%s W=%s",
                          default_music.exists(), os.access(default_music, os.R_OK) if default_music.exists() else False, os.access(default_music, os.W_OK) if default_music.exists() else False)
    return {"music_rw": music_rw, "dupes_rw": dupes_rw}


def _container_mounts_status() -> dict:
    """
    Check that all container mounts PMDA needs are present and have the expected access.
    Used for the fresh-config welcome message so users see at a glance if bindings are OK.
    Returns dict with config_rw, plex_db_ro, music_rw, dupes_rw (all bool).
    """
    config_dir = getattr(sys.modules[__name__], "CONFIG_DIR", Path("/config"))
    plex_db_path = getattr(sys.modules[__name__], "PLEX_DB_PATH", "/database")
    path_map = getattr(sys.modules[__name__], "PATH_MAP", {})
    dupe_root = getattr(sys.modules[__name__], "DUPE_ROOT", Path("/dupes"))

    config_rw = config_dir.exists() and os.access(config_dir, os.R_OK) and os.access(config_dir, os.W_OK)
    plex_db_path_p = Path(plex_db_path)
    plex_db_ro = plex_db_path_p.exists() and os.access(plex_db_path_p, os.R_OK)

    # When PATH_MAP is empty (fresh config), check the standard container mount /music (parent music folder)
    music_rw = True
    if path_map:
        for dest in path_map.values():
            p = Path(dest)
            if not p.exists() or not os.access(p, os.R_OK) or not os.access(p, os.W_OK):
                logging.debug("_container_mounts_status: music path %s exists=%s R=%s W=%s", p, p.exists(), os.access(p, os.R_OK) if p.exists() else False, os.access(p, os.W_OK) if p.exists() else False)
                music_rw = False
                break
    else:
        # No PATH_MAP yet: verify the typical bind mount /music (parent music folder) is RW
        default_music = Path("/music")
        music_rw = default_music.exists() and os.access(default_music, os.R_OK) and os.access(default_music, os.W_OK)
        if not music_rw:
            logging.debug("_container_mounts_status: default /music exists=%s R=%s W=%s",
                          default_music.exists(), os.access(default_music, os.R_OK) if default_music.exists() else False, os.access(default_music, os.W_OK) if default_music.exists() else False)

    dupes_rw = dupe_root.exists() and os.access(dupe_root, os.R_OK) and os.access(dupe_root, os.W_OK)

    return {"config_rw": config_rw, "plex_db_ro": plex_db_ro, "music_rw": music_rw, "dupes_rw": dupes_rw}

# (7) Export as module‚Äëlevel constants ----------------------------------------
PLEX_HOST      = merged["PLEX_HOST"]
PLEX_TOKEN     = merged["PLEX_TOKEN"]
SECTION_IDS    = SECTION_IDS
SECTION_ID     = SECTION_IDS[0] if SECTION_IDS else 0
PATH_MAP       = merged["PATH_MAP"]
import multiprocessing
SCAN_THREADS = merged["SCAN_THREADS"]
LOG_LEVEL      = merged["LOG_LEVEL"]
AI_PROVIDER    = merged["AI_PROVIDER"]
OPENAI_API_KEY = merged["OPENAI_API_KEY"]
OPENAI_MODEL   = merged["OPENAI_MODEL"]
ANTHROPIC_API_KEY = merged["ANTHROPIC_API_KEY"]
GOOGLE_API_KEY = merged["GOOGLE_API_KEY"]
OLLAMA_URL     = merged["OLLAMA_URL"]
DISCORD_WEBHOOK = merged["DISCORD_WEBHOOK"]

#
# State and cache DB always live in the config directory (STATE_DB_FILE defined early after CONFIG_DIR)
CACHE_DB_FILE = CONFIG_DIR / "cache.db"

# File-format preference order (from SQLite only)
FORMAT_PREFERENCE = merged["FORMAT_PREFERENCE"]

# Optional external log file (rotates @ 5 MB x 3) (from SQLite only)
LOG_FILE = merged["LOG_FILE"]

# Attach rotating file handler now that CONFIG_DIR / LOG_FILE are final
try:
    from logging.handlers import RotatingFileHandler
    _final_level = getattr(logging, LOG_LEVEL, logging.INFO)
    root_logger = logging.getLogger()
    root_logger.setLevel(_final_level)
    file_handler = RotatingFileHandler(LOG_FILE, maxBytes=5_000_000, backupCount=3, encoding="utf-8")
    root_logger.addHandler(file_handler)
except Exception as e:
    print(f"‚ö†Ô∏è  File logging disabled ‚Äì {e}", file=sys.stderr)


log_header("configuration")


def _is_sensitive_config_key(key: str) -> bool:
    """Return True for config keys that must never be logged in clear text."""
    k = (key or "").upper()
    sensitive_markers = ("TOKEN", "API_KEY", "SECRET", "PASSWORD", "WEBHOOK")
    return any(marker in k for marker in sensitive_markers)


def _mask_secret_value(value) -> str:
    """Mask secret values while keeping a short prefix for troubleshooting."""
    txt = str(value or "")
    if not txt:
        return ""
    if len(txt) <= 4:
        return "****"
    return txt[:4] + "‚Ä¶"


# Mask & dump effective config ------------------------------------------------
for k, src in ENV_SOURCES.items():
    val = merged.get(k)
    if _is_sensitive_config_key(k) and val:
        val = _mask_secret_value(val)
    logging.info("Config %-15s = %-30s (source: %s)", k, val, src)

logging.info("Config CROSS_LIBRARY_DEDUPE = %s (source: %s)", CROSS_LIBRARY_DEDUPE, ENV_SOURCES.get("CROSS_LIBRARY_DEDUPE", "default"))
if CROSS_LIBRARY_DEDUPE:
    logging.info("‚û°Ô∏è  Duplicate detection mode: cross-library (editions compared across ALL libraries)")
else:
    logging.info("‚û°Ô∏è  Duplicate detection mode: per-library only (no cross-library comparisons)")

if _level_num == logging.DEBUG:
    scrubbed = {k: ("***" if _is_sensitive_config_key(k) else v)
                for k, v in merged.items()}
    logging.debug("Full merged config:\n%s", json.dumps(scrubbed, indent=2))

# (9) Initialise AI clients based on provider ----------------------------------------

openai_client = None
anthropic_client = None
google_client = None
google_client_configured = False
ollama_url = None
ai_provider_ready = False

if AI_PROVIDER.lower() == "openai":
    if OPENAI_API_KEY:
        os.environ["OPENAI_API_KEY"] = OPENAI_API_KEY
        try:
            openai_client = OpenAI()
            ai_provider_ready = True
            logging.info("OpenAI client initialized")
        except Exception as e:
            logging.warning("OpenAI client init failed: %s", e)
    else:
        logging.info("No OPENAI_API_KEY provided; AI-driven selection disabled.")
elif AI_PROVIDER.lower() == "anthropic":
    if ANTHROPIC_API_KEY and anthropic:
        try:
            anthropic_client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)
            ai_provider_ready = True
            logging.info("Anthropic client initialized")
        except Exception as e:
            logging.warning("Anthropic client init failed: %s", e)
    else:
        if not anthropic:
            logging.warning("Anthropic SDK not installed. Install with: pip install anthropic")
        else:
            logging.info("No ANTHROPIC_API_KEY provided; AI-driven selection disabled.")
elif AI_PROVIDER.lower() == "google":
    if GOOGLE_API_KEY and genai:
        try:
            google_client = genai.Client(api_key=GOOGLE_API_KEY)
            google_client_configured = True
            ai_provider_ready = True
            logging.info("Google Gemini client configured")
        except Exception as e:
            logging.warning("Google client configuration failed: %s", e)
    else:
        if not genai:
            logging.warning("Google GenAI SDK not installed. Install with: pip install google-genai")
        else:
            logging.info("No GOOGLE_API_KEY provided; AI-driven selection disabled.")
elif AI_PROVIDER.lower() == "ollama":
    if OLLAMA_URL:
        ollama_url = OLLAMA_URL.rstrip("/")
        # Test connection
        try:
            response = requests.get(f"{ollama_url}/api/tags", timeout=5)
            if response.status_code == 200:
                ai_provider_ready = True
                logging.info("Ollama connection verified at %s", ollama_url)
            else:
                logging.warning("Ollama not accessible at %s (HTTP %d)", ollama_url, response.status_code)
        except Exception as e:
            logging.warning("Ollama connection test failed: %s", e)
    else:
        logging.info("No OLLAMA_URL provided; AI-driven selection disabled.")
else:
    logging.warning("Unknown AI_PROVIDER: %s. Supported: openai, anthropic, google, ollama", AI_PROVIDER)


def _reinit_ai_from_globals():
    """Re-initialize AI clients from current module globals (after settings save). No restart needed."""
    global openai_client, anthropic_client, google_client, google_client_configured, ollama_url, ai_provider_ready
    global RESOLVED_MODEL, RESOLVED_PARAM_STYLE, RESOLVED_STOP_OK, AI_FUNCTIONAL_ERROR_MSG
    mod = sys.modules[__name__]
    provider = (getattr(mod, "AI_PROVIDER", "") or "openai").strip().lower()
    openai_key = getattr(mod, "OPENAI_API_KEY", "") or ""
    anthropic_key = getattr(mod, "ANTHROPIC_API_KEY", "") or ""
    google_key = getattr(mod, "GOOGLE_API_KEY", "") or ""
    ollama_u = (getattr(mod, "OLLAMA_URL", "") or "").strip().rstrip("/")
    openai_model = getattr(mod, "OPENAI_MODEL", "gpt-4") or "gpt-4"
    _merged = getattr(mod, "merged", None) or {}

    openai_client = None
    anthropic_client = None
    google_client = None
    google_client_configured = False
    ollama_url = None
    ai_provider_ready = False

    if provider == "openai":
        if openai_key:
            os.environ["OPENAI_API_KEY"] = openai_key
            try:
                openai_client = OpenAI()
                ai_provider_ready = True
                logging.info("OpenAI client re-initialized (settings applied)")
            except Exception as e:
                logging.warning("OpenAI client re-init failed: %s", e)
        if openai_client:
            # One model only: probe the configured model to learn token param style and
            # ensure we get parseable output for PMDA. We do not auto-fallback to other models.
            cand = (openai_model or "").strip() or "gpt-4o-mini"
            style = _probe_model(cand)
            if style and _probe_ai_choose_best_response(cand):
                RESOLVED_MODEL = cand
                RESOLVED_PARAM_STYLE = style
                AI_FUNCTIONAL_ERROR_MSG = None
                logging.info(
                    "Using OpenAI model '%s' (%s, stop_ok=%s)",
                    RESOLVED_MODEL,
                    RESOLVED_PARAM_STYLE,
                    RESOLVED_STOP_OK,
                )
            else:
                ai_provider_ready = False
                if getattr(sys.modules[__name__], "OPENAI_LAST_PROBE_401", False):
                    AI_FUNCTIONAL_ERROR_MSG = (
                        "OpenAI API key is invalid or expired (401 Unauthorized). "
                        "Check your key in Settings ‚Üí AI and try again."
                    )
                else:
                    probe_detail = (getattr(sys.modules[__name__], "OPENAI_MODEL_PROBE_LAST_ERROR", "") or "").strip()
                    detail_suffix = f" Reason: {probe_detail}" if probe_detail else ""
                    AI_FUNCTIONAL_ERROR_MSG = (
                        f"AI disabled: OpenAI model '{cand}' failed PMDA preflight (unsupported params or empty/unparseable output)."
                        f"{detail_suffix}"
                    )
                RESOLVED_MODEL = cand
                RESOLVED_PARAM_STYLE = "mct"
                RESOLVED_STOP_OK = True
                logging.warning("OpenAI model probe failed; AI disabled. %s", AI_FUNCTIONAL_ERROR_MSG)
        else:
            logging.info("No OPENAI_API_KEY; AI-driven selection disabled.")
    elif provider == "anthropic":
        if anthropic_key and anthropic:
            try:
                anthropic_client = anthropic.Anthropic(api_key=anthropic_key)
                ai_provider_ready = True
                logging.info("Anthropic client re-initialized (settings applied)")
            except Exception as e:
                logging.warning("Anthropic client re-init failed: %s", e)
        else:
            logging.info("No ANTHROPIC_API_KEY; AI-driven selection disabled.")
    elif provider == "google":
        if google_key and genai:
            try:
                google_client = genai.Client(api_key=google_key)
                google_client_configured = True
                ai_provider_ready = True
                logging.info("Google Gemini client re-initialized (settings applied)")
            except Exception as e:
                logging.warning("Google client re-init failed: %s", e)
        else:
            logging.info("No GOOGLE_API_KEY; AI-driven selection disabled.")
    elif provider == "ollama":
        if ollama_u:
            ollama_url = ollama_u
            try:
                r = requests.get(f"{ollama_url}/api/tags", timeout=5)
                if r.status_code == 200:
                    ai_provider_ready = True
                    logging.info("Ollama re-verified at %s (settings applied)", ollama_url)
                else:
                    logging.warning("Ollama not accessible at %s (HTTP %d)", ollama_url, r.status_code)
            except Exception as e:
                logging.warning("Ollama re-check failed: %s", e)
        else:
            logging.info("No OLLAMA_URL; AI-driven selection disabled.")
    else:
        logging.warning("Unknown AI_PROVIDER: %s", provider)


def _reload_ai_config_and_reinit():
    """Reload AI config from settings.db and run _reinit_ai_from_globals().

    Used at scan start and provider preflight. This must read from SETTINGS_DB_FILE
    (single source of truth for configuration), not STATE_DB_FILE (scan data DB).
    """
    mod = sys.modules[__name__]
    ai_config_keys = ("AI_PROVIDER", "OPENAI_API_KEY", "OPENAI_MODEL",
                      "ANTHROPIC_API_KEY", "GOOGLE_API_KEY", "OLLAMA_URL")
    try:
        db_path = SETTINGS_DB_FILE
        if db_path.exists():
            con = sqlite3.connect(str(db_path), timeout=5)
            cur = con.cursor()
            placeholders = ",".join("?" for _ in ai_config_keys)
            cur.execute(
                f"SELECT key, value FROM settings WHERE key IN ({placeholders})",
                ai_config_keys,
            )
            for key, value in cur.fetchall():
                setattr(mod, key, (value or ""))
            con.close()
        _reinit_ai_from_globals()
    except Exception as e:
        logging.warning("_reload_ai_config_and_reinit failed: %s", e)


# --- Resolve a working OpenAI model (with price-aware fallbacks) -------------
RESOLVED_MODEL = OPENAI_MODEL
# Param style for the resolved model: "mct" -> max_completion_tokens, "mt" -> max_tokens
RESOLVED_PARAM_STYLE = "mct"
# Some models (e.g. reasoning/o-series) do not accept "stop"; when False, call_ai_provider omits stop.
RESOLVED_STOP_OK = True
# When no OpenAI candidate passes probe, set by _reinit_ai_from_globals for 503 / preflight message.
AI_FUNCTIONAL_ERROR_MSG = None
# Set by _probe_model when last failure was 401 so we can show a clear message.
OPENAI_LAST_PROBE_401 = False
# Best-effort error detail captured during OpenAI model probe so the UI can show an actionable reason.
OPENAI_MODEL_PROBE_LAST_ERROR = ""

def _probe_model(model_name: str) -> str | None:
    """Return param style ("mct" or "mt") if a 1-line ping works, else None.
    Tries max_completion_tokens first, then max_tokens; each with stop.
    If both fail with unsupported_parameter, retries without stop (some models do not accept stop).
    Sets global RESOLVED_STOP_OK to False when the model only works without stop.
    """
    global RESOLVED_STOP_OK, OPENAI_LAST_PROBE_401, AI_FUNCTIONAL_ERROR_MSG
    global OPENAI_MODEL_PROBE_LAST_ERROR
    OPENAI_LAST_PROBE_401 = False
    OPENAI_MODEL_PROBE_LAST_ERROR = ""
    if not (OPENAI_API_KEY and openai_client):
        return None
    # GPT-5 family models are known to reject some Chat Completions params (notably "stop"),
    # and can spend short completion budgets entirely on reasoning, producing empty text.
    # Probe them with a larger completion budget and without stop, and require a visible "PONG".
    try:
        mlow = (model_name or "").strip().lower()
    except Exception:
        mlow = ""
    if mlow.startswith("gpt-5"):
        try:
            resp = openai_client.chat.completions.create(
                model=model_name,
                messages=[
                    # For o1+ models (including GPT-5), prefer "developer" over "system".
                    {"role": "developer", "content": "Reply with exactly: PONG"},
                    {"role": "user", "content": "ping"},
                ],
                # Reduce reasoning to avoid spending the entire budget on hidden tokens.
                # This both lowers cost and makes it far more likely to get visible output.
                reasoning_effort="minimal",
                max_completion_tokens=96,
            )
            txt = (resp.choices[0].message.content or "").strip() if resp and resp.choices else ""
            if txt.upper().startswith("PONG"):
                RESOLVED_STOP_OK = False
                return "mct"
            OPENAI_MODEL_PROBE_LAST_ERROR = "Probe returned empty/unexpected output for a strict 'PONG' ping"
            return None
        except Exception as e:
            msg = str(e)
            OPENAI_MODEL_PROBE_LAST_ERROR = msg[:240]
            if "401" in msg or "unauthorized" in msg.lower():
                OPENAI_LAST_PROBE_401 = True
            logging.debug("Model probe (gpt-5 no-stop) failed for %s: %s", model_name, msg)
            return None
    # Try with max_completion_tokens + stop
    try:
        openai_client.chat.completions.create(
            model=model_name,
            messages=[{"role": "user", "content": "ping"}],
            max_completion_tokens=8,
            stop=["\n"],
        )
        RESOLVED_STOP_OK = True
        return "mct"
    except Exception as e:
        msg = str(e)
        if "401" in msg or "unauthorized" in msg.lower():
            OPENAI_LAST_PROBE_401 = True
        OPENAI_MODEL_PROBE_LAST_ERROR = msg[:240]
        logging.debug("Model probe (mct+stop) failed for %s: %s", model_name, msg)
    # Try legacy max_tokens + stop
    try:
        openai_client.chat.completions.create(
            model=model_name,
            messages=[{"role": "user", "content": "ping"}],
            max_tokens=8,
            stop=["\n"],
        )
        RESOLVED_STOP_OK = True
        return "mt"
    except Exception as e2:
        msg2 = str(e2)
        if "401" in msg2 or "unauthorized" in msg2.lower():
            OPENAI_LAST_PROBE_401 = True
        OPENAI_MODEL_PROBE_LAST_ERROR = msg2[:240]
        logging.debug("Model probe (mt+stop) failed for %s: %s", model_name, msg2)
    # If both failed with unsupported_parameter, try without stop (some reasoning models reject stop)
    low_msg = (msg or "").lower()
    low_msg2 = (msg2 or "").lower()
    unsupported = (
        ("unsupported_parameter" in low_msg) or ("unsupported_parameter" in low_msg2)
        or ("unsupported" in low_msg and "parameter" in low_msg)
        or ("unsupported" in low_msg2 and "parameter" in low_msg2)
        or ("stop" in low_msg and "unsupported" in low_msg)
        or ("stop" in low_msg2 and "unsupported" in low_msg2)
    )
    if not unsupported:
        return None
    try:
        openai_client.chat.completions.create(
            model=model_name,
            messages=[{"role": "user", "content": "ping"}],
            max_completion_tokens=8,
        )
        RESOLVED_STOP_OK = False
        logging.debug("Model probe: %s works with max_completion_tokens, without stop", model_name)
        return "mct"
    except Exception:
        pass
    try:
        openai_client.chat.completions.create(
            model=model_name,
            messages=[{"role": "user", "content": "ping"}],
            max_tokens=8,
        )
        RESOLVED_STOP_OK = False
        logging.debug("Model probe: %s works with max_tokens, without stop", model_name)
        return "mt"
    except Exception:
        return None


def call_ai_provider(provider: str, model: str, system_msg: str, user_msg: str, max_tokens: int = 256) -> str:
    """
    Call the appropriate AI provider with the given messages.
    Returns the text response from the AI.
    For OpenAI, uses RESOLVED_PARAM_STYLE (mct or mt) and RESOLVED_STOP_OK; on 400 unsupported
    parameter, retries with the other param or without stop.
    """
    provider_lower = provider.lower()

    if provider_lower == "openai":
        if not openai_client:
            raise ValueError("OpenAI client not initialized")
        # GPT-5 family models may spend a small completion budget entirely on reasoning
        # and return empty text. Enforce a sane minimum for PMDA's short-format outputs.
        try:
            if (model or "").strip().lower().startswith("gpt-5") and int(max_tokens or 0) < 128:
                max_tokens = 128
        except Exception:
            pass
        try:
            is_gpt5 = (model or "").strip().lower().startswith("gpt-5")
        except Exception:
            is_gpt5 = False
        param_style = getattr(sys.modules[__name__], "RESOLVED_PARAM_STYLE", "mct")
        stop_ok = getattr(sys.modules[__name__], "RESOLVED_STOP_OK", True)
        _kwargs = {
            "model": model,
            "messages": [
                # For o1+ models (including GPT-5), prefer "developer" messages for instructions.
                {"role": ("developer" if is_gpt5 else "system"), "content": system_msg},
                {"role": "user", "content": user_msg},
            ],
        }
        if is_gpt5:
            # Lower reasoning effort for cheap + deterministic, short, parseable outputs.
            _kwargs["reasoning_effort"] = "minimal"
        try:
            if stop_ok and not is_gpt5:
                _kwargs["stop"] = ["\n"]
            if param_style == "mct":
                _kwargs["max_completion_tokens"] = max_tokens
            else:
                _kwargs["max_tokens"] = max_tokens
            resp = openai_client.chat.completions.create(**_kwargs)
            out = (resp.choices[0].message.content or "").strip()
            if not out and is_gpt5 and int(max_tokens or 0) < 256:
                # One retry with a larger budget for GPT-5 family models.
                _kwargs_retry = dict(_kwargs)
                _kwargs_retry.pop("stop", None)
                _kwargs_retry.pop("max_tokens", None)
                _kwargs_retry["max_completion_tokens"] = 256
                resp2 = openai_client.chat.completions.create(**_kwargs_retry)
                out = (resp2.choices[0].message.content or "").strip()
            return out
        except Exception as e:
            err_msg = str(e).lower()
            # If reasoning_effort is not supported by the model, retry without it.
            if "reasoning_effort" in err_msg and ("unsupported_parameter" in err_msg or "400" in err_msg):
                _kwargs.pop("reasoning_effort", None)
                try:
                    resp = openai_client.chat.completions.create(**_kwargs)
                    return (resp.choices[0].message.content or "").strip()
                except Exception:
                    raise e
            # On 400 unsupported parameter, retry with the other param or without stop
            if "unsupported_parameter" not in err_msg and "400" not in err_msg:
                raise
            if "max_tokens" in err_msg and "max_completion_tokens" in err_msg:
                _kwargs.pop("max_tokens", None)
                _kwargs["max_completion_tokens"] = max_tokens
                try:
                    resp = openai_client.chat.completions.create(**_kwargs)
                    return (resp.choices[0].message.content or "").strip()
                except Exception:
                    raise e
            if "max_completion_tokens" in err_msg and ("max_tokens" in err_msg or "use" in err_msg):
                _kwargs.pop("max_completion_tokens", None)
                _kwargs["max_tokens"] = max_tokens
                try:
                    resp = openai_client.chat.completions.create(**_kwargs)
                    return (resp.choices[0].message.content or "").strip()
                except Exception:
                    raise e
            if "stop" in err_msg or "unsupported" in err_msg:
                _kwargs.pop("stop", None)
                # Persist: this model doesn't accept stop.
                try:
                    sys.modules[__name__].RESOLVED_STOP_OK = False
                except Exception:
                    pass
                try:
                    resp = openai_client.chat.completions.create(**_kwargs)
                    return (resp.choices[0].message.content or "").strip()
                except Exception:
                    raise e
            raise

    elif provider_lower == "anthropic":
        if not anthropic_client:
            raise ValueError("Anthropic client not initialized")
        resp = anthropic_client.messages.create(
            model=model,
            max_tokens=max_tokens,
            system=system_msg,
            messages=[{"role": "user", "content": user_msg}],
        )
        return resp.content[0].text.strip()

    elif provider_lower == "google":
        if not google_client_configured or not google_client:
            raise ValueError("Google client not configured")
        response = google_client.models.generate_content(
            model=model,
            contents=(user_msg or ""),
            config=genai.types.GenerateContentConfig(
                systemInstruction=(system_msg or ""),
                maxOutputTokens=max_tokens,
                stopSequences=["\n"],
            ),
        )
        return (getattr(response, "text", "") or "").strip()

    elif provider_lower == "ollama":
        if not ollama_url:
            raise ValueError("Ollama URL not configured")
        payload = {
            "model": model,
            "messages": [
                {"role": "system", "content": system_msg},
                {"role": "user", "content": user_msg},
            ],
            "options": {
                "num_predict": max_tokens,
                "stop": ["\n"],
            },
            "stream": False,
        }
        response = requests.post(f"{ollama_url}/api/chat", json=payload, timeout=60)
        if response.status_code != 200:
            raise Exception(f"Ollama API error: {response.status_code} - {response.text}")
        result = response.json()
        return result.get("message", {}).get("content", "").strip()

    else:
        raise ValueError(f"Unknown AI provider: {provider}")


def call_ai_provider_vision(
    provider: str,
    model: str,
    system_msg: str,
    user_msg: str,
    image_urls: Optional[List[str]] = None,
    image_base64: Optional[List[dict]] = None,
    max_tokens: int = 32,
) -> str:
    """
    Call AI with optional images (vision). Used for cover comparison: "Do these two covers represent the same album? Yes/No."
    image_urls: list of image URLs (e.g. Cover Art Archive, or PMDA-served local cover).
    image_base64: list of {"type": "image_url", "image_url": {"url": "data:image/...;base64,..."}} or provider-specific.
    Returns the text response. Only OpenAI is supported for vision; other providers fall back to text-only.
    """
    provider_lower = provider.lower()
    if provider_lower != "openai" or not openai_client:
        # Fallback: call without images
        return call_ai_provider(provider, model, system_msg, user_msg, max_tokens=max_tokens)
    content: List[dict] = [{"type": "text", "text": user_msg}]
    if image_urls:
        for url in image_urls[:10]:
            if url:
                content.append({"type": "image_url", "image_url": {"url": url}})
    if image_base64:
        for img in image_base64[:10]:
            if isinstance(img, dict) and img.get("type") == "image_url" and img.get("image_url", {}).get("url"):
                content.append(img)
    param_style = getattr(sys.modules[__name__], "RESOLVED_PARAM_STYLE", "mct")
    stop_ok = getattr(sys.modules[__name__], "RESOLVED_STOP_OK", True)
    try:
        is_gpt5 = (model or "").strip().lower().startswith("gpt-5")
    except Exception:
        is_gpt5 = False
    # GPT-5 family models may spend very small budgets entirely on reasoning and return empty output.
    # Enforce a sane minimum for short YES/NO style outputs.
    try:
        if is_gpt5 and int(max_tokens or 0) < 128:
            max_tokens = 128
    except Exception:
        pass
    _kwargs = {
        "model": model,
        "messages": [
            {"role": ("developer" if is_gpt5 else "system"), "content": system_msg},
            {"role": "user", "content": content},
        ],
    }
    if is_gpt5:
        _kwargs["reasoning_effort"] = "minimal"
    if stop_ok and not is_gpt5:
        _kwargs["stop"] = ["\n"]
    if param_style == "mct":
        _kwargs["max_completion_tokens"] = max_tokens
    else:
        _kwargs["max_tokens"] = max_tokens
    try:
        resp = openai_client.chat.completions.create(**_kwargs)
        out = (resp.choices[0].message.content or "").strip()
        if not out and is_gpt5 and int(max_tokens or 0) < 256:
            # One retry with a larger budget for GPT-5 family models.
            _kwargs_retry = dict(_kwargs)
            _kwargs_retry.pop("stop", None)
            _kwargs_retry.pop("max_tokens", None)
            _kwargs_retry["max_completion_tokens"] = 256
            resp2 = openai_client.chat.completions.create(**_kwargs_retry)
            out = (resp2.choices[0].message.content or "").strip()
        return out
    except Exception as e:
        logging.debug("[AI Vision] OpenAI vision call failed: %s", e)
        # Retry without reasoning_effort for models that don't support it.
        try:
            msg = str(e).lower()
        except Exception:
            msg = ""
        if "reasoning_effort" in msg and ("unsupported_parameter" in msg or "400" in msg):
            _kwargs.pop("reasoning_effort", None)
            try:
                resp = openai_client.chat.completions.create(**_kwargs)
                return (resp.choices[0].message.content or "").strip()
            except Exception:
                raise e
        # Retry without stop if the model rejects it (some reasoning models reject stop).
        if "stop" in msg and ("unsupported" in msg or "unsupported_parameter" in msg or "400" in msg):
            _kwargs.pop("stop", None)
            try:
                sys.modules[__name__].RESOLVED_STOP_OK = False
            except Exception:
                pass
            resp = openai_client.chat.completions.create(**_kwargs)
            return (resp.choices[0].message.content or "").strip()
        raise


def parse_ai_confidence(reply: str) -> tuple[str, Optional[int]]:
    """
    Strip optional (confidence: N) from the end of an AI reply. N is clamped to 0-100.
    Returns (reply_without_confidence, confidence_or_None).
    """
    if not (reply or isinstance(reply, str)):
        return (reply or "", None)
    text = reply.strip()
    m = re.search(r"\s*\(confidence:\s*(\d+)\)\s*$", text, re.I)
    if m:
        conf = min(100, max(0, int(m.group(1))))
        clean = text[: m.start()].strip()
        return (clean, conf)
    return (text, None)


def ai_verify_mb_match(
    artist: str,
    title_raw: Optional[str],
    title_norm: str,
    track_titles: Optional[List[str]],
    track_count: int,
    candidates: List[tuple],
    has_cover: bool = False,
    extra_sources: Optional[List[dict]] = None,
) -> tuple[Optional[tuple], Optional[int]]:
    """
    Ask the AI to pick which MusicBrainz candidate matches our album (or NONE).
    candidates: list of (rg, result_dict) where rg has 'title','id', result_dict has 'id','track_count'.
    extra_sources: optional list of {"source": "Discogs"|"Last.fm"|"Bandcamp", "title": ..., "artist": ...} for disambiguation.
    Returns (chosen_candidate_or_None, confidence_or_None). When confidence is below AI_CONFIDENCE_MIN, returns (None, conf) so caller can try other sources.
    """
    if not getattr(sys.modules[__name__], "USE_AI_FOR_MB_VERIFY", False):
        return (None, None)
    if not getattr(sys.modules[__name__], "ai_provider_ready", False):
        return (None, None)
    if not candidates:
        return (None, None)
    # Keep prompts bounded: MB search can return many candidates and full tracklists are token-expensive.
    # These caps dramatically reduce input tokens while preserving enough signal for disambiguation.
    max_candidates = int(getattr(sys.modules[__name__], "AI_MB_VERIFY_MAX_CANDIDATES", 8) or 8)
    max_local_tracks = int(getattr(sys.modules[__name__], "AI_MB_VERIFY_LOCAL_TRACK_PREVIEW", 12) or 12)
    max_mb_tracks = int(getattr(sys.modules[__name__], "AI_MB_VERIFY_MB_TRACK_PREVIEW", 5) or 5)
    letters = "ABCDEFGHIJKLMNOPQRSTUVWXYZ"
    choices = []
    for i, (rg, result_dict) in enumerate(candidates[:max_candidates]):
        tc = result_dict.get("track_count", "?")
        line = f"{letters[i]}: {rg.get('title', 'Unknown')} (id={rg.get('id', '')}, {tc} tracks)"
        mb_tracks = result_dict.get("track_titles")
        if mb_tracks:
            track_preview = ", ".join(mb_tracks[:max_mb_tracks]) + ("..." if len(mb_tracks) > max_mb_tracks else "")
            line += f" ‚Äî tracks: [{track_preview}]"
        choices.append(line)
    track_list_str = ", ".join((track_titles or [])[:max_local_tracks]) if track_titles else "(none)"
    if track_titles and len(track_titles) > max_local_tracks:
        track_list_str += ", ..."
    user_msg = (
        f"Album: artist={artist!r}; title={title_raw or title_norm!r}; norm={title_norm!r}; "
        f"tracks={track_count}; local_tracks=[{track_list_str}]; cover={has_cover}.\n"
        "Candidates:\n" + "\n".join(choices)
    )
    if extra_sources:
        other_lines = []
        for s in extra_sources:
            src = s.get("source", "?")
            t = s.get("title") or s.get("album") or "?"
            a = s.get("artist") or s.get("artist_name") or "?"
            other_lines.append(f"  {src}: {t!r} (artist: {a!r})")
        user_msg += "\n\nOther sources (for disambiguation):\n" + "\n".join(other_lines)
    user_msg += (
        "\n\nWhich candidate is the same release? Consider title variants (e.g. Volume I = volume i). "
        "Reply with only the letter (A, B, ...) or NONE if no candidate matches. "
        "Optionally end with (confidence: N) where N is 0-100."
    )
    system_msg = "Reply with only: a single letter (A, B, C, ...) or NONE. Optionally add (confidence: N)."
    try:
        model = getattr(sys.modules[__name__], "RESOLVED_MODEL", None) or getattr(sys.modules[__name__], "OPENAI_MODEL", "gpt-4")
        provider = getattr(sys.modules[__name__], "AI_PROVIDER", "openai")
        reply = call_ai_provider(provider, model, system_msg, user_msg, max_tokens=30)
        reply_clean, ai_confidence = parse_ai_confidence((reply or "").strip())
        reply_clean = reply_clean.upper()
        if ai_confidence is not None:
            logging.info("[MusicBrainz Verify] AI confidence: %d", ai_confidence)
        if reply_clean == "NONE":
            return (None, ai_confidence)
        letter = reply_clean[:1]
        idx = letters.find(letter)
        if 0 <= idx < len(candidates):
            logging.info("[MusicBrainz Verify] AI selected candidate %s: %s", letter, candidates[idx][0].get("title"))
            confidence_min = getattr(sys.modules[__name__], "AI_CONFIDENCE_MIN", 0)
            if confidence_min > 0 and ai_confidence is not None and ai_confidence < confidence_min:
                logging.info("[MusicBrainz Verify] Low confidence (%d < %d): rejecting match, caller may try other sources", ai_confidence, confidence_min)
                return (None, ai_confidence)
            return (candidates[idx], ai_confidence)
        return (None, ai_confidence)
    except Exception as e:
        logging.debug("[MusicBrainz Verify] AI verify failed: %s", e)
        return (None, None)


def _probe_ai_choose_best_response(model_name: str) -> bool:
    """
    Verify the model returns a parseable choose_best-style response (index|rationale|extras).
    If the model returns empty or unparseable output, scan would show Heuristic fallbacks.
    Return True only if we get a valid response so we block scan when AI would fail.
    """
    if not (OPENAI_API_KEY and openai_client and AI_PROVIDER and getattr(sys.modules[__name__], "AI_PROVIDER", "").strip().lower() == "openai"):
        return True  # Skip functional probe for non-OpenAI
    system_msg = (
        "You are an expert digital-music librarian.\n"
        "OUTPUT RULES (must follow exactly):\n"
        "- Return ONE single line only.\n"
        "- The line must contain EXACTLY two '|' characters.\n"
        "- Format: <index>|<brief rationale>|<comma-separated extra tracks>\n"
        "- If there are no extra tracks, still include the final pipe but leave it empty.\n"
        "- Do not add any other text, do not explain, do not add extra lines.\n"
        "Example of valid outputs:\n"
        "0|Preferred lossless|"
    )
    user_msg = "Candidate editions:\n0: fmt_score=1, bd=24, tracks=10, size_mb=200\n1: fmt_score=0, bd=16, tracks=10, size_mb=100\n"
    try:
        txt = call_ai_provider(AI_PROVIDER, model_name, system_msg, user_msg, max_tokens=256)
        if not txt:
            logging.debug("Functional probe: model %s returned empty response", model_name)
            try:
                sys.modules[__name__].OPENAI_MODEL_PROBE_LAST_ERROR = "Functional probe returned empty output for choose_best"
            except Exception:
                pass
            return False
        lines = [l.strip() for l in txt.replace("```", "").splitlines() if l.strip()]
        txt = lines[0] if lines else txt
        txt = re.sub(r"^(answer|r√©ponse)\s*:\s*", "", txt, flags=re.IGNORECASE).strip()
        m = re.match(r"^(\d+)\s*\|\s*(.*?)\s*\|\s*(.*)$", txt)
        if m:
            idx = int(m.group(1))
            if 0 <= idx <= 1:
                logging.debug("Functional probe: model %s returned parseable response (index=%s)", model_name, idx)
                return True
        logging.debug("Functional probe: model %s response not parseable: %r", model_name, txt[:80])
        try:
            sys.modules[__name__].OPENAI_MODEL_PROBE_LAST_ERROR = f"Functional probe output not parseable: {txt[:120]!r}"
        except Exception:
            pass
        return False
    except Exception as e:
        logging.debug("Functional probe failed for %s: %s", model_name, e)
        try:
            sys.modules[__name__].OPENAI_MODEL_PROBE_LAST_ERROR = f"Functional probe exception: {str(e)[:180]}"
        except Exception:
            pass
        return False


# Curated list of OpenAI Chat Completions models known to work with PMDA (parseable index|rationale|extras).
# Only these are shown in Settings. (We probe the model at runtime to verify token params + output format.)
OPENAI_COMPATIBLE_MODELS = [
    "gpt-5",
    "gpt-5-mini",
    "gpt-5-nano",
    "gpt-4o-mini",
    "gpt-4o",
    "gpt-4.1",
    "gpt-4.1-mini",
    "gpt-4.1-nano",
    "gpt-4-turbo",
    "gpt-4",
    "gpt-3.5-turbo",
]

# Probe the configured OpenAI model (no fallbacks). If it fails, AI is disabled and the UI
# will show a clear error. This keeps configuration simple: one model applies everywhere.
_startup_model = (OPENAI_MODEL or "").strip() or "gpt-4o-mini"
style = _probe_model(_startup_model)
if style and _probe_ai_choose_best_response(_startup_model):
    RESOLVED_MODEL = _startup_model
    RESOLVED_PARAM_STYLE = style
    logging.info("Using requested OpenAI model '%s' (%s)", RESOLVED_MODEL, RESOLVED_PARAM_STYLE)
else:
    if openai_client:
        ai_provider_ready = False
        _detail = (getattr(sys.modules[__name__], "OPENAI_MODEL_PROBE_LAST_ERROR", "") or "").strip()
        _suffix = f" Reason: {_detail}" if _detail else ""
        AI_FUNCTIONAL_ERROR_MSG = (
            f"AI disabled: OpenAI model '{_startup_model}' failed PMDA preflight (unsupported params or empty/unparseable output)."
            f"{_suffix}"
        )
        logging.warning("OpenAI: model probe failed; AI disabled. %s", AI_FUNCTIONAL_ERROR_MSG)

# (10) Validate Plex connection ------------------------------------------------
# (10) Validate Plex connection ------------------------------------------------
def _validate_plex_connection():
    """
    Perform a lightweight request to Plex `/library/sections` to make sure the
    server is reachable and the token is valid.  We only warn on failure so
    the application can still run in offline mode.
    """
    host = (PLEX_HOST or "").strip()
    if not host.startswith(("http://", "https://")):
        logging.debug("Skipping Plex connection check (no valid PLEX_HOST)")
        return
    url = f"{host.rstrip('/')}/library/sections"
    try:
        resp = requests.get(url, headers={"X-Plex-Token": PLEX_TOKEN}, timeout=10)
        if resp.status_code != 200:
            logging.warning(
                "‚ö†Ô∏è  Plex connection failed (HTTP %s) ‚Äì check PLEX_HOST and PLEX_TOKEN",
                resp.status_code,
            )
        else:
            logging.info("Plex connection OK (HTTP %s)", resp.status_code)
    except Exception as e:
        logging.warning("‚ö†Ô∏è  Plex connection failed ‚Äì %s", e)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ SELF‚ÄëDIAGNOSTIC ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def _self_diag() -> bool:
    """
    Runs a quick start‚Äëup check and prints a colour‚Äëcoded report:
    1) Plex DB reachability
    2) Coverage of every PATH_MAP entry
    3) R/W permissions on mapped music folders and /dupes
    4) Rough count of albums with no PATH_MAP match

    Returns *True* when every mandatory check passes, otherwise *False*.
    """
    log_header("self diagnostic")
    had_warning = False

    # 1) Plex DB readable?
    try:
        db = plex_connect()
        db.execute("SELECT 1").fetchone()
        logging.info(f"‚úì Plex DB reachable ({PLEX_DB_FILE})")
    except Exception as e:
        logging.error("‚úó Plex DB ERROR ‚Äì %s", e)
        return False

    # 0) /dupes sanity  (warn but do NOT hard‚Äëfail)
    if not (DUPE_ROOT.exists() and os.access(DUPE_ROOT, os.W_OK)):
        warn = ("‚ö† /dupes is missing or read‚Äëonly ‚Äì PMDA can't move duplicates.\n"
                "üëâ Please bind‚Äëmount a writable host folder, e.g.  -v /path/on/host:/dupes")
        logging.warning(warn)
        notify_discord(warn)
        had_warning = True

    # Compute exact album counts for each PATH_MAP prefix (no sampling)
    prefix_stats: dict[str, int] = {}
    for pre in PATH_MAP:
        cnt = db.execute(
            """
            SELECT COUNT(DISTINCT alb.id)
            FROM   media_parts      mp
            JOIN   media_items      mi  ON mi.id       = mp.media_item_id
            JOIN   metadata_items   tr  ON tr.id       = mi.metadata_item_id   -- track (type 10)
            JOIN   metadata_items   alb ON alb.id      = tr.parent_id          -- album (type 9)
            WHERE  mp.file LIKE ?
            """,
            (f"{pre}/%",)
        ).fetchone()[0]
        prefix_stats[pre] = cnt

    for pre, dest in PATH_MAP.items():
        albums_seen = prefix_stats.get(pre, 0)
        if albums_seen == 0:
            logging.warning("%s %s ‚Üí %s  (prefix not found in DB)",
                            colour('‚ö†', ANSI_YELLOW), pre, dest)
            had_warning = True
        elif not Path(dest).exists():
            logging.error("‚úó %s ‚Üí %s  (host path missing)", pre, dest)
            return False
        else:
            logging.info("%s %s ‚Üí %s  (%d albums)",
                         colour('‚úì', ANSI_GREEN), pre, dest, albums_seen)

    # 3) Permission checks
    for mount in [*PATH_MAP.values(), str(DUPE_ROOT), str(CONFIG_DIR)]:
        p = Path(mount)
        if not p.exists():
            continue
        rw = ("r" if os.access(p, os.R_OK) else "-") + \
             ("w" if os.access(p, os.W_OK) else "-")
        if rw != "rw":
            logging.warning("‚ö† %s permissions: %s", p, rw)
            had_warning = True
        else:
            logging.info("‚úì %s permissions: %s", p, rw)

    # 4) Albums with no mapping (skip if no PATH_MAP entries)
    if PATH_MAP:
        # Restrict the "un‚Äëmapped" check to the chosen MUSIC section(s) only
        where_clauses = " AND ".join(f"mp.file NOT LIKE '{pre}%'" for pre in PATH_MAP)
        placeholders = ",".join("?" for _ in SECTION_IDS)
        query = f"""
            SELECT COUNT(*)
            FROM   media_parts  mp
            JOIN   metadata_items md ON md.id = mp.media_item_id
            WHERE  md.library_section_id IN ({placeholders})
              AND  md.metadata_type     = 9          -- 9 = album
              AND  {where_clauses}
        """
        unmapped = db.execute(query, SECTION_IDS).fetchone()[0]
        if unmapped:
            logging.warning(
                "‚ö† %d albums have no PATH_MAP match; this is not necessarily an error. "
                "these albums may belong to Plex library sections you haven't included. "
                "to avoid this warning, set SECTION_IDS to include all relevant section IDs, separated by commas.",
                unmapped
            )
            had_warning = True
    else:
        logging.info("Skipping unmapped album check because PATH_MAP is empty")

    # 5) External service checks -------------------------------------------------
    openai_ok = False
    discord_ok = False

    # --- OpenAI key -------------------------------------------------------------
    if OPENAI_API_KEY and openai_client:
        try:
            # A small "ping" to verify the key/model works.
            # Some models (notably GPT-5) reject Chat Completions params like "stop".
            # Keep this check resilient; it should not block startup.
            is_gpt5 = (str(RESOLVED_MODEL or "").strip().lower().startswith("gpt-5"))
            stop_ok = bool(getattr(sys.modules[__name__], "RESOLVED_STOP_OK", True))
            _kwargs = {
                "model": RESOLVED_MODEL,
                "messages": [{"role": ("developer" if is_gpt5 else "user"), "content": "Reply with exactly: PONG"}],
            }
            if is_gpt5:
                _kwargs["reasoning_effort"] = "minimal"
            if stop_ok and not is_gpt5:
                _kwargs["stop"] = ["\n"]
            if RESOLVED_PARAM_STYLE == "mct":
                _kwargs["max_completion_tokens"] = 32 if is_gpt5 else 8
            else:
                _kwargs["max_tokens"] = 32 if is_gpt5 else 8
            try:
                resp = openai_client.chat.completions.create(**_kwargs)
                _txt = (resp.choices[0].message.content or "").strip() if resp and resp.choices else ""
            except Exception as e:
                # If stop is rejected, retry once without it and persist that property.
                msg = str(e).lower()
                if "stop" in msg and ("unsupported" in msg or "unsupported_parameter" in msg or "400" in msg):
                    _kwargs.pop("stop", None)
                    try:
                        sys.modules[__name__].RESOLVED_STOP_OK = False
                    except Exception:
                        pass
                    resp = openai_client.chat.completions.create(**_kwargs)
                    _txt = (resp.choices[0].message.content or "").strip() if resp and resp.choices else ""
                else:
                    raise
            # Keep the check permissive: any non-empty output indicates the key/model works.
            # For GPT-5, we prefer a visible PONG but don't hard-fail on formatting.
            logging.info("%s OpenAI API key valid ‚Äì model **%s** reachable",
                         colour("‚úì", ANSI_GREEN), RESOLVED_MODEL)
            openai_ok = True
        except Exception as e:
            logging.warning("%s OpenAI API key present but failed: %s",
                            colour("‚ö†", ANSI_YELLOW), e)
    else:
        logging.info("‚Ä¢ No OPENAI_API_KEY provided; AI features disabled.")

    # --- Discord webhook --------------------------------------------------------
    if DISCORD_WEBHOOK:
        try:
            resp = requests.post(DISCORD_WEBHOOK, json={"content": "üîî PMDA startup‚Ä¶"}, timeout=6)
            if resp.status_code == 204:
                logging.info("%s Discord webhook reachable",
                             colour("‚úì", ANSI_GREEN))
                discord_ok = True
            else:
                logging.warning("%s Discord webhook returned HTTP %s",
                                colour("‚ö†", ANSI_YELLOW), resp.status_code)
        except Exception as e:
            logging.warning("%s Discord webhook test failed: %s",
                            colour("‚ö†", ANSI_YELLOW), e)
    else:
        logging.info("‚Ä¢ No DISCORD_WEBHOOK configured.")

    # --- MusicBrainz connectivity check --------------------------------------
    if not USE_MUSICBRAINZ:
        logging.info("‚Ä¢ Skipping MusicBrainz connectivity check (USE_MUSICBRAINZ=False).")
    else:
        try:
            # Test with a real API call using musicbrainzngs (respects rate limiting)
            # Use a well-known release-group ID for testing
            test_mbid = "9162580e-5df4-32de-80cc-f45a8d8a9b1d"  # The Beatles - Abbey Road
            result = musicbrainzngs.get_release_group_by_id(test_mbid, includes=[])
            logging.info("‚úì MusicBrainz reachable and working ‚Äì tested with release-group %s", test_mbid)
            logging.info("‚úì MusicBrainz configured with email: %s", MUSICBRAINZ_EMAIL)
        except musicbrainzngs.WebServiceError as e:
            if "503" in str(e) or "rate" in str(e).lower():
                logging.warning("‚ö†Ô∏è MusicBrainz rate limited ‚Äì ensure rate limiting is configured (1 req/sec)")
            else:
                logging.warning("‚ö†Ô∏è MusicBrainz API error ‚Äì %s", e)
        except Exception as e:
            logging.warning("‚ö†Ô∏è MusicBrainz connectivity failed ‚Äì %s", e)

    # ---------------------------------------------------------------------------
    # ‚îÄ‚îÄ‚îÄ Build a richer Discord embed ------------------------------------
    if discord_ok:
        bindings_txt = "\n".join(
            f"`{src}` ‚Üí `{dst}`"
            for src, dst in PATH_MAP.items()
        )

        albums_txt = "\n".join(
            f"{src}: **{cnt}**"
            for src, cnt in prefix_stats.items()
        )

        fields = [
            {
                "name": "Libraries",
                "value": ", ".join(SECTION_NAMES.get(i, str(i)) for i in SECTION_IDS),
                "inline": False,
            },
            {
                "name": "Volume bindings",
                "value": bindings_txt or "n/a",
                "inline": False,
            },
            {
                "name": "Albums per bind",
                "value": albums_txt or "n/a",
                "inline": False,
            },
            {
                "name": "OpenAI",
                "value": "‚úÖ working" if openai_ok else "‚ùå disabled / error",
                "inline": True,
            },
            {
                "name": "Discord",
                "value": "‚úÖ webhook OK" if discord_ok else "‚ùå not configured",
                "inline": True,
            },
        ]

        notify_discord_embed(
            title="üü¢ PMDA started",
            description="All folder mappings look good ‚Äì ready to scan!",
            fields=fields
        )

    logging.info("‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ diagnostic complete ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ")
    if not had_warning:
        logging.info("%s ALL mapped folders contain albums ‚Äì ALL GOOD!", colour("‚úì", ANSI_GREEN))
    # ‚îÄ‚îÄ‚îÄ Log AI prompt for user review ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    try:
        if logging.getLogger().isEnabledFor(logging.DEBUG):
            prompt_text = AI_PROMPT_FILE.read_text(encoding="utf-8")
            logging.debug("Using ai_prompt.txt:\n%s", prompt_text)
    except Exception as e:
        logging.warning("Could not read ai_prompt.txt: %s", e)
    return True


# SQL fragment used by both startup cross-check and /api/paths/verify (same logic)
_PATH_VERIFY_EXTENSIONS = (
    "mp.file LIKE '%.flac' OR mp.file LIKE '%.wav' OR mp.file LIKE '%.m4a' OR mp.file LIKE '%.mp3'"
    " OR mp.file LIKE '%.ogg' OR mp.file LIKE '%.opus' OR mp.file LIKE '%.aac' OR mp.file LIKE '%.ape' OR mp.file LIKE '%.alac'"
    " OR mp.file LIKE '%.dsf' OR mp.file LIKE '%.aif' OR mp.file LIKE '%.aiff' OR mp.file LIKE '%.wma'"
    " OR mp.file LIKE '%.mp4' OR mp.file LIKE '%.m4b' OR mp.file LIKE '%.m4p' OR mp.file LIKE '%.aifc'"
)


def _run_path_verification(path_map: dict, db_file: str, samples: int):
    """
    Same logic as startup cross-check: for each PATH_MAP entry, sample audio files from Plex DB
    and verify they exist on disk (container paths). Returns list of result dicts for API.
    If 1 or 2 samples are missing (e.g. one file moved or encoding glitch), retry once with
    a new random sample for that root to avoid false failures. Does not modify config or global state.
    """
    if not Path(db_file).exists():
        return None
    results = []
    try:
        con = sqlite3.connect(f"file:{db_file}?mode=ro", uri=True, timeout=10)
        con.text_factory = lambda b: b.decode("utf-8", "surrogateescape")
        cur = con.cursor()
        for plex_root, host_root in path_map.items():
            try:
                def check_once():
                    cur.execute(
                        f"""
                        SELECT mp.file FROM media_parts mp
                        WHERE mp.file LIKE ? AND ({_PATH_VERIFY_EXTENSIONS})
                        ORDER BY RANDOM() LIMIT ?
                        """,
                        (f"{plex_root}/%", samples),
                    )
                    rows = [r[0] for r in cur.fetchall()]
                    if not rows:
                        return 0, 0, None
                    missing = 0
                    for src_path in rows:
                        rel = src_path[len(plex_root):].lstrip("/")
                        dst_path = os.path.join(host_root, rel)
                        if not Path(dst_path).exists():
                            missing += 1
                    return len(rows), missing, rows
                total, missing, _ = check_once()
                if total == 0:
                    # Path may be valid (Plex uses it) but this DB has no rows (e.g. different Plex server).
                    # If the path exists in the container and has audio files, treat as OK.
                    try:
                        p = Path(host_root)
                        if p.exists() and p.is_dir():
                            audio_count = sum(1 for _ in p.rglob("*") if AUDIO_RE.search(_.name))
                            if audio_count > 0:
                                results.append({
                                    "plex_root": plex_root,
                                    "host_root": host_root,
                                    "status": "ok",
                                    "samples_checked": 0,
                                    "message": "Path exists with audio files (no matching rows in this Plex DB)",
                                })
                                continue
                    except Exception:
                        pass
                    results.append({
                        "plex_root": plex_root,
                        "host_root": host_root,
                        "status": "fail",
                        "samples_checked": 0,
                        "message": "No audio files found in DB under this path",
                    })
                    continue
                if missing > 0 and missing <= 2:
                    total2, missing2, _ = check_once()
                    if total2 > 0 and missing2 == 0:
                        total, missing = total2, 0
                if missing == 0:
                    results.append({
                        "plex_root": plex_root,
                        "host_root": host_root,
                        "status": "ok",
                        "samples_checked": total,
                        "message": "OK",
                    })
                else:
                    results.append({
                        "plex_root": plex_root,
                        "host_root": host_root,
                        "status": "fail",
                        "samples_checked": total,
                        "message": f"{missing}/{total} sample(s) missing on disk",
                    })
            except Exception as path_err:
                logging.warning("Path verification failed for %s: %s", plex_root, path_err)
                results.append({
                    "plex_root": plex_root,
                    "host_root": host_root,
                    "status": "fail",
                    "samples_checked": 0,
                    "message": str(path_err) or "Verification error",
                })
        con.close()
    except Exception as e:
        logging.warning("Path verification failed: %s", e)
        return None
    return results


def _discover_bindings_by_content(path_map: dict, db_file: str, music_root: str, samples: int):
    """
    For each plex_root in path_map, sample audio paths from Plex DB and find which subdir of
    music_root actually contains those files (content-based match, ignores folder names).
    Returns (discovered_map, results) where discovered_map is plex_root -> resolved host_root
    and results is a list of { plex_root, host_root, status, samples_checked, message }.
    Only adds to discovered_map when at least one file is found for that plex_root.
    """
    if not path_map:
        return {}, []
    music_path = Path(music_root)
    if not music_path.exists() or not music_path.is_dir():
        return None
    if not Path(db_file).exists():
        return None
    discovered_map = {}
    results = []
    try:
        con = sqlite3.connect(f"file:{db_file}?mode=ro", uri=True, timeout=10)
        con.text_factory = lambda b: b.decode("utf-8", "surrogateescape")
        cur = con.cursor()
        candidates_cache = None

        for plex_root in path_map:
            cur.execute(
                f"""
                SELECT mp.file FROM media_parts mp
                WHERE mp.file LIKE ? AND ({_PATH_VERIFY_EXTENSIONS})
                ORDER BY RANDOM() LIMIT ?
                """,
                (f"{plex_root}/%", max(1, samples)),
            )
            rows = [r[0] for r in cur.fetchall()]
            if not rows:
                results.append({
                    "plex_root": plex_root,
                    "host_root": path_map[plex_root],
                    "status": "fail",
                    "samples_checked": 0,
                    "message": "No audio files found in DB under this path",
                })
                continue
            rels = [r[len(plex_root):].lstrip("/") for r in rows]
            if candidates_cache is None:
                candidates_cache = sorted([d for d in music_path.iterdir() if d.is_dir()], key=lambda p: str(p))
            candidates = candidates_cache

            best_path = None
            best_count = 0
            total = len(rels)
            for cand in candidates:
                count = 0
                for rel in rels:
                    if (cand / rel).exists():
                        count += 1
                if count > best_count:
                    best_count = count
                    best_path = str(cand)
                    if best_count == total:
                        break
            if best_count == 0:
                results.append({
                    "plex_root": plex_root,
                    "host_root": path_map[plex_root],
                    "status": "fail",
                    "samples_checked": total,
                    "message": "No matching folder under music root",
                })
                continue
            discovered_map[plex_root] = best_path
            status = "ok" if best_count == total else "fail"
            msg = "OK" if best_count == total else f"{best_count}/{total} files found"
            results.append({
                "plex_root": plex_root,
                "host_root": best_path,
                "status": status,
                "samples_checked": total,
                "message": msg,
            })
        con.close()
    except Exception as e:
        logging.warning("Discover bindings by content failed: %s", e)
        return None
    return (discovered_map, results)


def _discover_one_binding(plex_root: str, db_file: str, music_root: str, samples: int):
    """
    Resolve a single plex_root: find which subdir of music_root contains the sampled files.
    Returns (host_root or None, result_dict) for API. Returns None if DB or music_root invalid.
    Always uses DB + content-based discovery so the real folder (e.g. correct case like
    /music/Compilations) is found even when a wrong/empty folder exists (e.g. /music/compilations).
    """
    music_path = Path(music_root)
    if not music_path.exists() or not music_path.is_dir():
        return None
    if not Path(db_file).exists():
        return None
    try:
        con = sqlite3.connect(f"file:{db_file}?mode=ro", uri=True, timeout=10)
        con.text_factory = lambda b: b.decode("utf-8", "surrogateescape")
        cur = con.cursor()
        cur.execute(
            f"""
            SELECT mp.file FROM media_parts mp
            WHERE mp.file LIKE ? AND ({_PATH_VERIFY_EXTENSIONS})
            ORDER BY RANDOM() LIMIT ?
            """,
            (f"{plex_root}/%", max(1, samples)),
        )
        rows = [r[0] for r in cur.fetchall()]
        con.close()
    except Exception as e:
        logging.warning("Discover one failed for %s: %s", plex_root, e)
        return None
    if not rows:
        return (None, {
            "plex_root": plex_root,
            "host_root": plex_root,
            "status": "fail",
            "samples_checked": 0,
            "message": "No audio files found in DB under this path",
        })
    rels = [r[len(plex_root):].lstrip("/") for r in rows]
    candidates = sorted([d for d in music_path.iterdir() if d.is_dir()], key=lambda p: str(p))
    best_path = None
    best_count = 0
    total = len(rels)
    for cand in candidates:
        count = sum(1 for rel in rels if (cand / rel).exists())
        if count > best_count:
            best_count = count
            best_path = str(cand)
            if best_count == total:
                break
    if best_count == 0:
        # Fallback: recursive search by filename (same idea as _cross_check_bindings repair)
        logging.debug("Discover one: no match in immediate children of %s ‚Äì trying recursive search", music_root)
        candidate_counts: dict[str, int] = {}
        for _, rel in enumerate(rels):
            fname = os.path.basename(rel)
            try:
                for found in music_path.rglob(fname):
                    # Infer host root: go up as many levels as rel has parts
                    root = found
                    for _ in Path(rel).parts:
                        root = root.parent
                    root_str = str(root)
                    candidate_counts[root_str] = candidate_counts.get(root_str, 0) + 1
            except OSError as e:
                logging.debug("Discover one rglob for %s: %s", fname, e)
        if candidate_counts:
            best_path, best_count = max(candidate_counts.items(), key=lambda kv: kv[1])
            logging.info("Discover one: recursive search found best root %s with %d/%d matches", best_path, best_count, total)
        if best_count == 0:
            return (None, {
                "plex_root": plex_root,
                "host_root": plex_root,
                "status": "fail",
                "samples_checked": total,
                "message": "No matching folder under music root",
            })
    status = "ok" if best_count == total else "fail"
    msg = "OK" if best_count == total else f"{best_count}/{total} files found"
    return (best_path, {
        "plex_root": plex_root,
        "host_root": best_path,
        "status": status,
        "samples_checked": total,
        "message": msg,
    })


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ CROSS‚ÄëCHECK PATH BINDINGS ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def _cross_check_bindings(raise_on_abort: bool = True):
    """
    Verify that every PATH_MAP binding actually resolves to real audio
    files on the host.

    ‚Ä¢ Randomly samples CROSSCHECK_SAMPLES audio files per Plex root.
    ‚Ä¢ If all samples exist under the mapped host root, the binding is valid.
    ‚Ä¢ Otherwise performs a recursive search to locate the files; if they are
      all found under the same parent folder we treat that folder as the
      correct host root and patch PATH_MAP (memory + SQLite).
    ‚Ä¢ If any binding cannot be validated or repaired and raise_on_abort is True,
      raises SystemExit. When raise_on_abort is False (e.g. background thread),
      logs the error and returns without raising.
    """
    log_header("cross‚Äëcheck path bindings")

    con = plex_connect()
    cur = con.cursor()
    updates: dict[str, str] = {}
    abort = False

    for plex_root, host_root in PATH_MAP.items():
        # 1) Pull a random sample of audio files (same SQL as _run_path_verification / api paths verify)
        cur.execute(
            f'''
            SELECT mp.file FROM media_parts mp
            WHERE mp.file LIKE ? AND ({_PATH_VERIFY_EXTENSIONS})
            ORDER BY RANDOM() LIMIT ?
            ''',
            (f"{plex_root}/%", CROSSCHECK_SAMPLES)
        )
        rows = [r[0] for r in cur.fetchall()]

        target = len(rows)
        if target == 0:
            logging.error("No audio samples found under %s ‚Äì cannot validate this binding.", plex_root)
            abort = True
            continue
        if target < CROSSCHECK_SAMPLES:
            logging.warning("PATH CHECK: only %d/%d samples available under %s ‚Äì proceeding with reduced target.", target, CROSSCHECK_SAMPLES, plex_root)

        # 2) Direct existence test
        missing: list[tuple[str, str]] = []
        for src_path in rows:
            rel = src_path[len(plex_root):].lstrip("/")
            dst_path = os.path.join(host_root, rel)
            try:
                exists = Path(dst_path).exists()
            except OSError as e:
                logging.warning("PATH CHECK: I/O error checking %s ‚Äì skipping sample: %s", dst_path, e)
                continue
            if not exists:
                missing.append((src_path, rel))

        if not missing:
            logging.info("‚úì Binding verified: %s ‚Üí %s", plex_root, host_root)
            continue

        # Diagnostic: show a few concrete sample mappings that failed
        try:
            for i, (src_path, rel) in enumerate(missing[:5], 1):
                example_dst = os.path.join(host_root, rel)
                logging.debug(
                    "PATH CHECK example %d/%d: src=%s | rel=%s | dst=%s | exists(dst)=%s",
                    i, len(missing), src_path, rel, example_dst, os.path.exists(example_dst)
                )
        except Exception as diag_e:
            logging.debug("PATH CHECK example logging failed: %s", diag_e)

        logging.warning("Binding failed for %s ‚Üí %s ‚Äì attempting recursive search‚Ä¶", plex_root, host_root)

        # 3) Guess candidate roots by scanning *immediate* children of the search base
        search_base = Path(host_root).parent           # normally '/music'
        if not search_base.exists():
            search_base = Path("/")                    # last‚Äëchance fallback
        candidate_roots = [d for d in search_base.iterdir() if d.is_dir()]
        if not candidate_roots:
            logging.debug("No sub‚Äëdirectories under %s ‚Äì cannot guess roots", search_base)
        candidate_counts: dict[str, int] = {}
        for cand in candidate_roots:
            logging.debug("‚Üí checking %s ‚Ä¶", cand)
            ok = 0
            missing_target = len(missing)
            for idx, (_, rel) in enumerate(missing, 1):
                dst = cand / rel
                if dst.exists():
                    ok += 1
                    logging.debug("   %2d/%d matches so far (%s)", ok, missing_target, rel if ok <= 3 else "‚Ä¶")
                # early‚Äëexit: all samples matched
                if ok == missing_target:
                    break
            if ok:
                logging.info("   %s ‚Üí %d/%d samples matched", cand, ok, missing_target)
                candidate_counts[str(cand)] = ok
            else:
                logging.debug("   %s ‚Üí 0 matches", cand)
        # Remove unintended reassignment of target here (do not force target = CROSSCHECK_SAMPLES)
        # ‚îÄ‚îÄ‚îÄ‚ÄÜfallback: deep scan when nothing found above ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        if not candidate_counts:
            logging.debug("Immediate child scan found nothing ‚Äì performing deep filename search")
            missing_target = len(missing)
            for _, rel in missing:
                fname = os.path.basename(rel)
                for found in search_base.rglob(fname):
                    root = found
                    for _ in Path(rel).parts:
                        root = root.parent
                    root = str(root)
                    cnt = candidate_counts.get(root, 0) + 1
                    candidate_counts[root] = cnt
                    if cnt == max(1, missing_target // 2):
                        logging.info("   halfway there: %d/%d samples align under %s", cnt, missing_target, root)

        best_root, matched = (None, 0)
        if candidate_counts:
            best_root, matched = max(candidate_counts.items(), key=lambda kv: kv[1])
        logging.debug("Candidate roots and match counts: %s", candidate_counts)
        missing_target = len(missing)
        if matched:
            logging.info("Best candidate %s matched %d/%d samples", best_root, matched, missing_target)
        if matched == missing_target:
            updates[plex_root] = best_root
            logging.info("Resolved new host root for %s: %s", plex_root, best_root)
        elif matched > 0:
            logging.info("Partial match: %d/%d samples align under %s", matched, missing_target, best_root or "<none>")
        else:
            logging.error("Could not validate binding for %s ‚Äì matched %d/%d samples", plex_root, matched, missing_target)
            abort = True

    con.close()

    if abort:
        notify_discord("‚ùå PMDA startup aborted: PATH_MAP bindings failed cross‚Äëcheck.")
        if raise_on_abort:
            raise SystemExit("Cross‚Äëcheck PATH_MAP failed")
        logging.error("PATH_MAP cross-check failed (running in background ‚Äì check bindings in Settings).")
        return

    # Apply fixes
    if updates:
        PATH_MAP.update(updates)
        try:
            init_settings_db()
            con = sqlite3.connect(str(SETTINGS_DB_FILE), timeout=5)
            con.execute("INSERT OR REPLACE INTO settings(key, value) VALUES('PATH_MAP', ?)", (json.dumps(dict(PATH_MAP)),))
            con.commit()
            con.close()
        except Exception as e:
            logging.debug("Could not persist PATH_MAP to settings.db after cross-check: %s", e)
        msg = "\n".join(f"`{k}` ‚Üí `{v}`" for k, v in updates.items())
        notify_discord_embed(
            title="üîÑ PATH_MAP corrected",
            description=msg
        )
        logging.info("Updated PATH_MAP in memory and SQLite")

    logging.info("All PATH_MAP bindings verified OK.")


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ OTHER CONSTANTS ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
AUDIO_RE    = re.compile(r"\.(flac|ape|alac|wav|m4a|aac|mp3|ogg|opus|dsf|aif|aiff|wma|mp4|m4b|m4p|aifc)$", re.I)
# Derive format scores from user preference order
FMT_SCORE   = {ext: len(FORMAT_PREFERENCE)-i for i, ext in enumerate(FORMAT_PREFERENCE)}
OVERLAP_MIN = 0.85  # 85% track-title overlap minimum

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ STATE DB SETUP ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def init_state_db():
    con = sqlite3.connect(str(STATE_DB_FILE))
    # Enable WAL mode up‚Äëfront to allow concurrent reads/writes
    con.execute("PRAGMA journal_mode=WAL;")
    con.commit()
    cur = con.cursor()
    # Table for duplicate "best" entries
    cur.execute("""
        CREATE TABLE IF NOT EXISTS duplicates_best (
            artist      TEXT,
            album_id    INTEGER,
            title_raw   TEXT,
            album_norm  TEXT,
            folder      TEXT,
            fmt_text    TEXT,
            br          INTEGER,
            sr          INTEGER,
            bd          INTEGER,
            dur         INTEGER,
            discs       INTEGER,
            rationale   TEXT,
            merge_list  TEXT,
            ai_used     INTEGER DEFAULT 0,
            PRIMARY KEY (artist, album_id)
        )
    """)
    # Extend schema: add meta_json if missing
    cur.execute("PRAGMA table_info(duplicates_best)")
    cols = [r[1] for r in cur.fetchall()]
    if "meta_json" not in cols:
        cur.execute("ALTER TABLE duplicates_best ADD COLUMN meta_json TEXT")
    if "ai_provider" not in cols:
        cur.execute("ALTER TABLE duplicates_best ADD COLUMN ai_provider TEXT")
    if "ai_model" not in cols:
        cur.execute("ALTER TABLE duplicates_best ADD COLUMN ai_model TEXT")
    if "evidence_json" not in cols:
        cur.execute("ALTER TABLE duplicates_best ADD COLUMN evidence_json TEXT")
    if "size_mb" not in cols:
        cur.execute("ALTER TABLE duplicates_best ADD COLUMN size_mb INTEGER")
    if "track_count" not in cols:
        cur.execute("ALTER TABLE duplicates_best ADD COLUMN track_count INTEGER")
    if "match_verified_by_ai" not in cols:
        cur.execute("ALTER TABLE duplicates_best ADD COLUMN match_verified_by_ai INTEGER DEFAULT 0")
    # Add indexes for faster lookups
    try:
        cur.execute("CREATE INDEX IF NOT EXISTS idx_duplicates_best_artist ON duplicates_best(artist)")
        cur.execute("CREATE INDEX IF NOT EXISTS idx_duplicates_best_album_id ON duplicates_best(album_id)")
    except sqlite3.OperationalError:
        pass
    
    # Table for broken albums (missing tracks)
    cur.execute("""
        CREATE TABLE IF NOT EXISTS broken_albums (
            artist TEXT,
            album_id INTEGER,
            expected_track_count INTEGER,
            actual_track_count INTEGER,
            missing_indices TEXT,
            musicbrainz_release_group_id TEXT,
            detected_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            fixed_at TIMESTAMP,
            sent_to_lidarr BOOLEAN DEFAULT 0,
            PRIMARY KEY (artist, album_id)
        )
    """)
    # Table for monitored artists in Lidarr
    cur.execute("""
        CREATE TABLE IF NOT EXISTS monitored_artists (
            artist_id INTEGER PRIMARY KEY,
            artist_name TEXT,
            musicbrainz_artist_id TEXT,
            lidarr_artist_id INTEGER,
            monitored_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            UNIQUE(artist_id)
        )
    """)
    # Table for duplicate "loser" entries
    cur.execute("""
        CREATE TABLE IF NOT EXISTS duplicates_loser (
            artist      TEXT,
            album_id    INTEGER,
            folder      TEXT,
            fmt_text    TEXT,
            br          INTEGER,
            sr          INTEGER,
            bd          INTEGER,
            size_mb     INTEGER,
            FOREIGN KEY (artist, album_id) REFERENCES duplicates_best(artist, album_id)
        )
    """)
    # Add indexes for faster lookups
    try:
        cur.execute("CREATE INDEX IF NOT EXISTS idx_duplicates_loser_artist_album ON duplicates_loser(artist, album_id)")
    except sqlite3.OperationalError:
        pass
    # Migration: loser_album_id = Plex metadata_item id of this edition (loser). Required so /details
    # and load_scan_from_db have the correct album_id per edition (tracks, title_raw, path display).
    cur.execute("PRAGMA table_info(duplicates_loser)")
    loser_cols = {r[1] for r in cur.fetchall()}
    if "loser_album_id" not in loser_cols:
        cur.execute("ALTER TABLE duplicates_loser ADD COLUMN loser_album_id INTEGER")

    # AI cache for dupe selection decisions across scans.
    # This is separate from duplicates_best (scan results) because duplicates_best is cleared on each scan.
    cur.execute(
        """
        CREATE TABLE IF NOT EXISTS dupe_ai_cache (
            artist      TEXT NOT NULL,
            group_key   TEXT NOT NULL,
            best_folder TEXT,
            rationale   TEXT,
            merge_list  TEXT,
            ai_provider TEXT,
            ai_model    TEXT,
            confidence  INTEGER,
            created_at  REAL,
            updated_at  REAL,
            PRIMARY KEY (artist, group_key)
        )
        """
    )
    cur.execute("PRAGMA table_info(dupe_ai_cache)")
    dupe_ai_cols = {r[1] for r in cur.fetchall()}
    for col, col_type in (
        ("best_folder", "TEXT"),
        ("rationale", "TEXT"),
        ("merge_list", "TEXT"),
        ("ai_provider", "TEXT"),
        ("ai_model", "TEXT"),
        ("confidence", "INTEGER"),
        ("created_at", "REAL"),
        ("updated_at", "REAL"),
    ):
        if col not in dupe_ai_cols:
            try:
                cur.execute(f"ALTER TABLE dupe_ai_cache ADD COLUMN {col} {col_type}")
            except sqlite3.OperationalError:
                pass
    try:
        cur.execute("CREATE INDEX IF NOT EXISTS idx_dupe_ai_cache_artist ON dupe_ai_cache(artist)")
        cur.execute("CREATE INDEX IF NOT EXISTS idx_dupe_ai_cache_updated ON dupe_ai_cache(updated_at DESC)")
    except sqlite3.OperationalError:
        pass

    # User feedback loop for duplicates detection (pairs labeled as "dupe" or "not_dupe").
    # Stored by folder paths (stable across Files scans where album_id is run-local).
    cur.execute(
        """
        CREATE TABLE IF NOT EXISTS dupe_feedback_pairs (
            artist     TEXT NOT NULL,
            folder_a   TEXT NOT NULL,
            folder_b   TEXT NOT NULL,
            label      TEXT NOT NULL,
            updated_at REAL,
            note       TEXT,
            PRIMARY KEY (artist, folder_a, folder_b)
        )
        """
    )
    cur.execute("PRAGMA table_info(dupe_feedback_pairs)")
    dupe_fb_cols = {r[1] for r in cur.fetchall()}
    for col, col_type in (
        ("updated_at", "REAL"),
        ("note", "TEXT"),
    ):
        if col not in dupe_fb_cols:
            try:
                cur.execute(f"ALTER TABLE dupe_feedback_pairs ADD COLUMN {col} {col_type}")
            except sqlite3.OperationalError:
                pass
    try:
        cur.execute("CREATE INDEX IF NOT EXISTS idx_dupe_feedback_pairs_artist ON dupe_feedback_pairs(artist)")
        cur.execute("CREATE INDEX IF NOT EXISTS idx_dupe_feedback_pairs_updated ON dupe_feedback_pairs(updated_at DESC)")
    except sqlite3.OperationalError:
        pass
    # Table for stats like space_saved and removed_dupes
    cur.execute("""
        CREATE TABLE IF NOT EXISTS stats (
            key   TEXT PRIMARY KEY,
            value INTEGER
        )
    """)
    # Initialize stats if missing
    for stat_key in ("space_saved", "removed_dupes"):
        cur.execute("INSERT OR IGNORE INTO stats(key, value) VALUES(?, 0)", (stat_key,))
    # Small settings table in state.db for runtime values (e.g. last_completed_scan_id)
    cur.execute("""
        CREATE TABLE IF NOT EXISTS settings (
            key   TEXT PRIMARY KEY,
            value TEXT
        )
    """)
    # Table for scan history
    cur.execute("""
        CREATE TABLE IF NOT EXISTS scan_history (
            scan_id INTEGER PRIMARY KEY AUTOINCREMENT,
            start_time REAL NOT NULL,
            end_time REAL,
            duration_seconds INTEGER,
            albums_scanned INTEGER DEFAULT 0,
            duplicates_found INTEGER DEFAULT 0,
            artists_processed INTEGER DEFAULT 0,
            artists_total INTEGER DEFAULT 0,
            ai_used_count INTEGER DEFAULT 0,
            mb_used_count INTEGER DEFAULT 0,
            ai_enabled INTEGER DEFAULT 0,
            mb_enabled INTEGER DEFAULT 0,
            auto_move_enabled INTEGER DEFAULT 0,
            space_saved_mb INTEGER DEFAULT 0,
            albums_moved INTEGER DEFAULT 0,
            status TEXT DEFAULT 'completed',
            duplicate_groups_count INTEGER DEFAULT 0,
            total_duplicates_count INTEGER DEFAULT 0,
            broken_albums_count INTEGER DEFAULT 0,
            missing_albums_count INTEGER DEFAULT 0,
            albums_without_artist_image INTEGER DEFAULT 0,
            albums_without_album_image INTEGER DEFAULT 0,
            albums_without_complete_tags INTEGER DEFAULT 0,
            albums_without_mb_id INTEGER DEFAULT 0,
            albums_without_artist_mb_id INTEGER DEFAULT 0
        )
    """)
    # Add index for faster scan history queries
    try:
        cur.execute("CREATE INDEX IF NOT EXISTS idx_scan_history_start_time ON scan_history(start_time DESC)")
    except sqlite3.OperationalError:
        pass
    # Add new columns if they don't exist (migration for existing databases)
    cur.execute("PRAGMA table_info(scan_history)")
    cols = [r[1] for r in cur.fetchall()]
    new_cols = [
        ("duplicate_groups_count", "INTEGER DEFAULT 0"),
        ("total_duplicates_count", "INTEGER DEFAULT 0"),
        ("broken_albums_count", "INTEGER DEFAULT 0"),
        ("missing_albums_count", "INTEGER DEFAULT 0"),
        ("albums_without_artist_image", "INTEGER DEFAULT 0"),
        ("albums_without_album_image", "INTEGER DEFAULT 0"),
        ("albums_without_complete_tags", "INTEGER DEFAULT 0"),
        ("albums_without_mb_id", "INTEGER DEFAULT 0"),
        ("albums_without_artist_mb_id", "INTEGER DEFAULT 0"),
        ("summary_json", "TEXT"),
        ("entry_type", "TEXT DEFAULT 'scan'"),
    ]
    for col_name, col_type in new_cols:
        if col_name not in cols:
            cur.execute(f"ALTER TABLE scan_history ADD COLUMN {col_name} {col_type}")
    # Table for scan moves (tracking file movements)
    cur.execute("""
        CREATE TABLE IF NOT EXISTS scan_moves (
            move_id INTEGER PRIMARY KEY AUTOINCREMENT,
            scan_id INTEGER NOT NULL,
            artist TEXT NOT NULL,
            album_id INTEGER NOT NULL,
            original_path TEXT NOT NULL,
            moved_to_path TEXT NOT NULL,
            size_mb INTEGER,
            moved_at REAL NOT NULL,
            restored INTEGER DEFAULT 0,
            FOREIGN KEY (scan_id) REFERENCES scan_history(scan_id)
        )
    """)
    # Migrate scan_moves: add album_title and fmt_text if missing
    cur.execute("PRAGMA table_info(scan_moves)")
    move_cols = [r[1] for r in cur.fetchall()]
    for col_name, col_type in [("album_title", "TEXT"), ("fmt_text", "TEXT"), ("move_reason", "TEXT DEFAULT 'dedupe'")]:
        if col_name not in move_cols:
            cur.execute(f"ALTER TABLE scan_moves ADD COLUMN {col_name} {col_type}")
    # Table for incomplete-albums scan diagnostics (double-check Plex vs disk)
    cur.execute("""
        CREATE TABLE IF NOT EXISTS incomplete_album_diagnostics (
            run_id INTEGER NOT NULL,
            artist TEXT NOT NULL,
            album_id INTEGER NOT NULL,
            title_raw TEXT,
            folder TEXT,
            classification TEXT,
            missing_in_plex TEXT,
            missing_on_disk TEXT,
            track_titles TEXT,
            expected_track_count INTEGER,
            actual_track_count INTEGER,
            detected_at REAL,
            PRIMARY KEY (run_id, artist, album_id),
            FOREIGN KEY (run_id) REFERENCES scan_history(scan_id)
        )
    """)
    try:
        cur.execute("CREATE INDEX IF NOT EXISTS idx_incomplete_diagnostics_run ON incomplete_album_diagnostics(run_id)")
    except sqlite3.OperationalError:
        pass
    # Table for per-edition scan truth (Library, Tag Fixer read from here when available)
    cur.execute("""
        CREATE TABLE IF NOT EXISTS scan_editions (
            scan_id INTEGER NOT NULL,
            artist TEXT NOT NULL,
            album_id INTEGER NOT NULL,
            title_raw TEXT,
            folder TEXT,
            fmt_text TEXT,
            br INTEGER,
            sr INTEGER,
            bd INTEGER,
            meta_json TEXT,
            musicbrainz_id TEXT,
            is_broken INTEGER DEFAULT 0,
            expected_track_count INTEGER,
            actual_track_count INTEGER,
            missing_indices TEXT,
            has_cover INTEGER DEFAULT 0,
            missing_required_tags TEXT,
            PRIMARY KEY (scan_id, artist, album_id),
            FOREIGN KEY (scan_id) REFERENCES scan_history(scan_id)
        )
    """)
    # scan_editions provider identity columns (for Library badges/source-of-truth in Plex mode)
    try:
        cur.execute("PRAGMA table_info(scan_editions)")
        se_cols = [r[1] for r in cur.fetchall()]
        if "discogs_release_id" not in se_cols:
            cur.execute("ALTER TABLE scan_editions ADD COLUMN discogs_release_id TEXT")
        if "lastfm_album_mbid" not in se_cols:
            cur.execute("ALTER TABLE scan_editions ADD COLUMN lastfm_album_mbid TEXT")
        if "bandcamp_album_url" not in se_cols:
            cur.execute("ALTER TABLE scan_editions ADD COLUMN bandcamp_album_url TEXT")
        if "metadata_source" not in se_cols:
            cur.execute("ALTER TABLE scan_editions ADD COLUMN metadata_source TEXT")
    except sqlite3.OperationalError:
        pass
    # Persistent resume state for interrupted scans (artist-level status machine).
    cur.execute("""
        CREATE TABLE IF NOT EXISTS scan_resume_runs (
            run_id TEXT PRIMARY KEY,
            created_at REAL NOT NULL,
            updated_at REAL NOT NULL,
            mode TEXT NOT NULL,
            scan_type TEXT NOT NULL,
            source_signature TEXT NOT NULL,
            status TEXT NOT NULL DEFAULT 'running',
            scan_id INTEGER
        )
    """)
    cur.execute("""
        CREATE TABLE IF NOT EXISTS scan_resume_artists (
            run_id TEXT NOT NULL,
            artist_name TEXT NOT NULL,
            artist_signature TEXT NOT NULL,
            status TEXT NOT NULL DEFAULT 'pending',
            album_count INTEGER NOT NULL DEFAULT 0,
            updated_at REAL NOT NULL,
            error TEXT,
            PRIMARY KEY (run_id, artist_name),
            FOREIGN KEY (run_id) REFERENCES scan_resume_runs(run_id) ON DELETE CASCADE
        )
    """)
    # Fast incremental cache (folder fingerprint + quality flags) used by changed-only scans.
    cur.execute("""
        CREATE TABLE IF NOT EXISTS files_album_scan_cache (
            folder_path TEXT PRIMARY KEY,
            fingerprint TEXT NOT NULL,
            artist_name TEXT,
            album_title TEXT,
            has_cover INTEGER NOT NULL DEFAULT 0,
            has_artist_image INTEGER NOT NULL DEFAULT 0,
            has_complete_tags INTEGER NOT NULL DEFAULT 0,
            has_mbid INTEGER NOT NULL DEFAULT 0,
            has_identity INTEGER NOT NULL DEFAULT 0,
            identity_provider TEXT,
            musicbrainz_id TEXT,
            discogs_release_id TEXT,
            lastfm_album_mbid TEXT,
            bandcamp_album_url TEXT,
            metadata_source TEXT,
            missing_required_tags TEXT NOT NULL DEFAULT '[]',
            last_scan_id INTEGER,
            updated_at REAL NOT NULL
        )
    """)
    # Files watcher queue: pending changed folders/albums to speed up changed-only scans.
    cur.execute("""
        CREATE TABLE IF NOT EXISTS files_pending_changes (
            folder_path TEXT PRIMARY KEY,
            reason TEXT,
            first_seen REAL NOT NULL,
            last_seen REAL NOT NULL,
            event_count INTEGER NOT NULL DEFAULT 1
        )
    """)
    # Progressive files-library publication state (source for live Files index rebuilds during scan).
    cur.execute("""
        CREATE TABLE IF NOT EXISTS files_library_published_albums (
            folder_path TEXT PRIMARY KEY,
            scan_id INTEGER,
            artist_name TEXT NOT NULL,
            artist_norm TEXT NOT NULL,
            album_title TEXT NOT NULL,
            title_norm TEXT NOT NULL,
            year INTEGER,
            date_text TEXT,
            genre TEXT,
            label TEXT,
            tags_json TEXT NOT NULL DEFAULT '[]',
            format TEXT,
            is_lossless INTEGER NOT NULL DEFAULT 0,
            has_cover INTEGER NOT NULL DEFAULT 0,
            cover_path TEXT,
            has_artist_image INTEGER NOT NULL DEFAULT 0,
            artist_image_path TEXT,
            mb_identified INTEGER NOT NULL DEFAULT 0,
            musicbrainz_release_group_id TEXT,
            discogs_release_id TEXT,
            lastfm_album_mbid TEXT,
            bandcamp_album_url TEXT,
            primary_metadata_source TEXT,
            track_count INTEGER NOT NULL DEFAULT 0,
            total_duration_sec INTEGER NOT NULL DEFAULT 0,
            is_broken INTEGER NOT NULL DEFAULT 0,
            expected_track_count INTEGER,
            actual_track_count INTEGER,
            missing_indices_json TEXT NOT NULL DEFAULT '[]',
            missing_required_tags_json TEXT NOT NULL DEFAULT '[]',
            primary_tags_json TEXT NOT NULL DEFAULT '{}',
            tracks_json TEXT NOT NULL DEFAULT '[]',
            fingerprint TEXT,
            updated_at REAL NOT NULL
        )
    """)
    # Backward-compatible schema evolution for files_album_scan_cache.
    cur.execute("PRAGMA table_info(files_album_scan_cache)")
    files_cache_cols = {r[1] for r in cur.fetchall()}
    if "musicbrainz_id" not in files_cache_cols:
        cur.execute("ALTER TABLE files_album_scan_cache ADD COLUMN musicbrainz_id TEXT")
    for col_name, col_type in [
        ("has_identity", "INTEGER NOT NULL DEFAULT 0"),
        ("identity_provider", "TEXT"),
        ("discogs_release_id", "TEXT"),
        ("lastfm_album_mbid", "TEXT"),
        ("bandcamp_album_url", "TEXT"),
        ("metadata_source", "TEXT"),
    ]:
        if col_name not in files_cache_cols:
            cur.execute(f"ALTER TABLE files_album_scan_cache ADD COLUMN {col_name} {col_type}")
    # Backward-compatible schema evolution for files_library_published_albums.
    cur.execute("PRAGMA table_info(files_library_published_albums)")
    files_published_cols = {r[1] for r in cur.fetchall()}
    files_published_new_cols = [
        ("scan_id", "INTEGER"),
        ("artist_name", "TEXT NOT NULL DEFAULT ''"),
        ("artist_norm", "TEXT NOT NULL DEFAULT ''"),
        ("album_title", "TEXT NOT NULL DEFAULT ''"),
        ("title_norm", "TEXT NOT NULL DEFAULT ''"),
        ("year", "INTEGER"),
        ("date_text", "TEXT"),
        ("genre", "TEXT"),
        ("label", "TEXT"),
        ("tags_json", "TEXT NOT NULL DEFAULT '[]'"),
        ("format", "TEXT"),
        ("is_lossless", "INTEGER NOT NULL DEFAULT 0"),
        ("has_cover", "INTEGER NOT NULL DEFAULT 0"),
        ("cover_path", "TEXT"),
        ("has_artist_image", "INTEGER NOT NULL DEFAULT 0"),
        ("artist_image_path", "TEXT"),
        ("mb_identified", "INTEGER NOT NULL DEFAULT 0"),
        ("musicbrainz_release_group_id", "TEXT"),
        ("discogs_release_id", "TEXT"),
        ("lastfm_album_mbid", "TEXT"),
        ("bandcamp_album_url", "TEXT"),
        ("primary_metadata_source", "TEXT"),
        ("track_count", "INTEGER NOT NULL DEFAULT 0"),
        ("total_duration_sec", "INTEGER NOT NULL DEFAULT 0"),
        ("is_broken", "INTEGER NOT NULL DEFAULT 0"),
        ("expected_track_count", "INTEGER"),
        ("actual_track_count", "INTEGER"),
        ("missing_indices_json", "TEXT NOT NULL DEFAULT '[]'"),
        ("missing_required_tags_json", "TEXT NOT NULL DEFAULT '[]'"),
        ("primary_tags_json", "TEXT NOT NULL DEFAULT '{}'"),
        ("tracks_json", "TEXT NOT NULL DEFAULT '[]'"),
        ("fingerprint", "TEXT"),
        ("updated_at", "REAL NOT NULL DEFAULT 0"),
    ]
    for col_name, col_type in files_published_new_cols:
        if col_name not in files_published_cols:
            cur.execute(f"ALTER TABLE files_library_published_albums ADD COLUMN {col_name} {col_type}")
    try:
        cur.execute("CREATE INDEX IF NOT EXISTS idx_scan_resume_runs_source ON scan_resume_runs(source_signature, updated_at DESC)")
        cur.execute("CREATE INDEX IF NOT EXISTS idx_scan_resume_artists_run_status ON scan_resume_artists(run_id, status)")
        cur.execute("CREATE INDEX IF NOT EXISTS idx_files_album_scan_cache_updated ON files_album_scan_cache(updated_at DESC)")
        cur.execute("CREATE INDEX IF NOT EXISTS idx_files_published_artist_norm ON files_library_published_albums(artist_norm)")
        cur.execute("CREATE INDEX IF NOT EXISTS idx_files_published_updated ON files_library_published_albums(updated_at DESC)")
        cur.execute("CREATE INDEX IF NOT EXISTS idx_files_pending_changes_last_seen ON files_pending_changes(last_seen DESC)")
    except sqlite3.OperationalError:
        pass
    con.commit()
    con.close()


def init_settings_db():
    """Initialize the dedicated settings.db used for all persistent configuration."""
    con = sqlite3.connect(str(SETTINGS_DB_FILE))
    con.execute("PRAGMA journal_mode=WAL;")
    con.commit()
    cur = con.cursor()
    cur.execute("""
        CREATE TABLE IF NOT EXISTS settings (
            key   TEXT PRIMARY KEY,
            value TEXT
        )
    """)
    con.commit()
    con.close()


def migrate_settings_from_state_db():
    """One-time migration: copy configuration keys from legacy state.db.settings to settings.db."""
    # Only run if settings.db exists but has an empty settings table, and legacy state.db/settings exists
    try:
        if not STATE_DB_FILE.exists():
            return
        # Check legacy settings in state.db
        con_state = sqlite3.connect(str(STATE_DB_FILE))
        cur_state = con_state.cursor()
        cur_state.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='settings'")
        if not cur_state.fetchone():
            con_state.close()
            return
        cur_state.execute("SELECT key, value FROM settings")
        rows = cur_state.fetchall()
        con_state.close()
        if not rows:
            return
        # Ensure settings.db and its table exist
        init_settings_db()
        con_cfg = sqlite3.connect(str(SETTINGS_DB_FILE))
        cur_cfg = con_cfg.cursor()
        # Copy all keys except last_completed_scan_id (runtime-only)
        for key, value in rows:
            if key == "last_completed_scan_id":
                continue
            cur_cfg.execute(
                "INSERT OR IGNORE INTO settings(key, value) VALUES(?, ?)",
                (key, value),
            )
        con_cfg.commit()
        con_cfg.close()
        logging.info("Migrated legacy settings from state.db to settings.db")
    except Exception as e:
        logging.warning("Failed to migrate settings from state.db to settings.db: %s", e)

def get_stat(key: str) -> int:
    con = sqlite3.connect(str(STATE_DB_FILE))
    cur = con.cursor()
    try:
        cur.execute("SELECT value FROM stats WHERE key = ?", (key,))
    except sqlite3.OperationalError as e:
        # Legacy databases may not have the stats table yet; create it lazily and retry once.
        if "no such table: stats" in str(e):
            cur.execute("""
                CREATE TABLE IF NOT EXISTS stats (
                    key   TEXT PRIMARY KEY,
                    value INTEGER
                )
            """)
            con.commit()
            cur.execute("SELECT value FROM stats WHERE key = ?", (key,))
        else:
            con.close()
            raise
    row = cur.fetchone()
    con.close()
    return row[0] if row else 0

def set_stat(key: str, value: int):
    con = sqlite3.connect(str(STATE_DB_FILE))
    cur = con.cursor()
    try:
        cur.execute("UPDATE stats SET value = ? WHERE key = ?", (value, key))
    except sqlite3.OperationalError as e:
        # If stats table does not exist yet (legacy DB), create it and insert the row.
        if "no such table: stats" in str(e):
            cur.execute("""
                CREATE TABLE IF NOT EXISTS stats (
                    key   TEXT PRIMARY KEY,
                    value INTEGER
                )
            """)
            con.commit()
            cur.execute("INSERT OR REPLACE INTO stats(key, value) VALUES(?, ?)", (key, value))
        else:
            con.close()
            raise
    con.commit()
    con.close()

def increment_stat(key: str, delta: int):
    """Atomically add *delta* to a stat counter. Creates the row if it does not exist (upsert)."""
    con = sqlite3.connect(str(STATE_DB_FILE), timeout=30)
    con.execute("PRAGMA busy_timeout=30000;")
    cur = con.cursor()
    try:
        cur.execute(
            "INSERT INTO stats(key, value) VALUES(?, ?) ON CONFLICT(key) DO UPDATE SET value = value + ?",
            (key, delta, delta),
        )
    except sqlite3.OperationalError as e:
        # If stats table does not exist yet (legacy DB), create it and retry once.
        if "no such table: stats" in str(e):
            cur.execute("""
                CREATE TABLE IF NOT EXISTS stats (
                    key   TEXT PRIMARY KEY,
                    value INTEGER
                )
            """)
            con.commit()
            cur.execute(
                "INSERT INTO stats(key, value) VALUES(?, ?) ON CONFLICT(key) DO UPDATE SET value = value + ?",
                (key, delta, delta),
            )
        else:
            con.close()
            raise
    con.commit()
    con.close()

def get_last_completed_scan_id() -> Optional[int]:
    """Return the scan_id of the last completed scan, or None. Used by Library and Tag Fixer to read from scan_editions."""
    con = sqlite3.connect(str(STATE_DB_FILE))
    cur = con.cursor()
    cur.execute("PRAGMA table_info(settings)")
    cols = [r[1] for r in cur.fetchall()]
    # If PRAGMA table_info returns no rows, the table does not exist (legacy DB); otherwise it does.
    if not cols:
        row = None
    else:
        cur.execute("SELECT value FROM settings WHERE key = 'last_completed_scan_id'")
        row = cur.fetchone()
    con.close()
    if not row or not row[0]:
        return None
    try:
        return int(row[0])
    except (ValueError, TypeError):
        return None


def ensure_dedupe_scan_id() -> None:
    """If state has no scan_id, create a 'dedupe' scan_history row so moves are recorded. Used when user dedupes without a prior scan."""
    with lock:
        if state.get("scan_id") is not None:
            return
        start_time = time.time()
        con = sqlite3.connect(str(STATE_DB_FILE))
        cur = con.cursor()
        cur.execute("PRAGMA table_info(scan_history)")
        cols = [r[1] for r in cur.fetchall()]
        if "entry_type" in cols:
            cur.execute("""
                INSERT INTO scan_history
                (start_time, albums_scanned, artists_total, ai_enabled, mb_enabled, auto_move_enabled, status, entry_type)
                VALUES (?, 0, 0, 0, 0, 0, 'running', 'dedupe')
            """, (start_time,))
        else:
            cur.execute("""
                INSERT INTO scan_history
                (start_time, albums_scanned, artists_total, ai_enabled, mb_enabled, auto_move_enabled, status)
                VALUES (?, 0, 0, 0, 0, 0, 'running')
            """, (start_time,))
        scan_id = cur.lastrowid
        con.commit()
        con.close()
        state["scan_id"] = scan_id


def update_dedupe_scan_summary(scan_id: int, space_saved_mb: int, albums_moved: int) -> None:
    """Update a dedupe-only scan_history row with end time and stats. No-op if row is not entry_type='dedupe'."""
    con = sqlite3.connect(str(STATE_DB_FILE))
    cur = con.cursor()
    cur.execute("PRAGMA table_info(scan_history)")
    cols = [r[1] for r in cur.fetchall()]
    if "entry_type" not in cols:
        con.close()
        return
    cur.execute("SELECT entry_type FROM scan_history WHERE scan_id = ?", (scan_id,))
    row = cur.fetchone()
    if not row or row[0] != "dedupe":
        con.close()
        return
    end_time = time.time()
    cur.execute(
        "SELECT start_time FROM scan_history WHERE scan_id = ?", (scan_id,)
    )
    start_row = cur.fetchone()
    duration_seconds = int(end_time - start_row[0]) if start_row else 0
    cur.execute("""
        UPDATE scan_history
        SET end_time = ?, duration_seconds = ?, space_saved_mb = ?, albums_moved = ?, status = 'completed'
        WHERE scan_id = ?
    """, (end_time, duration_seconds, space_saved_mb, albums_moved, scan_id))
    con.commit()
    con.close()


init_state_db()
init_settings_db()
migrate_settings_from_state_db()

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ CACHE DB SETUP ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def init_cache_db():
    con = sqlite3.connect(str(CACHE_DB_FILE))
    # Enable WAL mode for concurrent reads/writes (same as state.db)
    con.execute("PRAGMA journal_mode=WAL;")
    con.commit()
    cur = con.cursor()
    cur.execute("""
        CREATE TABLE IF NOT EXISTS audio_cache (
            path       TEXT PRIMARY KEY,
            mtime      INTEGER,
            bit_rate   INTEGER,
            sample_rate INTEGER,
            bit_depth  INTEGER
        )
    """)
    # Add AcousticID columns if missing (for storing fingerprint + duration per track)
    try:
        cur.execute("PRAGMA table_info(audio_cache)")
        cols = [r[1] for r in cur.fetchall()]
        if "acoustid_fingerprint" not in cols:
            cur.execute("ALTER TABLE audio_cache ADD COLUMN acoustid_fingerprint TEXT")
        if "acoustid_duration" not in cols:
            cur.execute("ALTER TABLE audio_cache ADD COLUMN acoustid_duration REAL")
    except sqlite3.OperationalError:
        pass
    # Table for caching MusicBrainz release-group info (by MBID)
    cur.execute("""
        CREATE TABLE IF NOT EXISTS musicbrainz_cache (
            mbid       TEXT PRIMARY KEY,
            info_json  TEXT,
            created_at INTEGER
        )
    """)
    # Table for caching "artist+album -> MBID or not_found" so we don't re-search every scan
    cur.execute("""
        CREATE TABLE IF NOT EXISTS musicbrainz_album_lookup (
            artist_norm TEXT NOT NULL,
            album_norm  TEXT NOT NULL,
            mbid        TEXT,
            info_json   TEXT,
            created_at  INTEGER,
            PRIMARY KEY (artist_norm, album_norm)
        )
    """)
    # Provider fallback cache (Discogs/Last.fm/Bandcamp) with per-row TTL.
    cur.execute(
        """
        CREATE TABLE IF NOT EXISTS provider_album_lookup (
            provider    TEXT NOT NULL,
            artist_norm TEXT NOT NULL,
            album_norm  TEXT NOT NULL,
            status      TEXT NOT NULL,
            payload_json TEXT,
            created_at  INTEGER NOT NULL,
            expires_at  INTEGER NOT NULL,
            PRIMARY KEY (provider, artist_norm, album_norm)
        )
        """
    )
    
    # Add indexes for faster lookups
    try:
        cur.execute("CREATE INDEX IF NOT EXISTS idx_audio_cache_path ON audio_cache(path)")
        cur.execute("CREATE INDEX IF NOT EXISTS idx_mb_cache_mbid ON musicbrainz_cache(mbid)")
        cur.execute("CREATE INDEX IF NOT EXISTS idx_mb_album_lookup_key ON musicbrainz_album_lookup(artist_norm, album_norm)")
        cur.execute("CREATE INDEX IF NOT EXISTS idx_provider_album_lookup_key ON provider_album_lookup(provider, artist_norm, album_norm)")
        cur.execute("CREATE INDEX IF NOT EXISTS idx_provider_album_lookup_expires ON provider_album_lookup(expires_at)")
    except sqlite3.OperationalError:
        # Indexes may already exist, ignore
        pass
    
    con.commit()
    con.close()

def get_cached_info(path: str, mtime: int) -> Optional[tuple[int, int, int]]:
    # Open with a 30-second timeout so that concurrent reads/writes don't immediately error
    con = sqlite3.connect(str(CACHE_DB_FILE), timeout=30)
    cur = con.cursor()
    cur.execute("SELECT bit_rate, sample_rate, bit_depth, mtime FROM audio_cache WHERE path = ?", (path,))
    row = cur.fetchone()
    con.close()
    if row:
        br, sr, bd, cached_mtime = row
        if cached_mtime == mtime:
            return (br, sr, bd)
    return None

def set_cached_info(path: str, mtime: int, bit_rate: int, sample_rate: int, bit_depth: int):
    # Open with a 30-second timeout so concurrent writes wait instead of "database is locked"
    con = sqlite3.connect(str(CACHE_DB_FILE), timeout=30)
    cur = con.cursor()
    cur.execute("""
        INSERT INTO audio_cache(path, mtime, bit_rate, sample_rate, bit_depth)
        VALUES (?, ?, ?, ?, ?)
        ON CONFLICT(path) DO UPDATE SET
            mtime      = excluded.mtime,
            bit_rate   = excluded.bit_rate,
            sample_rate = excluded.sample_rate,
            bit_depth  = excluded.bit_depth
    """, (path, mtime, bit_rate, sample_rate, bit_depth))
    con.commit()
    con.close()


def get_cached_acoustid(path: str) -> Optional[tuple[float, str]]:
    """Return (duration, fingerprint) from cache if present. Otherwise None."""
    try:
        con = sqlite3.connect(str(CACHE_DB_FILE), timeout=30)
        cur = con.cursor()
        cur.execute("SELECT acoustid_duration, acoustid_fingerprint FROM audio_cache WHERE path = ?", (path,))
        row = cur.fetchone()
        con.close()
    except Exception:
        return None
    if row and row[0] is not None and row[1]:
        return (float(row[0]), str(row[1]))
    return None


def set_cached_acoustid(path: str, duration: float, fingerprint: str):
    """Store AcousticID fingerprint and duration for a track path. Creates row if missing."""
    con = sqlite3.connect(str(CACHE_DB_FILE), timeout=30)
    cur = con.cursor()
    cur.execute(
        "UPDATE audio_cache SET acoustid_fingerprint = ?, acoustid_duration = ? WHERE path = ?",
        (fingerprint, duration, path),
    )
    if cur.rowcount == 0:
        cur.execute(
            """INSERT INTO audio_cache(path, mtime, bit_rate, sample_rate, bit_depth, acoustid_fingerprint, acoustid_duration)
               VALUES (?, 0, 0, 0, 0, ?, ?)""",
            (path, fingerprint, duration),
        )
    con.commit()
    con.close()


init_cache_db()

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Files Library Index (PostgreSQL + Redis) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
_FILES_PG_SCHEMA_READY = False
_FILES_REDIS_CLIENT = None
_CACHE_TELEMETRY_SNAPSHOT = {"ts": 0.0, "payload": None}
_CACHE_TELEMETRY_LOCK = threading.Lock()

_LOSSLESS_FORMATS = {"FLAC", "ALAC", "APE", "WV", "WAV", "AIFF", "DSF"}
_ARTIST_IMAGE_NAMES = ("artist.jpg", "artist.jpeg", "artist.png", "artist.webp")


def _files_pg_dsn() -> str:
    return (
        f"host={PMDA_PG_HOST} port={PMDA_PG_PORT} dbname={PMDA_PG_DB} "
        f"user={PMDA_PG_USER} password={PMDA_PG_PASSWORD} connect_timeout=5"
    )


def _files_pg_connect(*, autocommit: bool = True):
    if psycopg is None:
        return None
    try:
        return psycopg.connect(_files_pg_dsn(), autocommit=autocommit)
    except Exception as e:
        logging.warning("Files PG connection failed: %s", e)
        return None


def _files_pg_init_schema() -> bool:
    global _FILES_PG_SCHEMA_READY
    if _FILES_PG_SCHEMA_READY:
        return True
    conn = _files_pg_connect(autocommit=True)
    if conn is None:
        return False
    try:
        with conn.cursor() as cur:
            try:
                cur.execute("CREATE EXTENSION IF NOT EXISTS pg_trgm")
            except Exception as e:
                logging.debug("pg_trgm extension unavailable (continuing without it): %s", e)
            cur.execute("""
                CREATE TABLE IF NOT EXISTS files_index_meta (
                    key TEXT PRIMARY KEY,
                    value TEXT,
                    updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
                )
            """)
            cur.execute("""
                CREATE TABLE IF NOT EXISTS files_artists (
                    id BIGSERIAL PRIMARY KEY,
                    name TEXT NOT NULL,
                    name_norm TEXT NOT NULL UNIQUE,
                    album_count INTEGER NOT NULL DEFAULT 0,
                    track_count INTEGER NOT NULL DEFAULT 0,
                    broken_albums_count INTEGER NOT NULL DEFAULT 0,
                    has_image BOOLEAN NOT NULL DEFAULT FALSE,
                    image_path TEXT,
                    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
                    updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
                )
            """)
            cur.execute("""
                CREATE TABLE IF NOT EXISTS files_albums (
                    id BIGSERIAL PRIMARY KEY,
                    artist_id BIGINT NOT NULL REFERENCES files_artists(id) ON DELETE CASCADE,
                    title TEXT NOT NULL,
                    title_norm TEXT NOT NULL,
                    folder_path TEXT NOT NULL UNIQUE,
                    year INTEGER,
                    date_text TEXT,
                    genre TEXT,
                    label TEXT,
                    tags_json TEXT NOT NULL DEFAULT '[]',
                    format TEXT,
                    is_lossless BOOLEAN NOT NULL DEFAULT FALSE,
                    has_cover BOOLEAN NOT NULL DEFAULT FALSE,
                    cover_path TEXT,
                    mb_identified BOOLEAN NOT NULL DEFAULT FALSE,
                    musicbrainz_release_group_id TEXT,
                    discogs_release_id TEXT,
                    lastfm_album_mbid TEXT,
                    bandcamp_album_url TEXT,
                    metadata_source TEXT,
                    track_count INTEGER NOT NULL DEFAULT 0,
                    total_duration_sec INTEGER NOT NULL DEFAULT 0,
                    is_broken BOOLEAN NOT NULL DEFAULT FALSE,
                    expected_track_count INTEGER,
                    actual_track_count INTEGER,
                    missing_indices_json TEXT NOT NULL DEFAULT '[]',
                    missing_required_tags_json TEXT NOT NULL DEFAULT '[]',
                    primary_tags_json TEXT NOT NULL DEFAULT '{}',
                    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
                    updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
                )
            """)
            # Backward-compatible schema evolution for files_albums.
            for col_name, col_sql in [
                ("label", "TEXT"),
                ("discogs_release_id", "TEXT"),
                ("lastfm_album_mbid", "TEXT"),
                ("bandcamp_album_url", "TEXT"),
                ("metadata_source", "TEXT"),
            ]:
                try:
                    cur.execute(f"ALTER TABLE files_albums ADD COLUMN IF NOT EXISTS {col_name} {col_sql}")
                except Exception:
                    pass
            cur.execute("""
                CREATE TABLE IF NOT EXISTS files_tracks (
                    id BIGSERIAL PRIMARY KEY,
                    album_id BIGINT NOT NULL REFERENCES files_albums(id) ON DELETE CASCADE,
                    file_path TEXT NOT NULL UNIQUE,
                    title TEXT NOT NULL,
                    disc_num INTEGER NOT NULL DEFAULT 1,
                    track_num INTEGER NOT NULL DEFAULT 0,
                    duration_sec INTEGER NOT NULL DEFAULT 0,
                    format TEXT,
                    bitrate INTEGER,
                    sample_rate INTEGER,
                    bit_depth INTEGER,
                    file_size_bytes BIGINT NOT NULL DEFAULT 0,
                    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
                    updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
                )
            """)
            cur.execute("""
                CREATE TABLE IF NOT EXISTS files_track_embeddings (
                    track_id BIGINT PRIMARY KEY REFERENCES files_tracks(id) ON DELETE CASCADE,
                    embed_json TEXT NOT NULL,
                    norm REAL NOT NULL DEFAULT 1.0,
                    source TEXT NOT NULL DEFAULT 'pmda_hash_v1',
                    updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
                )
            """)
            cur.execute("""
                CREATE TABLE IF NOT EXISTS files_reco_track_stats (
                    track_id BIGINT PRIMARY KEY REFERENCES files_tracks(id) ON DELETE CASCADE,
                    play_count BIGINT NOT NULL DEFAULT 0,
                    completion_count BIGINT NOT NULL DEFAULT 0,
                    partial_count BIGINT NOT NULL DEFAULT 0,
                    skip_count BIGINT NOT NULL DEFAULT 0,
                    last_event_at TIMESTAMPTZ,
                    updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
                )
            """)
            cur.execute("""
                CREATE TABLE IF NOT EXISTS files_reco_events (
                    id BIGSERIAL PRIMARY KEY,
                    session_id TEXT NOT NULL,
                    track_id BIGINT NOT NULL REFERENCES files_tracks(id) ON DELETE CASCADE,
                    album_id BIGINT REFERENCES files_albums(id) ON DELETE CASCADE,
                    artist_id BIGINT REFERENCES files_artists(id) ON DELETE CASCADE,
                    event_type TEXT NOT NULL,
                    played_seconds INTEGER NOT NULL DEFAULT 0,
                    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
                )
            """)
            # User listening telemetry (single-user install). This powers listening statistics charts.
            cur.execute("""
                CREATE TABLE IF NOT EXISTS files_playback_events (
                    id BIGSERIAL PRIMARY KEY,
                    user_id INTEGER NOT NULL DEFAULT 1,
                    track_id BIGINT NOT NULL REFERENCES files_tracks(id) ON DELETE CASCADE,
                    event_type TEXT NOT NULL,
                    played_seconds INTEGER NOT NULL DEFAULT 0,
                    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
                )
            """)
            cur.execute("""
                CREATE TABLE IF NOT EXISTS files_reco_sessions (
                    session_id TEXT PRIMARY KEY,
                    last_event_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
                    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
                    total_events BIGINT NOT NULL DEFAULT 0
                )
            """)
            cur.execute("""
                CREATE TABLE IF NOT EXISTS files_artist_profiles (
                    name_norm TEXT PRIMARY KEY,
                    artist_name TEXT NOT NULL,
                    bio TEXT,
                    short_bio TEXT,
                    tags_json TEXT NOT NULL DEFAULT '[]',
                    similar_json TEXT NOT NULL DEFAULT '[]',
                    source TEXT,
                    updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
                )
            """)
            cur.execute("""
                CREATE TABLE IF NOT EXISTS files_album_profiles (
                    artist_norm TEXT NOT NULL,
                    title_norm TEXT NOT NULL,
                    album_title TEXT NOT NULL,
                    description TEXT,
                    short_description TEXT,
                    tags_json TEXT NOT NULL DEFAULT '[]',
                    source TEXT,
                    updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
                    PRIMARY KEY (artist_norm, title_norm)
                )
            """)
            # Cached artist images that do not live on disk next to audio files.
            # This allows us to persist external artwork across index rebuilds and
            # to show images for "similar artists" even when they are not in the
            # local library.
            cur.execute("""
                CREATE TABLE IF NOT EXISTS files_external_artist_images (
                    name_norm TEXT PRIMARY KEY,
                    artist_name TEXT NOT NULL,
                    provider TEXT NOT NULL DEFAULT 'lastfm',
                    image_path TEXT,
                    image_url TEXT,
                    updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
                )
            """)
            cur.execute("CREATE INDEX IF NOT EXISTS idx_files_external_artist_images_updated_at ON files_external_artist_images(updated_at DESC)")
            # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Playlists (Files mode) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
            # Lightweight local playlists stored in PostgreSQL so they are fast and
            # available to both UI and assistant logic. Items are ordered via `position`.
            cur.execute("""
                CREATE TABLE IF NOT EXISTS files_playlists (
                    id BIGSERIAL PRIMARY KEY,
                    name TEXT NOT NULL,
                    description TEXT,
                    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
                    updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
                )
            """)
            cur.execute("""
                CREATE TABLE IF NOT EXISTS files_playlist_items (
                    id BIGSERIAL PRIMARY KEY,
                    playlist_id BIGINT NOT NULL REFERENCES files_playlists(id) ON DELETE CASCADE,
                    track_id BIGINT NOT NULL REFERENCES files_tracks(id) ON DELETE CASCADE,
                    position INTEGER NOT NULL DEFAULT 0,
                    added_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
                )
            """)
            cur.execute("CREATE INDEX IF NOT EXISTS idx_files_playlist_items_playlist_pos ON files_playlist_items(playlist_id, position ASC, id ASC)")
            cur.execute("CREATE INDEX IF NOT EXISTS idx_files_playlist_items_track_id ON files_playlist_items(track_id)")

            # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Likes / Favorites (Files mode) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
            # Persist user preferences (artist/album/track). Stored in PostgreSQL so the
            # assistant and recommendation engine can reuse the same preference signals.
            cur.execute("""
                CREATE TABLE IF NOT EXISTS files_entity_likes (
                    entity_type TEXT NOT NULL,
                    entity_id BIGINT NOT NULL,
                    liked BOOLEAN NOT NULL DEFAULT TRUE,
                    source TEXT,
                    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
                    updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
                    PRIMARY KEY (entity_type, entity_id)
                )
            """)
            cur.execute("CREATE INDEX IF NOT EXISTS idx_files_entity_likes_updated_at ON files_entity_likes(updated_at DESC)")
            cur.execute("CREATE INDEX IF NOT EXISTS idx_files_entity_likes_type_liked ON files_entity_likes(entity_type, liked)")

            # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Concert Cache (Files mode) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
            # Cached upcoming concerts for artist pages. Providers may be added later
            # (Songkick, etc.). We keep the raw-ish provider payload as JSON for
            # forward compatibility.
            cur.execute("""
                CREATE TABLE IF NOT EXISTS files_artist_concerts (
                    artist_id BIGINT PRIMARY KEY REFERENCES files_artists(id) ON DELETE CASCADE,
                    provider TEXT NOT NULL DEFAULT 'bandsintown',
                    events_json TEXT NOT NULL DEFAULT '[]',
                    source_url TEXT,
                    updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
                )
            """)
            cur.execute("CREATE INDEX IF NOT EXISTS idx_files_artist_concerts_updated_at ON files_artist_concerts(updated_at DESC)")
            # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Assistant (RAG + Chat Traces) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
            # Stored in PostgreSQL so "what the AI learned" is inspectable, attributable,
            # and can be garbage-collected safely without losing core library data.
            cur.execute("""
                CREATE TABLE IF NOT EXISTS assistant_sessions (
                    session_id TEXT PRIMARY KEY,
                    title TEXT,
                    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
                    updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
                )
            """)
            cur.execute("""
                CREATE TABLE IF NOT EXISTS assistant_messages (
                    id BIGSERIAL PRIMARY KEY,
                    session_id TEXT NOT NULL REFERENCES assistant_sessions(session_id) ON DELETE CASCADE,
                    role TEXT NOT NULL,
                    content TEXT NOT NULL,
                    context_json TEXT NOT NULL DEFAULT '{}',
                    metadata_json TEXT NOT NULL DEFAULT '{}',
                    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
                )
            """)
            cur.execute("""
                CREATE TABLE IF NOT EXISTS assistant_docs (
                    id BIGSERIAL PRIMARY KEY,
                    entity_type TEXT NOT NULL,
                    entity_id BIGINT NOT NULL,
                    doc_type TEXT NOT NULL,
                    source TEXT NOT NULL,
                    provider TEXT,
                    model TEXT,
                    title TEXT,
                    url TEXT,
                    lang TEXT,
                    content TEXT NOT NULL,
                    content_hash TEXT NOT NULL,
                    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
                    updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
                    UNIQUE (entity_type, entity_id, doc_type, source)
                )
            """)
            # Backward-compatible schema evolution for assistant_docs.
            for col_name, col_sql in [
                ("provider", "TEXT"),
                ("model", "TEXT"),
            ]:
                try:
                    cur.execute(f"ALTER TABLE assistant_docs ADD COLUMN IF NOT EXISTS {col_name} {col_sql}")
                except Exception:
                    pass
            cur.execute("""
                CREATE TABLE IF NOT EXISTS assistant_doc_chunks (
                    id BIGSERIAL PRIMARY KEY,
                    doc_id BIGINT NOT NULL REFERENCES assistant_docs(id) ON DELETE CASCADE,
                    chunk_index INTEGER NOT NULL,
                    content TEXT NOT NULL,
                    embed_json TEXT NOT NULL DEFAULT '[]',
                    norm REAL NOT NULL DEFAULT 1.0,
                    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
                    UNIQUE (doc_id, chunk_index)
                )
            """)
            cur.execute("""
                CREATE TABLE IF NOT EXISTS assistant_entity_facts (
                    entity_type TEXT NOT NULL,
                    entity_id BIGINT NOT NULL,
                    facts_json TEXT NOT NULL DEFAULT '{}',
                    evidence_json TEXT NOT NULL DEFAULT '[]',
                    source TEXT NOT NULL DEFAULT 'ai_extracted_v1',
                    provider TEXT,
                    model TEXT,
                    updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
                    PRIMARY KEY (entity_type, entity_id)
                )
            """)
            cur.execute("CREATE INDEX IF NOT EXISTS idx_files_artists_name ON files_artists(name)")
            cur.execute("CREATE INDEX IF NOT EXISTS idx_files_albums_artist_id ON files_albums(artist_id)")
            cur.execute("CREATE INDEX IF NOT EXISTS idx_files_albums_title ON files_albums(title)")
            cur.execute("CREATE INDEX IF NOT EXISTS idx_files_albums_title_norm ON files_albums(title_norm)")
            cur.execute("CREATE INDEX IF NOT EXISTS idx_files_albums_genre ON files_albums(genre)")
            cur.execute("CREATE INDEX IF NOT EXISTS idx_files_albums_label ON files_albums(label)")
            cur.execute("CREATE INDEX IF NOT EXISTS idx_files_tracks_album_id ON files_tracks(album_id)")
            cur.execute("CREATE INDEX IF NOT EXISTS idx_files_tracks_title ON files_tracks(title)")
            cur.execute("CREATE INDEX IF NOT EXISTS idx_files_tracks_order ON files_tracks(album_id, disc_num, track_num, id)")
            cur.execute("CREATE INDEX IF NOT EXISTS idx_files_track_embeddings_updated_at ON files_track_embeddings(updated_at DESC)")
            cur.execute("CREATE INDEX IF NOT EXISTS idx_files_reco_events_session_time ON files_reco_events(session_id, created_at DESC)")
            cur.execute("CREATE INDEX IF NOT EXISTS idx_files_reco_events_track_time ON files_reco_events(track_id, created_at DESC)")
            cur.execute("CREATE INDEX IF NOT EXISTS idx_files_playback_events_time ON files_playback_events(created_at DESC)")
            cur.execute("CREATE INDEX IF NOT EXISTS idx_files_playback_events_track_time ON files_playback_events(track_id, created_at DESC)")
            cur.execute("CREATE INDEX IF NOT EXISTS idx_files_reco_track_stats_score ON files_reco_track_stats(play_count DESC, completion_count DESC)")
            cur.execute("CREATE INDEX IF NOT EXISTS idx_files_reco_sessions_last_event ON files_reco_sessions(last_event_at DESC)")
            cur.execute("CREATE INDEX IF NOT EXISTS idx_files_artist_profiles_updated_at ON files_artist_profiles(updated_at DESC)")
            cur.execute("CREATE INDEX IF NOT EXISTS idx_files_album_profiles_updated_at ON files_album_profiles(updated_at DESC)")
            cur.execute("CREATE INDEX IF NOT EXISTS idx_assistant_messages_session_time ON assistant_messages(session_id, created_at DESC)")
            cur.execute("CREATE INDEX IF NOT EXISTS idx_assistant_messages_created_at ON assistant_messages(created_at DESC)")
            cur.execute("CREATE INDEX IF NOT EXISTS idx_assistant_docs_entity ON assistant_docs(entity_type, entity_id)")
            cur.execute("CREATE INDEX IF NOT EXISTS idx_assistant_doc_chunks_doc_id ON assistant_doc_chunks(doc_id)")
            cur.execute("CREATE INDEX IF NOT EXISTS idx_assistant_entity_facts_updated_at ON assistant_entity_facts(updated_at DESC)")
            try:
                cur.execute("CREATE INDEX IF NOT EXISTS idx_files_artists_name_trgm ON files_artists USING gin (name gin_trgm_ops)")
                cur.execute("CREATE INDEX IF NOT EXISTS idx_files_albums_title_trgm ON files_albums USING gin (title gin_trgm_ops)")
                cur.execute("CREATE INDEX IF NOT EXISTS idx_files_tracks_title_trgm ON files_tracks USING gin (title gin_trgm_ops)")
            except Exception:
                pass
            # Migrate legacy external-artist-image keys to the same loose normalization used by files_artists.name_norm.
            # This keeps joins fast and prevents repeated re-downloads when the library contains punctuation/&/unicode.
            try:
                _files_migrate_external_artist_images_norm_keys(cur)
            except Exception:
                pass
        _FILES_PG_SCHEMA_READY = True
        return True
    except Exception as e:
        logging.exception("Failed to initialize files PG schema: %s", e)
        return False
    finally:
        conn.close()


def _files_migrate_external_artist_images_norm_keys(cur) -> None:
    """
    Some builds cached external artist images using a stricter normalization than the Files library index.
    Files library uses a loose normalization for files_artists.name_norm (lower + collapse spaces, keep punctuation).
    This migrates files_external_artist_images.name_norm to the loose normalization so joins work and
    backfill can reuse cached images instead of re-downloading everything.
    """
    try:
        cur.execute("SELECT COALESCE(value, '') FROM files_index_meta WHERE key = 'external_artist_images_norm' LIMIT 1")
        row = cur.fetchone()
        if row and str(row[0] or "").strip() == "loose_v1":
            return
    except Exception:
        # If meta read fails, still attempt best-effort migration.
        pass

    try:
        cur.execute(
            """
            SELECT name_norm, artist_name, provider, COALESCE(image_path, ''), COALESCE(image_url, '')
            FROM files_external_artist_images
            """
        )
        rows = cur.fetchall()
    except Exception:
        rows = []
    if not rows:
        try:
            cur.execute(
                """
                INSERT INTO files_index_meta(key, value, updated_at)
                VALUES ('external_artist_images_norm', 'loose_v1', NOW())
                ON CONFLICT (key) DO UPDATE SET value = EXCLUDED.value, updated_at = NOW()
                """
            )
        except Exception:
            pass
        return

    def _path_size(p: str) -> int:
        try:
            pp = Path((p or "").strip())
            if pp.is_file():
                return int(pp.stat().st_size or 0)
        except Exception:
            return 0
        return 0

    migrated = 0
    for old_norm, artist_name, provider, image_path, image_url in rows:
        old_key = str(old_norm or "").strip()
        name = str(artist_name or "").strip()
        if not old_key or not name:
            continue
        new_key = _norm_artist_key(name) or old_key
        if not new_key or new_key == old_key:
            continue

        cand_path = str(image_path or "").strip()
        cand_url = str(image_url or "").strip()
        cand_size = _path_size(cand_path)

        # Decide whether to override an existing strict-key row.
        try:
            cur.execute(
                "SELECT COALESCE(image_path, ''), COALESCE(image_url, ''), COALESCE(provider, '') FROM files_external_artist_images WHERE name_norm = %s",
                (new_key,),
            )
            ex = cur.fetchone()
        except Exception:
            ex = None
        override = True
        if ex:
            ex_path = str(ex[0] or "").strip()
            ex_url = str(ex[1] or "").strip()
            ex_provider = str(ex[2] or "").strip()
            ex_size = _path_size(ex_path)
            # Prefer the larger on-disk cached image; also replace tiny placeholders.
            if ex_size >= 8192 and (ex_size >= cand_size or cand_size <= 0):
                override = False
            if ex_size < 8192 and cand_size >= 8192:
                override = True
            # If existing URL is a known placeholder but candidate is not, override.
            try:
                if ex_url and _is_probably_placeholder_artist_image_url(ex_url) and cand_url and not _is_probably_placeholder_artist_image_url(cand_url):
                    override = True
                elif ex_url and _is_probably_placeholder_artist_image_url(ex_url) and not cand_url:
                    override = False
            except Exception:
                pass
            # If existing row is empty but candidate has a usable file, override.
            if (not ex_path and cand_path) or (ex_size <= 0 and cand_size > 0):
                override = True
            # Prefer existing non-empty data when candidate is empty.
            if ex_path and not cand_path and ex_size > 0:
                override = False
            if ex_url and not cand_url and not cand_path:
                override = False
            # Preserve existing provider if we decide not to override.
            if not override:
                provider = ex_provider or provider

        if override:
            try:
                cur.execute(
                    """
                    INSERT INTO files_external_artist_images(name_norm, artist_name, provider, image_path, image_url, updated_at)
                    VALUES (%s, %s, %s, %s, %s, NOW())
                    ON CONFLICT (name_norm) DO UPDATE SET
                        artist_name = EXCLUDED.artist_name,
                        provider = EXCLUDED.provider,
                        image_path = EXCLUDED.image_path,
                        image_url = EXCLUDED.image_url,
                        updated_at = NOW()
                    """,
                    (
                        new_key,
                        name,
                        (provider or "").strip().lower() or "lastfm",
                        cand_path or None,
                        cand_url or None,
                    ),
                )
            except Exception:
                pass

        # Delete the legacy row regardless; the normalized-key row will be reused by joins.
        try:
            cur.execute("DELETE FROM files_external_artist_images WHERE name_norm = %s", (old_key,))
            migrated += 1
        except Exception:
            pass

    try:
        cur.execute(
            """
            INSERT INTO files_index_meta(key, value, updated_at)
            VALUES ('external_artist_images_norm', 'loose_v1', NOW())
            ON CONFLICT (key) DO UPDATE SET value = EXCLUDED.value, updated_at = NOW()
            """
        )
    except Exception:
        pass
    if migrated:
        logging.info("Migrated %d external artist image cache keys to loose normalization", migrated)


def _files_redis_client():
    global _FILES_REDIS_CLIENT
    if redis_lib is None:
        return None
    if _FILES_REDIS_CLIENT is not None:
        return _FILES_REDIS_CLIENT
    try:
        _FILES_REDIS_CLIENT = redis_lib.Redis(
            host=PMDA_REDIS_HOST,
            port=PMDA_REDIS_PORT,
            db=PMDA_REDIS_DB,
            password=(PMDA_REDIS_PASSWORD or None),
            decode_responses=True,
            socket_connect_timeout=0.3,
            socket_timeout=0.3,
        )
        _FILES_REDIS_CLIENT.ping()
    except Exception as e:
        logging.debug("Redis cache unavailable for files library: %s", e)
        _FILES_REDIS_CLIENT = None
    return _FILES_REDIS_CLIENT


def _files_cache_get_json(cache_key: str):
    cli = _files_redis_client()
    if cli is None:
        return None
    try:
        raw = cli.get(FILES_CACHE_PREFIX + cache_key)
        if not raw:
            return None
        return json.loads(raw)
    except Exception:
        return None


def _files_cache_set_json(cache_key: str, payload, ttl: int = 60) -> None:
    cli = _files_redis_client()
    if cli is None:
        return
    try:
        cli.setex(FILES_CACHE_PREFIX + cache_key, ttl, json.dumps(payload))
    except Exception:
        pass


def _files_cache_invalidate_all() -> None:
    cli = _files_redis_client()
    if cli is None:
        return
    try:
        keys = list(cli.scan_iter(f"{FILES_CACHE_PREFIX}*"))
        if keys:
            cli.delete(*keys)
    except Exception:
        pass


def _safe_int(value, default: int = 0) -> int:
    try:
        return int(value)
    except Exception:
        return default


def _read_first_int(paths: list[str]) -> Optional[int]:
    for raw in paths:
        p = Path(raw)
        if not p.exists():
            continue
        try:
            txt = p.read_text(encoding="utf-8", errors="ignore").strip()
        except Exception:
            continue
        if not txt:
            continue
        if txt.lower() == "max":
            return None
        try:
            return int(txt)
        except Exception:
            continue
    return None


def _current_process_rss_bytes() -> int:
    # Linux fast path.
    statm = Path("/proc/self/statm")
    if statm.exists():
        try:
            txt = statm.read_text(encoding="utf-8", errors="ignore").strip()
            parts = txt.split()
            if len(parts) >= 2:
                rss_pages = int(parts[1])
                page_size = os.sysconf("SC_PAGE_SIZE")
                return max(0, rss_pages * int(page_size))
        except Exception:
            pass
    return 0


def _read_container_memory_stats() -> dict:
    # cgroup v2 first.
    current = _read_first_int(
        [
            "/sys/fs/cgroup/memory.current",
            "/sys/fs/cgroup/memory/memory.usage_in_bytes",  # cgroup v1 fallback
        ]
    )
    limit = _read_first_int(
        [
            "/sys/fs/cgroup/memory.max",
            "/sys/fs/cgroup/memory/memory.limit_in_bytes",  # cgroup v1 fallback
        ]
    )
    # Some runtimes expose huge sentinel values when unlimited.
    if limit is not None and limit >= (1 << 60):
        limit = None
    used_pct = None
    if current is not None and limit and limit > 0:
        used_pct = round((float(current) / float(limit)) * 100.0, 2)
    return {
        "current_bytes": int(current or 0),
        "limit_bytes": int(limit or 0) if limit else 0,
        "used_pct": used_pct,
    }


def _path_size_bytes(path: Path) -> int:
    try:
        return int(path.stat().st_size)
    except Exception:
        return 0


def _sqlite_table_exists(cur, table_name: str) -> bool:
    cur.execute(
        "SELECT 1 FROM sqlite_master WHERE type='table' AND name = ? LIMIT 1",
        (table_name,),
    )
    return cur.fetchone() is not None


def _sqlite_scalar(cur, sql: str, args: tuple = ()) -> int:
    cur.execute(sql, args)
    row = cur.fetchone()
    if not row:
        return 0
    return _safe_int(row[0], 0)


def _scan_dir_usage(root: Path, max_files: int = 400000) -> dict:
    if not root.exists():
        return {
            "exists": False,
            "bytes_total": 0,
            "file_count": 0,
            "dir_count": 0,
            "walk_truncated": False,
            "walk_errors": 0,
        }
    total_bytes = 0
    file_count = 0
    dir_count = 0
    walk_errors = 0
    walk_truncated = False
    stack = [str(root)]
    max_files = max(10000, int(max_files or 400000))
    while stack:
        cur = stack.pop()
        try:
            with os.scandir(cur) as it:
                for entry in it:
                    try:
                        if entry.is_file(follow_symlinks=False):
                            file_count += 1
                            try:
                                total_bytes += int(entry.stat(follow_symlinks=False).st_size)
                            except Exception:
                                walk_errors += 1
                            if file_count >= max_files:
                                walk_truncated = True
                                break
                        elif entry.is_dir(follow_symlinks=False):
                            dir_count += 1
                            stack.append(entry.path)
                    except Exception:
                        walk_errors += 1
        except Exception:
            walk_errors += 1
        if walk_truncated:
            break
    return {
        "exists": True,
        "bytes_total": int(total_bytes),
        "file_count": int(file_count),
        "dir_count": int(dir_count),
        "walk_truncated": bool(walk_truncated),
        "walk_errors": int(walk_errors),
    }


def _read_media_cache_usage() -> dict:
    root = _media_cache_root_dir()
    album_root = root / "album"
    artist_root = root / "artist"
    total = _scan_dir_usage(root, max_files=PMDA_CACHE_TELEMETRY_MAX_WALK_FILES)
    album = _scan_dir_usage(album_root, max_files=PMDA_CACHE_TELEMETRY_MAX_WALK_FILES)
    artist = _scan_dir_usage(artist_root, max_files=PMDA_CACHE_TELEMETRY_MAX_WALK_FILES)
    return {
        "root": str(root),
        "total": total,
        "album": album,
        "artist": artist,
    }


def _read_sqlite_cache_metrics() -> dict:
    cache_db_wal = Path(str(CACHE_DB_FILE) + "-wal")
    cache_db_shm = Path(str(CACHE_DB_FILE) + "-shm")
    out = {
        "db_path": str(CACHE_DB_FILE),
        "db_bytes": _path_size_bytes(CACHE_DB_FILE),
        "wal_bytes": _path_size_bytes(cache_db_wal),
        "shm_bytes": _path_size_bytes(cache_db_shm),
        "audio_cache_rows": 0,
        "musicbrainz_cache_rows": 0,
        "musicbrainz_album_lookup_rows": 0,
        "musicbrainz_album_lookup_not_found_rows": 0,
        "provider_album_lookup_rows": 0,
        "provider_album_lookup_not_found_rows": 0,
    }
    if not CACHE_DB_FILE.exists():
        return out
    try:
        con = sqlite3.connect(str(CACHE_DB_FILE), timeout=5)
        cur = con.cursor()
        if _sqlite_table_exists(cur, "audio_cache"):
            out["audio_cache_rows"] = _sqlite_scalar(cur, "SELECT COUNT(*) FROM audio_cache")
        if _sqlite_table_exists(cur, "musicbrainz_cache"):
            out["musicbrainz_cache_rows"] = _sqlite_scalar(cur, "SELECT COUNT(*) FROM musicbrainz_cache")
        if _sqlite_table_exists(cur, "musicbrainz_album_lookup"):
            out["musicbrainz_album_lookup_rows"] = _sqlite_scalar(cur, "SELECT COUNT(*) FROM musicbrainz_album_lookup")
            out["musicbrainz_album_lookup_not_found_rows"] = _sqlite_scalar(
                cur,
                """
                SELECT COUNT(*)
                FROM musicbrainz_album_lookup
                WHERE mbid IS NULL OR TRIM(COALESCE(mbid, '')) = ''
                """,
            )
        if _sqlite_table_exists(cur, "provider_album_lookup"):
            out["provider_album_lookup_rows"] = _sqlite_scalar(cur, "SELECT COUNT(*) FROM provider_album_lookup")
            out["provider_album_lookup_not_found_rows"] = _sqlite_scalar(
                cur,
                "SELECT COUNT(*) FROM provider_album_lookup WHERE status = 'not_found'",
            )
        con.close()
    except Exception:
        logging.debug("Failed reading cache.db telemetry", exc_info=True)
    return out


def _read_state_cache_metrics() -> dict:
    state_db_wal = Path(str(STATE_DB_FILE) + "-wal")
    state_db_shm = Path(str(STATE_DB_FILE) + "-shm")
    out = {
        "db_path": str(STATE_DB_FILE),
        "db_bytes": _path_size_bytes(STATE_DB_FILE),
        "wal_bytes": _path_size_bytes(state_db_wal),
        "shm_bytes": _path_size_bytes(state_db_shm),
        "files_album_scan_cache_rows": 0,
        "files_album_scan_cache_healthy_rows": 0,
        "files_pending_changes_rows": 0,
        "files_library_published_rows": 0,
        "scan_resume_pending_artists_rows": 0,
    }
    if not STATE_DB_FILE.exists():
        return out
    try:
        con = sqlite3.connect(str(STATE_DB_FILE), timeout=5)
        cur = con.cursor()
        if _sqlite_table_exists(cur, "files_album_scan_cache"):
            out["files_album_scan_cache_rows"] = _sqlite_scalar(cur, "SELECT COUNT(*) FROM files_album_scan_cache")
            out["files_album_scan_cache_healthy_rows"] = _sqlite_scalar(
                cur,
                """
                SELECT COUNT(*)
                FROM files_album_scan_cache
                WHERE has_cover = 1
                  AND has_artist_image = 1
                  AND has_complete_tags = 1
                  AND has_identity = 1
                """,
            )
        if _sqlite_table_exists(cur, "files_pending_changes"):
            out["files_pending_changes_rows"] = _sqlite_scalar(cur, "SELECT COUNT(*) FROM files_pending_changes")
        if _sqlite_table_exists(cur, "files_library_published_albums"):
            out["files_library_published_rows"] = _sqlite_scalar(cur, "SELECT COUNT(*) FROM files_library_published_albums")
        if _sqlite_table_exists(cur, "scan_resume_artists"):
            out["scan_resume_pending_artists_rows"] = _sqlite_scalar(
                cur,
                "SELECT COUNT(*) FROM scan_resume_artists WHERE status IN ('pending', 'running', 'failed')",
            )
        con.close()
    except Exception:
        logging.debug("Failed reading state.db telemetry", exc_info=True)
    return out


def _read_redis_cache_metrics() -> dict:
    out = {
        "available": False,
        "host": PMDA_REDIS_HOST,
        "port": PMDA_REDIS_PORT,
        "db": PMDA_REDIS_DB,
        "db_keys": 0,
        "pmda_prefix_keys": 0,
        "pmda_prefix_scan_truncated": False,
        "used_memory_bytes": 0,
        "used_memory_peak_bytes": 0,
        "maxmemory_bytes": 0,
        "evicted_keys": 0,
        "keyspace_hits": 0,
        "keyspace_misses": 0,
        "keyspace_hit_rate_pct": None,
    }
    cli = _files_redis_client()
    if cli is None:
        return out
    out["available"] = True
    try:
        out["db_keys"] = _safe_int(cli.dbsize(), 0)
    except Exception:
        pass
    try:
        info_memory = cli.info("memory") or {}
        out["used_memory_bytes"] = _safe_int(info_memory.get("used_memory"), 0)
        out["used_memory_peak_bytes"] = _safe_int(info_memory.get("used_memory_peak"), 0)
        out["maxmemory_bytes"] = _safe_int(info_memory.get("maxmemory"), 0)
    except Exception:
        pass
    try:
        info_stats = cli.info("stats") or {}
        hits = _safe_int(info_stats.get("keyspace_hits"), 0)
        misses = _safe_int(info_stats.get("keyspace_misses"), 0)
        out["keyspace_hits"] = hits
        out["keyspace_misses"] = misses
        out["evicted_keys"] = _safe_int(info_stats.get("evicted_keys"), 0)
        total = hits + misses
        if total > 0:
            out["keyspace_hit_rate_pct"] = round((hits / total) * 100.0, 2)
    except Exception:
        pass
    try:
        max_scan = max(1000, int(PMDA_CACHE_TELEMETRY_MAX_REDIS_SCAN_KEYS or 200000))
        k = 0
        truncated = False
        for _ in cli.scan_iter(f"{FILES_CACHE_PREFIX}*"):
            k += 1
            if k >= max_scan:
                truncated = True
                break
        out["pmda_prefix_keys"] = int(k)
        out["pmda_prefix_scan_truncated"] = bool(truncated)
    except Exception:
        pass
    return out


def _read_pg_cache_metrics() -> dict:
    out = {
        "available": False,
        "db_size_bytes": 0,
        "db_cache_hit_rate_pct": None,
        "numbackends": 0,
        "table_estimated_rows": {},
        "table_total_bytes": {},
    }
    if not _files_pg_init_schema():
        return out
    conn = _files_pg_connect()
    if conn is None:
        return out
    out["available"] = True
    try:
        with conn.cursor() as cur:
            cur.execute("SELECT pg_database_size(current_database())")
            out["db_size_bytes"] = _safe_int((cur.fetchone() or [0])[0], 0)
            cur.execute(
                """
                SELECT numbackends, blks_hit, blks_read
                FROM pg_stat_database
                WHERE datname = current_database()
                """
            )
            row = cur.fetchone()
            if row:
                numbackends = _safe_int(row[0], 0)
                blks_hit = _safe_int(row[1], 0)
                blks_read = _safe_int(row[2], 0)
                out["numbackends"] = numbackends
                total_blks = blks_hit + blks_read
                if total_blks > 0:
                    out["db_cache_hit_rate_pct"] = round((blks_hit / total_blks) * 100.0, 2)
            table_names = [
                "files_artists",
                "files_albums",
                "files_tracks",
                "files_track_embeddings",
                "files_reco_events",
                "files_reco_sessions",
            ]
            cur.execute(
                """
                SELECT c.relname,
                       GREATEST(COALESCE(c.reltuples, 0), 0)::BIGINT AS est_rows,
                       pg_total_relation_size(c.oid) AS total_bytes
                FROM pg_class c
                JOIN pg_namespace n ON n.oid = c.relnamespace
                WHERE n.nspname = 'public'
                  AND c.relname = ANY(%s)
                """,
                (table_names,),
            )
            row_map = {}
            size_map = {}
            for relname, est_rows, total_bytes in cur.fetchall():
                row_map[str(relname)] = _safe_int(est_rows, 0)
                size_map[str(relname)] = _safe_int(total_bytes, 0)
            out["table_estimated_rows"] = row_map
            out["table_total_bytes"] = size_map
    except Exception:
        logging.debug("Failed reading PostgreSQL telemetry", exc_info=True)
    finally:
        conn.close()
    return out


def _collect_cache_control_metrics(force: bool = False) -> dict:
    now = time.time()
    ttl = max(2.0, float(PMDA_CACHE_TELEMETRY_TTL_SEC or 15.0))
    with _CACHE_TELEMETRY_LOCK:
        cached_payload = _CACHE_TELEMETRY_SNAPSHOT.get("payload")
        cached_ts = float(_CACHE_TELEMETRY_SNAPSHOT.get("ts") or 0.0)
        if not force and cached_payload is not None and (now - cached_ts) < ttl:
            return dict(cached_payload)

    with lock:
        files_watcher_state = dict(state.get("files_watcher") or {})
        scan_audio_hits = _safe_int(state.get("scan_audio_cache_hits"), 0)
        scan_audio_misses = _safe_int(state.get("scan_audio_cache_misses"), 0)
        scan_mb_hits = _safe_int(state.get("scan_mb_cache_hits"), 0)
        scan_mb_misses = _safe_int(state.get("scan_mb_cache_misses"), 0)

    payload = {
        "generated_at": int(now),
        "cache_policies": {
            "scan_disable_cache": bool(SCAN_DISABLE_CACHE),
            "mb_disable_cache": bool(MB_DISABLE_CACHE),
        },
        "runtime": {
            "library_mode": _get_library_mode(),
            "process_rss_bytes": _current_process_rss_bytes(),
            "container_memory": _read_container_memory_stats(),
        },
        "redis": _read_redis_cache_metrics(),
        "postgres": _read_pg_cache_metrics(),
        "sqlite_cache_db": _read_sqlite_cache_metrics(),
        "sqlite_state_db": _read_state_cache_metrics(),
        "media_cache": _read_media_cache_usage(),
        "scan_cache_counters_live": {
            "audio_hits": scan_audio_hits,
            "audio_misses": scan_audio_misses,
            "mb_hits": scan_mb_hits,
            "mb_misses": scan_mb_misses,
        },
        "files_watcher": {
            "running": bool(files_watcher_state.get("running")),
            "roots": list(files_watcher_state.get("roots") or []),
            "dirty_count": _safe_int(files_watcher_state.get("dirty_count"), 0),
            "last_event_at": files_watcher_state.get("last_event_at"),
            "last_event_path": files_watcher_state.get("last_event_path"),
        },
    }

    with _CACHE_TELEMETRY_LOCK:
        _CACHE_TELEMETRY_SNAPSHOT["ts"] = now
        _CACHE_TELEMETRY_SNAPSHOT["payload"] = dict(payload)
    return payload


def _normalize_identity_provider(value: str | None) -> str:
    raw = (value or "").strip().lower()
    if not raw:
        return ""
    if raw in {"mb", "mbid", "musicbrainz", "musicbrainz_rg", "musicbrainz_release_group"}:
        return "musicbrainz"
    if raw in {"discogs", "discog"}:
        return "discogs"
    if raw in {"lastfm", "last.fm", "last_fm"}:
        return "lastfm"
    if raw in {"bandcamp"}:
        return "bandcamp"
    return raw


def _record_files_pending_change(folder_path: str, reason: str) -> None:
    folder_key = (folder_path or "").strip()
    if not folder_key:
        return
    now = time.time()
    try:
        con = sqlite3.connect(str(STATE_DB_FILE), timeout=10)
        cur = con.cursor()
        cur.execute(
            """
            INSERT INTO files_pending_changes(folder_path, reason, first_seen, last_seen, event_count)
            VALUES (?, ?, ?, ?, 1)
            ON CONFLICT(folder_path) DO UPDATE SET
                reason = excluded.reason,
                last_seen = excluded.last_seen,
                event_count = files_pending_changes.event_count + 1
            """,
            (folder_key, (reason or "").strip()[:64], now, now),
        )
        con.commit()
        con.close()
    except Exception:
        logging.debug("Failed to record pending files change for %s", folder_key, exc_info=True)

    with lock:
        fw = dict(state.get("files_watcher") or {})
        fw["dirty_count"] = int(fw.get("dirty_count") or 0) + 1
        fw["last_event_at"] = now
        fw["last_event_path"] = folder_key
        state["files_watcher"] = fw


# Files watcher suppression ----------------------------------------------------
# The Files watcher can observe PMDA's own writes (tag updates, cover/artwork writes,
# and dupe/incomplete moves) and accidentally retrigger changed-only scans in a loop.
# We suppress events for recently-touched album folders to avoid scan storms.
_FILES_WATCHER_SUPPRESS_LOCK = threading.Lock()
_FILES_WATCHER_SUPPRESS_UNTIL: dict[str, float] = {}


def _files_watcher_suppress_folder(folder: Path | str, *, seconds: float = 90.0, reason: str = "pmda_write") -> None:
    try:
        key = _album_folder_cache_key(folder)
    except Exception:
        key = str(folder or "")
    key = (key or "").strip()
    if not key:
        return
    until = time.time() + max(5.0, float(seconds or 90.0))
    with _FILES_WATCHER_SUPPRESS_LOCK:
        _FILES_WATCHER_SUPPRESS_UNTIL[key] = max(_FILES_WATCHER_SUPPRESS_UNTIL.get(key, 0.0), until)


def _files_watcher_is_suppressed(folder_key: str) -> bool:
    key = (folder_key or "").strip()
    if not key:
        return False
    now = time.time()
    with _FILES_WATCHER_SUPPRESS_LOCK:
        # Opportunistic cleanup (keeps map small).
        if _FILES_WATCHER_SUPPRESS_UNTIL:
            expired = [k for k, v in _FILES_WATCHER_SUPPRESS_UNTIL.items() if float(v or 0.0) <= now]
            for k in expired[:5000]:
                _FILES_WATCHER_SUPPRESS_UNTIL.pop(k, None)
        until = float(_FILES_WATCHER_SUPPRESS_UNTIL.get(key, 0.0) or 0.0)
    return bool(until and until > now)


def _files_watcher_should_ignore_folder_key(folder_key: str) -> bool:
    """
    Ignore watcher events for folders that are never intended to be scanned (dupes,
    incomplete quarantine, config).
    """
    p = (folder_key or "").strip()
    if not p:
        return True
    low = p.lower()
    if low.startswith("/config/") or low == "/config":
        return True
    if low.startswith("/dupes/") or low == "/dupes":
        return True
    try:
        target_dir = str(_get_config_from_db("INCOMPLETE_ALBUMS_TARGET_DIR") or "/dupes/incomplete_albums").strip()
        if target_dir:
            target_norm = str(path_for_fs_access(Path(target_dir))).strip().lower()
            if target_norm and low.startswith(target_norm.lower().rstrip("/") + "/"):
                return True
    except Exception:
        pass
    return False


def _list_files_pending_changes(limit: int = 10000) -> list[dict]:
    out: list[dict] = []
    try:
        con = sqlite3.connect(str(STATE_DB_FILE), timeout=10)
        cur = con.cursor()
        cur.execute(
            """
            SELECT folder_path, reason, first_seen, last_seen, event_count
            FROM files_pending_changes
            ORDER BY last_seen DESC
            LIMIT ?
            """,
            (max(1, int(limit or 10000)),),
        )
        for row in cur.fetchall():
            folder_path = (row[0] or "").strip()
            if not folder_path:
                continue
            out.append(
                {
                    "folder_path": folder_path,
                    "reason": (row[1] or "").strip(),
                    "first_seen": float(row[2] or 0),
                    "last_seen": float(row[3] or 0),
                    "event_count": int(row[4] or 0),
                }
            )
        con.close()
    except Exception:
        logging.debug("Failed to list files_pending_changes", exc_info=True)
    return out


def _clear_files_pending_changes(folder_paths: list[str]) -> int:
    cleaned = [str(p or "").strip() for p in (folder_paths or []) if str(p or "").strip()]
    if not cleaned:
        return 0
    removed = 0
    try:
        con = sqlite3.connect(str(STATE_DB_FILE), timeout=10)
        cur = con.cursor()
        cur.executemany("DELETE FROM files_pending_changes WHERE folder_path = ?", [(p,) for p in cleaned])
        removed = int(cur.rowcount or 0)
        con.commit()
        con.close()
    except Exception:
        logging.debug("Failed to clear files_pending_changes", exc_info=True)
    return removed


def _folder_has_audio_files(folder: Path) -> bool:
    try:
        for p in folder.iterdir():
            if p.is_file() and AUDIO_RE.search(p.name):
                return True
    except Exception:
        return False
    return False


def _resolve_album_folders_from_event_path(raw_path: str) -> list[str]:
    """
    Resolve changed filesystem path to one or more album folders.
    - audio file -> parent album folder
    - cover file -> album folder
    - artist image file -> all immediate child folders containing audio
    """
    if not raw_path:
        return []
    try:
        path = path_for_fs_access(Path(raw_path))
    except Exception:
        return []

    candidates: list[Path] = []
    lowered = path.name.lower()
    path_exists = path.exists()
    if path_exists and path.is_file():
        if AUDIO_RE.search(path.name):
            candidates.append(path.parent)
        elif lowered.startswith(("cover", "folder", "front", "album", "artwork")):
            candidates.append(path.parent)
        elif lowered.startswith("artist."):
            parent = path.parent
            if parent.is_dir():
                for child in parent.iterdir():
                    if child.is_dir() and _folder_has_audio_files(child):
                        candidates.append(child)
    elif path_exists and path.is_dir():
        if _folder_has_audio_files(path):
            candidates.append(path)
        else:
            try:
                for child in path.iterdir():
                    if child.is_dir() and _folder_has_audio_files(child):
                        candidates.append(child)
            except Exception:
                pass
    else:
        # Deleted/moved paths are often no longer present on disk when watchdog fires.
        # Infer best-effort album folder from the event path itself.
        if AUDIO_RE.search(path.name):
            candidates.append(path.parent)
        elif lowered.startswith(("cover", "folder", "front", "album", "artwork")):
            candidates.append(path.parent)
        elif lowered.startswith("artist."):
            candidates.append(path.parent)
        else:
            candidates.append(path)

    out: list[str] = []
    seen: set[str] = set()
    for c in candidates:
        try:
            key = _album_folder_cache_key(c)
        except Exception:
            key = str(c)
        if key in seen:
            continue
        seen.add(key)
        out.append(key)
    return out


def _update_files_watcher_state(*, running: bool, roots: list[str] | None = None) -> None:
    with lock:
        fw = dict(state.get("files_watcher") or {})
        fw["running"] = bool(running)
        if roots is not None:
            fw["roots"] = [str(r) for r in roots]
        state["files_watcher"] = fw


def _files_watcher_available() -> bool:
    return bool(Observer is not None and FileSystemEventHandler is not None)


if FileSystemEventHandler is not None:
    class _FilesWatcherHandler(FileSystemEventHandler):
        def __init__(self):
            super().__init__()
            self._last_log_ts = 0.0

        def _handle(self, path: str, reason: str) -> None:
            folders = _resolve_album_folders_from_event_path(path)
            if not folders:
                return
            kept = 0
            for folder_key in folders:
                if _files_watcher_should_ignore_folder_key(folder_key):
                    continue
                if _files_watcher_is_suppressed(folder_key):
                    continue
                _record_files_pending_change(folder_key, reason)
                kept += 1
            now = time.time()
            if (now - self._last_log_ts) >= max(1.0, PMDA_FILES_WATCHER_LOG_COOLDOWN_SEC):
                self._last_log_ts = now
                if kept:
                    logging.info("[FILES watcher] queued %d changed album folder(s) (%s)", kept, reason)

        def on_created(self, event):
            self._handle(getattr(event, "src_path", ""), "created")

        def on_modified(self, event):
            self._handle(getattr(event, "src_path", ""), "modified")

        def on_moved(self, event):
            self._handle(getattr(event, "src_path", ""), "moved")
            self._handle(getattr(event, "dest_path", ""), "moved")

        def on_deleted(self, event):
            self._handle(getattr(event, "src_path", ""), "deleted")


def _stop_files_watcher() -> None:
    global _files_watcher_observer
    with _files_watcher_lock:
        obs = _files_watcher_observer
        _files_watcher_observer = None
    if obs is None:
        _update_files_watcher_state(running=False)
        return
    try:
        obs.stop()
        obs.join(timeout=3)
    except Exception:
        logging.debug("Failed to stop files watcher cleanly", exc_info=True)
    _update_files_watcher_state(running=False)


def _restart_files_watcher_if_needed() -> bool:
    if not PMDA_FILES_WATCHER_ENABLED:
        _stop_files_watcher()
        return False
    if _get_library_mode() != "files":
        _stop_files_watcher()
        return False
    if not FILES_ROOTS:
        _stop_files_watcher()
        return False
    if not _files_watcher_available():
        logging.info("FILES watcher unavailable (watchdog not installed); changed-only uses discovery fallback.")
        _update_files_watcher_state(running=False, roots=list(FILES_ROOTS))
        return False

    _stop_files_watcher()
    obs = Observer()
    handler = _FilesWatcherHandler()
    valid_roots: list[str] = []
    for root in FILES_ROOTS:
        if not root:
            continue
        p = Path(root)
        if not p.exists() or not p.is_dir():
            continue
        try:
            obs.schedule(handler, str(p), recursive=True)
            valid_roots.append(str(p))
        except Exception:
            logging.debug("Failed to watch root %s", p, exc_info=True)
    if not valid_roots:
        _update_files_watcher_state(running=False, roots=list(FILES_ROOTS))
        return False
    try:
        obs.start()
    except Exception:
        logging.warning("Unable to start files watcher; falling back to discovery scan.")
        return False

    global _files_watcher_observer
    with _files_watcher_lock:
        _files_watcher_observer = obs
    _update_files_watcher_state(running=True, roots=valid_roots)
    logging.info("FILES watcher started on %d root(s): %s", len(valid_roots), ", ".join(valid_roots))
    _start_auto_changed_only_scan_scheduler()
    return True


_auto_changed_only_scan_lock = threading.Lock()
_auto_changed_only_scan_thread = None
_auto_changed_only_scan_last_started_ts = 0.0


def _start_auto_changed_only_scan_scheduler() -> None:
    """Start a background scheduler that can trigger changed-only scans from watcher events."""
    global _auto_changed_only_scan_thread
    if not PMDA_AUTO_CHANGED_ONLY_SCAN:
        return
    if not PMDA_FILES_WATCHER_ENABLED:
        return
    if _get_library_mode() != "files":
        return
    if not _files_watcher_available():
        return
    with _auto_changed_only_scan_lock:
        if _auto_changed_only_scan_thread is not None and _auto_changed_only_scan_thread.is_alive():
            return
        t = threading.Thread(target=_auto_changed_only_scan_loop, daemon=True, name="auto-changed-only-scan")
        _auto_changed_only_scan_thread = t
        t.start()


def _auto_changed_only_scan_loop() -> None:
    """Debounce/coalesce filesystem events into an efficient changed-only scan."""
    global _auto_changed_only_scan_last_started_ts
    poll_sec = 10.0
    while True:
        try:
            time.sleep(poll_sec)
            if not PMDA_AUTO_CHANGED_ONLY_SCAN:
                continue
            if _get_library_mode() != "files":
                continue
            # Do not trigger while a scan is running/finalizing.
            with lock:
                scanning_now = bool(state.get("scanning"))
                finalizing_now = bool(state.get("scan_finalizing"))
                fw_state = dict(state.get("files_watcher") or {})
            if scanning_now or finalizing_now:
                continue

            # Require some pending changes.
            try:
                con = sqlite3.connect(str(STATE_DB_FILE), timeout=5)
                cur = con.cursor()
                cur.execute("SELECT COUNT(*) FROM files_pending_changes")
                pending = int((cur.fetchone() or [0])[0] or 0)
                cur.execute("SELECT MAX(last_seen) FROM files_pending_changes")
                last_seen_db = float((cur.fetchone() or [0])[0] or 0.0)
                con.close()
            except Exception:
                pending = 0
                last_seen_db = 0.0
            if pending < max(1, int(PMDA_AUTO_CHANGED_ONLY_SCAN_MIN_PENDING or 1)):
                continue

            now = time.time()
            last_event_at = float(fw_state.get("last_event_at") or 0.0)
            last_event_at = max(last_event_at, last_seen_db or 0.0)
            debounce = max(5.0, float(PMDA_AUTO_CHANGED_ONLY_SCAN_DEBOUNCE_SEC or 60.0))
            if last_event_at and (now - last_event_at) < debounce:
                continue

            cooldown = max(30.0, float(PMDA_AUTO_CHANGED_ONLY_SCAN_COOLDOWN_SEC or 300.0))
            if (now - float(_auto_changed_only_scan_last_started_ts or 0.0)) < cooldown:
                continue

            # Trigger scan.
            with lock:
                if state.get("scanning"):
                    continue
                state["run_improve_after"] = False
                state["scan_type"] = "changed_only"
                state["scan_auto_trigger"] = "files_watcher"
            _auto_changed_only_scan_last_started_ts = now
            logging.info("AUTO scan: starting changed-only scan (pending=%d)", pending)
            start_background_scan()
        except Exception:
            logging.debug("AUTO scan loop error", exc_info=True)


def _tokenize_reco_text(raw: str) -> list[str]:
    txt = (raw or "").strip().lower()
    if not txt:
        return []
    tokens = re.findall(r"[a-z0-9]+", txt)
    out = [t for t in tokens if len(t) >= 2]
    # Add bigrams for a light semantic boost on short names.
    for i in range(max(0, len(out) - 1)):
        out.append(f"{out[i]}_{out[i+1]}")
    return out


def _build_hashed_embedding(text: str, dims: int = RECO_EMBED_DIM) -> tuple[list[float], float]:
    vec = [0.0] * max(8, int(dims or RECO_EMBED_DIM))
    tokens = _tokenize_reco_text(text)
    if not tokens:
        return vec, 0.0
    for tok in tokens:
        digest = hashlib.sha1(tok.encode("utf-8", errors="ignore")).digest()
        idx = int.from_bytes(digest[:2], "big") % len(vec)
        sign = 1.0 if (digest[2] & 1) == 0 else -1.0
        # Slightly higher weight for longer tokens, capped.
        weight = min(2.5, 1.0 + (len(tok) / 12.0))
        vec[idx] += sign * weight
    norm = math.sqrt(sum(v * v for v in vec))
    if norm > 0.0:
        inv = 1.0 / norm
        vec = [v * inv for v in vec]
    return vec, norm


def _load_embedding_json(raw: str) -> list[float]:
    if not raw:
        return []
    try:
        arr = json.loads(raw)
    except Exception:
        return []
    if not isinstance(arr, list):
        return []
    out: list[float] = []
    for v in arr[:RECO_EMBED_DIM]:
        try:
            out.append(float(v))
        except Exception:
            out.append(0.0)
    if len(out) < RECO_EMBED_DIM:
        out.extend([0.0] * (RECO_EMBED_DIM - len(out)))
    return out


def _vec_cosine(a: list[float], b: list[float]) -> float:
    if not a or not b:
        return 0.0
    n = min(len(a), len(b))
    if n <= 0:
        return 0.0
    return float(sum((a[i] * b[i]) for i in range(n)))


def _reco_event_weight(event_type: str, played_seconds: int) -> float:
    et = (event_type or "").strip().lower()
    played = max(0, int(played_seconds or 0))
    if et == "like":
        return 1.8
    if et == "dislike":
        return -2.0
    if et == "play_complete":
        return 1.0
    if et == "play_partial":
        return 0.45
    if et == "play_start":
        return 0.25 if played >= 15 else 0.1
    if et == "skip":
        return -0.9
    if et == "stop":
        return 0.12 if played >= 20 else -0.25
    return 0.0


def _reco_build_track_embeddings(conn) -> int:
    """Rebuild deterministic per-track embeddings used for recommendation ranking."""
    inserted = 0
    with conn.cursor() as write_cur:
        write_cur.execute("DELETE FROM files_track_embeddings")
    with conn.cursor() as read_cur, conn.cursor() as write_cur:
        read_cur.execute(
            """
            SELECT
                tr.id,
                tr.title,
                COALESCE(alb.title, ''),
                COALESCE(alb.genre, ''),
                COALESCE(ar.name, ''),
                COALESCE(alb.year::text, ''),
                COALESCE(tr.format, ''),
                COALESCE(alb.tags_json, '[]')
            FROM files_tracks tr
            JOIN files_albums alb ON alb.id = tr.album_id
            JOIN files_artists ar ON ar.id = alb.artist_id
            ORDER BY tr.id ASC
            """
        )
        while True:
            rows = read_cur.fetchmany(2000)
            if not rows:
                break
            batch: list[tuple] = []
            for row in rows:
                track_id = int(row[0])
                title = row[1] or ""
                album_title = row[2] or ""
                genre = row[3] or ""
                artist = row[4] or ""
                year_text = row[5] or ""
                fmt = row[6] or ""
                tags_json = row[7] or "[]"
                try:
                    tags_list = json.loads(tags_json) if tags_json else []
                except Exception:
                    tags_list = []
                tags_part = " ".join(str(t or "").strip() for t in tags_list[:12] if str(t or "").strip())
                text = " ".join(
                    p for p in [artist, album_title, title, genre, tags_part, year_text, fmt] if p
                )
                vec, norm = _build_hashed_embedding(text, RECO_EMBED_DIM)
                if norm <= 0:
                    continue
                batch.append((track_id, json.dumps(vec, separators=(",", ":")), float(norm), RECO_EMBED_SOURCE))
            if batch:
                write_cur.executemany(
                    """
                    INSERT INTO files_track_embeddings(track_id, embed_json, norm, source, updated_at)
                    VALUES (%s, %s, %s, %s, NOW())
                    ON CONFLICT (track_id) DO UPDATE
                    SET embed_json = EXCLUDED.embed_json,
                        norm = EXCLUDED.norm,
                        source = EXCLUDED.source,
                        updated_at = NOW()
                    """,
                    batch,
                )
                inserted += len(batch)
    return inserted


def _reco_upsert_track_embeddings_for_album_ids(conn, album_ids: list[int]) -> int:
    """
    Rebuild deterministic embeddings only for tracks belonging to the provided album IDs.
    Used by granular Files index upserts to avoid full table rebuild each time.
    """
    cleaned_ids = sorted({int(a) for a in (album_ids or []) if int(a) > 0})
    if not cleaned_ids:
        return 0
    inserted = 0
    with conn.cursor() as write_cur:
        write_cur.execute(
            """
            DELETE FROM files_track_embeddings
            WHERE track_id IN (
                SELECT id FROM files_tracks WHERE album_id = ANY(%s)
            )
            """,
            (cleaned_ids,),
        )
    with conn.cursor() as read_cur, conn.cursor() as write_cur:
        read_cur.execute(
            """
            SELECT
                tr.id,
                tr.title,
                COALESCE(alb.title, ''),
                COALESCE(alb.genre, ''),
                COALESCE(ar.name, ''),
                COALESCE(alb.year::text, ''),
                COALESCE(tr.format, ''),
                COALESCE(alb.tags_json, '[]')
            FROM files_tracks tr
            JOIN files_albums alb ON alb.id = tr.album_id
            JOIN files_artists ar ON ar.id = alb.artist_id
            WHERE tr.album_id = ANY(%s)
            ORDER BY tr.id ASC
            """,
            (cleaned_ids,),
        )
        while True:
            rows = read_cur.fetchmany(2000)
            if not rows:
                break
            batch: list[tuple] = []
            for row in rows:
                track_id = int(row[0])
                title = row[1] or ""
                album_title = row[2] or ""
                genre = row[3] or ""
                artist = row[4] or ""
                year_text = row[5] or ""
                fmt = row[6] or ""
                tags_json = row[7] or "[]"
                try:
                    tags_list = json.loads(tags_json) if tags_json else []
                except Exception:
                    tags_list = []
                tags_part = " ".join(str(t or "").strip() for t in tags_list[:12] if str(t or "").strip())
                text = " ".join(p for p in [artist, album_title, title, genre, tags_part, year_text, fmt] if p)
                vec, norm = _build_hashed_embedding(text, RECO_EMBED_DIM)
                if norm <= 0:
                    continue
                batch.append((track_id, json.dumps(vec, separators=(",", ":")), float(norm), RECO_EMBED_SOURCE))
            if batch:
                write_cur.executemany(
                    """
                    INSERT INTO files_track_embeddings(track_id, embed_json, norm, source, updated_at)
                    VALUES (%s, %s, %s, %s, NOW())
                    ON CONFLICT (track_id) DO UPDATE
                    SET embed_json = EXCLUDED.embed_json,
                        norm = EXCLUDED.norm,
                        source = EXCLUDED.source,
                        updated_at = NOW()
                    """,
                    batch,
                )
                inserted += len(batch)
    return inserted


def _files_index_set_state(**updates) -> None:
    with lock:
        st = dict(state.get("files_index") or {})
        st.update(updates)
        state["files_index"] = st


def _files_index_get_state() -> dict:
    with lock:
        return dict(state.get("files_index") or {})


_MEDIA_CACHE_SIZES = (96, 320, 640)


def _media_cache_root_dir() -> Path:
    root = Path((MEDIA_CACHE_ROOT or "").strip() or str(CONFIG_DIR / "media_cache"))
    return root


def _ensure_media_cache_dirs() -> None:
    root = _media_cache_root_dir()
    try:
        (root / "album").mkdir(parents=True, exist_ok=True)
        (root / "artist").mkdir(parents=True, exist_ok=True)
        (root / "embedded").mkdir(parents=True, exist_ok=True)
    except Exception as e:
        logging.debug("Unable to initialize media cache directories at %s: %s", root, e)


def _image_ext_from_mime(mime: str) -> str:
    m = (mime or "").lower()
    if "png" in m:
        return ".png"
    if "webp" in m:
        return ".webp"
    return ".jpg"


def _media_cache_key_for_path(source_path: Path, kind: str, max_px: int) -> Optional[str]:
    try:
        st = source_path.stat()
    except OSError:
        return None
    payload = f"{kind}|{max_px}|{str(source_path)}|{int(st.st_mtime_ns)}|{int(st.st_size)}|v2"
    return hashlib.sha1(payload.encode("utf-8", errors="ignore")).hexdigest()


def _media_cache_path_for_key(key: str, kind: str, ext: str = ".webp") -> Path:
    root = _media_cache_root_dir()
    return root / kind / key[:2] / key[2:4] / f"{key}{ext}"


def _ensure_cached_image_for_path(source_path: Path, *, kind: str, max_px: int = 320) -> Optional[Path]:
    if not source_path or not source_path.exists() or not source_path.is_file():
        return None
    key = _media_cache_key_for_path(source_path, kind, max_px)
    if not key:
        return None
    target = _media_cache_path_for_key(key, kind, ".webp")
    if target.exists() and target.is_file():
        return target
    try:
        from PIL import Image
    except ImportError:
        return source_path
    try:
        target.parent.mkdir(parents=True, exist_ok=True)
        with Image.open(source_path) as img:
            if img.mode not in ("RGB", "L"):
                img = img.convert("RGB")
            img.thumbnail((max_px, max_px), Image.Resampling.LANCZOS)
            img.save(target, format="WEBP", quality=85, method=6)
        return target
    except Exception as e:
        logging.debug("Media cache generation failed for %s: %s", source_path, e)
        return source_path


def _ensure_cached_image_from_bytes(
    raw: bytes,
    mime: str,
    *,
    kind: str,
    cache_key_hint: str,
    max_px: int = 320,
) -> Optional[Path]:
    if not raw:
        return None
    try:
        digest = hashlib.sha1(
            f"{kind}|{max_px}|{cache_key_hint}|{len(raw)}|v2".encode("utf-8", errors="ignore")
        ).hexdigest()
    except Exception:
        return None
    target = _media_cache_path_for_key(digest, kind, ".webp")
    if target.exists() and target.is_file():
        return target
    try:
        from io import BytesIO
        from PIL import Image
    except ImportError:
        ext = _image_ext_from_mime(mime)
        target = _media_cache_path_for_key(digest, kind, ext)
        try:
            target.parent.mkdir(parents=True, exist_ok=True)
            target.write_bytes(raw)
            return target
        except Exception:
            return None
    try:
        target.parent.mkdir(parents=True, exist_ok=True)
        img = Image.open(BytesIO(raw))
        if img.mode not in ("RGB", "L"):
            img = img.convert("RGB")
        img.thumbnail((max_px, max_px), Image.Resampling.LANCZOS)
        img.save(target, format="WEBP", quality=85, method=6)
        return target
    except Exception as e:
        logging.debug("Embedded media cache generation failed: %s", e)
        return None


def _precache_files_media_assets(artists_map: dict[str, dict], albums_payload: list[dict]) -> tuple[int, int]:
    _ensure_media_cache_dirs()
    cover_paths: list[Path] = []
    artist_paths: list[Path] = []
    for album in albums_payload:
        raw = str(album.get("cover_path") or "").strip()
        if raw:
            p = path_for_fs_access(Path(raw))
            if p.exists() and p.is_file():
                cover_paths.append(p)
    for data in artists_map.values():
        raw = str((data or {}).get("image_path") or "").strip()
        if raw:
            p = path_for_fs_access(Path(raw))
            if p.exists() and p.is_file():
                artist_paths.append(p)
    cover_paths = list(dict.fromkeys(cover_paths))
    artist_paths = list(dict.fromkeys(artist_paths))

    covers_done = 0
    artists_done = 0

    for p in cover_paths:
        for size in _MEDIA_CACHE_SIZES:
            out = _ensure_cached_image_for_path(p, kind="album", max_px=size)
            if out:
                covers_done += 1
    for p in artist_paths:
        for size in _MEDIA_CACHE_SIZES:
            out = _ensure_cached_image_for_path(p, kind="artist", max_px=size)
            if out:
                artists_done += 1
    return covers_done, artists_done


def _parse_int_loose(value, default: int = 0) -> int:
    if value is None:
        return default
    if isinstance(value, (int, float)):
        return int(value)
    txt = str(value).strip()
    if not txt:
        return default
    if "/" in txt:
        txt = txt.split("/", 1)[0]
    m = re.search(r"-?\d+", txt)
    if not m:
        return default
    try:
        return int(m.group(0))
    except Exception:
        return default


def _parse_float_loose(value, default: float = 0.0) -> float:
    if value is None:
        return default
    if isinstance(value, (int, float)):
        return float(value)
    txt = str(value).strip()
    if not txt:
        return default
    try:
        return float(txt)
    except Exception:
        m = re.search(r"-?\d+(?:\.\d+)?", txt)
        if not m:
            return default
        try:
            return float(m.group(0))
        except Exception:
            return default


def _parse_duration_seconds_loose(value, default: float = 0.0) -> float:
    """
    Parse duration expressed either as seconds (float/int string) or clock style
    strings like MM:SS / HH:MM:SS(.mmm). Returns seconds.
    """
    if value is None:
        return default
    if isinstance(value, (int, float)):
        return float(value)

    txt = str(value).strip()
    if not txt:
        return default

    # Fast path for plain numeric values.
    try:
        return float(txt)
    except Exception:
        pass

    if ":" in txt:
        parts = [p.strip() for p in txt.split(":")]
        try:
            nums = [float(p) for p in parts]
        except Exception:
            nums = []
        if nums:
            if len(nums) == 3:
                h, m, s = nums
                return (h * 3600.0) + (m * 60.0) + s
            if len(nums) == 2:
                m, s = nums
                return (m * 60.0) + s
            if len(nums) == 1:
                return nums[0]

    return _parse_float_loose(txt, default)


def _parse_disc_track_loose(tags: dict | None, fallback_disc: int = 1, fallback_track: int = 0) -> tuple[int, int]:
    """
    Parse disc/track numbers from tags while tolerating vinyl-style values (A, B1, C-2).
    Returns (disc, track) with provided numeric fallbacks when parsing is impossible.
    """
    tags = tags or {}
    raw_disc = tags.get("disc") or tags.get("discnumber")
    raw_track = tags.get("track") or tags.get("tracknumber")

    disc = _parse_int_loose(raw_disc, 0)
    track = _parse_int_loose(raw_track, 0)

    track_txt = str(raw_track or "").strip().upper()
    if track_txt:
        # Accept values like A, A1, B-2, C_03, etc.
        m = re.match(r"^([A-Z])(?:\s*[-_.]?\s*(\d+))?$", track_txt)
        if m:
            if disc <= 0:
                disc = (ord(m.group(1)) - ord("A")) + 1
            if track <= 0:
                # Side-only markers (e.g. "A") become track 1 on that side.
                track = _parse_int_loose(m.group(2), 1)

    if disc <= 0:
        disc = fallback_disc
    if track <= 0:
        track = fallback_track

    return disc, track


def _normalize_meta_text(value) -> str:
    """Normalize loose metadata strings from tags (trim + collapse spaces)."""
    if value is None:
        return ""
    txt = str(value).strip()
    if not txt:
        return ""
    return re.sub(r"\s+", " ", txt)

def _first_scalar_meta_value(value):
    """Return the first non-empty scalar value from common tag shapes (list/tuple/scalar)."""
    if value is None:
        return None
    if isinstance(value, (list, tuple)):
        for item in value:
            v = _first_scalar_meta_value(item)
            if v is not None and str(v).strip():
                return v
        return None
    return value


def _pick_weighted_metadata_value(candidates: list[tuple[object, int]]) -> str:
    """
    Pick the most representative value from weighted metadata candidates.
    Higher score wins; ties keep first-seen value.
    """
    scores: dict[str, int] = {}
    first_seen: dict[str, int] = {}
    display: dict[str, str] = {}
    pos = 0
    for raw_value, weight in candidates:
        value = _normalize_meta_text(raw_value)
        if not value:
            continue
        key = value.casefold()
        scores[key] = scores.get(key, 0) + max(1, int(weight))
        if key not in first_seen:
            first_seen[key] = pos
            display[key] = value
        pos += 1
    if not scores:
        return ""
    best_key = min(scores.keys(), key=lambda k: (-scores[k], first_seen[k]))
    return display.get(best_key, "")


def _pick_album_artist_from_tag_dicts(tag_dicts: list[dict], default: str = "Unknown Artist") -> str:
    """Choose album-level artist from tags, preferring albumartist over per-track artist."""
    candidates: list[tuple[object, int]] = []
    for tags in tag_dicts or []:
        if not isinstance(tags, dict):
            continue
        candidates.append((tags.get("albumartist"), 4))
        candidates.append((tags.get("artist"), 1))
    picked = _pick_weighted_metadata_value(candidates)
    return picked or default


def _pick_album_title_from_tag_dicts(tag_dicts: list[dict], fallback: str = "Unknown Album") -> str:
    """Choose album title from tags (consensus), with fallback when missing."""
    candidates: list[tuple[object, int]] = []
    for tags in tag_dicts or []:
        if not isinstance(tags, dict):
            continue
        candidates.append((tags.get("album"), 4))
        candidates.append((tags.get("release"), 1))
    picked = _pick_weighted_metadata_value(candidates)
    return picked or (_normalize_meta_text(fallback) or "Unknown Album")

def _pick_album_label_from_tag_dicts(tag_dicts: list[dict]) -> str:
    """Pick record label/publisher from tags when available."""
    candidates: list[tuple[object, int]] = []
    keys = [
        ("label", 4),
        ("recordlabel", 3),
        ("record_label", 3),
        ("record label", 3),
        ("publisher", 2),
        ("organization", 1),
    ]
    for tags in tag_dicts or []:
        if not isinstance(tags, dict):
            continue
        for k, w in keys:
            candidates.append((_first_scalar_meta_value(tags.get(k)), w))
    return _pick_weighted_metadata_value(candidates)


def _split_genre_values(raw_value: str) -> list[str]:
    if not raw_value:
        return []
    parts = re.split(r"[;,/|]", str(raw_value))
    out = []
    for p in parts:
        v = re.sub(r"\s+", " ", (p or "").strip())
        if v:
            out.append(v)
    return out


def _first_artist_image_path(artist_folder: Path) -> Optional[Path]:
    if not artist_folder or not artist_folder.is_dir():
        return None
    try:
        for name in _ARTIST_IMAGE_NAMES:
            p = artist_folder / name
            if p.is_file():
                return p
    except OSError:
        return None
    return None


def _files_root_dir_strings() -> set[str]:
    """
    Return normalized string variants for FILES_ROOTS so we can reliably detect
    when we are at a library root while walking parent folders.
    """
    out: set[str] = set()
    for r in (FILES_ROOTS or []):
        rs = str(r or "").strip()
        if not rs:
            continue
        try:
            p = path_for_fs_access(Path(rs))
        except Exception:
            p = Path(rs)
        try:
            out.add(str(p.resolve()))
        except Exception:
            pass
        out.add(str(p))
    return out


def _files_is_files_root_dir(p: Path, *, root_dirs: Optional[set[str]] = None) -> bool:
    if not p:
        return False
    roots = root_dirs if isinstance(root_dirs, set) else _files_root_dir_strings()
    try:
        if str(p.resolve()) in roots:
            return True
    except Exception:
        pass
    return str(p) in roots


def _files_guess_artist_folder(
    album_folder: Path,
    artist_name: str,
    *,
    root_dirs: Optional[set[str]] = None,
    max_up: int = 6,
) -> Optional[Path]:
    """
    Guess the "artist folder" for an album folder in Files mode.

    Why: in flat libraries where albums are directly under FILES_ROOTS, blindly using
    `folder.parent` makes the FILES_ROOT look like the artist folder, causing a single
    `/root/artist.jpg` to be incorrectly assigned to every artist.

    Strategy: walk up from `album_folder` and return the first ancestor directory whose
    name matches the artist name (strict normalization). Never return FILES_ROOT itself.
    """
    if not album_folder:
        return None
    artist_disp = str(artist_name or "").strip()
    if not artist_disp:
        return None
    artist_norm = _normalize_identity_text_strict(artist_disp)
    if not artist_norm:
        return None
    artist_tokens = [t for t in artist_norm.split() if t]
    roots = root_dirs if isinstance(root_dirs, set) else _files_root_dir_strings()

    try:
        cur = path_for_fs_access(Path(album_folder))
    except Exception:
        cur = Path(album_folder)

    try:
        if not cur.is_dir():
            return None
    except Exception:
        return None

    # Walk from the album folder up to a few levels, stopping at FILES_ROOTS.
    for _ in range(max(0, int(max_up)) + 1):
        if _files_is_files_root_dir(cur, root_dirs=roots):
            break
        cand_norm = _normalize_identity_text_strict(cur.name)
        if cand_norm:
            if cand_norm == artist_norm:
                return cur
            # Tolerate simple token reordering (e.g. "Beatles, The" vs "The Beatles").
            cand_tokens = [t for t in cand_norm.split() if t]
            if len(cand_tokens) > 1 and len(artist_tokens) > 1 and sorted(cand_tokens) == sorted(artist_tokens):
                return cur
        parent = cur.parent
        if not parent or parent == cur:
            break
        cur = parent
    return None


def _files_index_read_counts() -> tuple[int, int, int]:
    if not _files_pg_init_schema():
        return (0, 0, 0)
    conn = _files_pg_connect()
    if conn is None:
        return (0, 0, 0)
    try:
        with conn.cursor() as cur:
            cur.execute("SELECT COUNT(*) FROM files_artists")
            artists = int(cur.fetchone()[0] or 0)
            cur.execute("SELECT COUNT(*) FROM files_albums")
            albums = int(cur.fetchone()[0] or 0)
            cur.execute("SELECT COUNT(*) FROM files_tracks")
            tracks = int(cur.fetchone()[0] or 0)
            return (artists, albums, tracks)
    except Exception:
        return (0, 0, 0)
    finally:
        conn.close()


def _files_index_read_track_and_embedding_counts() -> tuple[int, int]:
    if not _files_pg_init_schema():
        return (0, 0)
    conn = _files_pg_connect()
    if conn is None:
        return (0, 0)
    try:
        with conn.cursor() as cur:
            cur.execute("SELECT COUNT(*) FROM files_tracks")
            tracks = int((cur.fetchone() or [0])[0] or 0)
            cur.execute("SELECT COUNT(*) FROM files_track_embeddings")
            embeddings = int((cur.fetchone() or [0])[0] or 0)
            return (tracks, embeddings)
    except Exception:
        return (0, 0)
    finally:
        conn.close()


def _files_index_write_meta(cur, key: str, value: str) -> None:
    cur.execute(
        """
        INSERT INTO files_index_meta(key, value, updated_at)
        VALUES (%s, %s, NOW())
        ON CONFLICT (key) DO UPDATE SET value = EXCLUDED.value, updated_at = NOW()
        """,
        (key, value),
    )


def _rebuild_files_library_index_for_artist(
    artist_hint: str,
    *,
    reason: str = "manual_artist_upsert",
    wait_if_running: bool = False,
) -> dict:
    """
    Granular Files index rebuild for one artist from published rows.
    If no published rows exist for that artist, perform targeted cleanup only.
    """
    if _get_library_mode() != "files":
        return {"ok": False, "error": "LIBRARY_MODE is not 'files'"}
    if not FILES_ROOTS:
        return {"ok": False, "error": "FILES_ROOTS is empty"}
    if not _files_pg_init_schema():
        return {"ok": False, "error": "PostgreSQL schema unavailable"}

    artist_name = str(artist_hint or "").strip()
    if not artist_name:
        return {"ok": False, "error": "artist_hint is empty"}

    acquired = files_index_lock.acquire(blocking=wait_if_running)
    if not acquired:
        return {"ok": False, "running": True, "error": "Files index rebuild already running"}
    try:
        started_at = time.time()
        _files_index_set_state(
            running=True,
            started_at=started_at,
            finished_at=None,
            phase="writing",
            current_folder=f"artist:{artist_name}",
            error=None,
        )

        artists_map, albums_payload, payload_count = _load_files_library_published_payload_for_artist(artist_name)
        if payload_count <= 0:
            removed_artists = 0
            conn = _files_pg_connect()
            if conn is not None:
                try:
                    artist_norm = " ".join(artist_name.split()).lower()
                    artist_norm_alt = norm_album(artist_name or "")
                    with conn.transaction():
                        with conn.cursor() as cur:
                            cur.execute(
                                """
                                SELECT id
                                FROM files_artists
                                WHERE name_norm = %s
                                   OR name_norm = %s
                                   OR lower(name) = lower(%s)
                                """,
                                (artist_norm, artist_norm_alt or artist_norm, artist_name),
                            )
                            ids = [int(r[0]) for r in cur.fetchall() if r and r[0]]
                            if ids:
                                cur.execute("DELETE FROM files_artists WHERE id = ANY(%s)", (ids,))
                                removed_artists = len(ids)
                            cur.execute(
                                """
                                DELETE FROM files_artists
                                WHERE id NOT IN (SELECT DISTINCT artist_id FROM files_albums)
                                """
                            )
                            _files_index_write_meta(cur, "last_reason", reason)
                            _files_index_write_meta(cur, "last_build_ts", str(int(time.time())))
                            _files_index_write_meta(cur, "source", "published_rows_artist_cleanup")
                finally:
                    conn.close()
            _files_cache_invalidate_all()
            artists_count, albums_count, tracks_count = _files_index_read_counts()
            _files_index_set_state(
                running=False,
                finished_at=time.time(),
                phase="done",
                current_folder=None,
                artists=artists_count,
                albums=albums_count,
                tracks=tracks_count,
                error=None,
            )
            logging.info(
                "Files library index artist cleanup (%s, artist=%s): no published rows, removed %d artist row(s).",
                reason,
                artist_name,
                removed_artists,
            )
            return {
                "ok": True,
                "artists": artists_count,
                "albums": albums_count,
                "tracks": tracks_count,
                "artist_removed": removed_artists,
                "source": "published_rows_artist_cleanup",
            }

        total_tracks = 0
        embeddings_upserted = 0
        conn = _files_pg_connect()
        if conn is None:
            raise RuntimeError("PostgreSQL connection unavailable during granular rebuild")
        try:
            with conn.transaction():
                with conn.cursor() as cur:
                    artist_rows = [
                        (
                            data["name"],
                            norm,
                            bool(data.get("has_image")),
                            data.get("image_path") or "",
                        )
                        for norm, data in artists_map.items()
                    ]
                    if artist_rows:
                        cur.executemany(
                            """
                            INSERT INTO files_artists (name, name_norm, has_image, image_path, created_at, updated_at)
                            VALUES (%s, %s, %s, %s, NOW(), NOW())
                            ON CONFLICT (name_norm) DO UPDATE SET
                                name = EXCLUDED.name,
                                has_image = EXCLUDED.has_image,
                                image_path = CASE
                                    WHEN EXCLUDED.image_path IS NULL OR EXCLUDED.image_path = '' THEN files_artists.image_path
                                    ELSE EXCLUDED.image_path
                                END,
                                updated_at = NOW()
                            """,
                            artist_rows,
                        )
                    artist_norms = [norm for norm in artists_map.keys()]
                    cur.execute(
                        "SELECT id, name_norm FROM files_artists WHERE name_norm = ANY(%s)",
                        (artist_norms,),
                    )
                    artist_id_by_norm = {str(r[1]): int(r[0]) for r in cur.fetchall()}
                    target_artist_ids = sorted({aid for aid in artist_id_by_norm.values() if aid > 0})
                    if target_artist_ids:
                        cur.execute("DELETE FROM files_albums WHERE artist_id = ANY(%s)", (target_artist_ids,))

                    album_rows = []
                    for album in albums_payload:
                        artist_id = artist_id_by_norm.get(album["artist_norm"])
                        if not artist_id:
                            continue
                        album_rows.append(
                            (
                                artist_id,
                                album["title"],
                                album["title_norm"],
                                album["folder_path"],
                                album["year"],
                                album["date_text"],
                                album["genre"],
                                album.get("label") or "",
                                album["tags_json"],
                                album["format"],
                                bool(album["is_lossless"]),
                                bool(album["has_cover"]),
                                album["cover_path"],
                                bool(album["mb_identified"]),
                                album["musicbrainz_release_group_id"],
                                album.get("discogs_release_id") or "",
                                album.get("lastfm_album_mbid") or "",
                                album.get("bandcamp_album_url") or "",
                                _normalize_identity_provider(str(album.get("metadata_source") or "")),
                                album["track_count"],
                                album["total_duration_sec"],
                                bool(album["is_broken"]),
                                album["expected_track_count"],
                                album["actual_track_count"],
                                album["missing_indices_json"],
                                album["missing_required_tags_json"],
                                album["primary_tags_json"],
                            )
                        )
                    if album_rows:
                        cur.executemany(
                            """
                            INSERT INTO files_albums (
                                artist_id, title, title_norm, folder_path, year, date_text, genre, label, tags_json,
                                format, is_lossless, has_cover, cover_path, mb_identified, musicbrainz_release_group_id,
                                discogs_release_id, lastfm_album_mbid, bandcamp_album_url, metadata_source,
                                track_count, total_duration_sec, is_broken, expected_track_count, actual_track_count,
                                missing_indices_json, missing_required_tags_json, primary_tags_json,
                                created_at, updated_at
                            ) VALUES (
                                %s, %s, %s, %s, %s, %s, %s, %s, %s,
                                %s, %s, %s, %s, %s, %s,
                                %s, %s, %s, %s,
                                %s, %s, %s, %s, %s,
                                %s, %s, %s,
                                NOW(), NOW()
                            )
                            ON CONFLICT (folder_path) DO UPDATE SET
                                artist_id = EXCLUDED.artist_id,
                                title = EXCLUDED.title,
                                title_norm = EXCLUDED.title_norm,
                                year = EXCLUDED.year,
                                date_text = EXCLUDED.date_text,
                                genre = EXCLUDED.genre,
                                label = EXCLUDED.label,
                                tags_json = EXCLUDED.tags_json,
                                format = EXCLUDED.format,
                                is_lossless = EXCLUDED.is_lossless,
                                has_cover = EXCLUDED.has_cover,
                                cover_path = EXCLUDED.cover_path,
                                mb_identified = EXCLUDED.mb_identified,
                                musicbrainz_release_group_id = EXCLUDED.musicbrainz_release_group_id,
                                discogs_release_id = EXCLUDED.discogs_release_id,
                                lastfm_album_mbid = EXCLUDED.lastfm_album_mbid,
                                bandcamp_album_url = EXCLUDED.bandcamp_album_url,
                                metadata_source = EXCLUDED.metadata_source,
                                track_count = EXCLUDED.track_count,
                                total_duration_sec = EXCLUDED.total_duration_sec,
                                is_broken = EXCLUDED.is_broken,
                                expected_track_count = EXCLUDED.expected_track_count,
                                actual_track_count = EXCLUDED.actual_track_count,
                                missing_indices_json = EXCLUDED.missing_indices_json,
                                missing_required_tags_json = EXCLUDED.missing_required_tags_json,
                                primary_tags_json = EXCLUDED.primary_tags_json,
                                updated_at = NOW()
                            """,
                            album_rows,
                        )

                    folder_paths = [str(a.get("folder_path") or "") for a in albums_payload if str(a.get("folder_path") or "")]
                    cur.execute(
                        "SELECT id, folder_path FROM files_albums WHERE folder_path = ANY(%s)",
                        (folder_paths,),
                    )
                    album_id_by_folder = {str(r[1]): int(r[0]) for r in cur.fetchall()}
                    album_ids_written = sorted({aid for aid in album_id_by_folder.values() if aid > 0})

                    for album in albums_payload:
                        album_id = album_id_by_folder.get(str(album.get("folder_path") or ""))
                        if not album_id:
                            continue
                        track_rows = [
                            (
                                album_id,
                                t["file_path"],
                                t["title"],
                                t["disc_num"],
                                t["track_num"],
                                t["duration_sec"],
                                t["format"],
                                t["bitrate"],
                                t["sample_rate"],
                                t["bit_depth"],
                                t["file_size_bytes"],
                            )
                            for t in (album.get("tracks") or [])
                            if str(t.get("file_path") or "").strip()
                        ]
                        if not track_rows:
                            continue
                        cur.executemany(
                            """
                            INSERT INTO files_tracks (
                                album_id, file_path, title, disc_num, track_num, duration_sec, format,
                                bitrate, sample_rate, bit_depth, file_size_bytes, created_at, updated_at
                            ) VALUES (
                                %s, %s, %s, %s, %s, %s, %s,
                                %s, %s, %s, %s, NOW(), NOW()
                            )
                            ON CONFLICT (file_path) DO UPDATE SET
                                album_id = EXCLUDED.album_id,
                                title = EXCLUDED.title,
                                disc_num = EXCLUDED.disc_num,
                                track_num = EXCLUDED.track_num,
                                duration_sec = EXCLUDED.duration_sec,
                                format = EXCLUDED.format,
                                bitrate = EXCLUDED.bitrate,
                                sample_rate = EXCLUDED.sample_rate,
                                bit_depth = EXCLUDED.bit_depth,
                                file_size_bytes = EXCLUDED.file_size_bytes,
                                updated_at = NOW()
                            """,
                            track_rows,
                        )
                        total_tracks += len(track_rows)

                    if target_artist_ids:
                        cur.execute(
                            """
                            UPDATE files_artists a
                            SET album_count = COALESCE(s.album_count, 0),
                                track_count = COALESCE(s.track_count, 0),
                                broken_albums_count = COALESCE(s.broken_albums_count, 0),
                                updated_at = NOW()
                            FROM (
                                SELECT
                                    artist_id,
                                    COUNT(*) AS album_count,
                                    COALESCE(SUM(track_count), 0) AS track_count,
                                    COALESCE(SUM(CASE WHEN is_broken THEN 1 ELSE 0 END), 0) AS broken_albums_count
                                FROM files_albums
                                WHERE artist_id = ANY(%s)
                                GROUP BY artist_id
                            ) s
                            WHERE a.id = s.artist_id
                            """,
                            (target_artist_ids,),
                        )
                        cur.execute(
                            """
                            DELETE FROM files_artists
                            WHERE id = ANY(%s)
                              AND id NOT IN (SELECT DISTINCT artist_id FROM files_albums)
                            """,
                            (target_artist_ids,),
                        )
                    embeddings_upserted = _reco_upsert_track_embeddings_for_album_ids(conn, album_ids_written)
                    _files_index_write_meta(cur, "last_reason", reason)
                    _files_index_write_meta(cur, "last_build_ts", str(int(time.time())))
                    _files_index_write_meta(cur, "source", "published_rows_artist_upsert")
                    _files_index_write_meta(cur, "track_embeddings_source", RECO_EMBED_SOURCE)
        finally:
            conn.close()

        _files_index_set_state(phase="media_cache")
        covers_cached, artists_cached = _precache_files_media_assets(artists_map, albums_payload)
        _files_cache_invalidate_all()
        artists_count, albums_count, tracks_count = _files_index_read_counts()
        _tracks_count, embeddings_count = _files_index_read_track_and_embedding_counts()
        elapsed = round(time.time() - started_at, 2)
        _files_index_set_state(
            running=False,
            finished_at=time.time(),
            phase="done",
            current_folder=None,
            artists=artists_count,
            albums=albums_count,
            tracks=tracks_count,
            error=None,
        )
        logging.info(
            "Files library index upserted (%s, artist=%s): %d album(s), %d track row(s), %d embedding(s) in %.2fs (cached covers=%d, artist images=%d)",
            reason,
            artist_name,
            len(albums_payload),
            total_tracks,
            embeddings_upserted,
            elapsed,
            covers_cached,
            artists_cached,
        )
        return {
            "ok": True,
            "artists": artists_count,
            "albums": albums_count,
            "tracks": tracks_count,
            "track_embeddings": embeddings_count,
            "cached_covers": covers_cached,
            "cached_artist_images": artists_cached,
            "duration_sec": elapsed,
            "source": "published_rows_artist_upsert",
        }
    except Exception as e:
        logging.exception("Files index artist upsert failed: %s", e)
        _files_index_set_state(
            running=False,
            finished_at=time.time(),
            phase="error",
            current_folder=None,
            error=str(e),
        )
        return {"ok": False, "error": str(e)}
    finally:
        files_index_lock.release()


def _rebuild_files_library_index(reason: str = "manual", wait_if_running: bool = False) -> dict:
    if _get_library_mode() != "files":
        return {"ok": False, "error": "LIBRARY_MODE is not 'files'"}
    if not FILES_ROOTS:
        return {"ok": False, "error": "FILES_ROOTS is empty"}
    if not _files_pg_init_schema():
        return {"ok": False, "error": "PostgreSQL schema unavailable"}

    acquired = files_index_lock.acquire(blocking=wait_if_running)
    if not acquired:
        return {"ok": False, "running": True, "error": "Files index rebuild already running"}
    try:
        started_at = time.time()
        _files_index_set_state(
            running=True,
            started_at=started_at,
            finished_at=None,
            phase="discovering",
            current_folder=None,
            folders_processed=0,
            total_folders=0,
            artists=0,
            albums=0,
            tracks=0,
            error=None,
        )

        artists_map: dict[str, dict] = {}
        albums_payload: list[dict] = []
        reco_embeddings_count = 0
        published_artists, published_albums, published_count = _load_files_library_published_payload()
        with lock:
            files_scan_running = bool(state.get("scanning")) and _get_library_mode() == "files"
        use_published_payload = bool(published_count > 0 or files_scan_running)
        payload_source = "published_rows" if use_published_payload else "filesystem_roots"

        if use_published_payload:
            artists_map = published_artists
            albums_payload = published_albums
            _files_index_set_state(
                total_folders=max(published_count, 0),
                folders_processed=max(published_count, 0),
                phase="parsing",
                current_folder="published_scan_rows",
            )
        else:
            audio_files = _iter_audio_files_under_roots(FILES_ROOTS)
            by_folder: dict[Path, list[Path]] = defaultdict(list)
            for p in audio_files:
                by_folder[p.parent].append(p)
            folders = sorted(by_folder.items(), key=lambda x: str(x[0]).lower())
            _files_index_set_state(total_folders=len(folders), phase="parsing")
            root_dirs = _files_root_dir_strings()

            for idx, (folder, files) in enumerate(folders, start=1):
                _files_index_set_state(folders_processed=idx, current_folder=str(folder))
                if not files:
                    continue

                track_entries: list[dict] = []
                tag_dicts: list[dict] = []
                raw_genres: list[str] = []
                fmt_counts: dict[str, int] = defaultdict(int)
                first_tags: dict = {}
                total_duration_sec = 0

                for p in files:
                    tags = extract_tags(p) or {}
                    tag_dicts.append(tags)
                    if not first_tags:
                        first_tags = tags
                    title = (
                        (tags.get("title") or tags.get("name") or p.stem or "").strip()
                        or p.stem
                        or "Unknown Track"
                    )
                    disc_num = _parse_int_loose(tags.get("disc") or tags.get("discnumber"), 1) or 1
                    track_num = _parse_int_loose(tags.get("track") or tags.get("tracknumber"), 0)
                    duration_sec = int(max(0.0, _parse_float_loose(tags.get("duration"), 0.0)))
                    bitrate = _parse_int_loose(tags.get("bitrate") or tags.get("bit_rate"), 0)
                    sample_rate = _parse_int_loose(tags.get("sample_rate") or tags.get("samplerate"), 0)
                    bit_depth = _parse_int_loose(tags.get("bit_depth") or tags.get("bits_per_sample"), 0)
                    fmt = (p.suffix.lower().lstrip(".") or "UNKNOWN").upper()
                    fmt_counts[fmt] += 1
                    total_duration_sec += duration_sec
                    raw_genres.extend(_split_genre_values(tags.get("genre") or ""))
                    try:
                        file_size = int(p.stat().st_size)
                    except OSError:
                        file_size = 0
                    track_entries.append(
                        {
                            "file_path": str(p),
                            "title": title,
                            "disc_num": disc_num,
                            "track_num": track_num,
                            "duration_sec": duration_sec,
                            "format": fmt,
                            "bitrate": bitrate,
                            "sample_rate": sample_rate,
                            "bit_depth": bit_depth,
                            "file_size_bytes": file_size,
                        }
                    )

                track_entries.sort(key=lambda t: (t["disc_num"], t["track_num"], t["file_path"]))
                if not track_entries:
                    continue

                artist_name = _pick_album_artist_from_tag_dicts(tag_dicts, default="Unknown Artist")
                album_title = _pick_album_title_from_tag_dicts(
                    tag_dicts,
                    fallback=folder.name.replace("_", " ").strip() or "Unknown Album",
                )
                label = _pick_album_label_from_tag_dicts(tag_dicts)
                artist_norm = " ".join((artist_name or "").split()).lower() or "unknown artist"
                title_norm = norm_album_for_dedup(album_title, normalize_parenthetical=True)
                date_text = (first_tags.get("date") or first_tags.get("year") or "").strip()
                year = _parse_int_loose((date_text[:4] if date_text else first_tags.get("year")), 0) or None
                dominant_format = max(fmt_counts.items(), key=lambda x: x[1])[0] if fmt_counts else "UNKNOWN"
                is_lossless = dominant_format in _LOSSLESS_FORMATS
                cover_path = _first_cover_path(folder)
                has_cover = bool(cover_path and cover_path.is_file())
                artist_folder = _files_guess_artist_folder(folder, artist_name, root_dirs=root_dirs)
                artist_image_path = _first_artist_image_path(artist_folder) if artist_folder else None
                artist_has_image = bool(artist_image_path and artist_image_path.is_file())
                if artist_norm not in artists_map:
                    artists_map[artist_norm] = {
                        "name": artist_name,
                        "image_path": str(artist_image_path) if artist_image_path else None,
                        "has_image": artist_has_image,
                    }
                elif artist_has_image and not artists_map[artist_norm].get("has_image"):
                    artists_map[artist_norm]["image_path"] = str(artist_image_path)
                    artists_map[artist_norm]["has_image"] = True

                indices = [t["track_num"] for t in track_entries if t["track_num"] > 0]
                actual_track_count = len(track_entries)
                is_broken = False
                expected_track_count = None
                missing_indices: list[int] = []
                if indices:
                    # Track numbers can be garbage (e.g. "745" on a 15-track album). In that case
                    # gaps-based "broken" detection produces massive false positives and UI junk.
                    max_idx = max(indices)
                    coverage = (actual_track_count / max_idx) if max_idx else 1.0
                    if max_idx > max(120, actual_track_count * 3) and coverage < 0.5:
                        is_broken = False
                        expected_track_count = None
                        missing_indices = []
                    else:
                        is_broken, _actual_count_from_indices, gaps = _detect_gaps_in_indices(indices)
                        if is_broken:
                            expected_track_count = max_idx
                            for start_i, end_i in gaps:
                                # Defensive cap to avoid pathological arrays.
                                if (end_i - start_i) > 2000:
                                    continue
                                missing_indices.extend(list(range(start_i + 1, end_i)))
                                if len(missing_indices) > 5000:
                                    missing_indices = missing_indices[:5000]
                                    break

                inferred_genre = _infer_genre_from_bandcamp_tags(raw_genres) if raw_genres else None
                identity_fields = _extract_files_identity_fields(tags=first_tags, edition={}, cached={})
                mbid = identity_fields["musicbrainz_id"]
                missing_required = _check_required_tags(
                    first_tags or {},
                    REQUIRED_TAGS,
                    edition={"tracks": [{"title": t.get("title"), "index": t.get("track_num")} for t in track_entries]},
                )

                albums_payload.append(
                    {
                        "artist_norm": artist_norm,
                        "title": album_title,
                        "title_norm": title_norm,
                        "folder_path": str(folder),
                        "year": year,
                        "date_text": date_text[:32] if date_text else "",
                        "genre": inferred_genre or "",
                        "label": (label or "").strip(),
                        "tags_json": json.dumps(raw_genres[:20]),
                        "format": dominant_format,
                        "is_lossless": bool(is_lossless),
                        "has_cover": bool(has_cover),
                        "cover_path": str(cover_path) if cover_path else "",
                        "mb_identified": bool(identity_fields["has_mbid"]),
                        "musicbrainz_release_group_id": mbid,
                        "track_count": actual_track_count,
                        "total_duration_sec": total_duration_sec,
                        "is_broken": bool(is_broken),
                        "expected_track_count": expected_track_count,
                        "actual_track_count": actual_track_count,
                        "missing_indices_json": json.dumps(missing_indices),
                        "missing_required_tags_json": json.dumps(missing_required),
                        "primary_tags_json": json.dumps(first_tags or {}),
                        "tracks": track_entries,
                        "discogs_release_id": identity_fields["discogs_release_id"],
                        "lastfm_album_mbid": identity_fields["lastfm_album_mbid"],
                        "bandcamp_album_url": identity_fields["bandcamp_album_url"],
                        "metadata_source": identity_fields["identity_provider"] or identity_fields["metadata_source"],
                    }
                )

        _files_index_set_state(phase="writing", artists=len(artists_map), albums=len(albums_payload))

        conn = _files_pg_connect()
        if conn is None:
            raise RuntimeError("PostgreSQL connection unavailable during rebuild")
        try:
            with conn.transaction():
                with conn.cursor() as cur:
                    cur.execute("TRUNCATE TABLE files_tracks, files_albums, files_artists RESTART IDENTITY CASCADE")
                    ext_artist_images: dict[str, str] = {}
                    try:
                        norms = list(artists_map.keys())
                        if norms:
                            cur.execute(
                                """
                                SELECT name_norm, COALESCE(image_path, '')
                                FROM files_external_artist_images
                                WHERE name_norm = ANY(%s)
                                """,
                                (norms,),
                            )
                            for n, p in cur.fetchall():
                                nkey = str(n or "").strip()
                                pval = str(p or "").strip()
                                if nkey and pval:
                                    ext_artist_images[nkey] = pval
                    except Exception:
                        ext_artist_images = {}
                    artist_rows = [
                        (
                            data["name"],
                            norm,
                            bool(data["has_image"]) or bool(ext_artist_images.get(norm)),
                            (data.get("image_path") or "").strip() or ext_artist_images.get(norm) or "",
                        )
                        for norm, data in artists_map.items()
                    ]
                    if artist_rows:
                        cur.executemany(
                            """
                            INSERT INTO files_artists (name, name_norm, has_image, image_path, created_at, updated_at)
                            VALUES (%s, %s, %s, %s, NOW(), NOW())
                            """,
                            artist_rows,
                        )
                    cur.execute("SELECT id, name_norm FROM files_artists")
                    artist_id_by_norm = {str(r[1]): int(r[0]) for r in cur.fetchall()}

                    album_rows = []
                    for album in albums_payload:
                        artist_id = artist_id_by_norm.get(album["artist_norm"])
                        if not artist_id:
                            continue
                        album_rows.append(
                            (
                                artist_id,
                                album["title"],
                                album["title_norm"],
                                album["folder_path"],
                                album["year"],
                                album["date_text"],
                                album["genre"],
                                album.get("label") or "",
                                album["tags_json"],
                                album["format"],
                                bool(album["is_lossless"]),
                                bool(album["has_cover"]),
                                album["cover_path"],
                                bool(album["mb_identified"]),
                                album["musicbrainz_release_group_id"],
                                album.get("discogs_release_id") or "",
                                album.get("lastfm_album_mbid") or "",
                                album.get("bandcamp_album_url") or "",
                                album.get("metadata_source") or "",
                                album["track_count"],
                                album["total_duration_sec"],
                                bool(album["is_broken"]),
                                album["expected_track_count"],
                                album["actual_track_count"],
                                album["missing_indices_json"],
                                album["missing_required_tags_json"],
                                album["primary_tags_json"],
                            )
                        )
                    if album_rows:
                        cur.executemany(
                            """
                            INSERT INTO files_albums (
                                artist_id, title, title_norm, folder_path, year, date_text, genre, label, tags_json,
                                format, is_lossless, has_cover, cover_path, mb_identified, musicbrainz_release_group_id,
                                discogs_release_id, lastfm_album_mbid, bandcamp_album_url, metadata_source,
                                track_count, total_duration_sec, is_broken, expected_track_count, actual_track_count,
                                missing_indices_json, missing_required_tags_json, primary_tags_json,
                                created_at, updated_at
                            ) VALUES (
                                %s, %s, %s, %s, %s, %s, %s, %s, %s,
                                %s, %s, %s, %s, %s, %s,
                                %s, %s, %s, %s,
                                %s, %s, %s, %s, %s,
                                %s, %s, %s,
                                NOW(), NOW()
                            )
                            """,
                            album_rows,
                        )
                    cur.execute("SELECT id, folder_path FROM files_albums")
                    album_id_by_folder = {str(r[1]): int(r[0]) for r in cur.fetchall()}

                    total_tracks = 0
                    for album in albums_payload:
                        album_id = album_id_by_folder.get(album["folder_path"])
                        if not album_id:
                            continue
                        # Defensive: ensure file_path uniqueness within this batch.
                        track_by_path: dict[str, tuple] = {}
                        for t in album["tracks"]:
                            fp = str(t.get("file_path") or "").strip()
                            if not fp:
                                continue
                            if fp in track_by_path:
                                continue
                            track_by_path[fp] = (
                                album_id,
                                fp,
                                t.get("title") or "",
                                t.get("disc_num") or 1,
                                t.get("track_num") or 0,
                                t.get("duration_sec") or 0,
                                t.get("format") or "",
                                t.get("bitrate") or 0,
                                t.get("sample_rate") or 0,
                                t.get("bit_depth") or 0,
                                t.get("file_size_bytes") or 0,
                            )
                        track_rows = list(track_by_path.values())
                        if track_rows:
                            cur.executemany(
                                """
                                INSERT INTO files_tracks (
                                    album_id, file_path, title, disc_num, track_num, duration_sec, format,
                                    bitrate, sample_rate, bit_depth, file_size_bytes, created_at, updated_at
                                ) VALUES (
                                    %s, %s, %s, %s, %s, %s, %s,
                                    %s, %s, %s, %s, NOW(), NOW()
                                )
                                ON CONFLICT (file_path) DO UPDATE
                                SET album_id = EXCLUDED.album_id,
                                    title = EXCLUDED.title,
                                    disc_num = EXCLUDED.disc_num,
                                    track_num = EXCLUDED.track_num,
                                    duration_sec = EXCLUDED.duration_sec,
                                    format = EXCLUDED.format,
                                    bitrate = EXCLUDED.bitrate,
                                    sample_rate = EXCLUDED.sample_rate,
                                    bit_depth = EXCLUDED.bit_depth,
                                    file_size_bytes = EXCLUDED.file_size_bytes,
                                    updated_at = NOW()
                                """,
                                track_rows,
                            )
                            total_tracks += len(track_rows)

                    cur.execute("""
                        UPDATE files_artists a
                        SET album_count = s.album_count,
                            track_count = s.track_count,
                            broken_albums_count = s.broken_albums_count,
                            updated_at = NOW()
                        FROM (
                            SELECT
                                artist_id,
                                COUNT(*) AS album_count,
                                COALESCE(SUM(track_count), 0) AS track_count,
                                COALESCE(SUM(CASE WHEN is_broken THEN 1 ELSE 0 END), 0) AS broken_albums_count
                            FROM files_albums
                            GROUP BY artist_id
                        ) s
                        WHERE a.id = s.artist_id
                    """)
                    _files_index_write_meta(cur, "last_reason", reason)
                    _files_index_write_meta(cur, "last_build_ts", str(int(time.time())))
                    _files_index_write_meta(cur, "artists", str(len(artists_map)))
                    _files_index_write_meta(cur, "albums", str(len(albums_payload)))
                    _files_index_write_meta(cur, "tracks", str(total_tracks))
                    _files_index_write_meta(cur, "source", payload_source)
                _files_index_set_state(phase="embeddings")
                reco_embeddings_count = _reco_build_track_embeddings(conn)
                with conn.cursor() as cur:
                    _files_index_write_meta(cur, "track_embeddings", str(reco_embeddings_count))
        finally:
            conn.close()

        _files_index_set_state(phase="media_cache")
        covers_cached, artists_cached = _precache_files_media_assets(artists_map, albums_payload)
        _files_cache_invalidate_all()
        elapsed = round(time.time() - started_at, 2)
        _files_index_set_state(
            running=False,
            finished_at=time.time(),
            phase="done",
            current_folder=None,
            artists=len(artists_map),
            albums=len(albums_payload),
            tracks=sum(len(a["tracks"]) for a in albums_payload),
            error=None,
        )
        logging.info(
            "Files library index rebuilt (%s, source=%s): %d artist(s), %d album(s), %d track(s), %d embedding(s) in %.2fs (cached covers=%d, artist images=%d)",
            reason,
            payload_source,
            len(artists_map),
            len(albums_payload),
            sum(len(a["tracks"]) for a in albums_payload),
            reco_embeddings_count,
            elapsed,
            covers_cached,
            artists_cached,
        )
        # Post-build: backfill artist profiles (bios/tags/similar) and cache external images,
        # so browsing shows rich pages (no placeholders) without waiting for first click.
        try:
            if bool(_FILES_PROFILE_BACKFILL_ON_REBUILD):
                _trigger_files_profile_backfill_async(reason=f"after_files_index_rebuild:{reason}")
        except Exception:
            pass
        return {
            "ok": True,
            "artists": len(artists_map),
            "albums": len(albums_payload),
            "tracks": sum(len(a["tracks"]) for a in albums_payload),
            "track_embeddings": reco_embeddings_count,
            "cached_covers": covers_cached,
            "cached_artist_images": artists_cached,
            "duration_sec": elapsed,
            "source": payload_source,
        }
    except Exception as e:
        logging.exception("Files index rebuild failed: %s", e)
        _files_index_set_state(
            running=False,
            finished_at=time.time(),
            phase="error",
            current_folder=None,
            error=str(e),
        )
        return {"ok": False, "error": str(e)}
    finally:
        files_index_lock.release()


def _trigger_files_index_rebuild_async(reason: str = "manual") -> bool:
    if files_index_lock.locked():
        return False
    reason_norm = str(reason or "manual").strip()
    if reason_norm.startswith("scan_live_sync_"):
        # Per-artist publication already triggers granular upserts; avoid redundant full rebuilds.
        return False

    artist_hint = None
    if reason_norm.startswith("scan_artist_ready_"):
        artist_hint = reason_norm[len("scan_artist_ready_") :].strip() or None

    def _runner():
        if artist_hint:
            _rebuild_files_library_index_for_artist(
                artist_hint,
                reason=reason_norm,
                wait_if_running=False,
            )
            return
        _rebuild_files_library_index(reason=reason_norm, wait_if_running=False)

    tname = "files-index-rebuild-artist" if artist_hint else "files-index-rebuild"
    threading.Thread(target=_runner, name=tname, daemon=True).start()
    return True


def _rebuild_files_reco_embeddings(reason: str = "manual", wait_if_running: bool = False) -> dict:
    if _get_library_mode() != "files":
        return {"ok": False, "error": "LIBRARY_MODE is not 'files'"}
    if not _files_pg_init_schema():
        return {"ok": False, "error": "PostgreSQL schema unavailable"}

    acquired = files_index_lock.acquire(blocking=wait_if_running)
    if not acquired:
        return {"ok": False, "running": True, "error": "Files index rebuild already running"}
    try:
        started_at = time.time()
        _files_index_set_state(
            running=True,
            started_at=started_at,
            finished_at=None,
            phase="embeddings",
            current_folder=None,
            error=None,
        )
        conn = _files_pg_connect()
        if conn is None:
            raise RuntimeError("PostgreSQL connection unavailable during embedding rebuild")
        try:
            with conn.transaction():
                embedding_count = _reco_build_track_embeddings(conn)
                with conn.cursor() as cur:
                    _files_index_write_meta(cur, "track_embeddings", str(embedding_count))
                    _files_index_write_meta(cur, "track_embeddings_source", RECO_EMBED_SOURCE)
                    _files_index_write_meta(cur, "track_embeddings_reason", reason)
                    _files_index_write_meta(cur, "track_embeddings_ts", str(int(time.time())))
        finally:
            conn.close()
        _files_cache_invalidate_all()
        artists, albums, tracks = _files_index_read_counts()
        _files_index_set_state(
            running=False,
            finished_at=time.time(),
            phase="done",
            current_folder=None,
            artists=artists,
            albums=albums,
            tracks=tracks,
            error=None,
        )
        elapsed = round(time.time() - started_at, 2)
        logging.info(
            "Files reco embeddings rebuilt (%s): %d embedding(s) in %.2fs",
            reason,
            embedding_count,
            elapsed,
        )
        return {
            "ok": True,
            "track_embeddings": embedding_count,
            "duration_sec": elapsed,
        }
    except Exception as e:
        logging.exception("Files reco embedding rebuild failed: %s", e)
        _files_index_set_state(
            running=False,
            finished_at=time.time(),
            phase="error",
            current_folder=None,
            error=str(e),
        )
        return {"ok": False, "error": str(e)}
    finally:
        files_index_lock.release()


def _ensure_files_index_ready() -> tuple[bool, Optional[str]]:
    if _get_library_mode() != "files":
        return False, "LIBRARY_MODE is not 'files'"
    if not FILES_ROOTS:
        return False, "FILES_ROOTS is empty"
    if not _files_pg_init_schema():
        return False, "PostgreSQL is unavailable"
    with lock:
        files_scan_running = bool(state.get("scanning")) and _get_library_mode() == "files"
    artists, albums, tracks = _files_index_read_counts()
    if files_scan_running and (albums == 0 or tracks == 0):
        # During a running Files scan, the live index may be intentionally empty
        # before first artist publication. Do not force a full bootstrap rebuild.
        return True, None
    if albums > 0 and tracks > 0:
        track_count, embedding_count = _files_index_read_track_and_embedding_counts()
        min_expected = max(1, int(track_count * 0.85)) if track_count > 0 else 0
        if track_count > 0 and embedding_count < min_expected:
            if files_index_lock.locked():
                return True, None
            logging.info(
                "Files reco embeddings below threshold (%d/%d). Rebuilding embeddings...",
                embedding_count,
                track_count,
            )
            result = _rebuild_files_reco_embeddings(
                reason="auto_backfill_missing_embeddings",
                wait_if_running=True,
            )
            if not result.get("ok"):
                logging.warning("Files reco embedding auto-backfill failed: %s", result.get("error"))
        return True, None
    result = _rebuild_files_library_index(reason="auto_bootstrap", wait_if_running=True)
    if not result.get("ok"):
        return False, str(result.get("error") or "Files index build failed")
    return True, None


_FILES_PROFILE_MAX_AGE_SEC = 30 * 24 * 3600
_files_profile_jobs_lock = threading.Lock()
_files_profile_jobs_active: set[str] = set()


def _dt_to_epoch(value) -> float:
    if value is None:
        return 0.0
    if isinstance(value, (int, float)):
        return float(value)
    if hasattr(value, "timestamp"):
        try:
            return float(value.timestamp())
        except Exception:
            return 0.0
    return 0.0


def _is_profile_stale(updated_at) -> bool:
    ts = _dt_to_epoch(updated_at)
    if ts <= 0:
        return True
    return (time.time() - ts) > _FILES_PROFILE_MAX_AGE_SEC


def _files_get_artist_profile_cached(artist_name: str, artist_norm: str) -> dict:
    if not artist_norm:
        return {}
    cache_key = f"library:artist_profile:{artist_norm}"
    cached = _files_cache_get_json(cache_key)
    if isinstance(cached, dict):
        return cached
    conn = _files_pg_connect()
    if conn is None:
        return {}
    try:
        with conn.cursor() as cur:
            cur.execute(
                """
                SELECT bio, short_bio, tags_json, similar_json, source, updated_at
                FROM files_artist_profiles
                WHERE name_norm = %s
                """,
                (artist_norm,),
            )
            row = cur.fetchone()
        if not row:
            return {}
        try:
            tags = json.loads(row[2] or "[]") if row[2] else []
        except Exception:
            tags = []
        try:
            similar = json.loads(row[3] or "[]") if row[3] else []
        except Exception:
            similar = []
        payload = {
            "artist_name": artist_name,
            "bio": row[0] or "",
            "short_bio": row[1] or "",
            "tags": tags if isinstance(tags, list) else [],
            "similar_artists": similar if isinstance(similar, list) else [],
            "source": row[4] or "",
            "updated_at": int(_dt_to_epoch(row[5])) if row[5] else 0,
            "stale": _is_profile_stale(row[5]),
        }
        _files_cache_set_json(cache_key, payload, ttl=1800)
        return payload
    except Exception:
        return {}
    finally:
        conn.close()


def _files_get_album_profiles_cached(artist_norm: str, title_norms: list[str]) -> dict[str, dict]:
    if not artist_norm or not title_norms:
        return {}
    cache_key = f"library:album_profiles:{artist_norm}:{hashlib.sha1('|'.join(sorted(set(title_norms))).encode('utf-8', errors='ignore')).hexdigest()}"
    cached = _files_cache_get_json(cache_key)
    if isinstance(cached, dict):
        return cached
    conn = _files_pg_connect()
    if conn is None:
        return {}
    try:
        title_norms_clean = [t for t in title_norms if t]
        if not title_norms_clean:
            return {}
        placeholders = ",".join(["%s"] * len(title_norms_clean))
        with conn.cursor() as cur:
            cur.execute(
                f"""
                SELECT title_norm, description, short_description, tags_json, source, updated_at
                FROM files_album_profiles
                WHERE artist_norm = %s AND title_norm IN ({placeholders})
                """,
                [artist_norm, *title_norms_clean],
            )
            rows = cur.fetchall()
        out: dict[str, dict] = {}
        for row in rows:
            try:
                tags = json.loads(row[3] or "[]") if row[3] else []
            except Exception:
                tags = []
            out[str(row[0] or "")] = {
                "description": row[1] or "",
                "short_description": row[2] or "",
                "tags": tags if isinstance(tags, list) else [],
                "source": row[4] or "",
                "updated_at": int(_dt_to_epoch(row[5])) if row[5] else 0,
                "stale": _is_profile_stale(row[5]),
            }
        _files_cache_set_json(cache_key, out, ttl=1800)
        return out
    except Exception:
        return {}
    finally:
        conn.close()


def _files_upsert_artist_profile(conn, artist_norm: str, artist_name: str, profile: dict) -> None:
    tags_json = json.dumps((profile or {}).get("tags") or [])
    similar_json = json.dumps((profile or {}).get("similar") or (profile or {}).get("similar_artists") or [])
    with conn.cursor() as cur:
        cur.execute(
            """
            INSERT INTO files_artist_profiles(name_norm, artist_name, bio, short_bio, tags_json, similar_json, source, updated_at)
            VALUES (%s, %s, %s, %s, %s, %s, %s, NOW())
            ON CONFLICT (name_norm) DO UPDATE
            SET artist_name = EXCLUDED.artist_name,
                bio = EXCLUDED.bio,
                short_bio = EXCLUDED.short_bio,
                tags_json = EXCLUDED.tags_json,
                similar_json = EXCLUDED.similar_json,
                source = EXCLUDED.source,
                updated_at = NOW()
            """,
            (
                artist_norm,
                artist_name or "",
                (profile or {}).get("bio") or "",
                (profile or {}).get("short_bio") or "",
                tags_json,
                similar_json,
                (profile or {}).get("source") or "",
            ),
        )


def _files_upsert_album_profile(conn, artist_norm: str, title_norm: str, album_title: str, profile: dict) -> None:
    tags_json = json.dumps((profile or {}).get("tags") or [])
    with conn.cursor() as cur:
        cur.execute(
            """
            INSERT INTO files_album_profiles(artist_norm, title_norm, album_title, description, short_description, tags_json, source, updated_at)
            VALUES (%s, %s, %s, %s, %s, %s, %s, NOW())
            ON CONFLICT (artist_norm, title_norm) DO UPDATE
            SET album_title = EXCLUDED.album_title,
                description = EXCLUDED.description,
                short_description = EXCLUDED.short_description,
                tags_json = EXCLUDED.tags_json,
                source = EXCLUDED.source,
                updated_at = NOW()
            """,
            (
                artist_norm,
                title_norm,
                album_title or "",
                (profile or {}).get("description") or "",
                (profile or {}).get("short_description") or "",
                tags_json,
                (profile or {}).get("source") or "",
            ),
        )


_FILES_EXTERNAL_ARTIST_IMAGE_MAX_AGE_SEC = 120 * 24 * 3600


def _is_external_artist_image_stale(updated_at) -> bool:
    ts = _dt_to_epoch(updated_at)
    if ts <= 0:
        return True
    return (time.time() - ts) > _FILES_EXTERNAL_ARTIST_IMAGE_MAX_AGE_SEC


def _files_get_external_artist_images(conn, name_norms: list[str]) -> dict[str, dict]:
    """Return map name_norm -> {image_path,image_url,provider,updated_at,stale}."""
    norms = [str(n or "").strip() for n in (name_norms or []) if str(n or "").strip()]
    if not norms:
        return {}
    try:
        with conn.cursor() as cur:
            cur.execute(
                """
                SELECT name_norm, artist_name, provider, COALESCE(image_path, ''), COALESCE(image_url, ''), updated_at
                FROM files_external_artist_images
                WHERE name_norm = ANY(%s)
                """,
                (norms,),
            )
            rows = cur.fetchall()
        out: dict[str, dict] = {}
        for n, artist_name, provider, image_path, image_url, updated_at in rows:
            key = str(n or "").strip()
            if not key:
                continue
            img_path = (image_path or "").strip() or None
            img_url = (image_url or "").strip() or None
            stale = _is_external_artist_image_stale(updated_at)

            # Never serve known placeholder images (e.g. Last.fm "music note" missing-avatar).
            try:
                if img_url and _is_probably_placeholder_artist_image_url(img_url):
                    img_path = None
                    img_url = None
                    stale = True
            except Exception:
                pass

            # If the cached file disappeared, treat as stale so a refresh can fix it.
            if img_path:
                try:
                    p = Path(img_path)
                    if not p.exists():
                        img_path = None
                        stale = True
                    else:
                        # Tiny files are almost always placeholders (e.g. blank avatars); treat as stale.
                        try:
                            if int(p.stat().st_size or 0) < 8192:
                                img_path = None
                                stale = True
                        except Exception:
                            pass
                except Exception:
                    img_path = None
                    stale = True
            out[key] = {
                "artist_name": artist_name or "",
                "provider": provider or "",
                "image_path": img_path,
                "image_url": img_url,
                "updated_at": int(_dt_to_epoch(updated_at)) if updated_at else 0,
                "stale": stale,
            }
        return out
    except Exception:
        return {}


def _files_upsert_external_artist_image(
    conn,
    *,
    name_norm: str,
    artist_name: str,
    provider: str,
    image_path: Optional[str],
    image_url: Optional[str],
) -> None:
    key = str(name_norm or "").strip()
    if not key:
        return
    with conn.cursor() as cur:
        cur.execute(
            """
            INSERT INTO files_external_artist_images(name_norm, artist_name, provider, image_path, image_url, updated_at)
            VALUES (%s, %s, %s, %s, %s, NOW())
            ON CONFLICT (name_norm) DO UPDATE SET
                artist_name = EXCLUDED.artist_name,
                provider = EXCLUDED.provider,
                image_path = EXCLUDED.image_path,
                image_url = EXCLUDED.image_url,
                updated_at = NOW()
            """,
            (
                key,
                (artist_name or "").strip(),
                (provider or "").strip().lower() or "lastfm",
                (image_path or "").strip() if image_path else None,
                (image_url or "").strip() if image_url else None,
            ),
        )
        # Keep files_artists in sync so list endpoints relying on files_artists.has_image
        # show thumbnails immediately without requiring a full index rebuild.
        try:
            img_path = (image_path or "").strip() if image_path else ""
            if img_path:
                cur.execute("SELECT COALESCE(image_path, '') FROM files_artists WHERE name_norm = %s", (key,))
                row = cur.fetchone()
                existing_raw = str((row[0] if row else "") or "").strip()
                override = False
                if not existing_raw:
                    override = True
                else:
                    try:
                        existing_path = path_for_fs_access(Path(existing_raw))
                    except Exception:
                        existing_path = Path(existing_raw)
                    try:
                        if not existing_path.exists():
                            override = True
                    except Exception:
                        override = True
                    # Never keep a FILES_ROOT-level "artist.jpg" as the canonical artist image: it's almost certainly
                    # a flat-library artifact that would be incorrectly shared by many artists.
                    try:
                        if not override and existing_path.name.lower() in {n.lower() for n in _ARTIST_IMAGE_NAMES}:
                            if _files_is_files_root_dir(existing_path.parent):
                                override = True
                    except Exception:
                        pass
                if override:
                    cur.execute(
                        """
                        UPDATE files_artists
                        SET has_image = TRUE,
                            image_path = %s,
                            updated_at = NOW()
                        WHERE name_norm = %s
                        """,
                        (img_path, key),
                    )
                else:
                    cur.execute(
                        "UPDATE files_artists SET has_image = TRUE, updated_at = NOW() WHERE name_norm = %s",
                        (key,),
                    )
        except Exception:
            pass


def _files_cache_external_artist_image(
    conn,
    *,
    artist_name: str,
    provider: str,
    image_url: str,
    max_px: int = 640,
) -> Optional[str]:
    """
    Download + store an external artist image into the media cache and upsert the DB row.
    Returns the cached file path string, or None.
    """
    name = (artist_name or "").strip()
    if not name:
        return None
    key = _norm_artist_key(name)
    img_url = (image_url or "").strip()
    if not img_url:
        return None
    try:
        if _is_probably_placeholder_artist_image_url(img_url):
            return None
    except Exception:
        pass

    # Skip refresh if a recent cached image exists and the file is still present.
    existing = _files_get_external_artist_images(conn, [key]).get(key) or {}
    if existing and not bool(existing.get("stale")):
        p = (existing.get("image_path") or "").strip()
        if p:
            try:
                pp = Path(p)
                if pp.exists():
                    # If the cached file is tiny/low-res/placeholder-ish, force a refresh.
                    try:
                        if not _is_usable_artist_image_path(pp):
                            p = ""
                        else:
                            # Keep files_artists in sync even when we reuse an existing cached file.
                            try:
                                _files_upsert_external_artist_image(
                                    conn,
                                    name_norm=key,
                                    artist_name=name,
                                    provider=str(existing.get("provider") or provider),
                                    image_path=str(pp),
                                    image_url=str(existing.get("image_url") or img_url),
                                )
                            except Exception:
                                pass
                            return str(pp)
                    except Exception:
                        return str(pp)
            except Exception:
                pass

    dl = _download_best_cover_image(provider, img_url, cover_candidates=[img_url], timeout=12)
    if not dl:
        return None
    raw, mime, url_used = dl
    try:
        if not _is_usable_artist_image_bytes(raw):
            return None
    except Exception:
        # Keep permissive behavior if inspection fails unexpectedly.
        pass
    cached = _ensure_cached_image_from_bytes(
        raw,
        mime,
        kind="artist",
        cache_key_hint=f"external-artist-{key}",
        max_px=max_px,
    )
    if not cached:
        return None
    cached_path = str(cached)
    _files_upsert_external_artist_image(
        conn,
        name_norm=key,
        artist_name=name,
        provider=provider,
        image_path=cached_path,
        image_url=url_used,
    )
    return cached_path


def _files_attach_similar_artist_refs(conn, similar_artists: list, base_url: str) -> list:
    """
    Attach `artist_id` and `image_url` (prefer local, then cached external, then remote)
    to similar artist entries.
    """
    if not isinstance(similar_artists, list) or not similar_artists:
        return []
    base = (base_url or "").rstrip("/")
    # Build normalized name keys.
    norms: list[str] = []
    for entry in similar_artists:
        if not isinstance(entry, dict):
            continue
        n = _norm_artist_key(str(entry.get("name") or ""))
        if n:
            norms.append(n)
    norms = list(dict.fromkeys(norms))
    if not norms:
        return similar_artists

    local_map: dict[str, dict] = {}
    try:
        with conn.cursor() as cur:
            cur.execute(
                """
                SELECT id, name_norm, has_image
                FROM files_artists
                WHERE name_norm = ANY(%s)
                """,
                (norms,),
            )
            for aid, nn, has_img in cur.fetchall():
                key = str(nn or "").strip()
                if not key:
                    continue
                local_map[key] = {"artist_id": int(aid or 0), "has_image": bool(has_img)}
    except Exception:
        local_map = {}

    ext_map = _files_get_external_artist_images(conn, norms)

    out: list[dict] = []
    for entry in similar_artists:
        if not isinstance(entry, dict):
            continue
        name = str(entry.get("name") or "").strip()
        if not name:
            continue
        key = _norm_artist_key(name)
        patched = dict(entry)
        # Local link + image.
        local = local_map.get(key)
        local_has_image = False
        if local and int(local.get("artist_id") or 0) > 0:
            patched["artist_id"] = int(local["artist_id"])
            local_has_image = bool(local.get("has_image"))
            if local_has_image:
                patched["image_url"] = f"{base}/api/library/files/artist/{int(local['artist_id'])}/image?size=320"
        # Prefer cached external image over remote hotlinks when we don't have a local on-disk image.
        # This keeps browsing snappy and avoids broken hotlink/CDN behavior.
        if not local_has_image:
            ext = ext_map.get(key) or {}
            ext_path = str(ext.get("image_path") or "").strip()
            if ext_path:
                patched["image_url"] = f"{base}/api/library/external/artist-image/{quote(key, safe='')}?size=320"
        # Drop any known placeholder URLs so the UI never shows the "music note" missing-avatar.
        try:
            img = str(patched.get("image_url") or "").strip()
            if img and _is_probably_placeholder_artist_image_url(img):
                patched.pop("image_url", None)
        except Exception:
            pass
        out.append(patched)
    return out


def _files_similar_artists_by_genre(conn, artist_id: int, *, limit: int = 20) -> list[dict]:
    """
    Local fallback: suggest similar artists from the local library by overlapping inferred genres.
    This is a safe fallback when provider similar artists are missing or unreliable.
    """
    artist_id = int(artist_id or 0)
    limit = max(1, min(40, int(limit or 20)))
    if artist_id <= 0:
        return []
    try:
        with conn.cursor() as cur:
            cur.execute(
                """
                SELECT LOWER(TRIM(g.value)) AS g, COUNT(DISTINCT alb.id) AS c
                FROM files_albums alb
                CROSS JOIN LATERAL jsonb_array_elements_text(COALESCE(alb.tags_json, '[]')::jsonb) AS g(value)
                WHERE alb.artist_id = %s
                  AND COALESCE(TRIM(g.value), '') <> ''
                GROUP BY LOWER(TRIM(g.value))
                ORDER BY COUNT(DISTINCT alb.id) DESC, g ASC
                LIMIT 12
                """,
                (artist_id,),
            )
            target_genres = [str(r[0] or "").strip() for r in cur.fetchall() if str(r[0] or "").strip()]
        if not target_genres:
            return []
        with conn.cursor() as cur:
            cur.execute(
                """
                SELECT
                    a.id,
                    a.name,
                    COUNT(DISTINCT LOWER(TRIM(g.value))) AS score,
                    ARRAY_AGG(DISTINCT LOWER(TRIM(g.value))) AS matched_genres
                FROM files_artists a
                JOIN files_albums alb ON alb.artist_id = a.id
                CROSS JOIN LATERAL jsonb_array_elements_text(COALESCE(alb.tags_json, '[]')::jsonb) AS g(value)
                WHERE a.id <> %s
                  AND COALESCE(TRIM(g.value), '') <> ''
                  AND LOWER(TRIM(g.value)) = ANY(%s)
                GROUP BY a.id, a.name, a.album_count
                ORDER BY score DESC, a.album_count DESC, a.name ASC
                LIMIT %s
                """,
                (artist_id, target_genres, limit),
            )
            rows = cur.fetchall()
        out: list[dict] = []
        for aid, name, score, matched_genres in rows:
            nm = str(name or "").strip()
            if not nm:
                continue
            mg: list[str] = []
            if isinstance(matched_genres, list):
                for g in matched_genres:
                    gg = str(g or "").strip()
                    if gg:
                        mg.append(gg)
            label = f"Genre: {', '.join(mg[:2])}" if mg else "Genre match"
            out.append(
                {
                    "name": nm,
                    "artist_id": int(aid or 0),
                    "type": label,
                    "score": int(score or 0),
                }
            )
        return out[:limit]
    except Exception:
        return []


def _run_files_profile_enrichment_job(
    *,
    job_key: str,
    artist_name: str,
    artist_norm: str,
    albums: list[tuple[str, str]],
    skip_album_profiles: bool = False,
) -> None:
    try:
        conn = _files_pg_connect()
        if conn is None:
            return
        try:
            existing_profile = _files_get_artist_profile_cached(artist_name, artist_norm)
            existing_bio_text = ((existing_profile.get("bio") or existing_profile.get("short_bio") or "") if isinstance(existing_profile, dict) else "").strip()
            should_refresh_artist = (
                not bool(existing_profile.get("short_bio"))
                or bool(existing_profile.get("stale"))
                or _is_garbage_bio(existing_bio_text)
            )

            lastfm_info = None
            wiki_info = None

            artist_profile = None
            if should_refresh_artist:
                lastfm_info = _fetch_lastfm_artist_info(artist_name) or {}
                wiki_info = _fetch_wikipedia_artist_bio(artist_name, lang="en") or _fetch_wikipedia_artist_bio(artist_name, lang="fr") or {}

                # Start with Last.fm tags/similar; replace the bio if Wikipedia is better.
                artist_profile = {
                    "bio": (lastfm_info.get("bio") or "").strip(),
                    "short_bio": (lastfm_info.get("short_bio") or "").strip(),
                    "tags": lastfm_info.get("tags") or [],
                    "similar": lastfm_info.get("similar") or [],
                    "source": (lastfm_info.get("source") or "").strip() or "lastfm",
                }

                # Prefer Wikipedia when it provides a real intro paragraph (spec: >= 100 words).
                if isinstance(wiki_info, dict) and _is_acceptable_original_bio((wiki_info.get("bio") or "").strip()):
                    artist_profile["bio"] = (wiki_info.get("bio") or "").strip()
                    artist_profile["short_bio"] = (wiki_info.get("short_bio") or "").strip()
                    artist_profile["source"] = (wiki_info.get("source") or "").strip() or "wikipedia"
                else:
                    # Keep Last.fm only if it's not garbage; otherwise blank it so UI can fall back to AI.
                    if _is_garbage_bio(artist_profile.get("bio") or ""):
                        artist_profile["bio"] = ""
                    if _is_garbage_bio(artist_profile.get("short_bio") or ""):
                        artist_profile["short_bio"] = ""

                # Similar artists fallback via MusicBrainz (names only; images handled separately).
                if (not (artist_profile.get("similar") or [])) and USE_MUSICBRAINZ:
                    mbid = ""
                    try:
                        search_result = musicbrainzngs.search_artists(artist=artist_name, limit=3)
                        artist_list = search_result.get("artist-list") or []
                        if artist_list and isinstance(artist_list[0], dict):
                            cand_name = (artist_list[0].get("name") or "").strip()
                            if _provider_identity_text_score(artist_name, cand_name) >= 0.78:
                                mbid = (artist_list[0].get("id") or "").strip()
                    except Exception:
                        mbid = ""
                    if mbid:
                        try:
                            mb_similar = get_similar_artists_mb(mbid)
                            if mb_similar:
                                artist_profile["similar"] = mb_similar[:20]
                                # Keep the bio source; only override source if it was empty.
                                if not (artist_profile.get("source") or "").strip():
                                    artist_profile["source"] = "musicbrainz"
                        except Exception:
                            pass

                # Persist profile.
                with conn.transaction():
                    _files_upsert_artist_profile(conn, artist_norm, artist_name, artist_profile)

            # Cache external artist images (main + similar) so the UI never shows empty placeholders.
            # This is best-effort; failures should not break enrichment. Importantly, this should run
            # even when the bio/tags profile does not need a refresh (images can be missing/low quality).
            try:
                # Local artist image presence.
                with conn.cursor() as cur:
                    cur.execute("SELECT id, has_image, COALESCE(image_path, '') FROM files_artists WHERE name_norm = %s", (artist_norm,))
                    arow = cur.fetchone()
                artist_id = int(arow[0]) if arow else 0
                local_has_image = bool(arow[1]) if arow else False
                local_image_path = str((arow[2] if arow else "") or "").strip()
                if local_has_image and local_image_path:
                    try:
                        if not Path(local_image_path).exists():
                            local_has_image = False
                    except Exception:
                        pass

                # External cached image state (quality-aware) so we can decide whether a refresh is needed.
                needs_image = not local_has_image
                if needs_image:
                    ext_row = _files_get_external_artist_images(conn, [artist_norm]).get(artist_norm) or {}
                    ext_path = str(ext_row.get("image_path") or "").strip()
                    if ext_path and not bool(ext_row.get("stale")):
                        try:
                            if _is_usable_artist_image_path(Path(ext_path)):
                                needs_image = False
                        except Exception:
                            needs_image = False

                if needs_image:
                    # Lazy fetch sources only if we actually need an image.
                    if lastfm_info is None:
                        lastfm_info = _fetch_lastfm_artist_info(artist_name) or {}
                    if wiki_info is None:
                        wiki_info = _fetch_wikipedia_artist_bio(artist_name, lang="en") or _fetch_wikipedia_artist_bio(artist_name, lang="fr") or {}

                    # Persist Wikipedia intro for assistant RAG even if it is too short for the "Original" UI slot.
                    try:
                        if artist_id > 0 and isinstance(wiki_info, dict):
                            wtxt = (wiki_info.get("bio") or "").strip()
                            if wtxt:
                                _assistant_upsert_doc(
                                    conn,
                                    entity_type="artist",
                                    entity_id=int(artist_id),
                                    doc_type="artist_external_wikipedia_intro",
                                    source=str(wiki_info.get("source") or "wikipedia"),
                                    title=str(artist_name),
                                    url=str(wiki_info.get("url") or ""),
                                    lang=str(wiki_info.get("lang") or ""),
                                    content=wtxt,
                                )
                    except Exception:
                        pass

                    # Try providers in order until one yields a usable cached image.
                    # Prefer Fanart.tv when available (often better artwork than Last.fm placeholders).
                    candidates: list[tuple[str, str]] = []
                    try:
                        lf_mbid = str((lastfm_info.get("mbid") if isinstance(lastfm_info, dict) else "") or "").strip()
                    except Exception:
                        lf_mbid = ""
                    if lf_mbid:
                        try:
                            fu = (_fetch_artist_image_fanart(lf_mbid) or "").strip()
                        except Exception:
                            fu = ""
                        if fu:
                            candidates.append(("fanart", fu))
                    try:
                        img_url = (lastfm_info.get("image_url") or "").strip() if isinstance(lastfm_info, dict) else ""
                    except Exception:
                        img_url = ""
                    if img_url:
                        candidates.append(("lastfm", img_url))
                    try:
                        wiki_img_url = (wiki_info.get("image_url") or "").strip() if isinstance(wiki_info, dict) else ""
                    except Exception:
                        wiki_img_url = ""
                    if wiki_img_url:
                        candidates.append(("wikipedia", wiki_img_url))
                    try:
                        au = (_fetch_artist_image_audiodb(artist_name) or "").strip()
                    except Exception:
                        au = ""
                    if au:
                        candidates.append(("audiodb", au))
                    try:
                        du = (_fetch_artist_image_discogs(artist_name) or "").strip()
                    except Exception:
                        du = ""
                    if du:
                        candidates.append(("discogs", du))
                    try:
                        wu = (_fetch_artist_image_web(artist_name) or "").strip()
                    except Exception:
                        wu = ""
                    if wu:
                        candidates.append(("web", wu))

                    for prov, url in candidates:
                        try:
                            with conn.transaction():
                                outp = _files_cache_external_artist_image(
                                    conn,
                                    artist_name=artist_name,
                                    provider=prov,
                                    image_url=url,
                                    max_px=640,
                                )
                            if outp:
                                break
                        except Exception:
                            continue

                # Similar artists: warm top N images (use refreshed profile if present, else existing cached profile).
                profile_for_similar = artist_profile if isinstance(artist_profile, dict) else existing_profile if isinstance(existing_profile, dict) else {}
                sim_list = []
                if isinstance(profile_for_similar, dict):
                    sim_list = profile_for_similar.get("similar") or profile_for_similar.get("similar_artists") or []
                if isinstance(sim_list, list) and sim_list:
                    # Preload local/external image presence to avoid unnecessary network calls.
                    sim_norms: list[str] = []
                    for sim in sim_list[:12]:
                        if not isinstance(sim, dict):
                            continue
                        sname = (sim.get("name") or "").strip()
                        if not sname:
                            continue
                        sim_norms.append(_norm_artist_key(sname))
                    sim_norms = list(dict.fromkeys([n for n in sim_norms if n]))
                    local_sim_map: dict[str, dict] = {}
                    if sim_norms:
                        try:
                            with conn.cursor() as cur:
                                cur.execute(
                                    """
                                    SELECT name_norm, has_image
                                    FROM files_artists
                                    WHERE name_norm = ANY(%s)
                                    """,
                                    (sim_norms,),
                                )
                                for nn, has_img in cur.fetchall():
                                    key = str(nn or "").strip()
                                    if key:
                                        local_sim_map[key] = {"has_image": bool(has_img)}
                        except Exception:
                            local_sim_map = {}
                    ext_sim_map = _files_get_external_artist_images(conn, sim_norms) if sim_norms else {}

                    warmed = 0
                    for sim in sim_list[:12]:
                        if not isinstance(sim, dict):
                            continue
                        sname = (sim.get("name") or "").strip()
                        surl = (sim.get("image_url") or "").strip()
                        if not sname:
                            continue

                        # Fast path: provider already gave an image URL (but still validate/cache).
                        if surl and not _is_probably_placeholder_artist_image_url(surl):
                            try:
                                with conn.transaction():
                                    outp = _files_cache_external_artist_image(conn, artist_name=sname, provider="lastfm", image_url=surl, max_px=640)
                                if outp:
                                    continue
                            except Exception:
                                pass

                        # Otherwise: best-effort warm a small number of missing/poor images so UI doesn't show placeholders.
                        if warmed >= 8:
                            continue
                        key = _norm_artist_key(sname)
                        if not key:
                            continue
                        if local_sim_map.get(key, {}).get("has_image"):
                            continue
                        ext_row = ext_sim_map.get(key) or {}
                        ext_path = str(ext_row.get("image_path") or "").strip()
                        if ext_path and not bool(ext_row.get("stale")):
                            try:
                                if _is_usable_artist_image_path(Path(ext_path)):
                                    continue
                            except Exception:
                                continue

                        # Try providers in order until one succeeds.
                        candidates_sim: list[tuple[str, str]] = []
                        try:
                            lf = _fetch_lastfm_artist_info(sname) or {}
                            lf_img = (lf.get("image_url") or "").strip()
                            lf_mbid = str(lf.get("mbid") or "").strip()
                        except Exception:
                            lf = {}
                            lf_img = ""
                            lf_mbid = ""
                        if lf_mbid:
                            try:
                                fu = (_fetch_artist_image_fanart(lf_mbid) or "").strip()
                            except Exception:
                                fu = ""
                            if fu:
                                candidates_sim.append(("fanart", fu))
                        if lf_img:
                            candidates_sim.append(("lastfm", lf_img))
                        try:
                            w = _fetch_wikipedia_artist_bio(sname, lang="en") or _fetch_wikipedia_artist_bio(sname, lang="fr") or {}
                            w_img = (w.get("image_url") or "").strip() if isinstance(w, dict) else ""
                        except Exception:
                            w_img = ""
                        if w_img:
                            candidates_sim.append(("wikipedia", w_img))
                        try:
                            au = (_fetch_artist_image_audiodb(sname) or "").strip()
                        except Exception:
                            au = ""
                        if au:
                            candidates_sim.append(("audiodb", au))
                        try:
                            du = (_fetch_artist_image_discogs(sname) or "").strip()
                        except Exception:
                            du = ""
                        if du:
                            candidates_sim.append(("discogs", du))
                        try:
                            wu = (_fetch_artist_image_web(sname) or "").strip()
                        except Exception:
                            wu = ""
                        if wu:
                            candidates_sim.append(("web", wu))

                        success = False
                        for prov, url in candidates_sim:
                            try:
                                with conn.transaction():
                                    outp = _files_cache_external_artist_image(conn, artist_name=sname, provider=prov, image_url=url, max_px=640)
                                if outp:
                                    success = True
                                    break
                            except Exception:
                                continue
                        if success:
                            warmed += 1
            except Exception:
                pass

            # Album descriptions: fill missing/stale entries progressively (optional).
            if not bool(skip_album_profiles):
                album_pairs = [(str(title or ""), str(norm or "")) for title, norm in albums if str(norm or "").strip()]
                if album_pairs:
                    with conn.cursor() as cur:
                        placeholders = ",".join(["%s"] * len(album_pairs))
                        norms = [norm for _, norm in album_pairs]
                        cur.execute(
                            f"""
                            SELECT title_norm, updated_at
                            FROM files_album_profiles
                            WHERE artist_norm = %s AND title_norm IN ({placeholders})
                            """,
                            [artist_norm, *norms],
                        )
                        existing = {str(r[0] or ""): r[1] for r in cur.fetchall()}
                    to_fetch: list[tuple[str, str]] = []
                    for title, norm in album_pairs:
                        if norm not in existing or _is_profile_stale(existing.get(norm)):
                            to_fetch.append((title, norm))
                    for title, norm in to_fetch[:120]:
                        info = _fetch_lastfm_album_info(artist_name, title) or {}
                        if not info:
                            continue
                        full_desc = _strip_html_text((info.get("wiki_content") or info.get("wiki_summary") or "").strip())
                        short_desc = _truncate_text(info.get("wiki_summary") or full_desc, max_chars=280)
                        profile = {
                            "description": full_desc,
                            "short_description": short_desc,
                            "tags": info.get("toptags") or [],
                            "source": "lastfm",
                        }
                        with conn.transaction():
                            _files_upsert_album_profile(conn, artist_norm, norm, title, profile)
        finally:
            conn.close()
        _files_cache_invalidate_all()
    except Exception as e:
        logging.debug("Artist profile enrichment failed for %s: %s", artist_name, e)
    finally:
        with _files_profile_jobs_lock:
            _files_profile_jobs_active.discard(job_key)


def _enqueue_files_profile_enrichment(artist_name: str, artist_norm: str, albums: list[tuple[str, str]]) -> bool:
    if not artist_norm:
        return False
    job_key = f"{artist_norm}"
    with _files_profile_jobs_lock:
        if job_key in _files_profile_jobs_active:
            return True
        _files_profile_jobs_active.add(job_key)
    threading.Thread(
        target=_run_files_profile_enrichment_job,
        kwargs={
            "job_key": job_key,
            "artist_name": artist_name,
            "artist_norm": artist_norm,
            "albums": albums,
        },
        daemon=True,
        name=f"profile-enrich-{artist_norm[:24]}",
    ).start()
    return True


_files_similar_images_jobs_lock = threading.Lock()
_files_similar_images_jobs_active: set[str] = set()


def _run_files_similar_images_warm_job(*, job_key: str, artist_norm: str, names: list[str]) -> None:
    """
    Best-effort background job: cache external images for a list of artist names.
    Intended to make "Similar artists" grids look good even when providers return placeholders.
    """
    try:
        conn = _files_pg_connect()
        if conn is None:
            return
        try:
            # Deduplicate names while preserving order.
            uniq: list[str] = []
            seen: set[str] = set()
            for n in (names or [])[:24]:
                s = str(n or "").strip()
                if not s:
                    continue
                key = _norm_artist_key(s)
                if not key or key in seen:
                    continue
                seen.add(key)
                uniq.append(s)

            if not uniq:
                return

            for sname in uniq[:12]:
                key = _norm_artist_key(sname)
                if not key:
                    continue

                # Skip if local library already has a real on-disk artist image.
                try:
                    with conn.cursor() as cur:
                        cur.execute("SELECT has_image FROM files_artists WHERE name_norm = %s", (key,))
                        r = cur.fetchone()
                    if r and bool(r[0]):
                        continue
                except Exception:
                    pass

                # Skip if an external cached image already exists and is fresh.
                try:
                    ext = _files_get_external_artist_images(conn, [key]).get(key) or {}
                    if ext and not bool(ext.get("stale")) and str(ext.get("image_path") or "").strip():
                        continue
                except Exception:
                    pass

                # 1) Last.fm artist.getInfo (best coverage when configured).
                img_url = ""
                lf_mbid = ""
                try:
                    lf = _fetch_lastfm_artist_info(sname) or {}
                    img_url = str(lf.get("image_url") or "").strip()
                    lf_mbid = str(lf.get("mbid") or "").strip()
                except Exception:
                    img_url = ""
                    lf_mbid = ""

                # 1a) Fanart.tv (MBID-based) ‚Äì prefer when available (often higher quality than placeholders).
                if lf_mbid:
                    try:
                        f_url = (_fetch_artist_image_fanart(lf_mbid) or "").strip()
                    except Exception:
                        f_url = ""
                    if f_url:
                        try:
                            with conn.transaction():
                                outp = _files_cache_external_artist_image(conn, artist_name=sname, provider="fanart", image_url=f_url, max_px=640)
                            if outp:
                                continue
                        except Exception:
                            pass

                # 1b) Last.fm image (fallback).
                if img_url and not _is_probably_placeholder_artist_image_url(img_url):
                    try:
                        with conn.transaction():
                            outp = _files_cache_external_artist_image(conn, artist_name=sname, provider="lastfm", image_url=img_url, max_px=640)
                        if outp:
                            continue
                    except Exception:
                        pass

                # 2) Wikipedia lead image as fallback (no API key).
                try:
                    wiki_img = ""
                    for query, lang in (
                        (sname, "en"),
                        (f"{sname} musician", "en"),
                        (f"{sname} band", "en"),
                        (sname, "fr"),
                    ):
                        w = _fetch_wikipedia_artist_bio(query, lang=lang) or {}
                        if not isinstance(w, dict):
                            continue
                        w_img = str(w.get("image_url") or "").strip()
                        if w_img:
                            wiki_img = w_img
                            break
                except Exception:
                    wiki_img = ""
                if wiki_img:
                    try:
                        with conn.transaction():
                            outp = _files_cache_external_artist_image(conn, artist_name=sname, provider="wikipedia", image_url=wiki_img, max_px=640)
                        if outp:
                            continue
                    except Exception:
                        pass

                # 3) TheAudioDB (name-based) ‚Äì optional.
                try:
                    aurl = (_fetch_artist_image_audiodb(sname) or "").strip()
                except Exception:
                    aurl = ""
                if aurl:
                    try:
                        with conn.transaction():
                            outp = _files_cache_external_artist_image(conn, artist_name=sname, provider="audiodb", image_url=aurl, max_px=640)
                        if outp:
                            continue
                    except Exception:
                        pass

                # 4) Discogs artist image (often available when Wikipedia isn't).
                try:
                    durl = (_fetch_artist_image_discogs(sname) or "").strip()
                except Exception:
                    durl = ""
                if durl:
                    try:
                        with conn.transaction():
                            outp = _files_cache_external_artist_image(conn, artist_name=sname, provider="discogs", image_url=durl, max_px=640)
                        if outp:
                            continue
                    except Exception:
                        pass

                # 5) Web search fallback (Serper -> og:image). Last resort.
                try:
                    wurl = (_fetch_artist_image_web(sname) or "").strip()
                except Exception:
                    wurl = ""
                if wurl:
                    try:
                        with conn.transaction():
                            outp = _files_cache_external_artist_image(conn, artist_name=sname, provider="web", image_url=wurl, max_px=640)
                        if outp:
                            continue
                    except Exception:
                        pass

                time.sleep(0.12)
        finally:
            try:
                conn.close()
            except Exception:
                pass
            _files_cache_invalidate_all()
    except Exception:
        pass
    finally:
        with _files_similar_images_jobs_lock:
            _files_similar_images_jobs_active.discard(job_key)


def _enqueue_files_similar_images_warm(artist_norm: str, names: list[str]) -> bool:
    key = str(artist_norm or "").strip()
    if not key:
        return False
    job_key = f"similar-images:{key}"
    with _files_similar_images_jobs_lock:
        if job_key in _files_similar_images_jobs_active:
            return True
        _files_similar_images_jobs_active.add(job_key)
    threading.Thread(
        target=_run_files_similar_images_warm_job,
        kwargs={"job_key": job_key, "artist_norm": key, "names": list(names or [])},
        daemon=True,
        name=f"similar-img-{key[:24]}",
    ).start()
    return True


_FILES_PROFILE_BACKFILL_ON_REBUILD = True
_files_profile_backfill_lock = threading.Lock()
_files_profile_backfill_state: dict = {
    "running": False,
    "reason": "",
    "started_at": 0,
    "finished_at": 0,
    "current": 0,
    "total": 0,
    "current_artist": "",
    "errors": 0,
}


def _run_files_profile_backfill(*, reason: str = "manual", sleep_sec: float = 0.15) -> None:
    """
    Background job: progressively enrich all artists (bios/tags/similar) and cache external artist images.
    This is intentionally throttled to avoid provider rate limits.
    """
    sleep_sec = max(0.0, min(2.0, float(sleep_sec or 0.0)))
    try:
        conn = _files_pg_connect()
        if conn is None:
            return
        try:
            with conn.cursor() as cur:
                cur.execute(
                    """
                    SELECT id, name, name_norm
                    FROM files_artists
                    ORDER BY album_count DESC, name ASC
                    """
                )
                artists = [(int(r[0] or 0), str(r[1] or "").strip(), str(r[2] or "").strip()) for r in cur.fetchall()]
        finally:
            conn.close()

        with _files_profile_backfill_lock:
            _files_profile_backfill_state["running"] = True
            _files_profile_backfill_state["reason"] = str(reason or "manual")
            _files_profile_backfill_state["started_at"] = int(time.time())
            _files_profile_backfill_state["finished_at"] = 0
            _files_profile_backfill_state["current"] = 0
            _files_profile_backfill_state["total"] = int(len(artists))
            _files_profile_backfill_state["current_artist"] = ""
            _files_profile_backfill_state["errors"] = 0

        for idx, (_aid, artist_name, artist_norm) in enumerate(artists, start=1):
            if not artist_norm or not artist_name:
                continue
            with _files_profile_backfill_lock:
                if not bool(_files_profile_backfill_state.get("running")):
                    break
                _files_profile_backfill_state["current"] = int(idx)
                _files_profile_backfill_state["current_artist"] = artist_name
            # Avoid duplicate work when a user is browsing the same artist.
            with _files_profile_jobs_lock:
                if artist_norm in _files_profile_jobs_active:
                    continue
                _files_profile_jobs_active.add(artist_norm)
            try:
                _run_files_profile_enrichment_job(
                    job_key=artist_norm,
                    artist_name=artist_name,
                    artist_norm=artist_norm,
                    albums=[],
                    skip_album_profiles=True,
                )
            except Exception:
                with _files_profile_backfill_lock:
                    _files_profile_backfill_state["errors"] = int(_files_profile_backfill_state.get("errors") or 0) + 1
            if sleep_sec:
                time.sleep(sleep_sec)
    finally:
        with _files_profile_backfill_lock:
            _files_profile_backfill_state["running"] = False
            _files_profile_backfill_state["finished_at"] = int(time.time())
            _files_profile_backfill_state["current_artist"] = ""


def _trigger_files_profile_backfill_async(reason: str = "manual") -> bool:
    with _files_profile_backfill_lock:
        if bool(_files_profile_backfill_state.get("running")):
            return False
        # Mark running here to close race conditions (thread may start a bit later).
        _files_profile_backfill_state["running"] = True
        _files_profile_backfill_state["reason"] = str(reason or "manual")
        _files_profile_backfill_state["started_at"] = int(time.time())
        _files_profile_backfill_state["finished_at"] = 0
        _files_profile_backfill_state["current"] = 0
        _files_profile_backfill_state["total"] = 0
        _files_profile_backfill_state["current_artist"] = ""
        _files_profile_backfill_state["errors"] = 0
    threading.Thread(
        target=_run_files_profile_backfill,
        kwargs={"reason": str(reason or "manual")},
        daemon=True,
        name="files-profile-backfill",
    ).start()
    return True


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Assistant (RAG + Chat) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
_ASSISTANT_GC_INTERVAL_SEC = 24 * 3600
_ASSISTANT_SESSION_MAX_AGE_DAYS = 90
_ASSISTANT_SESSION_HARD_CAP = 200


def call_ai_provider_longform(provider: str, model: str, system_msg: str, user_msg: str, max_tokens: int = 800) -> str:
    """AI call that allows multi-line answers (unlike call_ai_provider which uses stop='\\n')."""
    provider_lower = (provider or "").strip().lower()
    if provider_lower == "openai":
        if not openai_client:
            raise ValueError("OpenAI client not initialized")
        param_style = getattr(sys.modules[__name__], "RESOLVED_PARAM_STYLE", "mct")
        _kwargs = {
            "model": model,
            "messages": [
                {"role": "system", "content": system_msg},
                {"role": "user", "content": user_msg},
            ],
        }
        if param_style == "mct":
            _kwargs["max_completion_tokens"] = max_tokens
        else:
            _kwargs["max_tokens"] = max_tokens
        try:
            resp = openai_client.chat.completions.create(**_kwargs)
            return (resp.choices[0].message.content or "").strip()
        except Exception as e:
            err_msg = str(e).lower()
            # Retry with other token parameter style when OpenAI errors on one of them.
            if "unsupported_parameter" in err_msg or "400" in err_msg:
                if "max_completion_tokens" in err_msg and ("max_tokens" in err_msg or "use" in err_msg):
                    _kwargs.pop("max_completion_tokens", None)
                    _kwargs["max_tokens"] = max_tokens
                    resp = openai_client.chat.completions.create(**_kwargs)
                    return (resp.choices[0].message.content or "").strip()
                if "max_tokens" in err_msg and "max_completion_tokens" in err_msg:
                    _kwargs.pop("max_tokens", None)
                    _kwargs["max_completion_tokens"] = max_tokens
                    resp = openai_client.chat.completions.create(**_kwargs)
                    return (resp.choices[0].message.content or "").strip()
            raise

    if provider_lower == "anthropic":
        if not anthropic_client:
            raise ValueError("Anthropic client not initialized")
        resp = anthropic_client.messages.create(
            model=model,
            max_tokens=max_tokens,
            system=system_msg,
            messages=[{"role": "user", "content": user_msg}],
        )
        return resp.content[0].text.strip()

    if provider_lower == "google":
        if not google_client_configured or not google_client:
            raise ValueError("Google client not configured")
        response = google_client.models.generate_content(
            model=model,
            contents=(user_msg or ""),
            config=genai.types.GenerateContentConfig(
                systemInstruction=(system_msg or ""),
                maxOutputTokens=max_tokens,
            ),
        )
        return (getattr(response, "text", "") or "").strip()

    if provider_lower == "ollama":
        if not ollama_url:
            raise ValueError("Ollama URL not configured")
        payload = {
            "model": model,
            "messages": [
                {"role": "system", "content": system_msg},
                {"role": "user", "content": user_msg},
            ],
            "options": {
                "num_predict": max_tokens,
            },
            "stream": False,
        }
        response = requests.post(f"{ollama_url}/api/chat", json=payload, timeout=120)
        if response.status_code != 200:
            raise Exception(f"Ollama API error: {response.status_code} - {response.text}")
        result = response.json()
        return (result.get("message", {}) or {}).get("content", "").strip()

    raise ValueError(f"Unknown AI provider: {provider}")


def _assistant_text_hash(text: str) -> str:
    return hashlib.sha1((text or "").encode("utf-8", errors="ignore")).hexdigest()


def _assistant_chunk_text(text: str, max_chars: int = 900) -> list[str]:
    """Chunk text roughly by paragraphs, capped by max_chars (ASCII-friendly)."""
    raw = (text or "").strip()
    if not raw:
        return []
    paras = [p.strip() for p in re.split(r"\n\s*\n+", raw) if p and p.strip()]
    chunks: list[str] = []
    buf = ""
    for p in paras:
        if not buf:
            buf = p
            continue
        if len(buf) + 2 + len(p) <= max_chars:
            buf = f"{buf}\n\n{p}"
            continue
        chunks.append(buf)
        buf = p
    if buf:
        chunks.append(buf)

    # Hard split any oversize chunk.
    out: list[str] = []
    for c in chunks:
        if len(c) <= max_chars:
            out.append(c)
            continue
        start = 0
        while start < len(c):
            out.append(c[start : start + max_chars].strip())
            start += max_chars
    return [c for c in out if c]


def _assistant_upsert_doc(
    conn,
    *,
    entity_type: str,
    entity_id: int,
    doc_type: str,
    source: str,
    provider: str = "",
    model: str = "",
    title: str = "",
    url: str = "",
    lang: str = "",
    content: str,
) -> int | None:
    """Upsert a document and (re)build its chunk embeddings when content changes."""
    et = (entity_type or "").strip().lower()
    dt = (doc_type or "").strip().lower()
    src = (source or "").strip().lower() or "unknown"
    if not et or int(entity_id or 0) <= 0 or not dt:
        return None
    body = (content or "").strip()
    if not body:
        return None
    h = _assistant_text_hash(body)
    title = (title or "").strip()
    url = (url or "").strip()
    lang = (lang or "").strip()
    provider = (provider or "").strip()
    model = (model or "").strip()

    with conn.cursor() as cur:
        cur.execute(
            """
            SELECT id, content_hash
            FROM assistant_docs
            WHERE entity_type = %s AND entity_id = %s AND doc_type = %s AND source = %s
            """,
            (et, int(entity_id), dt, src),
        )
        row = cur.fetchone()
        if row:
            doc_id = int(row[0])
            prev_hash = str(row[1] or "")
            if prev_hash == h:
                return doc_id
        else:
            doc_id = 0
            prev_hash = ""

    # Upsert document.
    with conn.transaction():
        with conn.cursor() as cur:
            cur.execute(
                """
                INSERT INTO assistant_docs(
                    entity_type, entity_id, doc_type, source, provider, model, title, url, lang, content, content_hash, created_at, updated_at
                )
                VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, NOW(), NOW())
                ON CONFLICT (entity_type, entity_id, doc_type, source) DO UPDATE
                SET provider = EXCLUDED.provider,
                    model = EXCLUDED.model,
                    title = EXCLUDED.title,
                    url = EXCLUDED.url,
                    lang = EXCLUDED.lang,
                    content = EXCLUDED.content,
                    content_hash = EXCLUDED.content_hash,
                    updated_at = NOW()
                RETURNING id
                """,
                (et, int(entity_id), dt, src, provider, model, title, url, lang, body, h),
            )
            doc_id = int((cur.fetchone() or [0])[0] or 0)
            if doc_id <= 0:
                return None
            if prev_hash and prev_hash == h:
                return doc_id

            # Rebuild chunks only when content changed.
            cur.execute("DELETE FROM assistant_doc_chunks WHERE doc_id = %s", (doc_id,))
            chunks = _assistant_chunk_text(body, max_chars=900)
            if not chunks:
                return doc_id
            rows = []
            for idx, chunk in enumerate(chunks):
                vec, norm = _build_hashed_embedding(chunk, RECO_EMBED_DIM)
                rows.append((doc_id, idx, chunk, json.dumps(vec), float(norm or 1.0)))
            cur.executemany(
                """
                INSERT INTO assistant_doc_chunks(doc_id, chunk_index, content, embed_json, norm, created_at)
                VALUES (%s, %s, %s, %s, %s, NOW())
                """,
                rows,
            )
    return doc_id


_ASSISTANT_LIBRARY_SNAPSHOT_REFRESH_SEC = 15 * 60


def _assistant_ingest_library_rag(conn) -> dict:
    """
    Ensure RAG docs exist for library-level questions (counts/facets).

    Keeps a lightweight "library snapshot" document in Postgres so the assistant can answer
    collection-wide questions without guessing.
    """
    # Throttle rebuilds; this runs in the request path.
    try:
        with conn.cursor() as cur:
            cur.execute("SELECT value FROM files_index_meta WHERE key = %s", ("assistant_library_snapshot_ts",))
            row = cur.fetchone()
        last_ts = float(row[0] or 0) if row and row[0] else 0.0
    except Exception:
        last_ts = 0.0
    if last_ts > 0 and (time.time() - last_ts) < _ASSISTANT_LIBRARY_SNAPSHOT_REFRESH_SEC:
        return {"skipped": True}

    try:
        with conn.cursor() as cur:
            cur.execute("SELECT COUNT(*) FROM files_artists")
            artists = int((cur.fetchone() or [0])[0] or 0)
            cur.execute("SELECT COUNT(*) FROM files_albums")
            albums = int((cur.fetchone() or [0])[0] or 0)
            cur.execute("SELECT COUNT(*) FROM files_tracks")
            tracks = int((cur.fetchone() or [0])[0] or 0)

            # Top genres (multi-genre tags_json first, then legacy alb.genre).
            cur.execute(
                """
                WITH genre_tokens AS (
                    SELECT
                        alb.id AS album_id,
                        LOWER(TRIM(g.value)) AS genre
                    FROM files_albums alb
                    CROSS JOIN LATERAL jsonb_array_elements_text(COALESCE(alb.tags_json, '[]')::jsonb) AS g(value)
                    WHERE COALESCE(TRIM(g.value), '') <> ''
                    UNION ALL
                    SELECT
                        alb.id AS album_id,
                        LOWER(TRIM(alb.genre)) AS genre
                    FROM files_albums alb
                    WHERE COALESCE(TRIM(alb.genre), '') <> ''
                      AND COALESCE(alb.tags_json, '[]') = '[]'
                )
                SELECT genre, COUNT(DISTINCT album_id) AS c
                FROM genre_tokens
                WHERE COALESCE(genre, '') <> ''
                GROUP BY genre
                ORDER BY c DESC, genre ASC
                LIMIT 20
                """
            )
            genre_rows = cur.fetchall()

            # Top labels.
            cur.execute(
                """
                SELECT TRIM(label) AS label, COUNT(*) AS c
                FROM files_albums
                WHERE COALESCE(TRIM(label), '') <> ''
                GROUP BY TRIM(label)
                ORDER BY c DESC, label ASC
                LIMIT 20
                """
            )
            label_rows = cur.fetchall()

            # Recently indexed albums (acts like "recently added").
            cur.execute(
                """
                SELECT
                    alb.id,
                    alb.title,
                    COALESCE(alb.year, 0) AS year,
                    ar.id AS artist_id,
                    ar.name AS artist_name
                FROM files_albums alb
                JOIN files_artists ar ON ar.id = alb.artist_id
                ORDER BY alb.created_at DESC, alb.id DESC
                LIMIT 20
                """
            )
            recent_rows = cur.fetchall()
    except Exception:
        logging.debug("Assistant library snapshot build failed", exc_info=True)
        return {}

    lines: list[str] = []
    lines.append("PMDA Library snapshot (local files)")
    lines.append(f"Artists: {artists}")
    lines.append(f"Albums: {albums}")
    lines.append(f"Tracks: {tracks}")

    if genre_rows:
        lines.append("")
        lines.append("Top genres (by #albums):")
        for g, c in genre_rows[:15]:
            gg = str(g or "").strip()
            if not gg:
                continue
            lines.append(f"- {gg}: {int(c or 0)}")

    if label_rows:
        lines.append("")
        lines.append("Top labels (by #albums):")
        for lab, c in label_rows[:12]:
            ll = str(lab or "").strip()
            if not ll:
                continue
            lines.append(f"- {ll}: {int(c or 0)}")

    if recent_rows:
        lines.append("")
        lines.append("Recently added albums (most recent first):")
        for aid, title, year, artist_id, artist_name in recent_rows[:15]:
            y = int(year or 0)
            year_txt = str(y) if y > 0 else "‚Äî"
            t = str(title or "").strip()
            a = str(artist_name or "").strip()
            if not (t and a):
                continue
            lines.append(f"- {a} ¬∑ {year_txt} ¬∑ {t} ¬∑ album_id={int(aid or 0)} ¬∑ artist_id={int(artist_id or 0)}")

    # Upsert the snapshot doc for RAG.
    try:
        _assistant_upsert_doc(
            conn,
            entity_type="library",
            entity_id=1,
            doc_type="library_snapshot",
            source="pmda_db",
            title="PMDA Library",
            content="\n".join(lines),
        )
        with conn.transaction():
            with conn.cursor() as cur:
                _files_index_write_meta(cur, "assistant_library_snapshot_ts", str(int(time.time())))
    except Exception:
        logging.debug("Assistant library snapshot upsert failed", exc_info=True)

    return {"artists": artists, "albums": albums, "tracks": tracks}


def _assistant_ingest_artist_rag(conn, artist_id: int) -> dict:
    """Ensure RAG docs exist for an artist. Returns minimal context info for citations."""
    artist_id = int(artist_id or 0)
    if artist_id <= 0:
        return {}
    with conn.cursor() as cur:
        cur.execute("SELECT id, name, name_norm FROM files_artists WHERE id = %s", (artist_id,))
        row = cur.fetchone()
        if not row:
            return {}
        artist_name = (row[1] or "").strip()
        artist_norm = (row[2] or "").strip() or " ".join((artist_name or "").split()).lower()

        cur.execute(
            """
            SELECT bio, short_bio, tags_json, similar_json, source, updated_at
            FROM files_artist_profiles
            WHERE name_norm = %s
            """,
            (artist_norm,),
        )
        prof_row = cur.fetchone()
        bio = (prof_row[0] or "").strip() if prof_row else ""
        short_bio = (prof_row[1] or "").strip() if prof_row else ""
        tags_json = (prof_row[2] or "").strip() if prof_row else ""
        similar_json = (prof_row[3] or "").strip() if prof_row else ""
        prof_source = (prof_row[4] or "").strip() if prof_row else ""
        prof_updated_at = prof_row[5] if prof_row else None

        try:
            tags = json.loads(tags_json) if tags_json else []
        except Exception:
            tags = []
        if not isinstance(tags, list):
            tags = []
        try:
            similar = json.loads(similar_json) if similar_json else []
        except Exception:
            similar = []
        if not isinstance(similar, list):
            similar = []

        cur.execute(
            """
            SELECT id, title, title_norm, COALESCE(year, 0) AS year, track_count, COALESCE(format, ''), is_lossless, has_cover
            FROM files_albums
            WHERE artist_id = %s
            ORDER BY COALESCE(year, 0) DESC, title ASC
            LIMIT 160
            """,
            (artist_id,),
        )
        album_rows = cur.fetchall()

        # Local genre cues to disambiguate common-name artists (e.g. Last.fm "multiple artists using this name").
        try:
            cur.execute(
                """
                WITH genre_tokens AS (
                    SELECT
                        LOWER(TRIM(g.value)) AS genre
                    FROM files_albums alb
                    CROSS JOIN LATERAL jsonb_array_elements_text(COALESCE(alb.tags_json, '[]')::jsonb) AS g(value)
                    WHERE alb.artist_id = %s
                      AND COALESCE(TRIM(g.value), '') <> ''
                    UNION ALL
                    SELECT
                        LOWER(TRIM(alb.genre)) AS genre
                    FROM files_albums alb
                    WHERE alb.artist_id = %s
                      AND COALESCE(TRIM(alb.genre), '') <> ''
                      AND COALESCE(alb.tags_json, '[]') = '[]'
                )
                SELECT genre, COUNT(*) AS c
                FROM genre_tokens
                WHERE COALESCE(genre, '') <> ''
                GROUP BY genre
                ORDER BY c DESC, genre ASC
                LIMIT 12
                """,
                (artist_id, artist_id),
            )
            genre_rows = cur.fetchall()
        except Exception:
            genre_rows = []

    if bio:
        _assistant_upsert_doc(
            conn,
            entity_type="artist",
            entity_id=artist_id,
            doc_type="artist_profile_bio",
            source=prof_source or "unknown",
            title=artist_name,
            content=bio,
        )
    if short_bio and (not bio or len(bio) < 140):
        _assistant_upsert_doc(
            conn,
            entity_type="artist",
            entity_id=artist_id,
            doc_type="artist_profile_short",
            source=prof_source or "unknown",
            title=artist_name,
            content=short_bio,
        )

    # Always ingest a local library snapshot for factual "what do I own" questions.
    lines = []
    lines.append(f"Artist: {artist_name}")
    lines.append(f"Local albums: {len(album_rows)}")
    try:
        if genre_rows:
            parts = []
            for g, c in genre_rows[:12]:
                gg = str(g or "").strip()
                if not gg:
                    continue
                try:
                    cc = int(c or 0)
                except Exception:
                    cc = 0
                parts.append(f"{gg} ({cc})" if cc > 1 else gg)
            if parts:
                lines.append("Local genres: " + ", ".join(parts[:12]))
    except Exception:
        pass
    for aid, title, title_norm, year, track_count, fmt, is_lossless, has_cover in album_rows[:160]:
        yr = int(year or 0)
        year_txt = str(yr) if yr > 0 else "‚Äî"
        fmt_txt = (fmt or "").strip().upper() or "‚Äî"
        loss = "lossless" if bool(is_lossless) else "lossy"
        cover = "cover" if bool(has_cover) else "no_cover"
        lines.append(f"- {year_txt} ¬∑ {title} ({int(track_count or 0)} tracks) ¬∑ {fmt_txt} ¬∑ {loss} ¬∑ {cover} ¬∑ album_id={int(aid)}")
    _assistant_upsert_doc(
        conn,
        entity_type="artist",
        entity_id=artist_id,
        doc_type="artist_library_snapshot",
        source="pmda_db",
        title=artist_name,
        content="\n".join(lines),
    )

    # Ingest tags + similar artists for conversational recommendations.
    try:
        if tags:
            tag_txt = ", ".join([str(t or "").strip() for t in tags if str(t or "").strip()][:30])
            if tag_txt:
                _assistant_upsert_doc(
                    conn,
                    entity_type="artist",
                    entity_id=artist_id,
                    doc_type="artist_tags",
                    source=prof_source or "unknown",
                    title=artist_name,
                    content=f"Tags: {tag_txt}",
                )
    except Exception:
        pass

    try:
        if similar:
            s_lines = ["Similar artists (from metadata providers):"]
            for s in similar[:40]:
                if not isinstance(s, dict):
                    continue
                nm = (s.get("name") or "").strip()
                if not nm:
                    continue
                typ = (s.get("type") or "").strip()
                if typ:
                    s_lines.append(f"- {nm} ({typ})")
                else:
                    s_lines.append(f"- {nm}")
            if len(s_lines) > 1:
                _assistant_upsert_doc(
                    conn,
                    entity_type="artist",
                    entity_id=artist_id,
                    doc_type="artist_similar_artists",
                    source=prof_source or "unknown",
                    title=artist_name,
                    content="\n".join(s_lines),
                )
    except Exception:
        pass

    # Ingest cached album snippets/reviews (local-only "journal" use-case).
    try:
        title_norms = [str(r[2] or "").strip() for r in album_rows if str(r[2] or "").strip()]
        title_norms = list(dict.fromkeys(title_norms))[:160]
        prof_map: dict[str, dict] = {}
        if title_norms:
            placeholders = ",".join(["%s"] * len(title_norms))
            with conn.cursor() as cur:
                cur.execute(
                    f"""
                    SELECT title_norm, short_description, source
                    FROM files_album_profiles
                    WHERE artist_norm = %s AND title_norm IN ({placeholders})
                    """,
                    [artist_norm, *title_norms],
                )
                for tn, sd, src in cur.fetchall():
                    key = str(tn or "").strip()
                    if not key:
                        continue
                    prof_map[key] = {"short_description": (sd or "").strip(), "source": (src or "").strip()}

        review_lines: list[str] = []
        for aid, title, title_norm, year, track_count, fmt, is_lossless, has_cover in album_rows[:120]:
            tn = str(title_norm or "").strip()
            if not tn:
                continue
            p = prof_map.get(tn) or {}
            sd = (p.get("short_description") or "").strip()
            if not sd:
                continue
            src = (p.get("source") or "").strip() or "unknown"
            yr = int(year or 0)
            year_txt = str(yr) if yr > 0 else "‚Äî"
            review_lines.append(f"- {year_txt} ¬∑ {title}: {sd} (source={src})")
            if len(review_lines) >= 80:
                break
        if review_lines:
            _assistant_upsert_doc(
                conn,
                entity_type="artist",
                entity_id=artist_id,
                doc_type="artist_album_snippets",
                source="pmda_db",
                title=artist_name,
                content="Album snippets:\n" + "\n".join(review_lines),
            )
    except Exception:
        pass

    # Ingest upcoming concerts if cached (or recently refreshed by the artist page).
    try:
        with conn.cursor() as cur:
            cur.execute(
                """
                SELECT provider, events_json, source_url, updated_at
                FROM files_artist_concerts
                WHERE artist_id = %s
                """,
                (artist_id,),
            )
            crow = cur.fetchone()
        if crow:
            provider = (crow[0] or "").strip().lower() or "bandsintown"
            source_url = (crow[2] or "").strip()
            try:
                events = json.loads(crow[1] or "[]") if crow[1] else []
            except Exception:
                events = []
            if not isinstance(events, list):
                events = []
            if events:
                c_lines = []
                for ev in events[:30]:
                    if not isinstance(ev, dict):
                        continue
                    dt = (ev.get("datetime") or ev.get("date") or "").strip()
                    venue = ev.get("venue") if isinstance(ev.get("venue"), dict) else {}
                    vname = (venue.get("name") or "").strip() if isinstance(venue, dict) else ""
                    city = (venue.get("city") or "").strip() if isinstance(venue, dict) else ""
                    country = (venue.get("country") or "").strip() if isinstance(venue, dict) else ""
                    where = ", ".join([x for x in [city, country] if x])
                    url = (ev.get("url") or "").strip()
                    line = " ¬∑ ".join([x for x in [dt, vname, where, url] if x])
                    if line:
                        c_lines.append(f"- {line}")
                if c_lines:
                    _assistant_upsert_doc(
                        conn,
                        entity_type="artist",
                        entity_id=artist_id,
                        doc_type="artist_concerts_upcoming",
                        source=provider,
                        title=artist_name,
                        url=source_url,
                        content="Upcoming concerts:\n" + "\n".join(c_lines),
                    )
    except Exception:
        pass

    # Ingest extracted facts (AKA/groups/labels/collabs) when available.
    try:
        with conn.cursor() as cur:
            cur.execute(
                """
                SELECT facts_json, source, provider, model, updated_at
                FROM assistant_entity_facts
                WHERE entity_type = 'artist' AND entity_id = %s
                """,
                (artist_id,),
            )
            frow = cur.fetchone()
        if frow:
            try:
                facts = json.loads(frow[0] or "{}") if frow[0] else {}
            except Exception:
                facts = {}
            if not isinstance(facts, dict):
                facts = {}
            src = (frow[1] or "").strip() or "facts"
            provider = (frow[2] or "").strip()
            model = (frow[3] or "").strip()
            lines = []
            for key, label in (
                ("aka", "AKA"),
                ("aliases", "Aliases"),
                ("member_of", "Groups"),
                ("collaborated_with", "Collaborations"),
                ("labels", "Labels"),
                ("notable_cities", "Cities"),
            ):
                val = facts.get(key)
                if not isinstance(val, list):
                    continue
                clean = [str(x or "").strip() for x in val if str(x or "").strip()]
                if not clean:
                    continue
                lines.append(f"{label}: {', '.join(clean[:30])}")
            if lines:
                _assistant_upsert_doc(
                    conn,
                    entity_type="artist",
                    entity_id=artist_id,
                    doc_type="artist_facts_extracted",
                    source=src,
                    provider=provider,
                    model=model,
                    title=artist_name,
                    content="\n".join(lines),
                )
    except Exception:
        pass

    return {
        "artist_id": artist_id,
        "artist_name": artist_name,
        "artist_norm": artist_norm,
        "profile_source": prof_source,
        "profile_updated_at": int(_dt_to_epoch(prof_updated_at)) if prof_updated_at else 0,
    }


def _assistant_find_artist_ids_for_query(conn, query: str, limit: int = 3) -> list[int]:
    q = (query or "").strip()
    if not q:
        return []
    limit = max(1, min(10, int(limit or 3)))
    with conn.cursor() as cur:
        # 1) Strong heuristic: if the message contains an artist name verbatim, prefer that.
        # This is robust for natural language queries like "Quels albums de Rod Stewart...".
        cur.execute(
            """
            SELECT id
            FROM files_artists
            WHERE length(name) >= 3
              AND position(lower(name) in lower(%s)) > 0
            ORDER BY length(name) DESC, album_count DESC, name ASC
            LIMIT %s
            """,
            (q, limit),
        )
        rows = cur.fetchall()
        if rows:
            return [int(r[0] or 0) for r in rows if int(r[0] or 0) > 0]

        # 2) Fallback: similarity / ILIKE when pg_trgm is available.
        like = f"%{q}%"
        try:
            cur.execute(
                """
                SELECT id
                FROM files_artists
                WHERE name ILIKE %s
                ORDER BY similarity(name, %s) DESC, album_count DESC, name ASC
                LIMIT %s
                """,
                (like, q, limit),
            )
        except Exception:
            cur.execute(
                """
                SELECT id
                FROM files_artists
                WHERE name ILIKE %s
                ORDER BY album_count DESC, name ASC
                LIMIT %s
                """,
                (like, limit),
            )
        rows = cur.fetchall()
    return [int(r[0] or 0) for r in rows if int(r[0] or 0) > 0]


def _assistant_fetch_session_messages(conn, session_id: str, limit: int = 12) -> list[dict]:
    session_id = (session_id or "").strip()
    if not session_id:
        return []
    limit = max(1, min(50, int(limit or 12)))
    with conn.cursor() as cur:
        cur.execute(
            """
            SELECT role, content
            FROM assistant_messages
            WHERE session_id = %s
            ORDER BY created_at DESC
            LIMIT %s
            """,
            (session_id, limit),
        )
        rows = cur.fetchall()
    out = []
    for role, content in reversed(rows):
        r = (role or "").strip().lower()
        if r not in {"user", "assistant", "system"}:
            continue
        out.append({"role": r, "content": (content or "").strip()})
    return out


def _assistant_retrieve_chunks(conn, query: str, *, artist_id: int | None = None, k: int = 8) -> dict:
    """Return top-k chunks with citation metadata (artist + library snapshot)."""
    query = (query or "").strip()
    k = max(1, min(16, int(k or 8)))
    artist_id = int(artist_id or 0) if artist_id else 0
    if not query:
        return {"chunks": [], "citations": []}

    doc_ids: list[int] = []
    # Always include library snapshot docs so collection-wide questions have grounding.
    try:
        with conn.cursor() as cur:
            cur.execute(
                """
                SELECT id
                FROM assistant_docs
                WHERE entity_type = 'library' AND entity_id = 1
                ORDER BY updated_at DESC
                """,
            )
            doc_ids.extend([int(r[0] or 0) for r in cur.fetchall() if int(r[0] or 0) > 0])
    except Exception:
        pass

    if artist_id > 0:
        try:
            with conn.cursor() as cur:
                cur.execute(
                    """
                    SELECT id
                    FROM assistant_docs
                    WHERE entity_type = 'artist' AND entity_id = %s
                    ORDER BY updated_at DESC
                    """,
                    (artist_id,),
                )
                doc_ids.extend([int(r[0] or 0) for r in cur.fetchall() if int(r[0] or 0) > 0])
        except Exception:
            pass
    # Deduplicate while preserving order.
    doc_ids = list(dict.fromkeys([int(x) for x in doc_ids if int(x) > 0]))
    if not doc_ids:
        return {"chunks": [], "citations": []}

    q_vec, _q_norm = _build_hashed_embedding(query, RECO_EMBED_DIM)

    with conn.cursor() as cur:
        cur.execute(
            """
            SELECT
                c.id,
                c.content,
                c.embed_json,
                d.entity_type,
                d.entity_id,
                d.doc_type,
                d.source,
                COALESCE(d.title, '')
            FROM assistant_doc_chunks c
            JOIN assistant_docs d ON d.id = c.doc_id
            WHERE c.doc_id = ANY(%s)
            """,
            (doc_ids,),
        )
        rows = cur.fetchall()

    scored = []
    artist_snapshot_item: dict | None = None
    library_snapshot_item: dict | None = None
    for chunk_id, content, embed_json, et, eid, doc_type, source, title in rows:
        emb = _load_embedding_json(embed_json or "[]")
        sim = _vec_cosine(q_vec, emb) if emb else 0.0
        txt = (content or "").strip()
        # Small lexical boost for exact substring match.
        if query.lower() in txt.lower():
            sim += 0.08
        item = (
            {
                "chunk_id": int(chunk_id or 0),
                "score": float(sim),
                "text": txt,
                "citation": {
                    "entity_type": str(et or ""),
                    "entity_id": int(eid or 0),
                    "doc_type": str(doc_type or ""),
                    "source": str(source or ""),
                    "title": str(title or ""),
                },
            }
        )
        scored.append(item)
        # Always keep snapshot chunks so "what do I own" questions are answerable without relying on
        # embedding similarity (artist inventory + collection snapshot).
        dt = str(doc_type or "").strip().lower()
        if dt == "artist_library_snapshot" and artist_snapshot_item is None:
            artist_snapshot_item = item
        if dt == "library_snapshot" and library_snapshot_item is None:
            library_snapshot_item = item
    scored.sort(key=lambda x: float(x.get("score") or 0.0), reverse=True)
    top = scored[: max(k, 1)]

    # Ensure snapshots are present (artist snapshot first, then library snapshot).
    merged = list(top)
    for snap in [artist_snapshot_item, library_snapshot_item]:
        if snap is None:
            continue
        snap_dt = (snap.get("citation") or {}).get("doc_type")
        if snap_dt and any((it.get("citation") or {}).get("doc_type") == snap_dt for it in merged):
            continue
        merged = [snap, *merged]
    if merged != top:
        # Put snapshots first so the model sees local inventory before external bios.
        seen = set()
        deduped = []
        for it in merged:
            cid = int(it.get("chunk_id") or 0)
            if cid and cid in seen:
                continue
            if cid:
                seen.add(cid)
            deduped.append(it)
        top = deduped[: min(14, max(k, 1) + 4)]
    citations = []
    for item in top:
        c = dict(item.get("citation") or {})
        c["chunk_id"] = int(item.get("chunk_id") or 0)
        c["score"] = round(float(item.get("score") or 0.0), 4)
        c["snippet"] = _truncate_text(item.get("text") or "", max_chars=240)
        citations.append(c)
    return {"chunks": top, "citations": citations}


def _assistant_maybe_gc(conn) -> None:
    """Daily GC for assistant sessions/messages to prevent unbounded growth."""
    try:
        with conn.cursor() as cur:
            cur.execute("SELECT value FROM files_index_meta WHERE key = %s", ("assistant_gc_ts",))
            row = cur.fetchone()
        last_ts = float(row[0] or 0) if row and row[0] else 0.0
    except Exception:
        last_ts = 0.0
    if last_ts > 0 and (time.time() - last_ts) < _ASSISTANT_GC_INTERVAL_SEC:
        return
    try:
        with conn.transaction():
            with conn.cursor() as cur:
                cur.execute(
                    f"DELETE FROM assistant_sessions WHERE updated_at < NOW() - INTERVAL '{int(_ASSISTANT_SESSION_MAX_AGE_DAYS)} days'"
                )
                # Hard cap: keep most recent sessions.
                cur.execute(
                    """
                    SELECT session_id
                    FROM assistant_sessions
                    ORDER BY updated_at DESC
                    OFFSET %s
                    """,
                    (int(_ASSISTANT_SESSION_HARD_CAP),),
                )
                extra = [str(r[0] or "").strip() for r in cur.fetchall() if str(r[0] or "").strip()]
                if extra:
                    cur.execute("DELETE FROM assistant_sessions WHERE session_id = ANY(%s)", (extra,))
                _files_index_write_meta(cur, "assistant_gc_ts", str(int(time.time())))
    except Exception:
        # Never block UI/API on GC.
        logging.debug("Assistant GC failed", exc_info=True)


def _assistant_ensure_session(conn, session_id: str | None) -> str:
    sid = (session_id or "").strip()
    if not sid:
        sid = str(uuid.uuid4())
    with conn.transaction():
        with conn.cursor() as cur:
            cur.execute("SELECT session_id FROM assistant_sessions WHERE session_id = %s", (sid,))
            if cur.fetchone():
                cur.execute("UPDATE assistant_sessions SET updated_at = NOW() WHERE session_id = %s", (sid,))
                return sid
            cur.execute(
                """
                INSERT INTO assistant_sessions(session_id, created_at, updated_at)
                VALUES (%s, NOW(), NOW())
                """,
                (sid,),
            )
    return sid


def _assistant_insert_message(conn, *, session_id: str, role: str, content: str, context: dict, metadata: dict) -> dict:
    sid = (session_id or "").strip()
    role_norm = (role or "").strip().lower()
    if role_norm not in {"user", "assistant", "system"}:
        role_norm = "user"
    ctx_json = json.dumps(context or {}, ensure_ascii=True)
    meta_json = json.dumps(metadata or {}, ensure_ascii=True)
    with conn.transaction():
        with conn.cursor() as cur:
            cur.execute(
                """
                INSERT INTO assistant_messages(session_id, role, content, context_json, metadata_json, created_at)
                VALUES (%s, %s, %s, %s, %s, NOW())
                RETURNING id, EXTRACT(EPOCH FROM created_at)::BIGINT
                """,
                (sid, role_norm, (content or "").strip(), ctx_json, meta_json),
            )
            row = cur.fetchone() or [0, 0]
            msg_id = int(row[0] or 0)
            created_at = int(row[1] or 0)
            cur.execute("UPDATE assistant_sessions SET updated_at = NOW() WHERE session_id = %s", (sid,))
    # Include context/metadata in the returned payload so the UI can render citations/links immediately
    # without requiring a separate history fetch.
    return {
        "id": msg_id,
        "role": role_norm,
        "content": (content or "").strip(),
        "created_at": created_at,
        "context": context if isinstance(context, dict) else {},
        "metadata": metadata if isinstance(metadata, dict) else {},
    }


def _assistant_build_prompt(
    *,
    user_message: str,
    retrieved: dict,
    history: list[dict],
    context_info: dict,
) -> tuple[str, str]:
    system_msg = (
        "You are PMDA Intelligence, an audiophile-focused music librarian for a LOCAL library.\n"
        "Rules:\n"
        "- Use only the provided context excerpts. Do not invent facts.\n"
        "- If the answer is not in context, say so and suggest a concrete next step (refresh profile, rescan, etc.).\n"
        "- If a current artist context is provided, focus only on that artist (do not enumerate other artists with the same name unless the user explicitly asks).\n"
        "- If a provider bio mentions multiple artists with the same name, DO NOT repeat the disambiguation; pick the variant that matches the local PMDA cues (local genres, local releases, known collaborators) and ignore the others.\n"
        "- When sources conflict about identity, prefer pmda_db excerpts as the identity anchor.\n"
        "- Answer in the same language as the user.\n"
        "- Prefer structured output (short sections / bullets) when helpful.\n"
    )
    ctx_lines = []
    chunks = retrieved.get("chunks") or []
    for i, item in enumerate(chunks[:12], start=1):
        cit = (item.get("citation") or {})
        label = f"C{i}"
        src = f"{cit.get('source') or 'unknown'}"
        dtype = f"{cit.get('doc_type') or 'doc'}"
        title = f"{cit.get('title') or ''}".strip()
        header = f"[{label}] ({dtype}, source={src}{', title='+title if title else ''})"
        ctx_lines.append(header)
        ctx_lines.append(item.get("text") or "")
        ctx_lines.append("")
    ctx_block = "\n".join(ctx_lines).strip()

    hist_lines = []
    for m in history[-12:]:
        role = (m.get("role") or "").strip().lower()
        content = (m.get("content") or "").strip()
        if not role or not content:
            continue
        prefix = "User" if role == "user" else ("Assistant" if role == "assistant" else "System")
        hist_lines.append(f"{prefix}: {content}")
    hist_block = "\n".join(hist_lines).strip()

    focus = ""
    if context_info.get("artist_name"):
        focus = f"Context: Current artist = {context_info.get('artist_name')}\n"

    user_prompt = (
        f"{focus}"
        f"User question:\n{(user_message or '').strip()}\n\n"
        f"Conversation (most recent last):\n{hist_block if hist_block else '(none)'}\n\n"
        f"Context excerpts:\n{ctx_block if ctx_block else '(none)'}\n"
    )
    return system_msg, user_prompt


def _assistant_simplify_for_intent(text: str) -> str:
    """Lowercase, strip accents, collapse whitespace (good enough for intent heuristics)."""
    t = (text or "").strip().lower()
    if not t:
        return ""
    try:
        t = unicodedata.normalize("NFKD", t)
        t = "".join(ch for ch in t if not unicodedata.combining(ch))
    except Exception:
        pass
    t = re.sub(r"[^a-z0-9]+", " ", t)
    return re.sub(r"\s+", " ", t).strip()


def _assistant_lang_for_message(user_message: str) -> str:
    s = _assistant_simplify_for_intent(user_message)
    if any(tok in s for tok in ["combien", "quel", "quelle", "quels", "bibliotheque", "bibliotheque", "morceau", "artiste", "album", "collection"]):
        return "fr"
    if any(tok in s for tok in ["how many", "library", "collection", "artist", "album", "track", "genre"]):
        return "en"
    try:
        return _assistant_preferred_lang()
    except Exception:
        return "en"


def _assistant_detect_tool_intent(user_message: str, *, context_artist_id: int) -> str | None:
    s = _assistant_simplify_for_intent(user_message)
    if not s:
        return None

    wants_count = any(tok in s for tok in ["combien", "nombre", "how many", "count"])
    if wants_count:
        if "artiste" in s or "artist" in s:
            return "library_count_artists"
        if "album" in s or "albums" in s:
            return "library_count_albums"
        if any(tok in s for tok in ["morceau", "track", "tracks", "titre", "song", "songs"]):
            return "library_count_tracks"

    # "Top" / "most present" / "most represented"
    if ("genre" in s) and any(tok in s for tok in ["plus present", "plus frequent", "most common", "most frequent", "most present"]):
        return "library_top_genres"
    if any(tok in s for tok in ["artistes les plus presents", "artistes les plus presents", "most represented artists", "top artists", "artistes les plus representes"]):
        return "library_top_artists"

    if context_artist_id > 0:
        if ("artiste similaire" in s) or ("artistes similaires" in s) or ("similar artists" in s) or ("similaires" in s and "artiste" in s):
            return "artist_similar_artists"
        if ("album" in s or "albums" in s) and any(tok in s for tok in ["liste", "list", "quels", "which", "dispose", "dans ma collection", "local"]):
            return "artist_list_albums"

    return None


def _assistant_tool_library_counts(conn) -> dict:
    with conn.cursor() as cur:
        cur.execute("SELECT COUNT(*) FROM files_artists")
        artists = int((cur.fetchone() or [0])[0] or 0)
        cur.execute("SELECT COUNT(*) FROM files_albums")
        albums = int((cur.fetchone() or [0])[0] or 0)
        cur.execute("SELECT COUNT(*) FROM files_tracks")
        tracks = int((cur.fetchone() or [0])[0] or 0)
    return {"artists": artists, "albums": albums, "tracks": tracks}


def _assistant_tool_library_top_genres(conn, *, limit: int = 10) -> list[tuple[str, int]]:
    limit = max(1, min(50, int(limit or 10)))
    with conn.cursor() as cur:
        cur.execute(
            """
            WITH genre_tokens AS (
                SELECT
                    alb.id AS album_id,
                    LOWER(TRIM(g.value)) AS genre
                FROM files_albums alb
                CROSS JOIN LATERAL jsonb_array_elements_text(COALESCE(alb.tags_json, '[]')::jsonb) AS g(value)
                WHERE COALESCE(TRIM(g.value), '') <> ''
                UNION ALL
                SELECT
                    alb.id AS album_id,
                    LOWER(TRIM(alb.genre)) AS genre
                FROM files_albums alb
                WHERE COALESCE(TRIM(alb.genre), '') <> ''
                  AND COALESCE(alb.tags_json, '[]') = '[]'
            )
            SELECT genre, COUNT(DISTINCT album_id) AS c
            FROM genre_tokens
            WHERE COALESCE(genre, '') <> ''
            GROUP BY genre
            ORDER BY c DESC, genre ASC
            LIMIT %s
            """,
            (int(limit),),
        )
        rows = cur.fetchall()
    out: list[tuple[str, int]] = []
    for g, c in rows:
        gg = str(g or "").strip()
        if not gg:
            continue
        out.append((gg, int(c or 0)))
    return out


def _assistant_tool_library_top_artists(conn, *, base_url: str, limit: int = 12) -> list[dict]:
    limit = max(1, min(40, int(limit or 12)))
    base = (base_url or "").rstrip("/")
    with conn.cursor() as cur:
        cur.execute(
            """
            SELECT
                a.id,
                a.name,
                a.name_norm,
                a.album_count,
                a.track_count,
                a.has_image,
                COALESCE(ext.image_path, '') AS ext_path
            FROM files_artists a
            LEFT JOIN files_external_artist_images ext ON ext.name_norm = a.name_norm
            ORDER BY a.album_count DESC, a.track_count DESC, a.name ASC
            LIMIT %s
            """,
            (int(limit),),
        )
        rows = cur.fetchall()
    out: list[dict] = []
    for aid, name, name_norm, album_count, track_count, has_image, ext_path in rows:
        artist_id = int(aid or 0)
        nm = str(name or "").strip()
        if artist_id <= 0 or not nm:
            continue
        thumb = None
        if bool(has_image):
            thumb = f"{base}/api/library/files/artist/{artist_id}/image?size=192"
        elif str(ext_path or "").strip():
            thumb = f"{base}/api/library/external/artist-image/{quote(str(name_norm or '').strip(), safe='')}?size=192"
        out.append(
            {
                "artist_id": artist_id,
                "artist_name": nm,
                "name_norm": str(name_norm or "").strip(),
                "album_count": int(album_count or 0),
                "track_count": int(track_count or 0),
                "thumb": thumb,
            }
        )
    return out


def _assistant_tool_artist_list_albums(conn, *, artist_id: int, limit: int = 80) -> tuple[str, list[dict]]:
    artist_id = int(artist_id or 0)
    limit = max(1, min(200, int(limit or 80)))
    with conn.cursor() as cur:
        cur.execute("SELECT name FROM files_artists WHERE id = %s", (artist_id,))
        row = cur.fetchone()
        artist_name = str((row[0] if row else "") or "").strip()
        cur.execute(
            """
            SELECT id, title, COALESCE(year, 0) AS year, track_count, COALESCE(format, '') AS fmt, is_lossless, has_cover
            FROM files_albums
            WHERE artist_id = %s
            ORDER BY COALESCE(year, 0) DESC, title ASC
            LIMIT %s
            """,
            (artist_id, int(limit)),
        )
        rows = cur.fetchall()
    albums: list[dict] = []
    for aid, title, year, track_count, fmt, is_lossless, has_cover in rows:
        albums.append(
            {
                "album_id": int(aid or 0),
                "title": str(title or "").strip(),
                "year": int(year or 0),
                "track_count": int(track_count or 0),
                "format": str(fmt or "").strip(),
                "is_lossless": bool(is_lossless),
                "has_cover": bool(has_cover),
            }
        )
    return artist_name, albums


def _assistant_tool_artist_similar(conn, *, artist_id: int, base_url: str, limit: int = 18) -> tuple[str, str, list[dict]]:
    """Return (artist_name, source, similar_items) with local ids + image_url when possible."""
    artist_id = int(artist_id or 0)
    limit = max(1, min(40, int(limit or 18)))
    base = (base_url or "").rstrip("/")
    with conn.cursor() as cur:
        cur.execute("SELECT name, name_norm FROM files_artists WHERE id = %s", (artist_id,))
        row = cur.fetchone()
        artist_name = str((row[0] if row else "") or "").strip()
        artist_norm = str((row[1] if row else "") or "").strip()
        cur.execute(
            """
            SELECT similar_json, source
            FROM files_artist_profiles
            WHERE name_norm = %s
            """,
            (artist_norm,),
        )
        prow = cur.fetchone()
    source = ""
    similar: list[dict] = []
    if prow:
        source = str(prow[1] or "").strip()
        try:
            sim = json.loads(prow[0] or "[]") if prow[0] else []
        except Exception:
            sim = []
        if isinstance(sim, list):
            for it in sim:
                if isinstance(it, dict) and str(it.get("name") or "").strip():
                    similar.append(dict(it))

    if not similar:
        # Local fallback: overlap genres from the library (works for Bandcamp-only libraries).
        similar = _files_similar_artists_by_genre(conn, artist_id, limit=limit) or []
        source = "pmda_db_genre"

    patched = _files_attach_similar_artist_refs(conn, similar[:limit], base)
    return artist_name, (source or "").strip(), (patched or [])[:limit]


def _assistant_try_handle_tool_query(conn, *, user_message: str, context_artist_id: int, base_url: str) -> dict:
    """
    Fast path: answer common DB-grounded questions without calling the LLM.

    Returns {handled: bool, assistant_text, citations, links, tool}.
    """
    intent = _assistant_detect_tool_intent(user_message, context_artist_id=context_artist_id)
    if not intent:
        return {"handled": False}

    lang = _assistant_lang_for_message(user_message)
    now_ts = int(time.time())

    def _cit(entity_type: str, entity_id: int, title: str, snippet: str, doc_type: str = "sql_tool") -> list[dict]:
        return [
            {
                "entity_type": entity_type,
                "entity_id": int(entity_id or 0),
                "doc_type": doc_type,
                "source": "pmda_db",
                "title": title,
                "chunk_id": -1,
                "score": 1.0,
                "snippet": _truncate_text(snippet or "", max_chars=240),
            }
        ]

    if intent in {"library_count_artists", "library_count_albums", "library_count_tracks"}:
        counts = _assistant_tool_library_counts(conn)
        if lang == "fr":
            text = (
                "Stats de ta bibliotheque locale (PMDA):\n"
                f"- Artistes: {counts['artists']}\n"
                f"- Albums: {counts['albums']}\n"
                f"- Morceaux: {counts['tracks']}\n"
            )
        else:
            text = (
                "Local library stats (PMDA):\n"
                f"- Artists: {counts['artists']}\n"
                f"- Albums: {counts['albums']}\n"
                f"- Tracks: {counts['tracks']}\n"
            )
        return {
            "handled": True,
            "tool": intent,
            "assistant_text": text.strip(),
            "citations": _cit("library", 1, "Library stats", f"artists={counts['artists']}, albums={counts['albums']}, tracks={counts['tracks']}"),
            "links": [],
            "ts": now_ts,
        }

    if intent == "library_top_genres":
        top = _assistant_tool_library_top_genres(conn, limit=12)
        if not top:
            text = "Je n'ai trouve aucun genre exploitable dans ta bibliotheque." if lang == "fr" else "I could not find any usable genres in your library."
            return {"handled": True, "tool": intent, "assistant_text": text, "citations": _cit("library", 1, "Library genres", "no genres found"), "links": [], "ts": now_ts}
        if lang == "fr":
            lines = ["Genres les plus presents (par #albums):"]
        else:
            lines = ["Most present genres (by #albums):"]
        for g, c in top[:12]:
            lines.append(f"- {g}: {c}")
        snippet = ", ".join([f"{g}={c}" for g, c in top[:6]])
        return {"handled": True, "tool": intent, "assistant_text": "\n".join(lines), "citations": _cit("library", 1, "Library genres", snippet), "links": [], "ts": now_ts}

    if intent == "library_top_artists":
        items = _assistant_tool_library_top_artists(conn, base_url=base_url, limit=16)
        if not items:
            text = "Je n'ai trouve aucun artiste dans la base." if lang == "fr" else "I couldn't find any artists in the database."
            return {"handled": True, "tool": intent, "assistant_text": text, "citations": _cit("library", 1, "Library artists", "no artists found"), "links": [], "ts": now_ts}
        if lang == "fr":
            lines = ["Artistes les plus presents (par #albums):"]
        else:
            lines = ["Most represented artists (by #albums):"]
        links: list[dict] = []
        for it in items[:16]:
            nm = str(it.get("artist_name") or "").strip()
            aid = int(it.get("artist_id") or 0)
            ac = int(it.get("album_count") or 0)
            if not nm or aid <= 0:
                continue
            lines.append(f"- {nm}: {ac} albums")
            links.append(
                {
                    "kind": "internal",
                    "label": nm,
                    "href": f"/library/artist/{aid}",
                    "entity_type": "artist",
                    "entity_id": aid,
                    "thumb": it.get("thumb"),
                }
            )
        snippet = "; ".join([f"{it.get('artist_name')}={it.get('album_count')}" for it in items[:6]])
        return {"handled": True, "tool": intent, "assistant_text": "\n".join(lines), "citations": _cit("library", 1, "Top artists", snippet), "links": links[:24], "ts": now_ts}

    if intent == "artist_list_albums" and int(context_artist_id or 0) > 0:
        artist_name, albums = _assistant_tool_artist_list_albums(conn, artist_id=int(context_artist_id), limit=120)
        if lang == "fr":
            lines = [f"Albums locaux pour {artist_name or f'Artist #{int(context_artist_id)}'} ({len(albums)}):"]
        else:
            lines = [f"Local albums for {artist_name or f'Artist #{int(context_artist_id)}'} ({len(albums)}):"]
        for a in albums[:120]:
            yr = int(a.get("year") or 0)
            year_txt = str(yr) if yr > 0 else "‚Äî"
            fmt = str(a.get("format") or "").strip().upper() or "‚Äî"
            loss = "lossless" if bool(a.get("is_lossless")) else "lossy"
            lines.append(f"- {year_txt} ¬∑ {a.get('title') or ''} ({int(a.get('track_count') or 0)} tracks) ¬∑ {fmt} ¬∑ {loss} ¬∑ album_id={int(a.get('album_id') or 0)}")
        snippet = ", ".join([str(a.get("title") or "") for a in albums[:6] if str(a.get("title") or "").strip()])
        return {"handled": True, "tool": intent, "assistant_text": "\n".join(lines).strip(), "citations": _cit("artist", int(context_artist_id), artist_name or "Artist albums", snippet), "links": [], "ts": now_ts}

    if intent == "artist_similar_artists" and int(context_artist_id or 0) > 0:
        artist_name, source, sim = _assistant_tool_artist_similar(conn, artist_id=int(context_artist_id), base_url=base_url, limit=20)
        if lang == "fr":
            lines = [f"Artistes similaires a {artist_name or f'Artist #{int(context_artist_id)}'} (source: {source or 'unknown'}):"]
        else:
            lines = [f"Similar artists to {artist_name or f'Artist #{int(context_artist_id)}'} (source: {source or 'unknown'}):"]
        links: list[dict] = []
        for it in (sim or [])[:20]:
            nm = str(it.get("name") or "").strip()
            if not nm:
                continue
            local_id = int(it.get("artist_id") or 0)
            typ = str(it.get("type") or "").strip()
            label = f"{nm} ({typ})" if typ else nm
            if local_id > 0:
                lines.append(f"- {label} (dans ta librairie)")
                links.append(
                    {
                        "kind": "internal",
                        "label": nm,
                        "href": f"/library/artist/{local_id}",
                        "entity_type": "artist",
                        "entity_id": local_id,
                        "thumb": it.get("image_url"),
                    }
                )
            else:
                lines.append(f"- {label}")
                # Generic external lookup (non-local artists).
                links.append(
                    {
                        "kind": "external",
                        "label": nm,
                        "href": f"https://bandcamp.com/search?q={quote(nm)}",
                        "entity_type": "artist",
                        "entity_id": 0,
                        "thumb": it.get("image_url"),
                    }
                )
        snippet = ", ".join([str(it.get("name") or "") for it in (sim or [])[:8] if str(it.get("name") or "").strip()])
        return {"handled": True, "tool": intent, "assistant_text": "\n".join(lines).strip(), "citations": _cit("artist", int(context_artist_id), artist_name or "Similar artists", snippet, doc_type="artist_similar_tool"), "links": links[:28], "ts": now_ts}

    return {"handled": False}


def _assistant_should_try_sql_agent(user_message: str) -> bool:
    """
    Return True when the user's question looks like a library/stats query where we should
    attempt a DB-backed SQL answer (instead of biography/interpretation questions).
    """
    s = _assistant_simplify_for_intent(user_message)
    if not s:
        return False

    # Avoid running the SQL agent for biography-like prompts; those should use RAG docs.
    if any(tok in s for tok in ["qui est", "who is", "biographie", "biography", "resume", "resumer", "summarize", "tell me about", "dis m en plus"]):
        return False

    # Strong signals that the question is about the user's local collection / statistics.
    strong = [
        "ma collection", "ma bibliotheque", "bibliotheque", "collection", "my library", "library",
        "dans ma", "dans mon", "j ai", "ai je", "do i have", "owned",
        "combien", "nombre", "how many", "count",
        "stat", "stats", "statistique", "statistiques",
        "ecoute", "ecoutes", "listened", "played", "playback", "plays", "lecture",
        "like", "dislike", "favori", "favorite", "playlist",
    ]
    if any(tok in s for tok in strong):
        return True

    # Weaker signals: entity words + possessive context.
    if any(tok in s for tok in ["artiste", "artist", "album", "morceau", "track", "genre", "label", "annee", "year"]):
        if any(tok in s for tok in ["ma", "mon", "mes", "my", "dans", "collection", "bibliotheque", "library"]):
            return True

    return False


def _assistant_extract_json_obj(text: str) -> dict:
    """Best-effort JSON object extractor for LLM outputs (no exceptions)."""
    raw = (text or "").strip()
    if not raw:
        return {}

    # Strip common fenced formats.
    raw = re.sub(r"^\s*```(?:json)?\s*", "", raw, flags=re.IGNORECASE).strip()
    raw = re.sub(r"\s*```\s*$", "", raw).strip()

    def _try_parse_obj(s: str) -> dict:
        s = (s or "").strip()
        if not s:
            return {}
        try:
            obj = json.loads(s)
            return obj if isinstance(obj, dict) else {}
        except Exception:
            pass
        # Some models still output Python-like dicts (single quotes). literal_eval is safe for literals.
        try:
            py = s
            py = re.sub(r"\bnull\b", "None", py, flags=re.IGNORECASE)
            py = re.sub(r"\btrue\b", "True", py, flags=re.IGNORECASE)
            py = re.sub(r"\bfalse\b", "False", py, flags=re.IGNORECASE)
            obj = ast.literal_eval(py)
            return obj if isinstance(obj, dict) else {}
        except Exception:
            return {}

    # If the output is already a clean JSON object, parse directly.
    direct = _try_parse_obj(raw)
    if direct:
        return direct

    # Otherwise, scan for the first balanced {...} object (ignoring braces in strings).
    start = None
    depth = 0
    in_str = False
    str_ch = ""
    esc = False
    for i, ch in enumerate(raw):
        if in_str:
            if esc:
                esc = False
                continue
            if ch == "\\":
                esc = True
                continue
            if ch == str_ch:
                in_str = False
                str_ch = ""
            continue
        if ch in ("\"", "'"):
            in_str = True
            str_ch = ch
            continue
        if ch == "{":
            if depth == 0:
                start = i
            depth += 1
            continue
        if ch == "}" and depth > 0:
            depth -= 1
            if depth == 0 and start is not None:
                cand = raw[start : i + 1]
                obj = _try_parse_obj(cand)
                if obj:
                    return obj
                start = None
    return {}


def _assistant_validate_readonly_sql(sql: str) -> tuple[bool, str]:
    """Reject obviously unsafe SQL. Only allow SELECT/CTE SELECT statements."""
    s = (sql or "").strip()
    if not s:
        return (False, "empty_sql")
    # Allow a single trailing semicolon, but reject any other semicolons.
    while s.endswith(";"):
        s = s[:-1].rstrip()
    if ";" in s:
        return (False, "multiple_statements_forbidden")
    low = s.lower()
    if not re.match(r"^(select|with)\b", low.strip()):
        return (False, "only_select_queries_allowed")
    # Disallow common mutating / privileged keywords.
    banned = [
        "insert", "update", "delete", "drop", "alter", "truncate", "create",
        "grant", "revoke", "copy", "call", "do", "execute", "vacuum",
        "analyze", "refresh", "listen", "notify",
    ]
    for kw in banned:
        if re.search(rf"\b{kw}\b", low):
            return (False, f"forbidden_keyword:{kw}")
    if re.search(r"\bpg_sleep\b", low):
        return (False, "forbidden_function:pg_sleep")
    return (True, "ok")


def _assistant_sql_agent_generate_query(
    *,
    user_message: str,
    context_artist_id: int,
    context_artist_name: str,
    provider: str,
    model: str,
    error_hint: str = "",
) -> dict:
    """
    Ask the LLM for a single read-only SQL query + params (positional).
    Returns dict with keys: sql, params (best-effort).
    """
    schema = (
        "files_artists(id, name, name_norm, album_count, track_count, broken_albums_count, has_image, image_path, created_at, updated_at)\n"
        "files_albums(id, artist_id, title, title_norm, folder_path, year, date_text, genre, label, tags_json, format, is_lossless, has_cover, cover_path, "
        "mb_identified, musicbrainz_release_group_id, discogs_release_id, lastfm_album_mbid, bandcamp_album_url, metadata_source, track_count, total_duration_sec, "
        "is_broken, expected_track_count, actual_track_count, missing_indices_json, missing_required_tags_json, primary_tags_json, created_at, updated_at)\n"
        "files_tracks(id, album_id, file_path, title, disc_num, track_num, duration_sec, format, bitrate, sample_rate, bit_depth, file_size_bytes, created_at, updated_at)\n"
        "files_playback_events(user_id, track_id, event_type, played_seconds, created_at)  # user_id is always 1\n"
        "files_entity_likes(entity_type, entity_id, liked, source, created_at, updated_at)\n"
        "files_reco_events(session_id, track_id, album_id, artist_id, event_type, played_seconds, created_at)\n"
        "files_artist_profiles(name_norm, artist_name, bio, short_bio, tags_json, similar_json, source, updated_at)\n"
        "files_album_profiles(artist_norm, title_norm, album_title, description, short_description, tags_json, source, updated_at)\n"
    )
    system_msg = (
        "You are a PostgreSQL query generator for PMDA (a local music library app).\n"
        "Output ONLY a JSON object, no extra text.\n"
        "JSON keys:\n"
        "- sql: string (a single read-only query; SELECT or WITH ... SELECT)\n"
        "- params: array (positional parameters for psycopg, use %s placeholders)\n"
        "- title: short string describing what the query answers\n"
        "Rules:\n"
        "- No semicolons. No multiple statements.\n"
        "- Never use INSERT/UPDATE/DELETE/CREATE/DROP/ALTER/TRUNCATE/COPY/CALL/DO.\n"
        "- Prefer parameterized values (%s) for user-provided strings.\n"
        "- For non-aggregate queries, include LIMIT <= 100.\n"
        "- If the user asks about their listening stats, use files_playback_events (user_id=1).\n"
        "Formatting rules for downstream UI:\n"
        "- If returning artists, include columns: artist_id, artist_name.\n"
        "- If returning labels, include a column named: label.\n"
        "- If returning albums, include columns: artist_id, artist_name, album_id, album_title (and bandcamp_album_url when available).\n"
        "Schema:\n"
        f"{schema}"
        "Join hints:\n"
        "- files_tracks.album_id -> files_albums.id\n"
        "- files_albums.artist_id -> files_artists.id\n"
    )
    ctx_line = ""
    if int(context_artist_id or 0) > 0 and (context_artist_name or "").strip():
        ctx_line = f"Current context artist: id={int(context_artist_id)} name={context_artist_name}\n"
    user_msg = (
        f"{ctx_line}"
        "User question:\n"
        f"{(user_message or '').strip()}\n"
    )
    if (error_hint or "").strip():
        user_msg = (
            f"{user_msg}\n"
            "Previous error (fix your SQL accordingly):\n"
            f"{_truncate_text(str(error_hint), max_chars=800)}\n"
        )

    out = ""
    provider_lower = (provider or "").strip().lower()
    if provider_lower == "openai" and openai_client:
        # Prefer JSON mode for reliability; fall back to normal call on older models.
        param_style = getattr(sys.modules[__name__], "RESOLVED_PARAM_STYLE", "mct")
        _kwargs = {
            "model": model,
            "messages": [
                {"role": "system", "content": system_msg},
                {"role": "user", "content": user_msg},
            ],
            "temperature": 0,
            "response_format": {"type": "json_object"},
        }
        if param_style == "mct":
            _kwargs["max_completion_tokens"] = 420
        else:
            _kwargs["max_tokens"] = 420
        try:
            resp = openai_client.chat.completions.create(**_kwargs)
            out = (resp.choices[0].message.content or "").strip()
        except Exception as e:
            # Retry with other token parameter style when OpenAI errors on one of them.
            err_msg = str(e).lower()
            try:
                if "unsupported_parameter" in err_msg or "400" in err_msg:
                    if "max_completion_tokens" in err_msg and ("max_tokens" in err_msg or "use" in err_msg):
                        _kwargs.pop("max_completion_tokens", None)
                        _kwargs["max_tokens"] = 420
                        resp = openai_client.chat.completions.create(**_kwargs)
                        out = (resp.choices[0].message.content or "").strip()
                    elif "max_tokens" in err_msg and "max_completion_tokens" in err_msg:
                        _kwargs.pop("max_tokens", None)
                        _kwargs["max_completion_tokens"] = 420
                        resp = openai_client.chat.completions.create(**_kwargs)
                        out = (resp.choices[0].message.content or "").strip()
            except Exception:
                out = ""
            if not out:
                logging.debug("Assistant SQL agent JSON-mode call failed, falling back: %s", str(e)[:240])
                out = call_ai_provider_longform(provider, model, system_msg, user_msg, max_tokens=420)
    else:
        out = call_ai_provider_longform(provider, model, system_msg, user_msg, max_tokens=420)

    obj = _assistant_extract_json_obj(out)
    if not obj:
        logging.debug("Assistant SQL agent produced no JSON plan. out=%s", _truncate_text(out, max_chars=600))
        return {}
    if "params" in obj and not isinstance(obj.get("params"), list):
        obj["params"] = []
    return obj


def _assistant_sql_agent_execute(conn, *, sql: str, params: list, max_rows: int = 100) -> tuple[list[str], list[tuple]]:
    """Execute SQL with a statement timeout. Returns (columns, rows)."""
    max_rows = max(1, min(500, int(max_rows or 100)))
    params = params if isinstance(params, list) else []
    with conn.cursor() as cur:
        # Keep requests responsive; reset after executing.
        try:
            cur.execute("SET statement_timeout TO '5000ms'")
        except Exception:
            pass
        try:
            cur.execute(sql, params)
            cols = [str(d.name or "") for d in (cur.description or []) if getattr(d, "name", None)]
            rows = cur.fetchmany(max_rows)
            return (cols, rows)
        finally:
            try:
                cur.execute("RESET statement_timeout")
            except Exception:
                pass


def _assistant_sql_agent_format_result(*, lang: str, title: str, cols: list[str], rows: list[tuple]) -> str:
    """Format SQL rows into a chat-friendly answer (deterministic; no hallucination)."""
    title = (title or "").strip() or ("Resultats (PMDA)" if lang == "fr" else "Results (PMDA)")
    if not rows:
        return f"{title}:\n- Aucun resultat." if lang == "fr" else f"{title}:\n- No results."

    if len(rows) == 1 and len(cols) == 1:
        val = rows[0][0]
        return f"{title}: {val}"

    # Special-case common 2-col shapes: (name, count)
    if len(cols) == 2:
        lines = [f"{title}:"]
        for r in rows[:20]:
            a = str(r[0] if len(r) > 0 else "").strip()
            b = r[1] if len(r) > 1 else ""
            if not a:
                continue
            lines.append(f"- {a}: {b}")
        return "\n".join(lines).strip()

    # Generic row formatter.
    lines = [f"{title}:"]
    for r in rows[:20]:
        parts = []
        for idx, col in enumerate(cols[:8]):
            try:
                v = r[idx]
            except Exception:
                v = None
            if v is None or v == "":
                continue
            parts.append(f"{col}={v}")
        if parts:
            lines.append(f"- " + " ¬∑ ".join(parts))
    return "\n".join(lines).strip()


def _assistant_sql_agent_links_from_result(*, cols: list[str], rows: list[tuple], base_url: str) -> list[dict]:
    """Create clickable links (internal + external) from common result shapes."""
    base = (base_url or "").rstrip("/")
    idx = {str(c or "").strip().lower(): i for i, c in enumerate(cols or []) if str(c or "").strip()}

    links: list[dict] = []
    seen: set[str] = set()

    def _add(link: dict) -> None:
        href = str(link.get("href") or "").strip()
        label = str(link.get("label") or "").strip()
        if not href or not label:
            return
        key = f"{href}||{label}"
        if key in seen:
            return
        seen.add(key)
        links.append(link)

    for r in (rows or [])[:60]:
        # Artist link
        if "artist_id" in idx:
            try:
                aid = int(r[idx["artist_id"]] or 0)
            except Exception:
                aid = 0
            if aid > 0:
                nm = ""
                if "artist_name" in idx:
                    try:
                        nm = str(r[idx["artist_name"]] or "").strip()
                    except Exception:
                        nm = ""
                if not nm:
                    nm = f"Artist #{aid}"
                _add(
                    {
                        "kind": "internal",
                        "label": nm,
                        "href": f"/library/artist/{aid}",
                        "entity_type": "artist",
                        "entity_id": aid,
                        "thumb": f"{base}/api/library/files/artist/{aid}/image?size=96",
                    }
                )

        # Label link
        if "label" in idx:
            try:
                lab = str(r[idx["label"]] or "").strip()
            except Exception:
                lab = ""
            if lab:
                _add(
                    {
                        "kind": "internal",
                        "label": lab,
                        "href": f"/library/label/{quote(lab)}",
                        "entity_type": "label",
                        "entity_id": 0,
                        "thumb": None,
                    }
                )

        # Bandcamp album link when present (nice "internet link" without needing extra crawling).
        if "bandcamp_album_url" in idx:
            try:
                url = str(r[idx["bandcamp_album_url"]] or "").strip()
            except Exception:
                url = ""
            if url and url.startswith(("http://", "https://")):
                title = ""
                for key in ("album_title", "title"):
                    if key in idx:
                        try:
                            title = str(r[idx[key]] or "").strip()
                        except Exception:
                            title = ""
                        if title:
                            break
                if not title:
                    title = "Bandcamp"
                _add(
                    {
                        "kind": "external",
                        "label": title,
                        "href": url,
                        "entity_type": "album",
                        "entity_id": 0,
                        "thumb": None,
                    }
                )

    return links[:18]


def _assistant_try_handle_sql_agent_query(conn, *, user_message: str, context_artist_id: int, base_url: str) -> dict:
    """
    Slow path (LLM-assisted): generate a safe SELECT query, execute, and format the result.
    Returns {handled: bool, assistant_text, citations, links, tool}.
    """
    if not _assistant_should_try_sql_agent(user_message):
        return {"handled": False}

    # Context artist name (optional).
    context_artist_id = int(context_artist_id or 0)
    context_artist_name = ""
    if context_artist_id > 0:
        try:
            with conn.cursor() as cur:
                cur.execute("SELECT name FROM files_artists WHERE id = %s", (context_artist_id,))
                row = cur.fetchone()
            context_artist_name = str((row[0] if row else "") or "").strip()
        except Exception:
            context_artist_name = ""

    provider = getattr(sys.modules[__name__], "AI_PROVIDER", "openai")
    model = getattr(sys.modules[__name__], "RESOLVED_MODEL", None) or getattr(sys.modules[__name__], "OPENAI_MODEL", "gpt-4o-mini")
    lang = _assistant_lang_for_message(user_message)

    last_err = ""
    plan: dict = {}
    sql = ""
    params: list = []
    title = ""
    cols: list[str] = []
    rows: list[tuple] = []

    for attempt in range(2):
        plan = _assistant_sql_agent_generate_query(
            user_message=user_message,
            context_artist_id=context_artist_id,
            context_artist_name=context_artist_name,
            provider=provider,
            model=model,
            error_hint=last_err if attempt > 0 else "",
        )
        sql = str(plan.get("sql") or "").strip()
        params = plan.get("params") if isinstance(plan.get("params"), list) else []
        title = str(plan.get("title") or "").strip()

        # Strip trailing semicolons (we still forbid multi-statement SQL).
        while sql.endswith(";"):
            sql = sql[:-1].rstrip()

        ok, reason = _assistant_validate_readonly_sql(sql)
        if not ok:
            logging.debug("Assistant SQL agent rejected query (%s): %s", reason, sql[:240])
            last_err = f"validation_failed:{reason}"
            continue

        # Add a hard LIMIT if the model forgot one (safe default).
        if not re.search(r"\blimit\b", sql, flags=re.IGNORECASE):
            sql = f"{sql.rstrip()} LIMIT 100"

        try:
            cols, rows = _assistant_sql_agent_execute(conn, sql=sql, params=params, max_rows=100)
            break
        except Exception as e:
            last_err = f"{type(e).__name__}: {str(e)[:400]}"
            logging.debug("Assistant SQL agent execute failed (attempt %s): %s", attempt + 1, last_err)
            cols, rows = ([], [])
            continue

    if not cols and not rows:
        return {"handled": False}

    assistant_text = _assistant_sql_agent_format_result(lang=lang, title=title, cols=cols, rows=rows)
    links = _assistant_sql_agent_links_from_result(cols=cols, rows=rows, base_url=base_url)

    # Attach a minimal citation so the UI can show "pmda_db" provenance.
    snippet = _truncate_text(assistant_text, max_chars=240)
    citations = [
        {
            "entity_type": "library" if context_artist_id <= 0 else "artist",
            "entity_id": 1 if context_artist_id <= 0 else int(context_artist_id),
            "doc_type": "sql_agent",
            "source": "pmda_db",
            "title": title or ("SQL result" if lang != "fr" else "Resultat SQL"),
            "chunk_id": -1,
            "score": 1.0,
            "snippet": snippet,
        }
    ]

    return {
        "handled": True,
        "tool": "sql_agent_v1",
        "assistant_text": assistant_text,
        "citations": citations,
        "links": links,
        "ts": int(time.time()),
    }


def _reco_genre_tokens(raw_genre: str) -> list[str]:
    tokens = []
    for part in re.split(r"[;,/|]+", (raw_genre or "").lower()):
        txt = re.sub(r"\s+", " ", (part or "").strip())
        if txt:
            tokens.append(txt)
    return tokens[:12]


def _reco_fetch_embeddings_map(conn, track_ids: list[int]) -> dict[int, list[float]]:
    if not track_ids:
        return {}
    with conn.cursor() as cur:
        cur.execute(
            """
            SELECT track_id, embed_json
            FROM files_track_embeddings
            WHERE track_id = ANY(%s)
            """,
            (track_ids,),
        )
        rows = cur.fetchall()
    out: dict[int, list[float]] = {}
    for track_id, embed_json in rows:
        out[int(track_id)] = _load_embedding_json(embed_json or "")
    return out


def _reco_build_session_profile(conn, session_id: str) -> dict:
    with conn.cursor() as cur:
        cur.execute(
            """
            SELECT
                e.id,
                e.event_type,
                e.track_id,
                e.played_seconds,
                EXTRACT(EPOCH FROM (NOW() - e.created_at))::DOUBLE PRECISION AS age_sec,
                COALESCE(e.artist_id, 0),
                COALESCE(e.album_id, 0),
                COALESCE(alb.genre, '')
            FROM files_reco_events e
            LEFT JOIN files_albums alb ON alb.id = e.album_id
            WHERE e.session_id = %s
            ORDER BY e.created_at DESC
            LIMIT 400
            """,
            (session_id,),
        )
        rows = cur.fetchall()
    profile = {
        "has_data": bool(rows),
        "recent_track_ids": [],
        "negative_track_ids": set(),
        "artist_weights": defaultdict(float),
        "genre_weights": defaultdict(float),
        "track_weights": defaultdict(float),
        "session_event_count": len(rows),
    }
    if not rows:
        profile["centroid"] = []
        return profile

    recent_limit = 40
    seen_recent = set()
    for idx, row in enumerate(rows):
        _event_id = int(row[0] or 0)
        event_type = str(row[1] or "").strip().lower()
        track_id = int(row[2] or 0)
        played_seconds = int(row[3] or 0)
        age_sec = max(0.0, float(row[4] or 0.0))
        artist_id = int(row[5] or 0)
        album_genre = str(row[7] or "")
        decay = math.exp(-(age_sec / (72.0 * 3600.0)))
        weight = _reco_event_weight(event_type, played_seconds) * decay

        if track_id > 0 and track_id not in seen_recent and len(profile["recent_track_ids"]) < recent_limit:
            profile["recent_track_ids"].append(track_id)
            seen_recent.add(track_id)
        if track_id > 0 and weight < -0.4:
            profile["negative_track_ids"].add(track_id)
        if track_id > 0 and abs(weight) > 0.01:
            profile["track_weights"][track_id] += weight
        if artist_id > 0 and abs(weight) > 0.01:
            profile["artist_weights"][artist_id] += weight
        for gt in _reco_genre_tokens(album_genre):
            profile["genre_weights"][gt] += weight

        # Extra recency emphasis on first events in the list.
        if idx < 6 and track_id > 0:
            profile["track_weights"][track_id] += 0.12

    # Normalize artist and genre affinity to approximately [-1, 1].
    for key in ("artist_weights", "genre_weights"):
        mapping = profile[key]
        if not mapping:
            continue
        max_abs = max(abs(v) for v in mapping.values()) or 1.0
        for mk in list(mapping.keys()):
            mapping[mk] = float(mapping[mk]) / float(max_abs)

    positive_track_ids = [int(tid) for tid, w in profile["track_weights"].items() if w > 0.03]
    emb_map = _reco_fetch_embeddings_map(conn, positive_track_ids)
    centroid = [0.0] * RECO_EMBED_DIM
    total_w = 0.0
    for tid, weight in profile["track_weights"].items():
        if weight <= 0.03:
            continue
        emb = emb_map.get(int(tid)) or []
        if not emb:
            continue
        w = float(weight)
        for i in range(min(RECO_EMBED_DIM, len(emb))):
            centroid[i] += emb[i] * w
        total_w += w
    norm = math.sqrt(sum(v * v for v in centroid))
    if norm > 0:
        inv = 1.0 / norm
        centroid = [v * inv for v in centroid]
    profile["centroid"] = centroid
    profile["positive_track_ids"] = positive_track_ids

    # Keep top affinities for SQL filtering.
    sorted_artists = sorted(
        ((int(k), float(v)) for k, v in profile["artist_weights"].items() if v > 0.08),
        key=lambda x: x[1],
        reverse=True,
    )
    sorted_genres = sorted(
        ((str(k), float(v)) for k, v in profile["genre_weights"].items() if v > 0.08),
        key=lambda x: x[1],
        reverse=True,
    )
    profile["top_artist_ids"] = [aid for aid, _ in sorted_artists[:12]]
    profile["top_genres"] = [g for g, _ in sorted_genres[:10]]
    return profile


def _reco_fetch_candidates(conn, profile: dict, limit: int) -> list[dict]:
    limit = max(100, min(5000, int(limit or 500)))
    recent_track_ids = [int(x) for x in (profile.get("recent_track_ids") or []) if int(x) > 0]
    top_artist_ids = [int(x) for x in (profile.get("top_artist_ids") or []) if int(x) > 0]
    top_genres = [str(x or "").strip() for x in (profile.get("top_genres") or []) if str(x or "").strip()]

    where_parts = ["1=1"]
    params: list = []
    if recent_track_ids:
        where_parts.append("t.id <> ALL(%s)")
        params.append(recent_track_ids[:120])

    pref_parts = []
    if top_artist_ids:
        pref_parts.append("ar.id = ANY(%s)")
        params.append(top_artist_ids[:24])
    if top_genres:
        for g in top_genres[:8]:
            pref_parts.append("alb.genre ILIKE %s")
            params.append(f"%{g}%")
    if pref_parts:
        where_parts.append("(" + " OR ".join(pref_parts) + ")")

    sql = f"""
        SELECT
            t.id,
            t.title,
            t.duration_sec,
            t.track_num,
            alb.id AS album_id,
            alb.title AS album_title,
            COALESCE(alb.genre, '') AS album_genre,
            COALESCE(alb.year, 0) AS album_year,
            ar.id AS artist_id,
            ar.name AS artist_name,
            alb.has_cover,
            COALESCE(st.play_count, 0) AS play_count,
            COALESCE(st.completion_count, 0) AS completion_count,
            COALESCE(st.skip_count, 0) AS skip_count,
            COALESCE(emb.embed_json, '[]') AS embed_json
        FROM files_tracks t
        JOIN files_albums alb ON alb.id = t.album_id
        JOIN files_artists ar ON ar.id = alb.artist_id
        LEFT JOIN files_reco_track_stats st ON st.track_id = t.id
        LEFT JOIN files_track_embeddings emb ON emb.track_id = t.id
        WHERE {" AND ".join(where_parts)}
        ORDER BY COALESCE(st.play_count, 0) DESC, COALESCE(alb.year, 0) DESC, t.id DESC
        LIMIT %s
    """
    params.append(limit)
    with conn.cursor() as cur:
        cur.execute(sql, params)
        rows = cur.fetchall()

    # If strict preferences yielded too few tracks, widen the net.
    if len(rows) < min(30, limit // 4):
        with conn.cursor() as cur:
            cur.execute(
                """
                SELECT
                    t.id,
                    t.title,
                    t.duration_sec,
                    t.track_num,
                    alb.id AS album_id,
                    alb.title AS album_title,
                    COALESCE(alb.genre, '') AS album_genre,
                    COALESCE(alb.year, 0) AS album_year,
                    ar.id AS artist_id,
                    ar.name AS artist_name,
                    alb.has_cover,
                    COALESCE(st.play_count, 0) AS play_count,
                    COALESCE(st.completion_count, 0) AS completion_count,
                    COALESCE(st.skip_count, 0) AS skip_count,
                    COALESCE(emb.embed_json, '[]') AS embed_json
                FROM files_tracks t
                JOIN files_albums alb ON alb.id = t.album_id
                JOIN files_artists ar ON ar.id = alb.artist_id
                LEFT JOIN files_reco_track_stats st ON st.track_id = t.id
                LEFT JOIN files_track_embeddings emb ON emb.track_id = t.id
                ORDER BY COALESCE(st.play_count, 0) DESC, COALESCE(alb.year, 0) DESC, t.id DESC
                LIMIT %s
                """,
                (limit,),
            )
            rows = cur.fetchall()

    out = []
    for row in rows:
        out.append(
            {
                "track_id": int(row[0]),
                "title": row[1] or "",
                "duration_sec": int(row[2] or 0),
                "track_num": int(row[3] or 0),
                "album_id": int(row[4] or 0),
                "album_title": row[5] or "",
                "album_genre": row[6] or "",
                "album_year": int(row[7] or 0),
                "artist_id": int(row[8] or 0),
                "artist_name": row[9] or "",
                "has_cover": bool(row[10]),
                "play_count": int(row[11] or 0),
                "completion_count": int(row[12] or 0),
                "skip_count": int(row[13] or 0),
                "embedding": _load_embedding_json(row[14] or "[]"),
            }
        )
    return out


def _reco_rank_candidates(profile: dict, candidates: list[dict], limit: int) -> list[dict]:
    limit = max(1, min(100, int(limit or 20)))
    if not candidates:
        return []
    centroid = profile.get("centroid") or []
    artist_weights = profile.get("artist_weights") or {}
    genre_weights = profile.get("genre_weights") or {}
    negative_track_ids = set(profile.get("negative_track_ids") or set())
    recent_track_ids = set(profile.get("recent_track_ids") or [])

    max_play_count = max((int(c.get("play_count") or 0) for c in candidates), default=0)
    log_den = math.log1p(max_play_count) if max_play_count > 0 else 1.0
    year_values = [int(c.get("album_year") or 0) for c in candidates if int(c.get("album_year") or 0) > 0]
    max_year = max(year_values) if year_values else 0
    min_year = min(year_values) if year_values else 0
    year_span = max(1, max_year - min_year) if max_year > 0 else 1

    scored: list[dict] = []
    for c in candidates:
        track_id = int(c.get("track_id") or 0)
        artist_id = int(c.get("artist_id") or 0)
        play_count = int(c.get("play_count") or 0)
        completion_count = int(c.get("completion_count") or 0)
        skip_count = int(c.get("skip_count") or 0)

        emb = c.get("embedding") or []
        emb_score = _vec_cosine(centroid, emb) if centroid and emb else 0.0
        artist_affinity = float(artist_weights.get(artist_id, 0.0))
        genre_affinity = 0.0
        for gt in _reco_genre_tokens(c.get("album_genre") or ""):
            genre_affinity += float(genre_weights.get(gt, 0.0))
        genre_affinity = max(-1.0, min(1.0, genre_affinity))

        pop_score = (math.log1p(play_count) / log_den) if play_count > 0 and log_den > 0 else 0.0
        completion_rate = (float(completion_count) / float(play_count)) if play_count > 0 else 0.0
        skip_rate = float(skip_count) / float(max(1, play_count + skip_count))

        year_val = int(c.get("album_year") or 0)
        recency = ((year_val - min_year) / year_span) if year_val > 0 and max_year > min_year else 0.0

        score = (
            0.58 * emb_score
            + 0.18 * artist_affinity
            + 0.12 * genre_affinity
            + 0.06 * pop_score
            + 0.03 * completion_rate
            + 0.03 * recency
            - 0.20 * skip_rate
        )
        if track_id in recent_track_ids:
            score -= 0.95
        if track_id in negative_track_ids:
            score -= 1.25

        reasons: list[str] = []
        if emb_score >= 0.40:
            reasons.append("embedding match")
        if artist_affinity >= 0.20:
            reasons.append("artist affinity")
        if genre_affinity >= 0.20:
            reasons.append("genre affinity")
        if pop_score >= 0.55:
            reasons.append("popular")
        c["score"] = float(score)
        c["reasons"] = reasons[:3]
        scored.append(c)

    scored.sort(key=lambda x: float(x.get("score", 0.0)), reverse=True)

    # Simple diversity pass to avoid too many tracks from the same album/artist.
    selected: list[dict] = []
    album_seen: dict[int, int] = defaultdict(int)
    artist_seen: dict[int, int] = defaultdict(int)
    for cand in scored:
        album_id = int(cand.get("album_id") or 0)
        artist_id = int(cand.get("artist_id") or 0)
        adjusted = float(cand.get("score") or 0.0)
        adjusted -= 0.10 * float(album_seen.get(album_id, 0))
        adjusted -= 0.05 * float(artist_seen.get(artist_id, 0))
        cand["score"] = adjusted
        if adjusted < -0.9:
            continue
        selected.append(cand)
        album_seen[album_id] += 1
        artist_seen[artist_id] += 1
        if len(selected) >= limit:
            break

    selected.sort(key=lambda x: float(x.get("score", 0.0)), reverse=True)
    return selected[:limit]


def _reco_record_event(conn, session_id: str, track_id: int, event_type: str, played_seconds: int = 0) -> tuple[bool, str]:
    et = str(event_type or "").strip().lower()
    allowed = {"play_start", "play_partial", "play_complete", "skip", "stop", "like", "dislike"}
    if et not in allowed:
        return False, "unsupported event_type"
    with conn.cursor() as cur:
        cur.execute(
            """
            SELECT t.id, t.album_id, alb.artist_id
            FROM files_tracks t
            JOIN files_albums alb ON alb.id = t.album_id
            WHERE t.id = %s
            """,
            (int(track_id or 0),),
        )
        row = cur.fetchone()
        if not row:
            return False, "track not found"
        resolved_track_id = int(row[0] or 0)
        album_id = int(row[1] or 0)
        artist_id = int(row[2] or 0)

        cur.execute(
            """
            INSERT INTO files_reco_events(session_id, track_id, album_id, artist_id, event_type, played_seconds, created_at)
            VALUES (%s, %s, %s, %s, %s, %s, NOW())
            """,
            (session_id, resolved_track_id, album_id, artist_id, et, max(0, int(played_seconds or 0))),
        )

        play_inc = 1 if et in {"play_complete", "like"} else 0
        completion_inc = 1 if et in {"play_complete", "like"} else 0
        partial_inc = 1 if et in {"play_partial", "play_start", "stop"} else 0
        skip_inc = 1 if et in {"skip", "dislike"} else 0
        cur.execute(
            """
            INSERT INTO files_reco_track_stats(
                track_id, play_count, completion_count, partial_count, skip_count, last_event_at, updated_at
            )
            VALUES (%s, %s, %s, %s, %s, NOW(), NOW())
            ON CONFLICT (track_id) DO UPDATE
            SET play_count = files_reco_track_stats.play_count + EXCLUDED.play_count,
                completion_count = files_reco_track_stats.completion_count + EXCLUDED.completion_count,
                partial_count = files_reco_track_stats.partial_count + EXCLUDED.partial_count,
                skip_count = files_reco_track_stats.skip_count + EXCLUDED.skip_count,
                last_event_at = NOW(),
                updated_at = NOW()
            """,
            (resolved_track_id, play_inc, completion_inc, partial_inc, skip_inc),
        )
        cur.execute(
            """
            INSERT INTO files_reco_sessions(session_id, last_event_at, created_at, total_events)
            VALUES (%s, NOW(), NOW(), 1)
            ON CONFLICT (session_id) DO UPDATE
            SET last_event_at = NOW(),
                total_events = files_reco_sessions.total_events + 1
            """,
            (session_id,),
        )
    return True, "ok"

# ----- Run summary tracking ---------------------------------------------------
def _count_rows(table: str) -> int:
    con = sqlite3.connect(str(STATE_DB_FILE))
    cur = con.cursor()
    cur.execute(f"SELECT COUNT(*) FROM {table}")
    n = cur.fetchone()[0]
    con.close()
    return n

RUN_START_TS = time.time()
RUN_BASELINE = {
    "removed_dupes": get_stat("removed_dupes"),
    "space_saved":  get_stat("space_saved"),
    "best_rows":    0,
    "loser_rows":   0,
}
try:
    RUN_BASELINE["best_rows"]  = _count_rows("duplicates_best")
    RUN_BASELINE["loser_rows"] = _count_rows("duplicates_loser")
except Exception:
    pass

SUMMARY_EMITTED = False

def emit_final_summary(reason: str = "normal"):
    global SUMMARY_EMITTED
    if SUMMARY_EMITTED:
        return

    # Compute duration and delta counters since start
    duration  = max(0, int(time.time() - RUN_START_TS))
    removed   = max(0, get_stat("removed_dupes") - RUN_BASELINE["removed_dupes"])
    saved_mb  = max(0, get_stat("space_saved")  - RUN_BASELINE["space_saved"])
    try:
        new_groups = max(0, _count_rows("duplicates_best")  - RUN_BASELINE.get("best_rows", 0))
        new_losers = max(0, _count_rows("duplicates_loser") - RUN_BASELINE.get("loser_rows", 0))
    except Exception:
        new_groups = new_losers = 0

    # Try to compute library‚Äëwide counts for the selected sections for extra context
    def _library_counts() -> tuple[int, int]:
        try:
            con = plex_connect()
            cur = con.cursor()
            placeholders = ",".join("?" for _ in SECTION_IDS)
            # Artists = metadata_type 8, Albums = metadata_type 9
            cur.execute(f"""
                SELECT
                    (SELECT COUNT(DISTINCT id) FROM metadata_items WHERE metadata_type = 8 AND library_section_id IN ({placeholders})),
                    (SELECT COUNT(DISTINCT id) FROM metadata_items WHERE metadata_type = 9 AND library_section_id IN ({placeholders}))
            """, (*SECTION_IDS, *SECTION_IDS))
            a, b = cur.fetchone()
            con.close()
            return int(a or 0), int(b or 0)
        except Exception:
            return 0, 0

    total_artists, total_albums = _library_counts()

    # Pretty banner with commas and consistent formatting
    bar = "‚îÄ" * 85
    logging.info("\n%s", bar)
    logging.info("FINAL SUMMARY")
    logging.info("Total artists           : %s", f"{total_artists:,}" if total_artists else "n/a")
    logging.info("Total albums            : %s", f"{total_albums:,}" if total_albums else "n/a")
    logging.info("Albums with dupes       : %s", f"{new_groups:,}")
    logging.info("Folders moved           : %s", f"{removed:,}")
    logging.info("Total space reclaimed   : %s MB", f"{saved_mb:,}")
    logging.info("Duration                : %s s", f"{duration:,}")
    logging.info("%s\n", bar)

    if DISCORD_WEBHOOK:
        fields = [
            {"name": "Artists",          "value": (f"{total_artists:,}" if total_artists else "n/a"), "inline": True},
            {"name": "Albums",           "value": (f"{total_albums:,}" if total_albums else "n/a"), "inline": True},
            {"name": "Groups",           "value": f"{new_groups:,}",                 "inline": True},
            {"name": "Removed",          "value": f"{removed:,}",                    "inline": True},
            {"name": "Reclaimed",        "value": f"{saved_mb:,} MB",               "inline": True},
            {"name": "Duration",         "value": f"{duration:,} s",                "inline": True},
        ]
        notify_discord_embed("‚úÖ PMDA ‚Äì Final summary", "Run completed.", fields=fields)

    SUMMARY_EMITTED = True

atexit.register(emit_final_summary)

# Shutdown MusicBrainz queue on exit
def _shutdown_mb_queue():
    global _mb_queue
    if _mb_queue is not None:
        _mb_queue.shutdown()
atexit.register(_shutdown_mb_queue)

# Shutdown ffprobe pool on exit
def _shutdown_ffprobe_pool():
    global _ffprobe_pool
    if _ffprobe_pool is not None:
        _ffprobe_pool.shutdown(wait=True)
atexit.register(_shutdown_ffprobe_pool)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ MusicBrainz Queue (Global Rate Limiting) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
class MusicBrainzQueue:
    """
    Global queue for MusicBrainz API calls to respect rate limiting (1 req/sec) 
    while allowing parallel submission from multiple threads.
    """
    def __init__(self, enabled: bool = True):
        self.enabled = enabled
        if not enabled:
            return
        self.queue: Queue = Queue()
        self.results: Dict[str, Tuple[Optional[dict], Optional[Exception]]] = {}
        self.locks: Dict[str, threading.Event] = {}
        self._lock = threading.Lock()
        self._stop_event = threading.Event()
        self.worker_thread = threading.Thread(target=self._worker, daemon=True, name="MBQueueWorker")
        self.worker_thread.start()
        logging.info("MusicBrainz queue initialized (rate limit: 1 req/sec)")
    
    def _worker(self):
        """Worker thread that processes MusicBrainz requests sequentially with rate limiting."""
        while not self._stop_event.is_set():
            try:
                # Get next request with timeout to allow checking stop event
                try:
                    item = self.queue.get(timeout=1.0)
                except:
                    continue
                
                if item is None:  # Shutdown signal
                    break
                
                request_id, callback = item
                result = None
                error = None
                
                try:
                    result = callback()
                except Exception as e:
                    error = e
                    logging.debug("[MB Queue] Request %s failed: %s", request_id, e)
                
                # Store result and notify waiting thread
                with self._lock:
                    self.results[request_id] = (result, error)
                    if request_id in self.locks:
                        self.locks[request_id].set()
                
                # Rate limit: 1 request per second
                time.sleep(1.0)
                
            except Exception as e:
                logging.error("[MB Queue] Worker error: %s", e, exc_info=True)
    
    def submit(self, request_id: str, callback) -> dict:
        """
        Submit a MusicBrainz request to the queue.
        Returns the result dict or raises exception.
        """
        if not self.enabled:
            # Direct call if queue disabled
            return callback()
        
        # Check if already in queue or processing
        with self._lock:
            if request_id in self.results:
                result, error = self.results[request_id]
                if error:
                    raise error
                return result
            
            # Create event for this request
            event = threading.Event()
            self.locks[request_id] = event
        
        # Submit to queue
        self.queue.put((request_id, callback))
        
        # Batch fetch (fetch_rg_*) can take 100+ seconds (100 pages √ó 1 s rate limit).
        # Single requests use a tighter timeout so scans can fall back to other providers faster.
        if request_id.startswith("fetch_rg_"):
            timeout_seconds = 300
        else:
            timeout_seconds = max(
                15,
                min(
                    45,
                    int(getattr(sys.modules[__name__], "MB_SEARCH_ALBUM_TIMEOUT_SEC", 20) or 20),
                ),
            )
        if event.wait(timeout=timeout_seconds):
            with self._lock:
                result, error = self.results.pop(request_id, (None, None))
                if request_id in self.locks:
                    del self.locks[request_id]
                
                if error:
                    raise error
                if result is None:
                    raise RuntimeError(f"MusicBrainz request {request_id} returned None")
                return result
        else:
            with self._lock:
                if request_id in self.locks:
                    del self.locks[request_id]
            raise TimeoutError(f"MusicBrainz request {request_id} timed out after {timeout_seconds} seconds")
    
    def shutdown(self):
        """Shutdown the queue worker."""
        if not self.enabled:
            return
        self._stop_event.set()
        self.queue.put(None)  # Signal worker to stop
        if self.worker_thread.is_alive():
            self.worker_thread.join(timeout=5.0)

# Global MusicBrainz queue instance
_mb_queue: Optional[MusicBrainzQueue] = None

def get_mb_queue() -> MusicBrainzQueue:
    """Get or create the global MusicBrainz queue."""
    global _mb_queue
    if _mb_queue is None:
        _mb_queue = MusicBrainzQueue(enabled=MB_QUEUE_ENABLED and USE_MUSICBRAINZ)
    return _mb_queue

# --- MusicBrainz cache helpers ---
def get_cached_mb_info(mbid: str) -> dict | None:
    con = sqlite3.connect(str(CACHE_DB_FILE), timeout=30)
    cur = con.cursor()
    cur.execute("SELECT info_json FROM musicbrainz_cache WHERE mbid = ?", (mbid,))
    row = cur.fetchone()
    con.close()
    if row:
        return json.loads(row[0])
    return None

def set_cached_mb_info(mbid: str, info: dict):
    con = sqlite3.connect(str(CACHE_DB_FILE), timeout=30)
    cur = con.cursor()
    cur.execute(
        "INSERT OR REPLACE INTO musicbrainz_cache (mbid, info_json, created_at) VALUES (?, ?, ?)",
        (mbid, json.dumps(info), int(time.time()))
    )
    con.commit()
    con.close()


def get_cached_mb_album_lookup(artist_norm: str, album_norm: str) -> tuple[str | None, dict | None]:
    """
    Return (mbid, info) for artist+album lookup cache.
    - (None, None) = not in cache
    - ("", None) = cached as "no MusicBrainz ID found"
    - (mbid, info_dict) = cached as found (info_dict may be None if only mbid was stored)
    """
    con = sqlite3.connect(str(CACHE_DB_FILE), timeout=30)
    cur = con.cursor()
    cur.execute(
        "SELECT mbid, info_json FROM musicbrainz_album_lookup WHERE artist_norm = ? AND album_norm = ?",
        (artist_norm, album_norm),
    )
    row = cur.fetchone()
    con.close()
    if row is None:
        return (None, None)
    mbid_val, info_json = row[0], row[1]
    if mbid_val is None or mbid_val == "":
        return ("", None)
    info = json.loads(info_json) if info_json else None
    return (mbid_val, info)


def set_cached_mb_album_lookup(artist_norm: str, album_norm: str, mbid: str | None, info: dict | None):
    """Cache result of artist+album lookup. mbid None or '' = not found."""
    con = sqlite3.connect(str(CACHE_DB_FILE), timeout=30)
    cur = con.cursor()
    cur.execute(
        """INSERT OR REPLACE INTO musicbrainz_album_lookup (artist_norm, album_norm, mbid, info_json, created_at)
           VALUES (?, ?, ?, ?, ?)""",
        (artist_norm, album_norm, mbid or "", json.dumps(info) if info else None, int(time.time())),
    )
    con.commit()
    con.close()


def _provider_cache_norm(value: str) -> str:
    txt = " ".join((value or "").strip().split())
    if not txt:
        return ""
    norm = norm_album(txt)
    return norm or txt.lower()


def get_cached_provider_album_lookup(provider: str, artist_name: str, album_title: str) -> tuple[str | None, dict | None]:
    """
    Return cached provider lookup:
    - (None, None): cache miss or expired
    - ("not_found", None): cached negative lookup
    - ("error", None): cached transient provider error
    - ("found", payload_dict): cached provider payload
    """
    provider_key = (provider or "").strip().lower()
    artist_norm = _provider_cache_norm(artist_name)
    album_norm = _provider_cache_norm(album_title)
    if not provider_key or not artist_norm or not album_norm:
        return (None, None)
    now = int(time.time())
    con = sqlite3.connect(str(CACHE_DB_FILE), timeout=30)
    cur = con.cursor()
    cur.execute(
        """
        SELECT status, payload_json, expires_at
        FROM provider_album_lookup
        WHERE provider = ? AND artist_norm = ? AND album_norm = ?
        """,
        (provider_key, artist_norm, album_norm),
    )
    row = cur.fetchone()
    con.close()
    if row is None:
        return (None, None)
    status = str(row[0] or "").strip().lower()
    payload_raw = row[1]
    expires_at = int(row[2] or 0)
    if expires_at and expires_at < now:
        try:
            con = sqlite3.connect(str(CACHE_DB_FILE), timeout=30)
            cur = con.cursor()
            cur.execute(
                "DELETE FROM provider_album_lookup WHERE provider = ? AND artist_norm = ? AND album_norm = ?",
                (provider_key, artist_norm, album_norm),
            )
            con.commit()
            con.close()
        except Exception:
            pass
        return (None, None)
    if status == "found":
        try:
            payload = json.loads(payload_raw) if payload_raw else None
        except Exception:
            payload = None
        return ("found", payload if isinstance(payload, dict) else None)
    if status in {"not_found", "error"}:
        return (status, None)
    return (None, None)


def set_cached_provider_album_lookup(
    provider: str,
    artist_name: str,
    album_title: str,
    status: str,
    payload: dict | None = None,
) -> None:
    provider_key = (provider or "").strip().lower()
    artist_norm = _provider_cache_norm(artist_name)
    album_norm = _provider_cache_norm(album_title)
    status_norm = (status or "").strip().lower()
    if not provider_key or not artist_norm or not album_norm:
        return
    if status_norm not in {"found", "not_found", "error"}:
        return
    if status_norm == "found":
        ttl = max(60, int(getattr(sys.modules[__name__], "PROVIDER_CACHE_FOUND_TTL_SEC", 60 * 60 * 24 * 30) or (60 * 60 * 24 * 30)))
    elif status_norm == "not_found":
        ttl = max(60, int(getattr(sys.modules[__name__], "PROVIDER_CACHE_NOT_FOUND_TTL_SEC", 60 * 60 * 8) or (60 * 60 * 8)))
    else:
        ttl = max(30, int(getattr(sys.modules[__name__], "PROVIDER_CACHE_ERROR_TTL_SEC", 60 * 20) or (60 * 20)))
    now = int(time.time())
    expires_at = now + int(ttl)
    payload_json = json.dumps(payload) if (status_norm == "found" and isinstance(payload, dict)) else None
    con = sqlite3.connect(str(CACHE_DB_FILE), timeout=30)
    cur = con.cursor()
    cur.execute(
        """
        INSERT OR REPLACE INTO provider_album_lookup
        (provider, artist_norm, album_norm, status, payload_json, created_at, expires_at)
        VALUES (?, ?, ?, ?, ?, ?, ?)
        """,
        (provider_key, artist_norm, album_norm, status_norm, payload_json, now, expires_at),
    )
    con.commit()
    con.close()


def fetch_provider_album_lookup_cached(
    provider: str,
    artist_name: str,
    album_title: str,
    fetcher,
) -> dict | None:
    """
    Cache wrapper around provider album fetchers.
    """
    if SCAN_DISABLE_CACHE:
        return fetcher(artist_name, album_title)
    cached_status, cached_payload = get_cached_provider_album_lookup(provider, artist_name, album_title)
    if cached_status == "found":
        return cached_payload
    if cached_status in {"not_found", "error"}:
        return None
    try:
        payload = fetcher(artist_name, album_title)
    except Exception:
        set_cached_provider_album_lookup(provider, artist_name, album_title, "error", None)
        raise
    if payload:
        set_cached_provider_album_lookup(provider, artist_name, album_title, "found", payload if isinstance(payload, dict) else None)
        return payload
    set_cached_provider_album_lookup(provider, artist_name, album_title, "not_found", None)
    return None

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ STATE IN MEMORY ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
state = {
    "scanning": False,
    "scan_type": "full",
    "scan_resume_run_id": None,
    "scan_progress": 0,
    "scan_total": 0,
    "deduping": False,
    "dedupe_progress": 0,
    "dedupe_total": 0,
    "dedupe_start_time": None,
    "dedupe_saved_this_run": 0,
    "dedupe_current_group": None,
    "dedupe_last_write": None,  # {"path": str, "at": float} after each move to /dupes
    # duplicates: { artist_name: [ { artist, album_id, best, losers } ] }
    "duplicates": {},
    # Scan details tracking
    "scan_artists_processed": 0,      # Nombre d'artistes trait√©s
    "scan_artists_total": 0,          # Total d'artistes
    "scan_detected_artists_total": 0, # Artists detected from source before resume/incremental filtering
    "scan_detected_albums_total": 0,  # Albums detected from source before resume/incremental filtering
    "scan_resume_skipped_artists": 0, # Artists skipped by resume logic for current run
    "scan_resume_skipped_albums": 0,  # Albums skipped by resume logic for current run
    "scan_ai_used_count": 0,          # Nombre de groupes o√π l'IA a √©t√© utilis√©e
    "scan_mb_used_count": 0,          # Nombre d'√©ditions enrichies avec MusicBrainz
    "scan_ai_enabled": False,         # Si l'IA est configur√©e et disponible
    "scan_mb_enabled": False,         # Si MusicBrainz est activ√©
    "scan_audio_cache_hits": 0,       # Nombre de fichiers audio trouv√©s en cache
    "scan_audio_cache_misses": 0,     # Nombre de fichiers audio n√©cessitant ffprobe
    "scan_mb_cache_hits": 0,         # Nombre de requ√™tes MusicBrainz trouv√©es en cache
    "scan_mb_cache_misses": 0,       # Nombre de requ√™tes MusicBrainz n√©cessitant API call
    # ETA tracking
    "scan_start_time": None,          # Timestamp du d√©but du scan
    "scan_last_update_time": None,    # Derni√®re mise √† jour pour calcul ETA
    "scan_last_progress": 0,          # Progression au dernier update
    "scan_format_done_count": 0,     # Albums that completed format (FFprobe) step
    "scan_mb_done_count": 0,         # Albums that completed MusicBrainz lookup step
    "scan_step_total": 0,            # Total steps for progress bar (3*albums + 2 or +3 if move)
    "scan_step_progress": 0,         # Steps completed (format + MB + compare + AI + finalize + move)
    "scan_active_artists": {},       # Dict {artist_name: {"start_time": float, "total_albums": int, "albums_processed": int}}
    "scan_post_processing": False,   # True while post-scan/post-artist metadata fixing is running
    "scan_post_total": 0,            # Total albums scheduled for post-processing
    "scan_post_done": 0,             # Albums processed in post-processing
    "scan_post_current_artist": None,
    "scan_post_current_album": None,
    # Files mode discovery counters (source walk before artist workers start).
    "scan_discovery_running": False,
    "scan_discovery_current_root": None,
    "scan_discovery_roots_done": 0,
    "scan_discovery_roots_total": 0,
    "scan_discovery_files_found": 0,
    "scan_discovery_folders_found": 0,
    "scan_discovery_albums_found": 0,
    "scan_discovery_artists_found": 0,
    "improve_all": None,              # { "running": bool, "artist_id": int, "current": int, "total": int, "log": [], "result": {}, "error": str } or None
    "last_fix_all_by_provider": None, # { "musicbrainz": {identified,covers,tags}, "discogs": ..., "lastfm": ..., "bandcamp": ... } from last global fix-all run
    "last_fix_all_total_albums": 0,   # Total albums processed in that run (for N/M match display)
    "lidarr_add_incomplete": None,    # { "running": bool, "current": int, "total": int, "current_album": str, "current_artist": str, "added": int, "failed": int, "result": {} } or None
    "last_lidarr_add_added": 0,
    "last_lidarr_add_failed": 0,
    "incomplete_scan": None,           # { "running": bool, "run_id": int, "progress": int, "total": int, "current_artist": str, "current_album": str, "count": int, "error": str } or None
    "files_editions_by_album_id": {},  # Populated by _build_scan_plan in Files mode for workers and export
    "export_progress": None,           # { "running": bool, "tracks_done": int, "total_tracks": int, "albums_done": int, "total_albums": int, "error": str } or None
    "files_index": {
        "running": False,
        "started_at": None,
        "finished_at": None,
        "phase": None,
        "current_folder": None,
        "folders_processed": 0,
        "total_folders": 0,
        "artists": 0,
        "albums": 0,
        "tracks": 0,
        "error": None,
    },
    "files_watcher": {
        "running": False,
        "roots": [],
        "dirty_count": 0,
        "last_event_at": None,
        "last_event_path": None,
    },
    "scan_dirty_folders_pending_clear": [],
    "scan_pipeline_flags": {},
    "scan_pipeline_sync_target": "none",
    "scan_incomplete_moved_count": 0,
    "scan_incomplete_moved_mb": 0,
    "scan_player_sync_target": None,
    "scan_player_sync_ok": None,
    "scan_player_sync_message": "",
}
lock = threading.Lock()
files_index_lock = threading.Lock()
_files_watcher_lock = threading.Lock()
_files_watcher_observer = None



# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ PLEX DB helper ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def plex_connect() -> sqlite3.Connection:
    """
    Open the Plex SQLite DB using UTF-8 *surrogate-escape* decoding so that any
    non-UTF-8 bytes are mapped to the U+DCxx range instead of throwing an error.

    We explicitly use immutable=1 to avoid any attempt by SQLite to write
    journal/WAL files on the Plex volume (which is mounted read-only in PMDA
    and can otherwise produce disk I/O errors on some filesystems).
    """
    # Open the Plex database in read-only + immutable mode to avoid write errors
    con = sqlite3.connect(f"file:{PLEX_DB_FILE}?mode=ro&immutable=1", uri=True, timeout=30)
    con.text_factory = lambda b: b.decode("utf-8", "surrogateescape")
    return con


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ UTILITIES ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def plex_api(path: str, method: str = "GET", **kw):
    headers = kw.pop("headers", {})
    headers["X-Plex-Token"] = PLEX_TOKEN
    return requests.request(method, f"{PLEX_HOST}{path}", headers=headers, timeout=60, **kw)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Discord notifications ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def notify_discord(content: str):
    """
    Fire‚Äëand‚Äëforget Discord webhook notifier.
    Disabled when DISCORD_WEBHOOK is empty.
    """
    if not DISCORD_WEBHOOK:
        return
    try:
        requests.post(DISCORD_WEBHOOK, json={"content": content}, timeout=10)
    except Exception as e:
        logging.warning("Discord notification failed: %s", e)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Discord embed notification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def notify_discord_embed(title: str, description: str, thumbnail_url: str = "", fields: list[dict] | None = None):
    """
    Send a nicely formatted Discord embed so we can show album artwork
    and keep the message tidy.
    """
    if not DISCORD_WEBHOOK:
        return
    embed: dict = {
        "title": title,
        "description": description,
    }
    if thumbnail_url:
        embed["thumbnail"] = {"url": thumbnail_url}
    if fields:
        embed["fields"] = fields[:25]   # Discord hard‚Äëlimit is 25 fields / embed
    try:
        requests.post(DISCORD_WEBHOOK, json={"embeds": [embed]}, timeout=10)
    except Exception as e:
        logging.warning("Discord embed failed: %s", e)

# ‚îÄ‚îÄ‚îÄ Run connection check & self‚Äëdiagnostic (called from main so WebUI can start first in serve mode) ‚îÄ‚îÄ‚îÄ
def run_startup_checks() -> None:
    """Run Plex validation, self-diagnostic and path cross-check. Call after starting the WebUI server in serve mode."""
    if not PLEX_CONFIGURED:
        logging.info("Skipping startup checks (Plex not configured ‚Äì use Settings to configure).")
        return
    _validate_plex_connection()
    if not _self_diag():
        raise SystemExit("Self‚Äëdiagnostic failed ‚Äì please fix the issues above and restart PMDA.")
    if not DISABLE_PATH_CROSSCHECK:
        _cross_check_bindings()
    else:
        logging.info("PATH cross-check skipped (DISABLE_PATH_CROSSCHECK=true).")


def container_to_host(p: str) -> Optional[Path]:
    for pre, real in PATH_MAP.items():
        if p.startswith(pre):
            return Path(real) / p[len(pre):].lstrip("/")
    return None

def path_for_fs_access(p: Path) -> Path:
    """
    Return a path that the current process can read (e.g. container path when
    running in Docker). If *p* exists, return it. Otherwise, if *p* matches
    a PATH_MAP value (real/host side), convert it to the corresponding key
    (plex/container side) so that safe_folder_size and similar can succeed.
    """
    try:
        if p.exists():
            return p
    except Exception:
        pass
    sp = str(p)
    for plex_prefix, real_prefix in PATH_MAP.items():
        if sp.startswith(real_prefix):
            suffix = sp[len(real_prefix):].lstrip("/")
            return Path(plex_prefix) / suffix if suffix else Path(plex_prefix)
    return p

def relative_path_under_known_roots(path: Path) -> Optional[Path]:
    """
    Return the relative path of *path* under any known PATH_MAP root (host or container).
    Falls back to ``None`` when the path is outside every configured root.
    """
    try:
        resolved = path.resolve()
    except Exception:
        resolved = path

    roots: list[Path] = []
    for value in PATH_MAP.values():
        roots.append(Path(value))
    for key in PATH_MAP.keys():
        roots.append(Path(key))

    for root in roots:
        candidates = {root}
        try:
            candidates.add(root.resolve())
        except Exception:
            pass
        for candidate in candidates:
            try:
                return resolved.relative_to(candidate)
            except ValueError:
                continue
    return None

def _sanitize_path_component(s: str, max_len: int = 120) -> str:
    """Sanitize artist or album title for use in a path component (no slashes, no leading/trailing dots)."""
    if not s:
        return "Unknown"
    out = re.sub(r'[/\\:*?"<>|]', "_", str(s).strip())
    out = out.strip(" .") or "Unknown"
    return out[:max_len] if len(out) > max_len else out


def _sanitize_album_title_display(title: str) -> str:
    """Normalize album titles for display/indexing (avoid obvious provider artifacts like trailing commas)."""
    t = str(title or "").strip()
    t = re.sub(r"\s+", " ", t).strip()
    # Strip trailing punctuation that is almost certainly an artifact.
    while t and t[-1] in {",", ";", ":"}:
        t = t[:-1].rstrip()
    return t


def _backup_album_folder_before_fix(folder: Path, artist: str, album_title: str) -> Optional[Path]:
    """
    Copy the album folder to DUPE_ROOT/original_version/Artist/Album (with suffix if exists).
    Returns the destination path on success, None on failure. Logs and adds to detailed log via steps (caller adds step).
    """
    dupe_root = getattr(sys.modules[__name__], "DUPE_ROOT", Path("/dupes"))
    base = dupe_root / "original_version" / _sanitize_path_component(artist) / _sanitize_path_component(album_title)
    # We only ever want a single backup per album. If a backup folder already
    # exists for this artist/album, reuse it and do not create numbered
    # variants on subsequent runs.
    dst = base
    if dst.exists():
        logging.info(
            "[Backup before fix] Backup already exists for %s ‚Äì reusing %s and skipping new copy",
            folder,
            dst,
        )
        return dst
    try:
        logging.info("[Backup before fix] Copying %s -> %s", folder, dst)
        dst.parent.mkdir(parents=True, exist_ok=True)
        shutil.copytree(str(folder), str(dst))
        logging.info("[Backup before fix] Backed up album to %s", dst)
        return dst
    except Exception as e:
        logging.error("[Backup before fix] Backup failed: %s -> %s: %s", folder, dst, e)
        return None


def build_dupe_destination(src_folder: Path) -> Path:
    """
    Compute the destination path under DUPE_ROOT while preserving the relative
    artist/album structure whenever possible.
    """
    rel = relative_path_under_known_roots(src_folder)
    if rel is None or str(rel).strip() == "":
        rel = Path(src_folder.name)
    return DUPE_ROOT / rel

def folder_size(p: Path) -> int:
    return sum(f.stat().st_size for f in p.rglob("*") if f.is_file())

def safe_folder_size(p: Path) -> int:
    """Return folder size in bytes, or 0 if path missing or not readable."""
    try:
        if not p.exists() or not p.is_dir():
            return 0
        return folder_size(p)
    except Exception:
        return 0

def score_format(ext: str) -> int:
    return FMT_SCORE.get(ext.lower(), 0)

def norm_album(title: str) -> str:
    """
    Normalise an album title for duplicate grouping.
    ‚Ä¢ Remove parenthetical/bracketed content conservatively.
    ‚Ä¢ Collapse whitespace and lowercase.
    ‚Ä¢ If the result is empty or too short (<3), fall back to the raw title (lowercased).
    ‚Ä¢ If still empty, return a unique placeholder so different unknown titles don't collide.
    """
    raw = (title or "").strip()
    # Remove any content in parentheses or brackets
    cleaned = re.sub(r"[\(\[][^(\)\]]*[\)\]]", "", raw)
    cleaned = " ".join(cleaned.split()).lower()

    if len(cleaned) >= 3:
        return cleaned

    # Fallback to raw (lowercased) if cleaning erased the useful bits
    fallback = raw.lower()
    fallback = " ".join(fallback.split())
    if len(fallback) >= 3:
        return fallback

    # Last resort: avoid collapsing different untitled releases together
    if raw:
        h = hashlib.sha1(raw.encode("utf-8", "ignore")).hexdigest()[:8]
        return f"__untitled__-{h}"
    return "__untitled__"


# Regex for format/version parenthetical suffixes: (flac), (mp3), (EP), (flac, EP), etc.
_PARENTHETICAL_SUFFIX_RE = re.compile(r"(?:\s*\([\w\s,]+\))+$", re.IGNORECASE)


def strip_parenthetical_suffixes(s: str) -> str:
    """
    Remove trailing parenthetical format/version segments from a string.
    Examples: "Album (flac)" -> "Album", "Album (flac) (EP)" -> "Album", "Album (flac, EP)" -> "Album".
    """
    if not s or not s.strip():
        return (s or "").strip()
    out = _PARENTHETICAL_SUFFIX_RE.sub("", (s or "").strip())
    return out.strip()


def norm_album_for_dedup(title: str, normalize_parenthetical: bool) -> str:
    """
    Normalise an album title for duplicate grouping, with optional parenthetical handling.
    When normalize_parenthetical is True: strip format/version parentheticals (flac), (mp3), (EP), etc.
    so that "Lemodie (Flac)" and "Lemodie" group together. When False, do not strip them
    (treat "Lemodie (Flac)" and "Lemodie" as different).
    """
    raw = (title or "").strip()
    if normalize_parenthetical:
        raw = strip_parenthetical_suffixes(raw) or raw
    cleaned = " ".join(raw.split()).lower()
    if len(cleaned) >= 3:
        return cleaned
    fallback = (title or "").strip().lower()
    fallback = " ".join(fallback.split())
    if len(fallback) >= 3:
        return fallback
    if raw or (title or "").strip():
        h = hashlib.sha1((raw or title or "").encode("utf-8", "ignore")).hexdigest()[:8]
        return f"__untitled__-{h}"
    return "__untitled__"


# Dupe Detection v2: stronger album-title normalization for grouping (recall) while keeping
# "edition markers" available for explainability / classification.
_DUPE_NOISE_WORDS = {
    # Sources / release pipeline noise
    "web", "web-flac", "retail", "scene", "rip",
    # Formats / containers / codecs
    "flac", "mp3", "wav", "aiff", "alac", "ape", "wv", "ogg", "opus", "m4a", "aac",
    "dsd", "dsf", "dff", "mqa", "sacd",
    # Media
    "vinyl", "cassette", "cd", "digital",
}
_DUPE_EDITION_MARKERS = {
    # Variants that are usually the same "core" album for duplicate grouping
    "remaster", "remastered", "remastering",
    "deluxe", "expanded", "anniversary", "edition", "reissue", "re-release", "rerelease",
    "special", "limited", "extended", "enhanced",
    "bonus", "extras", "outtakes",
    "mono", "stereo",
    "explicit", "clean",
}
_DUPE_CONTENT_MARKERS = {
    # Markers that often mean a *different* album (avoid over-grouping by stripping them)
    "live", "demo", "soundtrack", "ost", "score",
}


def _dupe_extract_edition_tokens(title: str) -> list[str]:
    """Extract edition/variant markers from noisy album titles (kept for explainability)."""
    raw = (title or "").strip()
    if not raw:
        return []
    low = raw.lower()
    found: list[str] = []

    # Pull tokens from bracket/parenthetical segments too.
    for seg in re.findall(r"[\(\[]([^)\]]+)[\)\]]", low):
        seg = re.sub(r"\s+", " ", (seg or "").strip())
        if not seg:
            continue
        for tok in re.split(r"[\s,/|;]+", seg):
            t = (tok or "").strip()
            if not t:
                continue
            if t in _DUPE_NOISE_WORDS or t in _DUPE_EDITION_MARKERS or t in _DUPE_CONTENT_MARKERS:
                found.append(t)
            # Hi-res patterns like 24-96, 16/44.1
            if re.fullmatch(r"\d{1,2}[-/]\d{2,3}(?:\.\d)?", t):
                found.append(t)
            # Bitrate like 320kbps
            if re.fullmatch(r"\d{3,4}kbps", t):
                found.append(t)
            # "24bit", "16-bit"
            if re.fullmatch(r"\d{1,2}\s*[- ]?bit", t):
                found.append(t.replace(" ", ""))

    # Whole-title scan for common markers.
    for w in sorted(_DUPE_EDITION_MARKERS | _DUPE_CONTENT_MARKERS | _DUPE_NOISE_WORDS):
        if re.search(rf"\\b{re.escape(w)}\\b", low):
            found.append(w)

    # Resolution patterns in free text: "24-96", "24/96", "16-44.1"
    for m in re.findall(r"\b\d{1,2}\s*[-/]\s*\d{2,3}(?:\.\d)?\b", low):
        found.append(re.sub(r"\s+", "", m))

    # Dedupe keep order
    out: list[str] = []
    seen = set()
    for t in found:
        tt = (t or "").strip().lower()
        if not tt or tt in seen:
            continue
        seen.add(tt)
        out.append(tt)
    return out[:20]


def norm_album_for_dedup_loose(title: str) -> str:
    """
    Aggressive title normalization for dupe candidate grouping.
    Removes common release pipeline noise (WEB-FLAC, 24-96, etc.) and edition markers
    (Remastered/Deluxe/Expanded/Anniversary...) while keeping content markers like "live".
    """
    raw = (title or "").strip()
    if not raw:
        return "__untitled__"

    s = raw.replace("_", " ")
    # Drop bracketed segments entirely (often pure noise).
    s = re.sub(r"\[[^\]]*\]", " ", s)
    # Drop parenthetical segments (keep tokens separately via _dupe_extract_edition_tokens()).
    s = re.sub(r"\([^)]*\)", " ", s)

    low = s.lower()
    # Normalize separators
    low = re.sub(r"[_‚Ä¢¬∑]+", " ", low)
    # Remove common hi-res / bitrate markers
    low = re.sub(r"\b\d{1,2}\s*[-/]\s*\d{2,3}(?:\.\d)?\b", " ", low)  # 24-96, 16/44.1
    low = re.sub(r"\b\d{3,4}\s*kbps\b", " ", low)
    low = re.sub(r"\b\d{1,2}\s*[- ]?bit\b", " ", low)
    low = re.sub(r"\b\d{2,3}(?:\.\d)?\s*khz\b", " ", low)

    # Remove noise words + edition markers, but keep content markers (live, soundtrack, etc.)
    drop_words = (_DUPE_NOISE_WORDS | _DUPE_EDITION_MARKERS) - _DUPE_CONTENT_MARKERS
    for w in sorted(drop_words, key=len, reverse=True):
        low = re.sub(rf"\\b{re.escape(w)}\\b", " ", low)

    # Strip catalog-like tokens (heuristic): ABC-1234, abc1234, etc.
    low = re.sub(r"\b[a-z]{2,6}[- ]?\d{2,6}\b", " ", low)

    # Collapse punctuation and whitespace
    low = re.sub(r"[\"'`]", "", low)
    low = re.sub(r"[^\w\s]+", " ", low)
    low = " ".join(low.split()).strip()

    if len(low) >= 3:
        return low

    # Fallback to strict normalization if loose collapsed too much.
    try:
        return norm_album_for_dedup(raw, normalize_parenthetical=True)
    except Exception:
        return low or "__untitled__"

def derive_album_title(plex_title: str, meta: Dict[str, str], folder: Path, album_id: int) -> Tuple[str, str]:
    """
    Pick the most trustworthy album title available and return (title, source).
    Priority: Plex DB ‚Üí embedded tags ‚Üí folder name ‚Üí unique placeholder.
    """
    if plex_title:
        title = plex_title.strip()
        if title:
            return (title, "plex")

    for key in ("album", "title", "release", "albumartist"):
        candidate = meta.get(key, "")
        if candidate:
            title = candidate.strip()
            if title:
                return (title, f"tag:{key}")

    folder_title = folder.name.replace("_", " ").strip()
    if folder_title:
        return (folder_title, "folder")

    return (f"Untitled Album #{album_id}", "placeholder")

def get_primary_format(folder: Path) -> str:
    try:
        for f in folder.rglob("*"):
            if AUDIO_RE.search(f.name):
                return f.suffix[1:].upper()
    except OSError as e:
        logging.debug("get_primary_format I/O error for %s: %s", folder, e)
    return "UNKNOWN"

def thumb_url(album_id: int) -> str:
    return f"{PLEX_HOST}/library/metadata/{album_id}/thumb?X-Plex-Token={PLEX_TOKEN}"

def build_cards() -> list[dict]:
    """
    Convert the live state["duplicates"] structure into the list of card
    dictionaries expected by the front-end.  Called both by the initial
    page render and the /api/duplicates endpoint so that new cards appear
    incrementally while a scan is running.
    """
    cards: list[dict] = []
    for artist, groups in state["duplicates"].items():
        for g in groups:
            best = g["best"]
            # Ensure "date" is present in each version for modal rendering
            if "meta" in best:
                best["date"] = best["meta"].get("date") or best["meta"].get("originaldate") or ""
            for l in g.get("losers", []):
                if "meta" in l:
                    l["date"] = l["meta"].get("date") or l["meta"].get("originaldate") or ""
            best_fmt = best.get("fmt_text") or get_primary_format(Path(best["folder"]))
            cards.append(
                {
                    "artist_key": artist.replace(" ", "_"),
                    "artist": artist,
                    "album_id": best["album_id"],
                    "n": len(g["losers"]) + 1,
                    "best_thumb": thumb_url(best["album_id"]),
                    "best_title": best["title_raw"],
                    "best_fmt": best_fmt,
                    "formats": [best_fmt]
                    + [
                        l.get("fmt_text") or l.get("fmt") or get_primary_format(Path(l["folder"]))
                        for l in g["losers"]
                    ],
                    "used_ai": best.get("used_ai", False),
                }
            )
    return cards
@app.route("/api/edition_details")
def edition_details():
    album_id = int(request.args["album_id"])
    folder   = Path(request.args["folder"])
    tracks   = get_tracks(plex_connect(), album_id)
    fmt_score, br, sr, bd, _ = analyse_format(folder)
    info = (fmt_score, br, sr, bd)  # Return 4-tuple for backward compatibility
    track_list = [{"idx": t.idx, "title": t.title, "dur": t.dur} for t in tracks]
    return jsonify({"tracks": track_list, "info": info})

@app.route("/api/dedupe_manual", methods=["POST"])
def dedupe_manual():
    r = _requires_config()
    if r is not None:
        return r
    req = request.get_json(force=True)
    for item in req:
        # reuse existing purge logic
        _purge_invalid_edition({
            "folder"   : item["folder"],
            "artist"   : "",           # not needed for purge
            "title_raw": "",
            "album_id" : int(item["album_id"])
        })
    return jsonify({"status":"ok"})

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ DATABASE HELPERS ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
class Track(NamedTuple):
    title: str
    idx: int
    disc: int
    dur: int  # duration in ms

def get_tracks(db_conn, album_id: int) -> List[Track]:
    has_parent = any(r[1] == "parent_index"
                     for r in db_conn.execute("PRAGMA table_info(metadata_items)"))
    sql = f"""
      SELECT tr.title, tr."index",
             {'tr.parent_index' if has_parent else 'NULL'} AS disc_no,
             mp.duration
      FROM metadata_items tr
      JOIN media_items mi ON mi.metadata_item_id = tr.id
      JOIN media_parts mp ON mp.media_item_id = mi.id
      WHERE tr.parent_id = ? AND tr.metadata_type = 10
    """
    rows = db_conn.execute(sql, (album_id,)).fetchall()
    return [Track(t.lower().strip(), i or 0, d or 1, dur or 0)
            for t, i, d, dur in rows]

def get_tracks_with_ids(db_conn, album_id: int) -> List[dict]:
    """Return list of track dicts with id, title, index, duration_ms for library playback API."""
    has_parent = any(r[1] == "parent_index"
                     for r in db_conn.execute("PRAGMA table_info(metadata_items)"))
    sql = f"""
      SELECT tr.id, tr.title, tr."index",
             {'tr.parent_index' if has_parent else 'NULL'} AS disc_no,
             COALESCE(mp.duration, tr.duration, 0) AS duration
      FROM metadata_items tr
      JOIN media_items mi ON mi.metadata_item_id = tr.id
      JOIN media_parts mp ON mp.media_item_id = mi.id
      WHERE tr.parent_id = ? AND tr.metadata_type = 10
      ORDER BY tr."index"
    """
    rows = db_conn.execute(sql, (album_id,)).fetchall()
    return [
        {"id": r[0], "title": (r[1] or "").strip(), "index": r[2] or 0, "duration_ms": r[4] or 0}
        for r in rows
    ]

def _stream_columns(db_conn) -> tuple[str, str] | None:
    """If media_streams exists with codec/bitrate, return (codec_col, bitrate_col) else None."""
    try:
        info = db_conn.execute("PRAGMA table_info(media_streams)").fetchall()
        cols = {r[1].lower() for r in info}
        if "codec" in cols and "bitrate" in cols:
            return ("codec", "bitrate")
        return None
    except Exception:
        return None

def get_tracks_for_details(db_conn, album_id: int) -> List[dict]:
    """
    Return list of track dicts for API: name, title, idx, duration (seconds), dur (ms),
    format (codec), bitrate (kbps), for use in /details editions.
    """
    has_parent = any(r[1] == "parent_index"
                     for r in db_conn.execute("PRAGMA table_info(metadata_items)"))
    stream_cols = _stream_columns(db_conn)
    if stream_cols is None:
        # No media_streams codec/bitrate: return basic track info (duration from part or metadata_items)
        rows = db_conn.execute(f"""
          SELECT tr.title, tr."index",
                 {'tr.parent_index' if has_parent else 'NULL'} AS disc_no,
                 COALESCE(mp.duration, tr.duration) AS duration, mp.file
          FROM metadata_items tr
          JOIN media_items mi ON mi.metadata_item_id = tr.id
          JOIN media_parts mp ON mp.media_item_id = mi.id
          WHERE tr.parent_id = ? AND tr.metadata_type = 10
          ORDER BY tr."index"
        """, (album_id,)).fetchall()
        return [
            {
                "name": (t or "").strip(),
                "title": (t or "").strip(),
                "idx": i or 0,
                "duration": (dur or 0) // 1000,
                "dur": dur or 0,
                "format": None,
                "bitrate": None,
                "path": (raw_path or "").strip() or None,
            }
            for t, i, _d, dur, raw_path in rows
        ]
    codec_col, bitrate_col = stream_cols
    # stream_type_id 2 = audio in Plex; one row per track (pick stream with max bitrate if several)
    sql = f"""
      SELECT tr.title, tr."index",
             {'tr.parent_index' if has_parent else 'NULL'} AS disc_no,
             COALESCE(mp.duration, tr.duration), ms.{codec_col}, ms.{bitrate_col}, mp.file
      FROM metadata_items tr
      JOIN media_items mi ON mi.metadata_item_id = tr.id
      JOIN media_parts mp ON mp.media_item_id = mi.id
      LEFT JOIN media_streams ms ON ms.media_part_id = mp.id AND ms.stream_type_id = 2
        AND ms.id = (
          SELECT ms2.id FROM media_streams ms2
          WHERE ms2.media_part_id = mp.id AND ms2.stream_type_id = 2
          ORDER BY COALESCE(ms2.{bitrate_col}, 0) DESC LIMIT 1
        )
      WHERE tr.parent_id = ? AND tr.metadata_type = 10
      ORDER BY tr."index"
    """
    try:
        rows = db_conn.execute(sql, (album_id,)).fetchall()
    except Exception as e:
        logging.warning(
            "get_tracks_for_details (streams) failed for album_id=%s: %s",
            album_id, e
        )
        tracks = get_tracks(db_conn, album_id)
        return [
            {"name": t.title, "title": t.title, "idx": t.idx, "duration": t.dur // 1000, "dur": t.dur, "format": None, "bitrate": None, "path": None}
            for t in tracks
        ]
    out = []
    seen_index = set()
    for row in rows:
        t, i, _d, dur, codec, bitrate, raw_path = row
        idx = i or 0
        if idx in seen_index:
            continue
        seen_index.add(idx)
        title = (t or "").strip()
        dur_ms = dur or 0
        # Plex DB: bitrate is in bps (e.g. 867234 -> 867 kbps)
        br_kbps = None
        if bitrate is not None:
            b = int(bitrate)
            br_kbps = b if b < 100000 else b // 1000
        out.append({
            "name": title,
            "title": title,
            "idx": idx,
            "duration": dur_ms // 1000,
            "dur": dur_ms,
            "format": (codec or "").strip().upper() or None,
            "bitrate": br_kbps,
            "path": (raw_path or "").strip() or None,
        })
    return out

def album_title(db_conn, album_id: int) -> str:
    row = db_conn.execute(
        "SELECT title FROM metadata_items WHERE id = ?", (album_id,)
    ).fetchone()
    return row[0] if row else ""

def first_part_path(db_conn, album_id: int) -> Optional[Path]:
    sql = """
      SELECT mp.file
      FROM metadata_items tr
      JOIN media_items mi ON mi.metadata_item_id = tr.id
      JOIN media_parts mp ON mp.media_item_id = mi.id
      WHERE tr.parent_id = ? LIMIT 1
    """
    row = db_conn.execute(sql, (album_id,)).fetchone()
    if not row:
        return None

    raw_path = row[0]
    # Try to map to host path, fallback to container path if mapping missing
    host_loc = container_to_host(raw_path)
    if host_loc is None:
        return Path(raw_path).parent

    return host_loc.parent


def _album_path_under_dupes(db_conn, album_id: int) -> bool:
    """Return True if the album's path is under DUPE_ROOT (already moved). Used to skip library-only groups that were already deduped."""
    sql = """
      SELECT mp.file FROM metadata_items tr
      JOIN media_items mi ON mi.metadata_item_id = tr.id
      JOIN media_parts mp ON mp.media_item_id = mi.id
      WHERE tr.parent_id = ? LIMIT 1
    """
    row = db_conn.execute(sql, (album_id,)).fetchone()
    if not row or not row[0]:
        return False
    raw = (row[0] or "").replace("\\", "/")
    if str(DUPE_ROOT) in raw or raw.strip().startswith("/dupes") or "/dupes/" in raw:
        return True
    if "dupes" in raw.lower() and ("/dupes" in raw or "Music_dupes" in raw or "dupes/" in raw):
        return True
    return False


# Cover filenames we consider "has cover" (same as create_pmda_test_files.sh)
_COVER_NAMES = (
    "cover.jpg", "cover.png", "cover.jpeg",
    "folder.jpg", "Folder.jpg", "AlbumArt.jpg", "AlbumArtSmall.jpg",
    "front.jpg", "artwork.jpg",
)

def album_folder_has_cover(folder: Path) -> bool:
    """Return True if the album folder contains any known cover image file."""
    if not folder or not folder.is_dir():
        return False
    try:
        for name in _COVER_NAMES:
            if (folder / name).is_file():
                return True
        return False
    except OSError:
        return False


def _first_cover_path(folder: Path) -> Optional[Path]:
    """Return the path of the first existing cover file in the folder, or None."""
    if not folder or not folder.is_dir():
        return None
    try:
        for name in _COVER_NAMES:
            p = folder / name
            if p.is_file():
                return p
        return None
    except OSError:
        return None


def _extract_embedded_cover_from_audio(audio_path: Path) -> Optional[tuple[bytes, str]]:
    """
    Extract the first embedded cover image from an audio file (FLAC, MP3, M4A, etc.).
    Returns (image_bytes, mime_type) or None. Used when no cover file exists in the folder.
    """
    if not audio_path or not audio_path.is_file():
        return None
    try:
        from mutagen import File as MutagenFile
        f = MutagenFile(str(audio_path))
        if f is None:
            return None
        # FLAC: pictures
        if hasattr(f, "pictures") and f.pictures:
            pic = f.pictures[0]
            mime = getattr(pic, "mime", "image/jpeg") or "image/jpeg"
            return (pic.data, mime)
        # MP3: ID3 APIC
        if hasattr(f, "tags") and f.tags:
            apics = f.tags.getall("APIC") if hasattr(f.tags, "getall") else []
            if not apics and "APIC:Cover" in f.tags:
                apics = [f.tags["APIC:Cover"]]
            for apic in apics:
                if getattr(apic, "data", None):
                    mime = getattr(apic, "mime", "image/jpeg") or "image/jpeg"
                    return (bytes(apic.data), mime)
        # MP4/M4A: covr
        if hasattr(f, "get") and f.get("covr"):
            covr = f["covr"][0]
            if isinstance(covr, bytes):
                return (covr, "image/jpeg")
            if hasattr(covr, "rawdata"):
                return (bytes(covr.rawdata), "image/jpeg")
    except Exception as e:
        logging.debug("[Vision] Extract embedded cover from %s failed: %s", audio_path, e)
    return None


def _get_local_cover_data_uri_for_vision(folder: Path) -> Optional[str]:
    """
    Return a data URI for the album cover for vision comparison.
    First tries cover files in the folder (folder.jpg, cover.jpg, etc.);
    if none found, extracts embedded cover from the first audio file in the folder.
    """
    if not folder or not folder.is_dir():
        return None
    cover_path = _first_cover_path(folder)
    if cover_path:
        return _encode_local_cover_to_data_uri(cover_path)
    try:
        first_audio = next(
            (p for p in sorted(folder.rglob("*")) if AUDIO_RE.search(p.name)),
            None,
        )
        if not first_audio:
            return None
        result = _extract_embedded_cover_from_audio(first_audio)
        if not result:
            return None
        data, mime = result
        if len(data) > _MAX_COVER_SIZE_BYTES or mime != "image/jpeg":
            data, mime = _resize_cover_for_vision(data, mime)
        if not data:
            return None
        b64 = base64.b64encode(data).decode("ascii")
        return f"data:{mime};base64,{b64}"
    except Exception as e:
        logging.debug("[Vision] Fallback embedded cover failed: %s", e)
    return None


# Vision API: keep covers tiny (comparison does not need high resolution). Resize if larger.
_MAX_COVER_SIZE_BYTES = 100 * 1024  # 100 KB max payload for vision
_MAX_COVER_PIXELS = 256  # Max width/height; enough for album cover comparison


def _resize_cover_for_vision(data: bytes, mime: str) -> tuple[bytes, str]:
    """
    Resize/compress image to stay under _MAX_COVER_SIZE_BYTES. Returns (jpeg_bytes, "image/jpeg").
    Uses Pillow; on failure returns original data if small enough, else empty.
    """
    if len(data) <= _MAX_COVER_SIZE_BYTES and mime == "image/jpeg":
        return (data, mime)
    try:
        from io import BytesIO
        from PIL import Image
        img = Image.open(BytesIO(data)).convert("RGB")
        w, h = img.size
        if w > _MAX_COVER_PIXELS or h > _MAX_COVER_PIXELS:
            img.thumbnail((_MAX_COVER_PIXELS, _MAX_COVER_PIXELS), Image.Resampling.LANCZOS)
        buf = BytesIO()
        quality = 85
        img.save(buf, "JPEG", quality=quality, optimize=True)
        out = buf.getvalue()
        while len(out) > _MAX_COVER_SIZE_BYTES and quality > 25:
            quality -= 15
            buf = BytesIO()
            img.save(buf, "JPEG", quality=quality, optimize=True)
            out = buf.getvalue()
        return (out, "image/jpeg")
    except Exception as e:
        logging.debug("[Vision] Resize cover failed: %s", e)
        if len(data) <= _MAX_COVER_SIZE_BYTES:
            return (data, mime)
        return (b"", "")


def _encode_local_cover_to_data_uri(cover_path: Path) -> Optional[str]:
    """
    Read the cover file, resize if needed to stay under ~100 KB, encode as base64 data URI.
    Vision only needs small images for cover comparison.
    """
    if not cover_path or not cover_path.is_file():
        return None
    try:
        raw = cover_path.read_bytes()
        suffix = cover_path.suffix.lower()
        mime_map = {".jpg": "image/jpeg", ".jpeg": "image/jpeg", ".png": "image/png", ".gif": "image/gif", ".webp": "image/webp"}
        mime = mime_map.get(suffix, "image/jpeg")
        if len(raw) > _MAX_COVER_SIZE_BYTES or mime != "image/jpeg":
            raw, mime = _resize_cover_for_vision(raw, mime)
        if not raw:
            return None
        b64 = base64.b64encode(raw).decode("ascii")
        return f"data:{mime};base64,{b64}"
    except Exception as e:
        logging.debug("[Vision] Failed to encode cover %s: %s", cover_path, e)
        return None

def extract_tags(audio_path: Path) -> dict[str, str]:
    """
    Return *all* container‚Äëlevel metadata tags for the given audio file
    (FLAC/MP3/M4A/‚Ä¶).

    Uses ffprobe so no external Python deps are required.
    """
    try:
        out = subprocess.check_output(
            [
                "ffprobe", "-v", "error",
                "-show_entries", "format_tags",
                "-of", "default=noprint_wrappers=1",
                str(audio_path)
            ],
            stderr=subprocess.DEVNULL,
            text=True,
            timeout=10
        )
        tags = {}
        for line in out.splitlines():
            if "=" in line:
                k, v = line.split("=", 1)
                # ffprobe returns TAG:KEY=VAL sometimes ‚Äì strip the prefix
                if k.startswith("TAG:"):
                    k = k[4:]
                tags[k.lower()] = v.strip()
        return tags
    except Exception:
        return {}


def _iter_audio_files_under_roots(
    roots: list[str],
    *,
    progress_cb=None,
    progress_every: int = 250,
    heartbeat_seconds: float = 10.0,
) -> list[Path]:
    """
    Return a list of audio files under the given filesystem roots.
    This helper is backend‚Äëagnostic and only cares about AUDIO_RE matches.
    """
    out: list[Path] = []
    seen_paths: set[str] = set()
    files_found = 0
    entries_scanned = 0
    roots_total = len([r for r in (roots or []) if r])
    roots_done = 0
    heartbeat_seconds = float(heartbeat_seconds or 0.0)
    last_heartbeat = time.monotonic()
    for root in roots:
        if not root:
            continue
        base = Path(root)
        root_entries_scanned = 0
        if not base.exists():
            logging.debug("FILES_ROOTS entry %s does not exist; skipping", root)
            roots_done += 1
            if callable(progress_cb):
                try:
                    progress_cb(
                        {
                            "root": str(base),
                            "roots_done": roots_done,
                            "roots_total": roots_total,
                            "files_found": files_found,
                            "entries_scanned": entries_scanned,
                            "root_entries_scanned": root_entries_scanned,
                            "done": roots_done >= roots_total,
                        }
                    )
                except Exception:
                    pass
            continue
        for p in base.rglob("*"):
            entries_scanned += 1
            root_entries_scanned += 1
            if callable(progress_cb) and heartbeat_seconds > 0:
                now = time.monotonic()
                if (now - last_heartbeat) >= heartbeat_seconds:
                    last_heartbeat = now
                    try:
                        progress_cb(
                            {
                                "root": str(base),
                                "roots_done": roots_done,
                                "roots_total": roots_total,
                                "files_found": files_found,
                                "entries_scanned": entries_scanned,
                                "root_entries_scanned": root_entries_scanned,
                                "done": False,
                            }
                        )
                    except Exception:
                        pass
            try:
                if p.is_file() and AUDIO_RE.search(p.name):
                    sp = str(p)
                    if sp in seen_paths:
                        continue
                    seen_paths.add(sp)
                    out.append(p)
                    files_found += 1
                    if callable(progress_cb) and progress_every > 0 and (files_found % progress_every == 0):
                        try:
                            progress_cb(
                                {
                                    "root": str(base),
                                    "roots_done": roots_done,
                                    "roots_total": roots_total,
                                    "files_found": files_found,
                                    "entries_scanned": entries_scanned,
                                    "root_entries_scanned": root_entries_scanned,
                                    "done": False,
                                }
                            )
                        except Exception:
                            pass
            except (OSError, PermissionError):
                continue
        roots_done += 1
        if callable(progress_cb):
            try:
                progress_cb(
                    {
                        "root": str(base),
                        "roots_done": roots_done,
                        "roots_total": roots_total,
                        "files_found": files_found,
                        "entries_scanned": entries_scanned,
                        "root_entries_scanned": root_entries_scanned,
                        "done": roots_done >= roots_total,
                    }
                )
            except Exception:
                pass
    return out

# Global ffprobe pool for parallel processing
_ffprobe_pool: Optional[ThreadPoolExecutor] = None

def get_ffprobe_pool() -> ThreadPoolExecutor:
    """Get or create the global ffprobe pool."""
    global _ffprobe_pool
    if _ffprobe_pool is None:
        _ffprobe_pool = ThreadPoolExecutor(max_workers=FFPROBE_POOL_SIZE, thread_name_prefix="ffprobe")
        logging.debug(f"Created ffprobe pool with {FFPROBE_POOL_SIZE} workers")
    return _ffprobe_pool

def _run_ffprobe(fpath: str) -> tuple[int, int, int]:
    """
    Run ffprobe on a single file and return (bit_rate, sample_rate, bit_depth).
    This is the actual work function that will be run in the pool.
    """
    cmd = [
        "ffprobe", "-v", "error",
        "-select_streams", "a:0",
        "-show_entries",
        "format=bit_rate:stream=bit_rate,sample_rate,bits_per_raw_sample,bits_per_sample,sample_fmt",
        "-of", "default=noprint_wrappers=1",
        fpath,
    ]
    
    br = sr = bd = 0
    try:
        out = subprocess.check_output(
            cmd, stderr=subprocess.DEVNULL, text=True, timeout=10
        )
        for line in out.splitlines():
            key, _, val = line.partition("=")
            if key == "bit_rate":
                try:
                    v = int(val)
                    if v > br:
                        br = v  # keep highest bit‚Äërate seen
                except ValueError:
                    pass
            elif key == "sample_rate":
                try:
                    sr = int(val)
                except ValueError:
                    pass
            elif key in ("bits_per_raw_sample", "bits_per_sample"):
                try:
                    bd = int(val)
                except ValueError:
                    pass
            elif key == "sample_fmt" and not bd:
                m = re.match(r"s(\d+)", val)
                if m:
                    bd = int(m.group(1))
    except Exception:
        # leave br/sr/bd at 0 on failure
        pass
    
    return (br, sr, bd)


def _run_ffprobe_duration_sec(fpath: str) -> int:
    """Return media duration (seconds) via ffprobe. Best-effort; returns 0 on failure."""
    cmd = [
        "ffprobe", "-v", "error",
        "-show_entries", "format=duration",
        "-of", "default=noprint_wrappers=1:nokey=1",
        fpath,
    ]
    try:
        out = subprocess.check_output(cmd, stderr=subprocess.DEVNULL, text=True, timeout=20).strip()
        if not out:
            return 0
        return int(max(0.0, float(out)))
    except Exception:
        return 0

def analyse_format(folder: Path) -> tuple[int, int, int, int, bool]:
    """
    Inspect up to **three** audio files inside *folder* and return a 4‚Äëtuple:

        (fmt_score, bit_rate, sample_rate, bit_depth)

    *   **fmt_score** derives from the global FORMAT_PREFERENCE list.
    *   **bit_rate** is in **bps** (`0` when not reported, e.g. lossless FLAC).
    *   **sample_rate** is in **Hz**.
    *   **bit_depth** is 16 / 24 / 32 when derivable, otherwise 0.

    Rationale for retry logic
    -------------------------
    A single, transient ffprobe failure (network share hiccup, race during mount,
    etc.) previously led to a *false ¬´ invalid ¬ª* verdict because all tech values
    were 0.
    We now:

    1. Collect *all* audio files under the folder (breadth‚Äëfirst, glob pattern
       from `AUDIO_RE`).
    2. Probe **up to three distinct files** or **two attempts per file** (cache +
       fresh call) until we obtain at least one non‚Äëzero technical metric.
    3. Only if **every attempt** yields `(0, 0, 0)` do we fall back to the
       "invalid" classification.

    Each `(path, mtime)` result ‚Äì even the all‚Äëzero case ‚Äì is cached so we
    never hammer ffprobe, but a later scan still re‚Äëprobes if the file changes.
    
    Non-cached ffprobe calls are now processed in parallel using a thread pool.
    """
    audio_files = [p for p in folder.rglob("*") if AUDIO_RE.search(p.name)]
    if not audio_files:
        return (0, 0, 0, 0, False)

    # First pass: check cache for all files (unless global scan setting disables cache usage)
    use_cache = not getattr(sys.modules[__name__], "SCAN_DISABLE_CACHE", False)
    files_to_probe = []
    for audio_file in audio_files[:3]:
        ext   = audio_file.suffix[1:].lower()
        fpath = str(audio_file)
        mtime = int(audio_file.stat().st_mtime)

        # Check cache first (when enabled)
        if use_cache:
            cached = get_cached_info(fpath, mtime)
            if cached and not (cached == (0, 0, 0) and ext == "flac"):
                br, sr, bd = cached
                if br or sr or bd:
                    # Track cache hit (will be aggregated in scan_duplicates)
                    return (score_format(ext), br, sr, bd, True)  # True = cache hit
        
        # File not in cache or cache miss, add to probe list
        files_to_probe.append((audio_file, ext, fpath, mtime))
    
    # Second pass: probe files in parallel if pool is enabled
    if files_to_probe and FFPROBE_POOL_SIZE > 1:
        futures = {}
        pool = get_ffprobe_pool()
        
        for audio_file, ext, fpath, mtime in files_to_probe:
            future = pool.submit(_run_ffprobe, fpath)
            futures[future] = (audio_file, ext, fpath, mtime)
        
        # Wait for results (with timeout per file)
        for future in as_completed(futures):
            audio_file, ext, fpath, mtime = futures[future]
            try:
                br, sr, bd = future.result(timeout=15)  # Slightly longer timeout for pool
            except Exception:
                br, sr, bd = 0, 0, 0
            
            # Cache the result
            set_cached_info(fpath, mtime, br, sr, bd)
            
            if br or sr or bd:  # success on this file ‚Üí done
                return (score_format(ext), br, sr, bd, False)  # False = cache miss
    else:
        # Sequential processing (fallback or pool disabled)
        for audio_file, ext, fpath, mtime in files_to_probe:
            br, sr, bd = _run_ffprobe(fpath)
            
            # Cache the result
            set_cached_info(fpath, mtime, br, sr, bd)
            
            if br or sr or bd:  # success on this file ‚Üí done
                return (score_format(ext), br, sr, bd, False)  # False = cache miss

    # After probing up to 3 files and still nothing usable ‚Üí treat as invalid
    if audio_files:
        first_ext = audio_files[0].suffix[1:].lower()
        return (score_format(first_ext), 0, 0, 0, False)
    return (0, 0, 0, 0, False)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ DUPLICATE DETECTION ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def signature(tracks: List[Track]) -> tuple:
    # round durations to seconds before grouping
    return tuple(sorted((
        t.disc,
        t.idx,
        t.title,
        int(round(t.dur/1000))
    ) for t in tracks))

def overlap(a: set, b: set) -> float:
    return len(a & b) / max(len(a), len(b))

def _dupe_norm_track_title(s: str) -> str:
    """Normalize a track title for dupe similarity (robust to tags/filename noise)."""
    raw = (s or "").strip().lower()
    if not raw:
        return ""
    # Drop bracket/parenthetical segments and leading index patterns.
    raw = re.sub(r"[\(\[][^)\]]*[\)\]]", " ", raw)
    raw = re.sub(r"^\s*\d+\s*[-.)]\s*", "", raw)
    raw = re.sub(r"\s+", " ", raw)
    raw = raw.strip()
    # Strip punctuation, keep letters/numbers/spaces.
    raw = re.sub(r"[^\w\s]+", " ", raw)
    raw = " ".join(raw.split())
    return raw[:240]


def _dupe_track_title_set(tracks: list) -> set[str]:
    """Return a normalized set of track titles for an edition."""
    out: set[str] = set()
    for t in tracks or []:
        title = ""
        try:
            if isinstance(t, dict):
                title = str(t.get("title") or t.get("name") or "")
            else:
                title = str(getattr(t, "title", "") or "")
        except Exception:
            title = ""
        nt = _dupe_norm_track_title(title)
        if nt:
            out.add(nt)
    return out


def _dupe_jaccard(a: set[str], b: set[str]) -> float:
    if not a and not b:
        return 1.0
    if not a or not b:
        return 0.0
    inter = len(a & b)
    union = len(a | b)
    return (float(inter) / float(union)) if union else 0.0


def _dupe_track_count_ratio(a_tracks: list, b_tracks: list) -> float:
    a_n = len(a_tracks or [])
    b_n = len(b_tracks or [])
    if a_n <= 0 and b_n <= 0:
        return 1.0
    if a_n <= 0 or b_n <= 0:
        return 0.0
    return float(min(a_n, b_n)) / float(max(a_n, b_n))


def _dupe_folder_key_str(folder) -> str:
    """Stable-ish folder identity for caching/feedback (prefer resolved absolute path)."""
    if not folder:
        return ""
    try:
        return str(Path(str(folder)).resolve())
    except Exception:
        return str(folder)


def _dupe_group_key_from_editions(editions: list[dict]) -> str:
    """Compute a stable group key from the set of edition folders (order-independent)."""
    keys: list[str] = []
    for e in editions or []:
        k = _dupe_folder_key_str((e or {}).get("folder"))
        if k:
            keys.append(k)
    keys = sorted(set(keys))
    payload = "dupe_v2\n" + "\n".join(keys)
    return hashlib.sha1(payload.encode("utf-8", errors="ignore")).hexdigest()


def _dupe_feedback_pair_key(folder_a: str, folder_b: str) -> tuple[str, str]:
    a = (folder_a or "").strip()
    b = (folder_b or "").strip()
    if not a or not b:
        return ("", "")
    if a == b:
        return (a, b)
    return (a, b) if a < b else (b, a)


def _dupe_load_feedback_pairs_for_artist(artist: str) -> dict[tuple[str, str], str]:
    """Return {(folder_a, folder_b): label} for one artist."""
    out: dict[tuple[str, str], str] = {}
    artist_name = (artist or "").strip()
    if not artist_name:
        return out
    try:
        con = sqlite3.connect(str(STATE_DB_FILE), timeout=10)
        cur = con.cursor()
        cur.execute(
            "SELECT folder_a, folder_b, label FROM dupe_feedback_pairs WHERE artist = ?",
            (artist_name,),
        )
        rows = cur.fetchall()
        con.close()
    except Exception:
        return out
    for fa, fb, lab in rows or []:
        a = (fa or "").strip()
        b = (fb or "").strip()
        if not a or not b:
            continue
        key = _dupe_feedback_pair_key(a, b)
        if not key[0] or not key[1]:
            continue
        out[key] = (lab or "").strip().lower()
    return out


def _dupe_ai_cache_get(artist: str, group_key: str) -> Optional[dict]:
    if not artist or not group_key:
        return None
    try:
        con = sqlite3.connect(str(STATE_DB_FILE), timeout=10)
        cur = con.cursor()
        cur.execute(
            """
            SELECT best_folder, rationale, merge_list, ai_provider, ai_model, confidence
            FROM dupe_ai_cache
            WHERE artist = ? AND group_key = ?
            """,
            ((artist or "").strip(), (group_key or "").strip()),
        )
        row = cur.fetchone()
        con.close()
    except Exception:
        return None
    if not row:
        return None
    best_folder, rationale, merge_list_json, provider, model, confidence = row
    try:
        merge_list = json.loads(merge_list_json) if merge_list_json else []
        if not isinstance(merge_list, list):
            merge_list = []
    except Exception:
        merge_list = []
    conf = None
    try:
        if confidence is not None:
            conf = int(confidence)
    except Exception:
        conf = None
    return {
        "best_folder": (best_folder or "").strip(),
        "rationale": (rationale or "").strip(),
        "merge_list": merge_list,
        "ai_provider": (provider or "").strip(),
        "ai_model": (model or "").strip(),
        "confidence": conf,
    }


def _dupe_ai_cache_put(
    *,
    artist: str,
    group_key: str,
    best_folder: str,
    rationale: str,
    merge_list: list[str],
    ai_provider: str,
    ai_model: str,
    confidence: int | None,
) -> None:
    if not artist or not group_key:
        return
    now = time.time()
    try:
        con = sqlite3.connect(str(STATE_DB_FILE), timeout=10)
        cur = con.cursor()
        cur.execute(
            """
            INSERT INTO dupe_ai_cache
              (artist, group_key, best_folder, rationale, merge_list, ai_provider, ai_model, confidence, created_at, updated_at)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ON CONFLICT(artist, group_key) DO UPDATE SET
              best_folder = excluded.best_folder,
              rationale   = excluded.rationale,
              merge_list  = excluded.merge_list,
              ai_provider = excluded.ai_provider,
              ai_model    = excluded.ai_model,
              confidence  = excluded.confidence,
              updated_at  = excluded.updated_at
            """,
            (
                (artist or "").strip(),
                (group_key or "").strip(),
                (best_folder or "").strip(),
                (rationale or "").strip(),
                json.dumps(list(merge_list or [])),
                (ai_provider or "").strip(),
                (ai_model or "").strip(),
                int(confidence) if confidence is not None else None,
                now,
                now,
            ),
        )
        con.commit()
        con.close()
    except Exception:
        # Cache failures must never break the scan.
        return


def _dupe_choose_best_heuristic(editions: list[dict]) -> tuple[Optional[dict], str, list[str], bool]:
    """
    Deterministic fallback for choosing the "best" edition without AI.
    Returns (best, rationale, merge_list, confident).

    "Confident" is intentionally conservative: we only return confident=True when
    the pick is obviously superior (or identical track counts with better technical quality),
    so we can skip expensive AI calls without increasing wrong auto-moves.
    """
    if not editions:
        return (None, "", [], False)

    def _metrics(e: dict) -> tuple:
        # Deterministic quality/health signals (cheap):
        # - prefer editions with provider identity and MBIDs (less ambiguous)
        # - prefer editions with complete tags and a cover (better user experience and safer dedupe)
        has_identity = 1 if (
            _dupe_get_mb_release_group_id(e)
            or _dupe_get_mb_release_id(e)
            or _dupe_get_discogs_id(e)
            or _dupe_get_lastfm_mbid(e)
            or _dupe_get_bandcamp_url(e)
        ) else 0
        missing_required = e.get("missing_required_tags") or []
        has_complete_tags = 1 if (not missing_required) else 0
        has_cover = 1 if bool(e.get("has_cover")) else 0
        has_artist_image = 1 if bool(e.get("has_artist_image")) else 0
        missing_count_neg = -int(len(missing_required))

        # Prefer "clean" folder names when everything else is equal.
        # This is especially helpful for test sets and for user-created variants like "(no tags)".
        folder_raw = e.get("folder")
        folder_name = ""
        try:
            folder_name = str(Path(folder_raw).name if folder_raw else "").lower()
        except Exception:
            folder_name = str(folder_raw or "").lower()
        noisy_tokens = (
            "no tags",
            "no cover",
            "gaps",
            "incomplete",
            "broken",
            "[dupe]",
            " dupe",
            "(dupe)",
        )
        variant_clean = 1
        for tok in noisy_tokens:
            if tok and tok in folder_name:
                variant_clean = 0
                break

        fmt_score = int(e.get("fmt_score") or 0)
        bd = int(e.get("bd") or 0)
        sr = int(e.get("sr") or 0)
        br = int(e.get("br") or 0)
        track_count = len(e.get("tracks") or [])
        file_count = int(e.get("file_count") or 0)
        return (
            has_identity,
            has_complete_tags,
            has_cover,
            has_artist_image,
            variant_clean,
            missing_count_neg,
            fmt_score,
            bd,
            sr,
            br,
            track_count,
            file_count,
        )

    ranked = sorted(list(editions), key=_metrics, reverse=True)
    best = ranked[0]
    if len(ranked) == 1:
        fmt_score, bd, sr, br, track_count, file_count = _metrics(best)
        rationale = (
            f"Heuristic: single edition (fmt_score={fmt_score}, bd={bd}, sr={sr}, br={br}, tracks={track_count}, files={file_count})"
        )
        return (best, rationale, [], True)

    second = ranked[1]
    (b_ident, b_tags, b_cov, b_img, b_clean, b_miss, b_fmt, b_bd, b_sr, b_br, b_tr, b_fc) = _metrics(best)
    (s_ident, s_tags, s_cov, s_img, s_clean, s_miss, s_fmt, s_bd, s_sr, s_br, s_tr, s_fc) = _metrics(second)

    # Conservative confidence rules.
    confident = False

    # Never be "confident" if the best has fewer tracks than runner-up (deluxe/bonus vs standard ambiguity).
    if b_tr < s_tr:
        confident = False
    else:
        # Strong health signals should be enough to avoid AI.
        if (b_ident, b_tags, b_cov, b_img, b_clean) != (s_ident, s_tags, s_cov, s_img, s_clean):
            # If we have a clear improvement on identity/tags/cover, it's safe and deterministic.
            if (b_ident > s_ident) or (b_tags > s_tags) or (b_cov > s_cov) or (b_img > s_img) or (b_clean > s_clean):
                confident = True
        # Big codec/container class difference (e.g., FLAC vs MP3).
        if not confident and b_fmt >= (s_fmt + 2):
            confident = True
        # Same track count + better technical quality.
        elif (not confident) and b_tr == s_tr:
            if b_fmt > s_fmt:
                confident = True
            elif b_fmt == s_fmt:
                if b_bd >= (s_bd + 8):
                    confident = True
                elif b_sr >= (s_sr + 22050):
                    confident = True
                elif (b_bd == s_bd) and (b_sr == s_sr) and (b_br >= (s_br + 200000)):
                    confident = True
        # Slightly more tracks and not worse quality: often the same album + bonus.
        elif (not confident) and b_tr > s_tr and (b_tr - s_tr) <= 2:
            if (b_fmt >= s_fmt) and (b_bd >= s_bd) and (b_sr >= s_sr):
                confident = True

    # If all technical/health metrics are essentially tied but the group shares a strong identity signal,
    # treat the heuristic pick as confident to avoid unnecessary AI spend.
    if not confident and len(editions) >= 2:
        try:
            discogs_ids = {_dupe_get_discogs_id(e) for e in editions}
            discogs_ids = {x for x in discogs_ids if x}
            lastfm_ids = {_dupe_get_lastfm_mbid(e) for e in editions}
            lastfm_ids = {x for x in lastfm_ids if x}
            bandcamp_urls = {_dupe_get_bandcamp_url(e) for e in editions}
            bandcamp_urls = {x for x in bandcamp_urls if x}
            mb_rg_ids = {_dupe_get_mb_release_group_id(e) for e in editions}
            mb_rg_ids = {x for x in mb_rg_ids if x}
            strong = (
                (len(mb_rg_ids) == 1 and mb_rg_ids)
                or (len(discogs_ids) == 1 and discogs_ids)
                or (len(lastfm_ids) == 1 and lastfm_ids)
                or (len(bandcamp_urls) == 1 and bandcamp_urls)
            )
            if strong and b_tr == s_tr and b_fc == s_fc:
                confident = True
        except Exception:
            pass

    rationale = (
        "Heuristic pick: "
        f"identity {b_ident} vs {s_ident}, "
        f"tags {b_tags} vs {s_tags}, "
        f"cover {b_cov} vs {s_cov}, "
        f"artist_img {b_img} vs {s_img}, "
        f"clean_name {b_clean} vs {s_clean}, "
        f"fmt_score {b_fmt} vs {s_fmt}, "
        f"bd {b_bd} vs {s_bd}, "
        f"sr {b_sr} vs {s_sr}, "
        f"br {b_br} vs {s_br}, "
        f"tracks {b_tr} vs {s_tr}"
    )
    return (best, rationale, [], confident)


def _dupe_get_mb_release_group_id(e: dict) -> str:
    try:
        rg = (e.get("rg_info") or {}).get("id")
    except Exception:
        rg = None
    if rg:
        return str(rg).strip()
    if e.get("musicbrainz_id"):
        return str(e.get("musicbrainz_id") or "").strip()
    meta = e.get("meta") or {}
    if isinstance(meta, dict):
        for k in ("musicbrainz_releasegroupid", "musicbrainz_release_group_id"):
            v = (meta.get(k) or "").strip()
            if v:
                return v
        # Some pipelines store RGID into albumid; accept as last resort.
        for k in ("musicbrainz_albumid", "musicbrainz_releaseid"):
            v = (meta.get(k) or "").strip()
            if v and re.fullmatch(r"[0-9a-fA-F-]{36}", v):
                return v
    return ""


def _dupe_get_mb_release_id(e: dict) -> str:
    meta = e.get("meta") or {}
    if isinstance(meta, dict):
        for k in ("musicbrainz_releaseid", "musicbrainz_albumid", "musicbrainz_originalreleaseid"):
            v = (meta.get(k) or "").strip()
            if v and re.fullmatch(r"[0-9a-fA-F-]{36}", v):
                return v
    return ""


def _dupe_get_discogs_id(e: dict) -> str:
    v = (e.get("discogs_release_id") or "").strip()
    if v:
        return v
    meta = e.get("meta") or {}
    if isinstance(meta, dict):
        v = (meta.get("discogs_release_id") or meta.get("discogs_releaseid") or "").strip()
        if v:
            return v
    return ""


def _dupe_get_lastfm_mbid(e: dict) -> str:
    v = (e.get("lastfm_album_mbid") or "").strip()
    if v:
        return v
    meta = e.get("meta") or {}
    if isinstance(meta, dict):
        v = (meta.get("lastfm_album_mbid") or "").strip()
        if v:
            return v
    return ""


def _dupe_get_bandcamp_url(e: dict) -> str:
    v = (e.get("bandcamp_album_url") or "").strip()
    if v:
        return v
    meta = e.get("meta") or {}
    if isinstance(meta, dict):
        v = (meta.get("bandcamp_album_url") or "").strip()
        if v:
            return v
    return ""


def _dupe_audio_fp_set_for_edition(e: dict, *, max_tracks: int = 12) -> set[str]:
    """
    Return a set of chromaprint fingerprints for the edition (cached in cache.db).
    Used as high-precision evidence for duplicates when titles/tags are messy.
    """
    cached = e.get("_dupe_audio_fp_set")
    if isinstance(cached, set):
        return cached

    folder = e.get("folder")
    if not folder:
        e["_dupe_audio_fp_set"] = set()
        return set()
    try:
        folder_path = path_for_fs_access(Path(folder))
    except Exception:
        folder_path = Path(folder)
    if not folder_path or not folder_path.exists():
        e["_dupe_audio_fp_set"] = set()
        return set()

    paths: list[Path] = []
    try:
        ordered = e.get("ordered_paths") or []
        if ordered:
            for p in ordered:
                try:
                    pp = path_for_fs_access(Path(p))
                except Exception:
                    pp = Path(p)
                if pp and pp.is_file() and AUDIO_RE.search(pp.name):
                    paths.append(pp)
    except Exception:
        paths = []
    if not paths:
        try:
            paths = sorted([p for p in folder_path.rglob("*") if p.is_file() and AUDIO_RE.search(p.name)], key=lambda p: str(p))[: max(1, int(max_tracks or 12))]
        except Exception:
            paths = []

    fps: set[str] = set()
    for p in paths[: max(1, int(max_tracks or 12))]:
        try:
            path_str = str(p)
            cached_track = get_cached_acoustid(path_str)
            if cached_track:
                _dur, fp = cached_track
                if fp:
                    fps.add(str(fp))
                    continue
            # Compute via fpcalc (subprocess) and store.
            res = _fpcalc_fingerprint_file(path_str, length_sec=120, timeout_sec=45)
            if not res:
                continue
            dur, fp = res
            if fp:
                set_cached_acoustid(path_str, dur, fp)
                fps.add(str(fp))
        except Exception:
            continue

    e["_dupe_audio_fp_set"] = fps
    return fps


def _dupe_audio_sig_for_edition(
    e: dict,
    *,
    max_tracks: int = 10,
    min_fps: int = 3,
    compute_missing: bool = False,
) -> str:
    """
    Return a stable-ish album audio signature derived from a small set of track fingerprints.
    - When compute_missing=False, only uses cached fingerprints (cheap).
    - When compute_missing=True, will compute missing fingerprints via fpcalc and cache them.
    """
    cached_sig = e.get("_dupe_audio_sig")
    if isinstance(cached_sig, str) and cached_sig:
        return cached_sig

    folder = e.get("folder")
    if not folder:
        e["_dupe_audio_sig"] = ""
        return ""
    try:
        folder_path = path_for_fs_access(Path(folder))
    except Exception:
        folder_path = Path(folder)
    if not folder_path or not folder_path.exists():
        e["_dupe_audio_sig"] = ""
        return ""

    paths: list[Path] = []
    try:
        ordered = e.get("ordered_paths") or []
        if ordered:
            for p in ordered:
                try:
                    pp = path_for_fs_access(Path(p))
                except Exception:
                    pp = Path(p)
                if pp and pp.is_file() and AUDIO_RE.search(pp.name):
                    paths.append(pp)
    except Exception:
        paths = []
    if not paths:
        try:
            paths = sorted(
                [p for p in folder_path.rglob("*") if p.is_file() and AUDIO_RE.search(p.name)],
                key=lambda p: str(p),
            )
        except Exception:
            paths = []

    fps: list[str] = []
    for p in (paths or [])[: max(1, int(max_tracks or 10))]:
        try:
            path_str = str(p)
            cached_track = get_cached_acoustid(path_str)
            if cached_track:
                _dur, fp = cached_track
                if fp:
                    fps.append(str(fp))
                    continue
            if not compute_missing:
                continue
            res = _fpcalc_fingerprint_file(path_str, length_sec=120, timeout_sec=45)
            if not res:
                continue
            dur, fp = res
            if fp:
                set_cached_acoustid(path_str, dur, fp)
                fps.append(str(fp))
        except Exception:
            continue

    # Require some fingerprints to avoid collisions.
    if len(fps) < max(1, int(min_fps or 3)):
        e["_dupe_audio_sig"] = ""
        return ""

    # Hash fingerprints to keep payload small and stable.
    digests: list[str] = []
    for fp in fps:
        try:
            digests.append(hashlib.sha1(fp.encode("utf-8", errors="ignore")).hexdigest())
        except Exception:
            continue
    digests = sorted(set(digests))
    payload = "audio_sig_v1\n" + "\n".join(digests) + f"\nN={len(digests)}"
    sig = hashlib.sha1(payload.encode("utf-8", errors="ignore")).hexdigest()
    e["_dupe_audio_sig"] = sig
    return sig


def _dupe_split_editions_by_similarity(
    editions: list[dict],
    *,
    min_jaccard: float = 0.82,
    min_ratio: float = 0.75,
    allow_audio_fp: bool = True,
    audio_min_overlap: float = 0.87,
) -> list[list[dict]]:
    """
    Split a noisy candidate group into one or more coherent clusters using track-title
    similarity (and optionally chromaprint overlap as a tie-break).
    """
    if not editions:
        return []
    if len(editions) <= 1:
        return [editions]

    # Local union-find over list indices.
    n = len(editions)
    parent = list(range(n))
    rank = [0] * n

    def find(i: int) -> int:
        while parent[i] != i:
            parent[i] = parent[parent[i]]
            i = parent[i]
        return i

    def union(i: int, j: int) -> None:
        ri = find(i)
        rj = find(j)
        if ri == rj:
            return
        if rank[ri] < rank[rj]:
            parent[ri] = rj
        elif rank[ri] > rank[rj]:
            parent[rj] = ri
        else:
            parent[rj] = ri
            rank[ri] += 1

    title_sets: list[set[str]] = []
    track_lists: list[list] = []
    for e in editions:
        tr = e.get("tracks") or []
        track_lists.append(tr)
        cached_titles = e.get("_dupe_track_title_set")
        if isinstance(cached_titles, set):
            title_sets.append(cached_titles)
        else:
            ts = _dupe_track_title_set(tr)
            e["_dupe_track_title_set"] = ts
            title_sets.append(ts)

    # Pairwise comparisons within this small candidate set only.
    for i in range(n):
        for j in range(i + 1, n):
            ratio = _dupe_track_count_ratio(track_lists[i], track_lists[j])
            if ratio < 0.55:
                continue
            jac = _dupe_jaccard(title_sets[i], title_sets[j])
            if jac >= float(min_jaccard) and ratio >= float(min_ratio):
                union(i, j)
                continue
            # Optional tie-break: if track titles are messy but audio overlaps, keep together.
            if allow_audio_fp and jac >= 0.55 and ratio >= 0.60:
                a = _dupe_audio_fp_set_for_edition(editions[i])
                b = _dupe_audio_fp_set_for_edition(editions[j])
                if a and b:
                    ov = (len(a & b) / max(len(a), len(b))) if max(len(a), len(b)) else 0.0
                    if ov >= float(audio_min_overlap):
                        union(i, j)

    comps: dict[int, list[dict]] = defaultdict(list)
    for i, e in enumerate(editions):
        comps[find(i)].append(e)
    return [c for c in comps.values() if c]

def editions_share_confident_signal(ed_list: List[dict]) -> bool:
    """
    Determine whether a potential duplicate group has enough evidence to be trusted.
    Accept when at least two editions have high-confidence titles, all track
    signatures match, they share the same MusicBrainz release-group ID, or
    all have the same album_norm (e.g. folder-derived titles like "Album [dupe]").
    """
    if len(ed_list) < 2:
        return False

    high_conf_prefixes = {"plex", "tag"}
    high_conf_titles = sum(
        1 for e in ed_list
        if e.get("title_source", "").partition(":")[0] in high_conf_prefixes
    )
    if high_conf_titles >= 2:
        return True

    sigs = {e.get("sig") for e in ed_list if e.get("sig")}
    if len(sigs) == 1 and sigs:
        return True

    rg_ids = {e.get("rg_info", {}).get("id") for e in ed_list if e.get("rg_info", {}).get("id")}
    if len(rg_ids) == 1 and rg_ids:
        return True

    # Dupe Detection v2: accept when provider IDs match even if rg_info is absent.
    mb_rg_ids = {_dupe_get_mb_release_group_id(e) for e in ed_list}
    mb_rg_ids = {x for x in mb_rg_ids if x}
    if len(mb_rg_ids) == 1 and mb_rg_ids:
        return True

    discogs_ids = {_dupe_get_discogs_id(e) for e in ed_list}
    discogs_ids = {x for x in discogs_ids if x}
    if len(discogs_ids) == 1 and discogs_ids:
        return True

    lastfm_mbids = {_dupe_get_lastfm_mbid(e) for e in ed_list}
    lastfm_mbids = {x for x in lastfm_mbids if x}
    if len(lastfm_mbids) == 1 and lastfm_mbids:
        return True

    bandcamp_urls = {_dupe_get_bandcamp_url(e) for e in ed_list}
    bandcamp_urls = {x for x in bandcamp_urls if x}
    if len(bandcamp_urls) == 1 and bandcamp_urls:
        return True

    # Same normalized title (e.g. "Night Cycle" and "Night Cycle [dupe]" -> "night cycle")
    norms = {e.get("album_norm") for e in ed_list if e.get("album_norm")}
    if len(norms) == 1 and norms:
        return True

    # Same Plex-normalized title (we grouped by this; accept so scan results match library)
    plex_norms = {e.get("plex_norm") for e in ed_list if e.get("plex_norm")}
    if len(plex_norms) == 1 and plex_norms:
        return True

    # Dupe Detection v2: same loose normalized title (aggressive noise stripping).
    loose_norms = {(e.get("_dupe_title_norm_loose") or "").strip() for e in ed_list if e.get("_dupe_title_norm_loose")}
    loose_norms = {x for x in loose_norms if x and not x.startswith("__untitled__")}
    if len(loose_norms) == 1 and loose_norms:
        return True

    return False

def detect_broken_album(db_conn, album_id: int, tracks: List[Track], mb_release_group_info: dict | None) -> tuple[bool, int | None, int, list]:
    """
    Detect if an album is broken (missing tracks).
    Returns (is_broken, expected_track_count, actual_track_count, missing_indices)
    
    Detection methods:
    1. MusicBrainz comparison: if expected track count is known and actual < 90% of expected
    2. Heuristic: gaps in track indices (configurable thresholds)
    """
    actual_count = len(tracks)
    if actual_count == 0:
        return True, 0, 0, []
    
    # Ignore non-positive indices (Plex sometimes stores missing track numbers as 0).
    # Note: in multi-disc albums, Plex track indices are per-disc; we currently only use
    # the heuristic to catch obviously broken/tag-garbage cases like a single track "11".
    track_indices = [int(t.idx or 0) for t in tracks if int(t.idx or 0) > 0]
    if not track_indices:
        return False, None, actual_count, []
    track_indices = sorted(track_indices)
    max_idx = max(track_indices)
    coverage = (actual_count / max_idx) if max_idx else 1.0
    # Skip "broken" detection when track numbering is obviously corrupt (prevents huge false positives).
    if max_idx > max(120, actual_count * 3) and coverage < 0.5:
        return False, None, actual_count, []

    # Check for gaps (including a leading gap when the smallest index is > 1).
    gaps = []
    if track_indices and track_indices[0] > 1:
        gaps.append((0, track_indices[0]))
    for i in range(len(track_indices) - 1):
        if track_indices[i + 1] - track_indices[i] > 1:
            gaps.append((track_indices[i], track_indices[i + 1]))
    
    # Method 1: MusicBrainz comparison
    if mb_release_group_info:
        # If we have a track_count from the MB matching step, use it as an "expected" size signal.
        # This catches "tail-truncated" albums (no gaps in indices) which the heuristic cannot detect.
        try:
            expected = int(mb_release_group_info.get("track_count") or 0)
        except Exception:
            expected = 0
        if expected > 0:
            # Allow small mismatches for bonus tracks, alt editions, etc. (only treat as broken when notably short).
            if actual_count < int(expected * 0.90):
                # Best-effort missing indices: combine gaps + missing tail indices.
                missing_indices: list[int] = []
                try:
                    indices_set = set(track_indices or [])
                    # Cap to avoid giant arrays on very large releases.
                    if expected <= 500:
                        missing_indices = [i for i in range(1, expected + 1) if i not in indices_set]
                    else:
                        # Only report a small preview for huge track counts.
                        missing_indices = [i for i in range(1, min(expected, 200) + 1) if i not in indices_set]
                except Exception:
                    missing_indices = []
                return True, expected, actual_count, missing_indices

    # Method 2: Heuristic (gaps) - using configurable thresholds
    if gaps:
        # Check if gap > configured consecutive threshold
        large_gaps = [g for g in gaps if g[1] - g[0] > BROKEN_ALBUM_CONSECUTIVE_THRESHOLD]
        if large_gaps:
            missing_indices = []
            for start_i, end_i in gaps:
                try:
                    missing_indices.extend(list(range(int(start_i) + 1, int(end_i))))
                except Exception:
                    continue
                if len(missing_indices) > 5000:
                    missing_indices = missing_indices[:5000]
                    break
            return True, max_idx, actual_count, missing_indices
        
        # Check if gaps represent > configured percentage threshold
        total_missing = sum(g[1] - g[0] - 1 for g in gaps)
        if total_missing > actual_count * BROKEN_ALBUM_PERCENTAGE_THRESHOLD:
            missing_indices = []
            for start_i, end_i in gaps:
                try:
                    missing_indices.extend(list(range(int(start_i) + 1, int(end_i))))
                except Exception:
                    continue
                if len(missing_indices) > 5000:
                    missing_indices = missing_indices[:5000]
                    break
            return True, max_idx, actual_count, missing_indices
    
    return False, None, actual_count, []


def _detect_gaps_in_indices(indices: list) -> tuple[bool, int, list]:
    """
    Given sorted track indices, detect if album has gaps (incomplete).
    Returns (is_broken, actual_count, gaps) where gaps is list of (start, end) pairs.
    Uses same thresholds as detect_broken_album.
    """
    if not indices:
        return False, 0, []
    track_indices = sorted([int(i) for i in indices if int(i) > 0])
    if not track_indices:
        return False, 0, []
    actual_count = len(track_indices)
    gaps = []
    # Leading gap: a single track numbered "11" should count as missing 1..10.
    if track_indices[0] > 1:
        gaps.append((0, track_indices[0]))
    for i in range(len(track_indices) - 1):
        if track_indices[i + 1] - track_indices[i] > 1:
            gaps.append((track_indices[i], track_indices[i + 1]))
    if not gaps:
        return False, actual_count, []
    large_gaps = [g for g in gaps if g[1] - g[0] > BROKEN_ALBUM_CONSECUTIVE_THRESHOLD]
    if large_gaps:
        return True, actual_count, gaps
    total_missing = sum(g[1] - g[0] - 1 for g in gaps)
    if total_missing > actual_count * BROKEN_ALBUM_PERCENTAGE_THRESHOLD:
        return True, actual_count, gaps
    return False, actual_count, []


def _incomplete_album_disk_crosscheck(
    db_conn,
    artist: str,
    album_id: int,
    tracks: List[Track],
    folder: Path,
    album_title_str: str,
) -> dict:
    """
    Cross-check a broken album: Plex vs disk. Returns a diagnostic dict with
    classification (DISK_MISSING, DISK_HAS_MORE, DISK_HAS_TAG_SPLIT), missing_in_plex,
    missing_on_disk, track_titles, etc. for storage in incomplete_album_diagnostics.
    """
    import json
    classifications: List[str] = []
    missing_in_plex: List[int] = []   # Plex track indices with no file on disk
    missing_on_disk: List[str] = []    # Disk filenames that don't match Plex tracks
    track_titles = [(t.idx, t.title) for t in tracks]
    plex_indices = {t.idx for t in tracks}

    if not folder.exists():
        classifications.append("DISK_MISSING")
        return {
            "classification": ",".join(classifications),
            "missing_in_plex": json.dumps(list(plex_indices)),
            "missing_on_disk": json.dumps([]),
            "track_titles": json.dumps(track_titles),
            "expected_track_count": len(tracks),
            "actual_track_count": 0,
        }

    audio_files = [p for p in folder.rglob("*") if AUDIO_RE.search(p.name)]
    disk_by_index: Dict[int, Path] = {}
    disk_extra: List[str] = []
    tag_album: Optional[str] = None
    tag_artist: Optional[str] = None

    for p in audio_files:
        tags = extract_tags(p)
        idx = None
        if "tracknumber" in tags:
            raw = tags["tracknumber"].strip().split("/")[0].strip()
            try:
                idx = int(raw)
            except ValueError:
                pass
        if idx is not None and idx in plex_indices:
            disk_by_index[idx] = p
        else:
            disk_extra.append(p.name)
        if tag_album is None and "album" in tags:
            tag_album = (tags["album"] or "").strip()
        if tag_artist is None and "artist" in tags:
            tag_artist = (tags["artist"] or "").strip()

    for idx in plex_indices:
        if idx not in disk_by_index:
            missing_in_plex.append(idx)
    if missing_in_plex:
        classifications.append("DISK_MISSING")
    if disk_extra:
        classifications.append("DISK_HAS_MORE")
    if (tag_album or tag_artist) and album_title_str:
        album_norm = (album_title_str or "").strip().lower()
        artist_norm = (artist or "").strip().lower()
        tag_album_norm = (tag_album or "").strip().lower()
        tag_artist_norm = (tag_artist or "").strip().lower()
        if tag_album_norm and tag_album_norm != album_norm:
            classifications.append("DISK_HAS_TAG_SPLIT")
        elif tag_artist_norm and tag_artist_norm != artist_norm:
            classifications.append("DISK_HAS_TAG_SPLIT")

    if not classifications:
        classifications.append("DISK_MISSING")

    return {
        "classification": ",".join(classifications),
        "missing_in_plex": json.dumps(missing_in_plex),
        "missing_on_disk": json.dumps(missing_on_disk[:50]),
        "track_titles": json.dumps(track_titles),
        "expected_track_count": len(tracks),
        "actual_track_count": len(audio_files),
    }


def resolve_mbid_to_release_group(mbid: str, tag_source: str = "", use_queue: bool = True) -> Optional[str]:
    """
    Return a MusicBrainz release-group ID for use with get_release_group_by_id / fetch_mb_release_group_info.
    - If tag_source is 'musicbrainz_releasegroupid', mbid is already a release-group ID: return it.
    - If tag_source is 'musicbrainz_releaseid' or 'musicbrainz_albumid' (or empty/unknown), mbid is a release ID:
      fetch the release, extract release-group id, return it. On API error, log and return None.
    - use_queue: if False, call API directly (required when already inside MB queue worker to avoid deadlock).
    """
    if not mbid or not mbid.strip():
        return None
    mbid = mbid.strip()
    if tag_source == "musicbrainz_releasegroupid":
        return mbid
    # Release ID (musicbrainz_releaseid, musicbrainz_albumid) or unknown: resolve via get_release_by_id
    # MusicBrainz API expects "release-groups" (plural) for release lookup includes
    try:
        def _fetch_release():
            return musicbrainzngs.get_release_by_id(mbid, includes=["release-groups"])["release"]
        if use_queue and MB_QUEUE_ENABLED and USE_MUSICBRAINZ:
            rel = get_mb_queue().submit(f"rel_{mbid}", _fetch_release)
        else:
            rel = _fetch_release()
        rg = rel.get("release-group")
        if rg and isinstance(rg.get("id"), str):
            return rg["id"]
        return None
    except Exception as e:
        logging.debug("resolve_mbid_to_release_group: failed for mbid=%s tag_source=%s: %s", mbid, tag_source, e)
        return None


def fetch_mb_release_group_info(mbid: str) -> tuple[dict, bool]:
    """
    Fetch primary type, secondary-types, and optional format summary from MusicBrainz release-group.
    Uses musicbrainzngs for proper rate-limiting. Only inc=releases is used (inc=media is not valid
    for release-group and causes 400); format_summary may be empty unless releases include media.
    Returns (info_dict, cache_hit) where cache_hit is True if found in cache.
    """
    # Attempt to reuse cached MusicBrainz release-group info
    cached = get_cached_mb_info(mbid)
    if cached:
        logging.debug("[MusicBrainz RG Info] using cached info for MBID %s", mbid)
        return cached, True  # True = cache hit
    
    # Use queue for rate-limited API call.
    # Note: for release-group, include "releases" and "artist-credits". Avoid "media" here (use release endpoint instead).
    def _fetch():
        try:
            result = musicbrainzngs.get_release_group_by_id(
                mbid,
                includes=["releases", "artist-credits"]
            )["release-group"]
            return result
        except musicbrainzngs.WebServiceError as e:
            error_msg = str(e)
            if "503" in error_msg or "rate" in error_msg.lower():
                logging.warning("[MusicBrainz] Rate limited for MBID %s, will retry after delay", mbid)
                time.sleep(1.5)
                try:
                    result = musicbrainzngs.get_release_group_by_id(mbid, includes=["releases", "artist-credits"])["release-group"]
                    return result
                except musicbrainzngs.WebServiceError as e2:
                    raise RuntimeError(f"MusicBrainz lookup failed for {mbid} after retry: {e2}") from None
            if "404" in error_msg:
                # mbid may be a release ID; resolve to release-group and retry
                resolved = resolve_mbid_to_release_group(mbid, "")
                if resolved and resolved != mbid:
                    result = musicbrainzngs.get_release_group_by_id(
                        resolved, includes=["releases", "artist-credits"]
                    )["release-group"]
                    return result
            raise RuntimeError(f"MusicBrainz lookup failed for {mbid}: {e}") from None
    
    try:
        if MB_QUEUE_ENABLED and USE_MUSICBRAINZ:
            result = get_mb_queue().submit(f"rg_{mbid}", _fetch)
        else:
            result = _fetch()
    except Exception as e:
        raise

    primary = result.get("primary-type", "")
    secondary = result.get("secondary-types", [])
    formats = set()

    # Each release may have multiple medium entries
    for release in result.get("releases", []):
        for medium in release.get("media", []):
            fmt = medium.get("format")
            qty = medium.get("track-count") or medium.get("position") or medium.get("discs-count") or medium.get("count")
            # Some media entries include 'track-count' and 'format'
            if fmt:
                # quantity fallback: if medium["discs"] is present
                if isinstance(medium.get("format"), str):
                    quantity = medium.get("track-count", 1)
                else:
                    quantity = 1
                formats.add(f"{quantity}√ó{fmt}")

    format_summary = ", ".join(sorted(formats))
    logging.debug("[MusicBrainz RG Info] raw response for MBID %s: %s", mbid, result)
    logging.debug("[MusicBrainz RG Info] parsed primary_type=%s, secondary_types=%s, format_summary=%s", primary, secondary, format_summary)
    info = {
        "id": mbid,  # Include the MBID in the info dict
        "title": result.get("title", ""),
        # Keep artist names so strict identity checks can validate MBIDs sourced from providers (e.g. Last.fm).
        "mb_artist_names": _extract_mb_artist_names(result),
        "primary_type": primary,
        "secondary_types": secondary,
        "format_summary": format_summary
    }
    # Cache the lookup result
    set_cached_mb_info(mbid, info)
    return info, False  # False = cache miss

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ MusicBrainz search fallback ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def _extract_track_titles_from_mb_release(release_response: dict) -> List[str]:
    """Extract track titles from a MusicBrainz release response (get_release_by_id with includes=['recordings'])."""
    titles: List[str] = []
    try:
        release = release_response.get("release") if isinstance(release_response.get("release"), dict) else release_response
        for medium in release.get("medium-list", []):
            for track in medium.get("track-list", []):
                rec = track.get("recording") or {}
                t = rec.get("title") if isinstance(rec, dict) else None
                if t:
                    titles.append(str(t))
    except Exception:
        pass
    return titles


def _mb_track_count_from_rg_info(info: dict) -> int:
    """
    Return total track count from release-group info (release-list / medium-list / track-count).
    When the API returns no medium-list (track count 0) but we have release-list, fetch the first
    release with includes=['recordings'] to get the real count (release-group lookup often omits media).
    Tolerates both "release-list"/"medium-list" and "releases"/"media" key names (musicbrainzngs/API variants).
    """
    releases = info.get("release-list") or info.get("releases") or []
    count = 0
    for release in releases:
        media = release.get("medium-list") or release.get("media") or []
        for medium in media:
            count += int(medium.get("track-count", 0) or 0)
    if count > 0:
        return count
    if not releases:
        return 0
    first_id = releases[0].get("id")
    if not first_id:
        return 0
    try:
        rel_resp = musicbrainzngs.get_release_by_id(first_id, includes=["recordings"])
        release = rel_resp.get("release") if isinstance(rel_resp.get("release"), dict) else rel_resp
        n = 0
        for medium in release.get("medium-list") or release.get("media") or []:
            track_list = medium.get("track-list") or medium.get("tracks") or []
            if track_list:
                n += len(track_list)
            else:
                n += int(medium.get("track-count", 0) or 0)
        return n
    except Exception:
        return 0


def _is_likely_live_album(folder: Optional[Path], title: Optional[str]) -> bool:
    """
    Return True if the album is likely a live recording based on folder path and title.
    Used to skip MB assignment (SKIP_MB_FOR_LIVE_ALBUMS) or only accept MB release-groups
    with secondary type Live (LIVE_ALBUMS_MB_STRICT).
    """
    combined = " ".join(
        filter(None, [str(folder) if folder else "", (title or "").strip()])
    ).lower()
    if not combined:
        return False
    live_phrases = [
        " live at ",
        " live in ",
        " (live)",
        " (other live)",
        " (concert)",
        " (bootleg)",
        " live)",
    ]
    if any(p in combined for p in live_phrases):
        return True
    if combined.rstrip().endswith(" live"):
        return True
    return False


def _crosscheck_tracklist(local_titles: List[str], release_titles: List[str]) -> float:
    """
    Compare local track titles with release tracklist. Returns a score in [0, 1]:
    fraction of local titles that match at least one release title (normalized).
    Used to only assign a release when confidence is above TRACKLIST_MATCH_MIN.
    """
    if not local_titles:
        return 1.0

    def _norm(s: str) -> str:
        return (s or "").lower().strip()[:200]

    local_n = [_norm(t) for t in local_titles if (t or "").strip()]
    release_n = [_norm(t) for t in release_titles if (t or "").strip()]
    if not local_n:
        return 1.0

    matches = 0
    for ln in local_n:
        for rn in release_n:
            if ln == rn:
                matches += 1
                break
            if len(ln) > 2 and len(rn) > 2 and (ln in rn or rn in ln):
                matches += 1
                break
    return matches / len(local_n)


def _crosscheck_tracklist_perfect(local_titles: List[str], release_titles: List[str]) -> float:
    """
    More conservative tracklist match: normalize titles and require exact equality (no substring).
    This is intended for "certain match" fast-paths where we want to avoid false positives.
    """
    if not local_titles:
        return 1.0

    def _norm(s: str) -> str:
        raw = (s or "").strip().lower()
        if not raw:
            return ""
        had_remaster = bool(re.search(r"\bremaster", raw))
        # Drop leading index patterns.
        raw = re.sub(r"^\s*\d+\s*[-.)]\s*", "", raw)
        raw = raw.replace("&", " and ")
        # Strip punctuation, keep letters/numbers/spaces.
        raw = re.sub(r"[^\w\s]+", " ", raw)
        raw = " ".join(raw.split()).strip()
        if not raw:
            return ""
        if had_remaster:
            tokens = raw.split()
            tokens = [t for t in tokens if not re.fullmatch(r"remaster(?:ed|ing)?", t)]
            # Common tag noise: "... 2011 Remaster" -> keep the song title, drop trailing year.
            if len(tokens) >= 2 and re.fullmatch(r"(19|20)\d{2}", tokens[-1] or ""):
                tokens = tokens[:-1]
            raw = " ".join(tokens).strip()
        return raw[:240]

    local_n = [_norm(t) for t in local_titles if (t or "").strip()]
    release_n = [_norm(t) for t in release_titles if (t or "").strip()]
    if not local_n:
        return 1.0
    release_set = {t for t in release_n if t}
    if not release_set:
        return 0.0
    matches = 0
    for ln in local_n:
        if ln and ln in release_set:
            matches += 1
    return matches / len(local_n)


def _prepare_mb_submission_payload(artist: str, title: str, date: str, tracklist: List[str], source: str = "discogs") -> dict:
    """
    Build a payload suitable for preparing a MusicBrainz submission (manual or via MB edit API).
    Used when we have a high-confidence match from Discogs/Bandcamp but no MB release-group.
    Returns dict with artist, title, date, tracks (list of {position, title}), source.
    """
    tracks = [{"position": i + 1, "title": t} for i, t in enumerate(tracklist or [])]
    return {
        "artist": artist or "",
        "title": title or "",
        "date": (date or "").strip()[:10],
        "tracks": tracks,
        "source": source,
    }

def _fpcalc_fingerprint_file(path_str: str, *, length_sec: int = 120, timeout_sec: int = 45) -> Optional[tuple[float, str]]:
    """Compute chromaprint fingerprint via `fpcalc` subprocess.

    Important: We never call chromaprint in-process for scanning because certain
    audio files can trigger a hard abort inside chromaprint (assertion failure),
    killing the entire PMDA server. By isolating fingerprinting in a subprocess,
    we can safely skip problematic files and continue the scan.
    """
    fpcalc = shutil.which("fpcalc")
    if not fpcalc:
        return None
    try:
        length_sec_i = max(1, int(length_sec or 120))
    except Exception:
        length_sec_i = 120

    args = [fpcalc, "-json", "-length", str(length_sec_i), path_str]
    try:
        proc = subprocess.run(args, capture_output=True, text=True, timeout=timeout_sec)
    except subprocess.TimeoutExpired:
        logging.debug("[AcousticID] fpcalc timeout for %s", Path(path_str).name)
        return None
    except Exception as e:
        logging.debug("[AcousticID] fpcalc failed for %s: %s", Path(path_str).name, e)
        return None

    if proc.returncode != 0:
        err = (proc.stderr or proc.stdout or "").strip().replace("\n", " ")[:200]
        logging.debug("[AcousticID] fpcalc rc=%s for %s: %s", proc.returncode, Path(path_str).name, err)
        return None

    out = (proc.stdout or "").strip()
    if not out:
        out = (proc.stderr or "").strip()
    if not out:
        return None

    # Prefer JSON output (fpcalc -json).
    try:
        if out.lstrip().startswith("{"):
            data = json.loads(out)
            fingerprint = data.get("fingerprint")
            duration = data.get("duration")
            if fingerprint:
                try:
                    return float(duration or 0.0), str(fingerprint)
                except Exception:
                    return 0.0, str(fingerprint)
    except Exception as e:
        logging.debug("[AcousticID] fpcalc JSON parse failed for %s: %s", Path(path_str).name, e)

    # Fallback: KEY=VALUE output (older fpcalc).
    duration = None
    fingerprint = None
    for line in out.splitlines():
        up = (line or "").strip().upper()
        if up.startswith("DURATION="):
            try:
                duration = float(line.split("=", 1)[1].strip())
            except Exception:
                duration = None
        elif up.startswith("FINGERPRINT="):
            fingerprint = line.split("=", 1)[1].strip()
    if fingerprint:
        try:
            return float(duration or 0.0), str(fingerprint)
        except Exception:
            return 0.0, str(fingerprint)
    return None


def _store_acoustid_fingerprints_for_folder(folder: Path, max_tracks: int = 20) -> int:
    """
    Compute and store AcousticID fingerprint + duration for each audio file in folder (up to max_tracks).
    Used during scan so fingerprints are in DB for later lookup. Returns number of files stored/computed.
    """
    if not getattr(sys.modules[__name__], "USE_ACOUSTID", False):
        return 0
    try:
        import acoustid
    except ImportError:
        return 0
    audio_files = sorted([p for p in folder.rglob("*") if AUDIO_RE.search(p.name)])
    if not audio_files:
        return 0
    stored = 0
    for path in audio_files[:max_tracks]:
        path_str = str(path)
        if get_cached_acoustid(path_str):
            continue
        try:
            res = _fpcalc_fingerprint_file(path_str, length_sec=120, timeout_sec=45)
            if not res:
                continue
            duration, fingerprint = res
            set_cached_acoustid(path_str, duration, fingerprint)
            stored += 1
        except Exception:
            pass
    return stored


def _identify_album_by_acoustic_id(
    folder: Path,
    artist: str,
    album_norm: str,
) -> tuple[Optional[dict], bool]:
    """
    Identify album via AcoustID fingerprint when tags are missing. Fingerprints audio files,
    looks up recordings, maps to MusicBrainz release-groups. Returns (rg_info, verified_by_ai).
    Logs and updates step_summary/step_response via caller.
    """
    if not getattr(sys.modules[__name__], "USE_ACOUSTID", False):
        return (None, False)
    api_key = (getattr(sys.modules[__name__], "ACOUSTID_API_KEY", "") or "").strip()
    if not api_key:
        logging.debug("[AcousticID] Skipped: no API key")
        return (None, False)
    try:
        import acoustid
    except ImportError as ie:
        logging.warning("[AcousticID] pyacoustid not available: %s", ie)
        return (None, False)
    audio_files = sorted([p for p in folder.rglob("*") if AUDIO_RE.search(p.name)])
    if not audio_files:
        logging.debug("[AcousticID] No audio files in %s", folder)
        return (None, False)
    # Limit to first 20 tracks to avoid long runtime (e.g. live albums)
    max_tracks = 20
    files_to_use = audio_files[:max_tracks]
    recording_scores: List[tuple[str, float]] = []  # (recording_mbid, score)
    for path in files_to_use:
        path_str = str(path)
        cached = get_cached_acoustid(path_str)
        if cached:
            duration, fingerprint = cached
        else:
            try:
                res = _fpcalc_fingerprint_file(path_str, length_sec=120, timeout_sec=45)
                if not res:
                    continue
                duration, fingerprint = res
                set_cached_acoustid(path_str, duration, fingerprint)
            except Exception as ex:
                logging.debug("[AcousticID] fingerprint_file failed for %s: %s", path.name, ex)
                continue
        try:
            response = acoustid.lookup(api_key, fingerprint, duration)
            for score, recording_id, _title, _artist in acoustid.parse_lookup_result(response):
                if recording_id and score >= 0.5:
                    recording_scores.append((recording_id, score))
                    break
        except Exception as ex:
            logging.debug("[AcousticID] lookup failed for %s: %s", path.name, ex)
    if not recording_scores:
        logging.info("[AcousticID] No matches for folder %s (%d files fingerprinted)", folder, len(files_to_use))
        return (None, False)
    lines = [
        "[AcousticID] Folder %s: %d file(s) fingerprinted ‚Üí %d recording(s) from lookup"
        % (folder, len(files_to_use), len(recording_scores)),
        *["  recording %s score=%.2f" % (rec_id, sc) for rec_id, sc in recording_scores[:15]],
    ]
    if len(recording_scores) > 15:
        lines.append("  ... and %d more" % (len(recording_scores) - 15))
    logging.info("\n".join(lines))
    # Map each recording to release-group via MusicBrainz
    rg_counts: Dict[str, int] = {}
    rg_scores: Dict[str, float] = {}
    for recording_id, score in recording_scores:
        try:
            rec = musicbrainzngs.get_recording_by_id(recording_id, includes=["releases"])
            rec_data = rec.get("recording") if isinstance(rec.get("recording"), dict) else rec
            release_list = rec_data.get("release-list") or rec_data.get("releases") or []
            for rel in release_list[:5]:
                rg = rel.get("release-group")
                if isinstance(rg, dict):
                    rg_id = rg.get("id")
                elif isinstance(rg, str):
                    rg_id = rg
                else:
                    rg_id = rel.get("release-group-id")
                if rg_id:
                    rg_counts[rg_id] = rg_counts.get(rg_id, 0) + 1
                    rg_scores[rg_id] = max(rg_scores.get(rg_id, 0), score)
        except Exception as ex:
            logging.debug("[AcousticID] get_recording_by_id %s failed: %s", recording_id, ex)
    if not rg_counts:
        logging.info("[AcousticID] No release-groups from recordings for %s", folder)
        return (None, False)
    lines = [
        "[AcousticID] Release-groups from recordings (%d):" % len(rg_counts),
        *["  %s count=%d max_score=%.2f" % (rg_id, rg_counts[rg_id], rg_scores.get(rg_id, 0)) for rg_id in sorted(rg_counts.keys(), key=lambda x: (-rg_counts[x], -rg_scores.get(x, 0)))],
    ]
    logging.info("\n".join(lines))
    # Pick best release-group: most frequent, then highest score
    best_rg_id = max(rg_counts.keys(), key=lambda x: (rg_counts[x], rg_scores.get(x, 0)))
    candidates = [(rg_id, rg_counts[rg_id], rg_scores.get(rg_id, 0)) for rg_id in rg_counts]
    if len(candidates) > 1 and getattr(sys.modules[__name__], "USE_AI_FOR_MB_VERIFY", False) and getattr(sys.modules[__name__], "ai_provider_ready", False):
        # Multiple candidates: ask AI to pick (simplified ‚Äì we only have rg IDs, so fetch titles and ask)
        try:
            choices = []
            for rg_id in list(rg_counts.keys())[:10]:
                info = musicbrainzngs.get_release_group_by_id(rg_id, includes=[])
                rg = info.get("release-group", {}) or {}
                title = rg.get("title", "?")
                artist_credit = rg.get("artist-credit") or []
                ac_name = ""
                if artist_credit and isinstance(artist_credit[0], dict):
                    ac_name = (artist_credit[0].get("artist") or {}).get("name", "") or artist_credit[0].get("name", "")
                elif artist_credit and isinstance(artist_credit[0], str):
                    ac_name = artist_credit[0]
                choices.append((rg_id, title, ac_name))
            if len(choices) >= 2:
                letters = "ABCDEFGHIJKLMNOPQRSTUVWXYZ"
                lines = ["[AcousticID] Candidates for AI choice (folder has no tags):"]
                for i, (rg_id, title, ac_name) in enumerate(choices):
                    lines.append("  %s) %s by %s (id=%s)" % (letters[i], title or "?", ac_name or "?", rg_id))
                logging.info("\n".join(lines))
                context_line = (
                    "The album folder is named: \"%s\". Artist from path: \"%s\". Album (normalized): \"%s\".\n\n"
                    % (folder.name, artist or "?", album_norm or "?")
                )
                prompt = (
                    "AcousticID candidates for an album (folder has no tags):\n"
                    + context_line
                    + "\n".join(
                        f"{letters[i]}) {title} by {ac_name} (id={rg_id})" for i, (rg_id, title, ac_name) in enumerate(choices)
                    )
                    + "\n\nWhich release group matches this album? Reply with one letter or the MBID or NONE. Optionally end with (confidence: N) where N is 0-100."
                )
                reply = call_ai_provider(
                    getattr(sys.modules[__name__], "AI_PROVIDER", "openai"),
                    getattr(sys.modules[__name__], "RESOLVED_MODEL", "gpt-4o-mini"),
                    "Reply with a single letter (A,B,...) or an MBID (UUID) or NONE. Optionally end with (confidence: N).",
                    prompt,
                    max_tokens=40,
                )
                reply_clean, ai_confidence = parse_ai_confidence((reply or "").strip())
                if ai_confidence is not None:
                    logging.info("[AcousticID] AI candidate choice confidence: %d", ai_confidence)
                reply = reply_clean.upper()
                if reply and reply != "NONE":
                    idx = letters.find(reply[:1])
                    if 0 <= idx < len(choices):
                        best_rg_id = choices[idx][0]
                    else:
                        mbid_match = re.search(r"[0-9A-Fa-f]{8}-[0-9A-Fa-f]{4}-[0-9A-Fa-f]{4}-[0-9A-Fa-f]{4}-[0-9A-Fa-f]{12}", reply)
                        if mbid_match and any(c[0] == mbid_match.group(0) for c in choices):
                            best_rg_id = mbid_match.group(0)
        except Exception as ex:
            logging.debug("[AcousticID] AI verify failed: %s", ex)
    try:
        rg_info, _ = fetch_mb_release_group_info(best_rg_id)
        if rg_info:
            logging.info(
                "[AcousticID] Identified folder %s as release group %s (%s)",
                folder,
                best_rg_id,
                rg_info.get("title", "?"),
            )
            return (rg_info, len(candidates) > 1)
    except Exception as ex:
        logging.warning("[AcousticID] fetch_mb_release_group_info %s failed: %s", best_rg_id, ex)
    return (None, False)


def _search_mb_rg_candidates(artist: str, release_query: str, strict: bool) -> List[dict]:
    """Run MusicBrainz search_release_groups; return list of release-group dicts (no details).
    release_query can be normalized title or raw title (e.g. 'Isolette') for better API match."""
    result = musicbrainzngs.search_release_groups(
        artist=artist,
        release=release_query,
        limit=15,
        strict=strict
    )
    logging.debug("[MusicBrainz Search] artist=%r release=%r strict=%s -> %d results", artist, release_query, strict, len(result.get('release-group-list', [])))
    return result.get('release-group-list', [])


def fetch_all_mb_release_groups_for_artist(artist_name: str) -> List[dict]:
    """
    Fetch all release-groups for an artist from MusicBrainz (paginated browse).
    Used to build a per-artist index so we avoid one search+browse per album.
    Returns list of dicts with 'id' and 'title'; cap at 100 pages (10k RGs) for huge artists.
    """
    if not USE_MUSICBRAINZ:
        return []

    def _do_fetch() -> List[dict]:
        try:
            search_result = musicbrainzngs.search_artists(artist=artist_name, limit=1)
            artist_list = search_result.get("artist-list", [])
            if not artist_list:
                return []
            artist_mbid = artist_list[0]["id"]
            all_rgs: List[dict] = []
            offset = 0
            limit = 100
            max_pages = 100
            time.sleep(1.0)  # Rate limit: 1 req/s (avoid 2nd request right after search_artists)
            for _ in range(max_pages):
                result = musicbrainzngs.browse_release_groups(artist=artist_mbid, limit=limit, offset=offset)
                rg_list = result.get("release-group-list", [])
                for rg in rg_list:
                    title = (rg.get("title") or "").strip()
                    if title:
                        all_rgs.append({"id": rg.get("id"), "title": title})
                if len(rg_list) < limit:
                    break
                offset += limit
                time.sleep(1.0)
            logging.debug("[MusicBrainz] fetch_all_mb_release_groups_for_artist %r -> %d RGs", artist_name, len(all_rgs))
            return all_rgs
        except Exception as e:
            logging.debug("[MusicBrainz] fetch_all_mb_release_groups_for_artist failed for %r: %s", artist_name, e)
            return []

    if MB_QUEUE_ENABLED and USE_MUSICBRAINZ:
        safe_key = re.sub(r"[^a-zA-Z0-9_-]", "_", artist_name[:60])
        return get_mb_queue().submit(f"fetch_rg_{safe_key}", _do_fetch)
    return _do_fetch()


def _build_mb_rg_index_for_artist(all_rgs: List[dict]) -> dict:
    """Build norm_title -> [rg_dict, ...] for matching album_norm against pre-fetched RGs."""
    index: dict = {}
    for rg in all_rgs:
        title = rg.get("title") or ""
        if not title:
            continue
        key = norm_album(title)
        index.setdefault(key, []).append(rg)
    return index


def _match_album_norm_to_mb_index(album_norm: str, index: dict) -> List[dict]:
    """Return list of RG dicts (id, title) that match album_norm: exact then substring."""
    if not index or not (album_norm or "").strip():
        return []
    album_norm = (album_norm or "").strip().lower()
    exact = index.get(album_norm)
    if exact:
        return list(exact)
    candidates = []
    for key, rgs in index.items():
        if not key:
            continue
        if album_norm in key or key in album_norm:
            candidates.extend(rgs)
    return candidates


def _browse_mb_rg_by_artist(artist: str, album_norm: str) -> List[dict]:
    """Get release-group candidates by browsing artist's release groups; filter by title match."""
    if not USE_MUSICBRAINZ:
        return []
    try:
        search_result = musicbrainzngs.search_artists(artist=artist, limit=1)
        artist_list = search_result.get("artist-list", [])
        if not artist_list:
            return []
        artist_mbid = artist_list[0]["id"]
        artist_data = musicbrainzngs.get_artist_by_id(artist_mbid, includes=["release-groups"])
        rg_list = artist_data.get("artist", {}).get("release-group-list", [])
        candidates = []
        for rg in rg_list:
            title = (rg.get("title") or "").strip()
            if not title:
                continue
            rg_norm = norm_album(title)
            if rg_norm == album_norm or album_norm in rg_norm or rg_norm in album_norm:
                candidates.append(rg)
        logging.debug("[MusicBrainz Browse] artist=%s album_norm=%s -> %d title-matched release groups", artist, album_norm, len(candidates))
        return candidates
    except Exception as e:
        logging.debug("[MusicBrainz Browse] failed for '%s' / '%s': %s", artist, album_norm, e)
        return []


def _fetch_album_provider_fallbacks_parallel(artist: str, album_title: str) -> dict:
    """
    Fetch album metadata fallbacks concurrently (Discogs, Last.fm, Bandcamp).
    Returns a dict with raw provider payloads and an `extra_sources` list for AI disambiguation.
    """
    out = {
        "discogs": None,
        "lastfm": None,
        "bandcamp": None,
        "extra_sources": [],
    }
    artist_name = (artist or "").strip()
    title = (album_title or "").strip()
    if not artist_name or not title:
        return out

    try:
        from concurrent.futures import ThreadPoolExecutor, as_completed
    except Exception:
        # Fallback to sequential when concurrent futures is unavailable.
        try:
            if USE_DISCOGS:
                out["discogs"] = fetch_provider_album_lookup_cached(
                    "discogs",
                    artist_name,
                    title,
                    _fetch_discogs_release,
                )
            if USE_LASTFM:
                out["lastfm"] = fetch_provider_album_lookup_cached(
                    "lastfm",
                    artist_name,
                    title,
                    _fetch_lastfm_album_info,
                )
            if USE_BANDCAMP:
                out["bandcamp"] = fetch_provider_album_lookup_cached(
                    "bandcamp",
                    artist_name,
                    title,
                    _fetch_bandcamp_album_info,
                )
        except Exception:
            pass
    else:
        tasks = {}
        with ThreadPoolExecutor(max_workers=3, thread_name_prefix="pmda-provider-fallback") as pool:
            if USE_DISCOGS:
                tasks[pool.submit(fetch_provider_album_lookup_cached, "discogs", artist_name, title, _fetch_discogs_release)] = "discogs"
            if USE_LASTFM:
                tasks[pool.submit(fetch_provider_album_lookup_cached, "lastfm", artist_name, title, _fetch_lastfm_album_info)] = "lastfm"
            if USE_BANDCAMP:
                tasks[pool.submit(fetch_provider_album_lookup_cached, "bandcamp", artist_name, title, _fetch_bandcamp_album_info)] = "bandcamp"

            for fut in as_completed(tasks):
                key = tasks[fut]
                try:
                    out[key] = fut.result()
                except Exception as e:
                    logging.debug("[Providers] %s fetch failed for %r - %r: %s", key, artist_name, title, e)

    discogs_info = out.get("discogs")
    if discogs_info:
        out["extra_sources"].append(
            {
                "source": "Discogs",
                "title": discogs_info.get("title"),
                "artist_name": discogs_info.get("artist_name"),
            }
        )
    lastfm_info = out.get("lastfm")
    if lastfm_info:
        out["extra_sources"].append(
            {
                "source": "Last.fm",
                "title": lastfm_info.get("title"),
                "artist": lastfm_info.get("artist"),
            }
        )
    bandcamp_info = out.get("bandcamp")
    if bandcamp_info:
        out["extra_sources"].append(
            {
                "source": "Bandcamp",
                "title": bandcamp_info.get("title"),
                "artist_name": bandcamp_info.get("artist_name"),
            }
        )
    return out


def _normalize_identity_text_strict(value: str | None) -> str:
    """
    Normalize identity text for strict equality checks.
    Keeps unicode letters but removes punctuation/extra spaces and decodes HTML entities.
    """
    raw = html.unescape(str(value or ""))
    if not raw:
        return ""
    text = unicodedata.normalize("NFKC", raw).casefold().strip()
    if not text:
        return ""
    text = text.replace("&", " and ").replace("_", " ")
    text = re.sub(r"[`¬¥‚Äô']", "", text)
    text = re.sub(r"[^\w\s]+", " ", text, flags=re.UNICODE)
    text = " ".join(text.split())
    return text


def _normalize_identity_album_strict(value: str | None) -> str:
    """
    Normalize album title for strict identity checks while ignoring format/version suffixes.
    """
    raw = html.unescape(str(value or ""))
    if not raw:
        return ""
    dedup_norm = norm_album_for_dedup(raw, normalize_parenthetical=True)
    return _normalize_identity_text_strict(dedup_norm)


def _extract_mb_artist_names(payload: dict | None) -> list[str]:
    """Extract readable artist names from a MusicBrainz payload."""
    if not isinstance(payload, dict):
        return []
    out: list[str] = []
    artist_credit = payload.get("artist-credit")
    if isinstance(artist_credit, list):
        for item in artist_credit:
            if not isinstance(item, dict):
                continue
            nm = (item.get("name") or (item.get("artist") or {}).get("name") or "").strip()
            if nm:
                out.append(nm)
    phrase = (payload.get("artist-credit-phrase") or "").strip()
    if phrase:
        out.append(phrase)
    deduped: list[str] = []
    seen = set()
    for name in out:
        key = _normalize_identity_text_strict(name)
        if key and key not in seen:
            deduped.append(name)
            seen.add(key)
    base_names = list(deduped)
    if len(base_names) >= 2:
        deduped.append(" & ".join(base_names))
        deduped.append("; ".join(base_names))
    return deduped


def _strict_identity_match_details(
    *,
    local_artist: str,
    local_title: str,
    candidate_artist: str | list[str] | tuple[str, ...] | None,
    candidate_title: str,
) -> tuple[bool, str]:
    """
    Strict identity gate:
    - normalized artist must match exactly
    - normalized album title must match exactly
    Returns (ok, reason).
    """
    local_artist_norm = _normalize_identity_text_strict(local_artist)
    local_title_norm = _normalize_identity_album_strict(local_title)
    candidate_title_norm = _normalize_identity_album_strict(candidate_title)
    if isinstance(candidate_artist, (list, tuple)):
        candidate_artist_values = [str(x or "") for x in candidate_artist]
    else:
        candidate_artist_values = [str(candidate_artist or "")]
    candidate_artist_norms = [
        n for n in (_normalize_identity_text_strict(v) for v in candidate_artist_values) if n
    ]
    artist_ok = bool(
        local_artist_norm
        and candidate_artist_norms
        and local_artist_norm in candidate_artist_norms
    )
    title_ok = bool(local_title_norm and candidate_title_norm and local_title_norm == candidate_title_norm)
    if artist_ok and title_ok:
        return (True, "strict identity ok (artist/title exact)")
    reasons: list[str] = []
    if not local_artist_norm:
        reasons.append("local artist missing")
    if not local_title_norm:
        reasons.append("local title missing")
    if not candidate_artist_norms:
        reasons.append("candidate artist missing")
    elif not artist_ok:
        reasons.append(
            f"artist mismatch local={local_artist_norm!r} candidate={candidate_artist_norms[:3]!r}"
        )
    if not candidate_title_norm:
        reasons.append("candidate title missing")
    elif not title_ok:
        reasons.append(
            f"title mismatch local={local_title_norm!r} candidate={candidate_title_norm!r}"
        )
    return (False, "; ".join(reasons) if reasons else "strict identity failed")


def _provider_identity_text_score(local_value: str, provider_value: str) -> float:
    local_norm = norm_album(local_value or "")
    provider_norm = norm_album(provider_value or "")
    if not local_norm or not provider_norm:
        return 0.0
    if local_norm == provider_norm:
        return 1.0
    if local_norm in provider_norm or provider_norm in local_norm:
        return 0.9
    try:
        from difflib import SequenceMatcher

        return float(SequenceMatcher(None, local_norm, provider_norm).ratio())
    except Exception:
        return 0.0


def _provider_candidate_id(payload: dict, provider: str) -> str:
    p = (provider or "").strip().lower()
    if not isinstance(payload, dict):
        return ""
    if p == "discogs":
        return str(payload.get("release_id") or payload.get("master_id") or "").strip()
    if p == "lastfm":
        return str(payload.get("mbid") or "").strip()
    if p == "bandcamp":
        return str(payload.get("album_url") or payload.get("url") or "").strip()
    return ""


def _build_provider_identity_candidates(
    artist_name: str,
    album_title: str,
    local_track_titles: List[str],
    provider_payloads: dict,
) -> list[dict]:
    """
    Build scored provider identity candidates (Discogs/Last.fm/Bandcamp).
    """
    out: list[dict] = []
    provider_order = ("discogs", "lastfm", "bandcamp")
    for provider in provider_order:
        payload = provider_payloads.get(provider)
        if not isinstance(payload, dict):
            continue
        src_title = (
            (payload.get("title") if provider != "lastfm" else payload.get("title") or payload.get("album"))
            or ""
        ).strip()
        src_artist = (
            (payload.get("artist_name") if provider != "lastfm" else payload.get("artist") or payload.get("artist_name"))
            or ""
        ).strip()
        if not src_title:
            continue
        strict_ok, strict_reason = _strict_identity_match_details(
            local_artist=artist_name,
            local_title=album_title,
            candidate_artist=src_artist,
            candidate_title=src_title,
        )
        title_score = _provider_identity_text_score(album_title, src_title)
        artist_score = _provider_identity_text_score(artist_name, src_artist) if src_artist else 0.7
        track_score = 0.0
        src_tracks = payload.get("tracklist") or []
        if local_track_titles and isinstance(src_tracks, list) and src_tracks:
            try:
                track_score = float(_crosscheck_tracklist(local_track_titles, src_tracks))
            except Exception:
                track_score = 0.0
        confidence = round((title_score * 0.55) + (artist_score * 0.20) + (track_score * 0.25), 4)
        provider_id = _provider_candidate_id(payload, provider)
        out.append(
            {
                "provider": provider,
                "payload": payload,
                "provider_id": provider_id,
                "title_score": title_score,
                "artist_score": artist_score,
                "track_score": track_score,
                "confidence": confidence,
                "title": src_title,
                "artist": src_artist,
                "strict_ok": strict_ok,
                "strict_reason": strict_reason,
            }
        )
    out.sort(key=lambda c: (-float(c.get("confidence") or 0.0), -float(c.get("track_score") or 0.0), str(c.get("provider") or "")))
    return out


def _ai_choose_provider_identity_candidate(
    artist_name: str,
    album_title: str,
    local_track_titles: List[str],
    candidates: list[dict],
) -> tuple[dict | None, int | None]:
    """
    Ask AI to pick provider identity candidate when heuristics are ambiguous.
    """
    if not candidates:
        return (None, None)
    if not getattr(sys.modules[__name__], "ai_provider_ready", False):
        return (None, None)
    letters = "ABCDEFGHIJKLMNOPQRSTUVWXYZ"
    choices = []
    for i, cand in enumerate(candidates[:12]):
        choices.append(
            (
                f"{letters[i]}) provider={cand.get('provider')} "
                f"title={cand.get('title')!r} artist={cand.get('artist')!r} "
                f"track_score={float(cand.get('track_score') or 0.0):.2f} "
                f"heuristic={float(cand.get('confidence') or 0.0):.2f}"
            )
        )
    tracks_preview = ", ".join(local_track_titles[:30]) if local_track_titles else "(none)"
    if local_track_titles and len(local_track_titles) > 30:
        tracks_preview += ", ..."
    prompt = (
        f"Album identity arbitration.\n"
        f"Local album: artist={artist_name!r}, title={album_title!r}, tracks=[{tracks_preview}].\n\n"
        f"Provider candidates:\n" + "\n".join(choices) + "\n\n"
        "Pick the best matching provider candidate for this same release. "
        "Reply with one letter (A/B/...) or NONE if no candidate is reliable enough. "
        "Optionally append (confidence: N) with N from 0 to 100."
    )
    system_msg = "Reply with a single letter (A, B, C, ...) or NONE. Optionally end with (confidence: N). No explanation."
    try:
        provider = getattr(sys.modules[__name__], "AI_PROVIDER", "openai")
        model = getattr(sys.modules[__name__], "RESOLVED_MODEL", None) or getattr(sys.modules[__name__], "OPENAI_MODEL", "gpt-4o-mini")
        reply = call_ai_provider(provider, model, system_msg, prompt, max_tokens=30)
        reply_clean, ai_confidence = parse_ai_confidence((reply or "").strip())
        cleaned = (reply_clean or "").strip().upper()
        if cleaned == "NONE":
            return (None, ai_confidence)
        idx = letters.find(cleaned[:1])
        if 0 <= idx < len(candidates):
            conf_min = max(
                int(getattr(sys.modules[__name__], "AI_CONFIDENCE_MIN", 0) or 0),
                55,
            )
            if ai_confidence is not None and ai_confidence < conf_min:
                logging.info(
                    "[Providers Arbitration] AI candidate rejected due to low confidence (%s < %s)",
                    ai_confidence,
                    conf_min,
                )
                return (None, ai_confidence)
            return (candidates[idx], ai_confidence)
    except Exception as e:
        logging.debug("[Providers Arbitration] AI choice failed: %s", e)
    return (None, None)


def _arbitrate_provider_identity(
    artist_name: str,
    album_title: str,
    local_track_titles: List[str],
    provider_payloads: dict,
) -> dict | None:
    """
    Decide which provider can be accepted as source-of-truth identity when MB is unavailable.
    Returns dict with provider/payload/provider_id/confidence, or None.
    """
    candidates = _build_provider_identity_candidates(artist_name, album_title, local_track_titles, provider_payloads)
    if not candidates:
        logging.info(
            "[Providers Arbitration] %r ‚Äì %r: rejected (no provider candidates)",
            artist_name,
            album_title,
        )
        return None
    strict_candidates = [c for c in candidates if bool(c.get("strict_ok"))]
    if not strict_candidates:
        rejected = "; ".join(
            f"{c.get('provider')} -> {c.get('strict_reason') or 'strict identity failed'}"
            for c in candidates[:3]
        )
        logging.info(
            "[Providers Arbitration] %r ‚Äì %r: rejected (strict identity failed for all candidates: %s)",
            artist_name,
            album_title,
            rejected or "no reason",
        )
        return None
    strict = bool(getattr(sys.modules[__name__], "PROVIDER_IDENTITY_STRICT", True))
    min_score = float(getattr(sys.modules[__name__], "PROVIDER_IDENTITY_MIN_SCORE", 0.72) or 0.72)
    min_margin = float(getattr(sys.modules[__name__], "PROVIDER_IDENTITY_SCORE_MARGIN", 0.08) or 0.08)
    top = strict_candidates[0]
    second = strict_candidates[1] if len(strict_candidates) > 1 else None
    top_score = float(top.get("confidence") or 0.0)
    margin = top_score - float(second.get("confidence") or 0.0) if second else top_score
    top_track = float(top.get("track_score") or 0.0)
    heuristics_ok = (top_score >= min_score) and ((not strict) or margin >= min_margin or top_track >= 0.9)
    if heuristics_ok:
        logging.info(
            "[Providers Arbitration] %r ‚Äì %r: accepted %s via heuristic (score=%.2f, margin=%.2f, track=%.2f)",
            artist_name,
            album_title,
            top.get("provider"),
            top_score,
            margin,
            top_track,
        )
        return {
            "provider": top.get("provider"),
            "payload": top.get("payload"),
            "provider_id": top.get("provider_id") or "",
            "confidence": top_score,
            "confidence_source": "heuristic",
            "title_score": float(top.get("title_score") or 0.0),
            "artist_score": float(top.get("artist_score") or 0.0),
            "track_score": top_track,
        }
    if not bool(getattr(sys.modules[__name__], "PROVIDER_IDENTITY_USE_AI", True)):
        logging.info(
            "[Providers Arbitration] %r ‚Äì %r: rejected (heuristic below threshold, AI disabled)",
            artist_name,
            album_title,
        )
        return None
    ai_choice, ai_conf = _ai_choose_provider_identity_candidate(
        artist_name,
        album_title,
        local_track_titles,
        strict_candidates[:8],
    )
    if ai_choice is None:
        logging.info(
            "[Providers Arbitration] %r ‚Äì %r: rejected (AI returned no reliable strict candidate)",
            artist_name,
            album_title,
        )
        return None
    logging.info(
        "[Providers Arbitration] %r ‚Äì %r: accepted %s via AI (ai_conf=%s, heuristic=%.2f)",
        artist_name,
        album_title,
        ai_choice.get("provider"),
        ai_conf if ai_conf is not None else "n/a",
        float(ai_choice.get("confidence") or 0.0),
    )
    return {
        "provider": ai_choice.get("provider"),
        "payload": ai_choice.get("payload"),
        "provider_id": ai_choice.get("provider_id") or "",
        "confidence": float(ai_choice.get("confidence") or 0.0),
        "confidence_source": "ai",
        "ai_confidence": ai_conf,
        "title_score": float(ai_choice.get("title_score") or 0.0),
        "artist_score": float(ai_choice.get("artist_score") or 0.0),
        "track_score": float(ai_choice.get("track_score") or 0.0),
    }


def search_mb_release_group_by_metadata(
    artist: str,
    album_norm: str,
    tracks: set[str],
    title_raw: Optional[str] = None,
    album_folder: Optional[Path] = None,
) -> tuple[dict | None, bool]:
    """
    Fallback search on MusicBrainz by artist name, normalized album title, and optional track titles.
    Tries: (1) search with strict=True, (2) search with strict=False, (3) browse by artist and match title,
    (4) if still no candidates and title_raw differs from album_norm, retry search/browse with title_raw (e.g. "Isolette").
    If multiple candidates and USE_AI_FOR_MB_MATCH, uses AI to pick best match.
    When USE_AI_VISION_FOR_COVER and album_folder are set, compares local cover to Cover Art Archive (vision).
    When 0 candidates or AI returns NONE, can use Bandcamp + web search (Serper) + AI to suggest MBID.
    Returns (release-group info dict or None, verified_by_ai: bool). verified_by_ai is True when match was chosen by USE_AI_FOR_MB_VERIFY.
    """
    if album_folder is not None and not isinstance(album_folder, Path):
        album_folder = Path(album_folder) if album_folder else None
    mb_search_started = time.perf_counter()
    mb_budget_sec = max(10, int(getattr(sys.modules[__name__], "MB_SEARCH_ALBUM_TIMEOUT_SEC", 20) or 20))
    candidate_fetch_limit = max(1, int(getattr(sys.modules[__name__], "MB_CANDIDATE_FETCH_LIMIT", 4) or 4))
    tracklist_fetch_limit = max(0, int(getattr(sys.modules[__name__], "MB_TRACKLIST_FETCH_LIMIT", 2) or 2))
    fast_fallback_mode = bool(getattr(sys.modules[__name__], "MB_FAST_FALLBACK_MODE", True))
    provider_fallback_cache: dict | None = None
    use_ai_for_mb_match = bool(getattr(sys.modules[__name__], "USE_AI_FOR_MB_MATCH", False))
    use_ai_for_mb_verify = bool(getattr(sys.modules[__name__], "USE_AI_FOR_MB_VERIFY", False))
    use_ai_for_mb = bool(use_ai_for_mb_match or use_ai_for_mb_verify)

    def _mb_budget_exceeded() -> bool:
        return (time.perf_counter() - mb_search_started) >= mb_budget_sec

    def _provider_fallbacks() -> dict:
        nonlocal provider_fallback_cache
        if provider_fallback_cache is None:
            provider_fallback_cache = _fetch_album_provider_fallbacks_parallel(
                artist,
                (title_raw or album_norm or "").strip(),
            )
        return provider_fallback_cache

    def _fetch_rg_details(rg_id: str):
        """Fetch release group details by ID. On 404, try treating rg_id as a release ID and resolve to release-group."""
        try:
            info = musicbrainzngs.get_release_group_by_id(
                rg_id, includes=["releases", "artist-credits"]
            )["release-group"]
            return info
        except musicbrainzngs.WebServiceError as e:
            if "404" not in str(e):
                raise
            # API returned 404: rg_id may be a release ID (search/browse can occasionally return release refs)
            # use_queue=False to avoid deadlock (we are already inside the MB queue worker)
            resolved = resolve_mbid_to_release_group(rg_id, "musicbrainz_releaseid", use_queue=False)
            if resolved and resolved != rg_id:
                info = musicbrainzngs.get_release_group_by_id(
                    resolved, includes=["releases", "artist-credits"]
                )["release-group"]
                return info
            raise

    seen_ids: set[str] = set()
    candidates: List[dict] = []

    def _collect_candidates(search_results: List[dict]) -> None:
        for rg in search_results:
            rg_id = rg.get("id")
            if rg_id and rg_id not in seen_ids:
                seen_ids.add(rg_id)
                candidates.append(rg)

    def _run_search_and_browse(release_query: str) -> None:
        if _mb_budget_exceeded():
            return
        if MB_QUEUE_ENABLED and USE_MUSICBRAINZ:
            try:
                _collect_candidates(get_mb_queue().submit(f"search_{artist}_{release_query}_1", lambda: _search_mb_rg_candidates(artist, release_query, True)))
            except Exception as e:
                logging.debug("[MusicBrainz Search] strict query failed for %r - %r: %s", artist, release_query, e)
            if not _mb_budget_exceeded():
                try:
                    _collect_candidates(get_mb_queue().submit(f"search_{artist}_{release_query}_0", lambda: _search_mb_rg_candidates(artist, release_query, False)))
                except Exception as e:
                    logging.debug("[MusicBrainz Search] relaxed query failed for %r - %r: %s", artist, release_query, e)
        else:
            try:
                _collect_candidates(_search_mb_rg_candidates(artist, release_query, True))
            except Exception as e:
                logging.debug("[MusicBrainz Search] strict query failed for %r - %r: %s", artist, release_query, e)
            if not _mb_budget_exceeded():
                try:
                    _collect_candidates(_search_mb_rg_candidates(artist, release_query, False))
                except Exception as e:
                    logging.debug("[MusicBrainz Search] relaxed query failed for %r - %r: %s", artist, release_query, e)
        if not candidates and not _mb_budget_exceeded():
            browse_list = _browse_mb_rg_by_artist(artist, release_query)
            _collect_candidates(browse_list)

    try:
        # 1‚Äì3) Search (strict, non-strict) and browse by artist with normalized title
        _run_search_and_browse(album_norm)

        # 4) If still no candidates, try with raw title (e.g. "Isolette") ‚Äî MusicBrainz may match exact casing
        if (not candidates) and (not _mb_budget_exceeded()) and title_raw and (title_raw.strip() != album_norm):
            raw_clean = title_raw.strip()
            if raw_clean:
                _run_search_and_browse(raw_clean)

        if candidates:
            letters = "ABCDEFGHIJKLMNOPQRSTUVWXYZ"
            lines = [
                "[MusicBrainz] %s ‚Äì %r: %d candidate(s) from search"
                % (artist, title_raw or album_norm or "?", len(candidates)),
                *["  %s) %s (id=%s)" % (letters[i], (rg.get("title") or "?"), rg.get("id") or "?") for i, rg in enumerate(candidates[:20])],
            ]
            if len(candidates) > 20:
                lines.append("  ... and %d more" % (len(candidates) - 20))
            logging.info("\n".join(lines))

        all_fetched: List[tuple] = []
        # Intentionally avoid an "AI-first" pick here. We fetch a limited number of candidates and
        # only use AI later if disambiguation is genuinely ambiguous.

        if len(candidates) > candidate_fetch_limit:
            logging.info(
                "[MusicBrainz] %s ‚Äì %r: %d candidate(s); limiting detailed fetch to top %d for speed",
                artist,
                title_raw or album_norm or "?",
                len(candidates),
                candidate_fetch_limit,
            )

        for idx, rg in enumerate(candidates[:candidate_fetch_limit]):
            if _mb_budget_exceeded():
                logging.info(
                    "[MusicBrainz] %s ‚Äì %r: MB budget reached after %.1fs (budget=%ds); stopping detailed candidate fetch and using fallback flow",
                    artist,
                    title_raw or album_norm or "?",
                    time.perf_counter() - mb_search_started,
                    mb_budget_sec,
                )
                break
            try:
                rg_id = rg['id']
                if MB_QUEUE_ENABLED and USE_MUSICBRAINZ:
                    info = get_mb_queue().submit(f"rg_{rg_id}", lambda rid=rg_id: _fetch_rg_details(rid))
                else:
                    info = _fetch_rg_details(rg_id)
                mb_track_count = _mb_track_count_from_rg_info(info)
                formats = set()
                for release in info.get('release-list', []):
                    for medium in release.get('medium-list', []):
                        fmt = medium.get('format')
                        qty = medium.get('track-count', 1)
                        if fmt:
                            formats.add(f"{qty}√ó{fmt}")
                rg_id_final = info.get('id', rg['id'])
                mb_title = str(info.get("title") or rg.get("title") or "").strip()
                mb_artists = _extract_mb_artist_names(info) or _extract_mb_artist_names(rg)
                strict_ok, strict_reason = _strict_identity_match_details(
                    local_artist=artist,
                    local_title=title_raw or album_norm,
                    candidate_artist=mb_artists,
                    candidate_title=mb_title,
                )
                if not strict_ok:
                    log_mb(
                        "Album %s ‚Äì \"%s\": candidate %s rejected before arbitration (%s)",
                        artist,
                        title_raw or album_norm,
                        rg_id_final,
                        strict_reason,
                    )
                    continue
                result_dict = {
                    'primary_type': info.get('primary-type', ''),
                    'secondary_types': info.get('secondary-types', []),
                    'format_summary': ', '.join(sorted(formats)),
                    'id': rg_id_final,
                    'track_count': mb_track_count,
                    # Phase 4: Cover Art Archive URL for optional vision comparison
                    'cover_url': f"https://coverartarchive.org/release-group/{rg_id_final}/front",
                    'mb_title': mb_title,
                    'mb_artist_names': mb_artists,
                }
                # Optionally fetch recording-level track titles for deterministic tracklist crosschecks
                # (no LLM cost, but this endpoint is heavier; controlled by MB_TRACKLIST_FETCH_LIMIT).
                if info.get('release-list') and idx < tracklist_fetch_limit and tracks:
                    first_release_id = info['release-list'][0].get('id')
                    if first_release_id:
                        try:
                            def _fetch_release_recordings(rel_id: str):
                                return musicbrainzngs.get_release_by_id(rel_id, includes=["recordings"])
                            if MB_QUEUE_ENABLED and USE_MUSICBRAINZ:
                                rel_resp = get_mb_queue().submit(f"rel_rec_{first_release_id}", lambda rid=first_release_id: _fetch_release_recordings(rid))
                            else:
                                rel_resp = _fetch_release_recordings(first_release_id)
                            result_dict['track_titles'] = _extract_track_titles_from_mb_release(rel_resp)
                        except musicbrainzngs.WebServiceError:
                            pass
                all_fetched.append((rg, result_dict))
            except musicbrainzngs.WebServiceError:
                continue

        if all_fetched:
            letters = "ABCDEFGHIJKLMNOPQRSTUVWXYZ"
            lines = [
                "[MusicBrainz] %s ‚Äì %r: fetched details for %d candidate(s)"
                % (artist, title_raw or album_norm or "?", len(all_fetched)),
                *[
                    "  %s) %s | %s | %d tracks | %s"
                    % (
                        letters[i],
                        (r[0].get("title") or "?"),
                        r[1].get("id") or "?",
                        r[1].get("track_count", 0),
                        (r[1].get("format_summary") or "")[:40],
                    )
                    for i, r in enumerate(all_fetched[:20])
                ],
            ]
            if len(all_fetched) > 20:
                lines.append("  ... and %d more" % (len(all_fetched) - 20))
            logging.info("\n".join(lines))

        if not all_fetched:
            title_for_zero = (title_raw or album_norm or "").strip()
            if fast_fallback_mode:
                if _mb_budget_exceeded():
                    reason = f"budget exceeded (budget={mb_budget_sec}s)"
                else:
                    reason = "no MB candidates/details"
                logging.info(
                    "[MusicBrainz] %s ‚Äì %r: fast fallback mode -> skipping web+AI MBID hunt (%s)",
                    artist,
                    title_for_zero or album_norm or "?",
                    reason,
                )
                return (None, False)

            provider_info = _provider_fallbacks() if title_for_zero else {}
            provider_sources = list(provider_info.get("extra_sources") or [])
            if provider_sources:
                lines = ["[Providers] %s ‚Äì %r: no usable MusicBrainz match yet; fallback sources ready:" % (artist, title_for_zero or "?")]
                for src in provider_sources:
                    src_name = src.get("source", "?")
                    src_title = src.get("title") or src.get("album") or "?"
                    src_artist = src.get("artist") or src.get("artist_name") or "?"
                    lines.append("  %s: %r by %s" % (src_name, src_title, src_artist))
                logging.info("\n".join(lines))

            if (not use_ai_for_mb) or (not getattr(sys.modules[__name__], "ai_provider_ready", False)):
                return (None, False)

            # Optional slow path: try web + AI to infer MBID even when MB has no usable candidate.
            web_snippets = ""
            web_results = []
            if USE_WEB_SEARCH_FOR_MB and SERPER_API_KEY.strip() and title_for_zero:
                q = f"{artist} {title_for_zero} album"
                web_results = _web_search_serper(q, num=10)
                if web_results:
                    parts = [f"Web {i+1}) {r.get('title')} ‚Äî {r.get('snippet')} ({r.get('link')})" for i, r in enumerate(web_results[:10])]
                    web_snippets = "\n".join(parts)
            bandcamp_text = ""
            bc_info = provider_info.get("bandcamp") if provider_info else None
            if bc_info:
                bandcamp_text = f"Bandcamp: title={bc_info.get('title')}, artist={bc_info.get('artist_name')}"

            if bandcamp_text or web_snippets:
                prompt = f"Our album: artist={artist}, title={title_for_zero}. No usable MusicBrainz candidate.\n"
                if bandcamp_text:
                    prompt += bandcamp_text + "\n"
                if web_snippets:
                    prompt += "Web results:\n" + web_snippets + "\n"
                prompt += "Suggest a MusicBrainz release group ID (MBID, UUID format) if you can identify it from the above, or reply exactly NONE. Optionally end with (confidence: N) where N is 0-100."
                try:
                    provider = getattr(sys.modules[__name__], "AI_PROVIDER", "openai")
                    model = getattr(sys.modules[__name__], "RESOLVED_MODEL", None) or getattr(sys.modules[__name__], "OPENAI_MODEL", "gpt-4o-mini")
                    reply = call_ai_provider(provider, model, "You reply with a single MBID (UUID) or the word NONE. Optionally end with (confidence: N).", prompt, max_tokens=70)
                    reply_clean, ai_confidence = parse_ai_confidence((reply or "").strip())
                    if ai_confidence is not None:
                        logging.info("[MusicBrainz] Bandcamp/web AI MBID suggestion confidence: %d", ai_confidence)
                    reply = reply_clean.upper()
                    if reply and reply != "NONE":
                        mbid_match = re.search(r"[0-9A-Fa-f]{8}-[0-9A-Fa-f]{4}-[0-9A-Fa-f]{4}-[0-9A-Fa-f]{4}-[0-9A-Fa-f]{12}", reply)
                        if mbid_match:
                            suggested_mbid = mbid_match.group(0)
                            try:
                                info = _fetch_rg_details(suggested_mbid)
                                mb_track_count = _mb_track_count_from_rg_info(info)
                                formats = set()
                                for release in info.get("release-list", []):
                                    for medium in release.get("medium-list", []):
                                        fmt = medium.get("format")
                                        qty = medium.get("track-count", 1)
                                        if fmt:
                                            formats.add(f"{qty}√ó{fmt}")
                                rg_id_final = info.get("id", suggested_mbid)
                                mb_title = str(info.get("title") or "").strip()
                                mb_artists = _extract_mb_artist_names(info)
                                strict_ok, strict_reason = _strict_identity_match_details(
                                    local_artist=artist,
                                    local_title=title_for_zero or album_norm,
                                    candidate_artist=mb_artists,
                                    candidate_title=mb_title,
                                )
                                if not strict_ok:
                                    log_mb(
                                        "Album %s ‚Äì \"%s\": web+AI suggested MBID %s rejected (%s)",
                                        artist,
                                        title_for_zero or album_norm,
                                        rg_id_final,
                                        strict_reason,
                                    )
                                    raise RuntimeError("strict identity mismatch")
                                result_dict = {
                                    "primary_type": info.get("primary-type", ""),
                                    "secondary_types": info.get("secondary-types", []),
                                    "format_summary": ", ".join(sorted(formats)),
                                    "id": rg_id_final,
                                    "track_count": mb_track_count,
                                    "cover_url": f"https://coverartarchive.org/release-group/{rg_id_final}/front",
                                    "mb_title": mb_title,
                                    "mb_artist_names": mb_artists,
                                }
                                set_cached_mb_info(rg_id_final, result_dict)
                                logging.info("[MusicBrainz] MBID suggested by AI from Bandcamp/web for artist=%r album=%r: %s", artist, album_norm, rg_id_final)
                                return (result_dict, True)
                            except RuntimeError:
                                pass
                            except musicbrainzngs.WebServiceError:
                                logging.debug("[MusicBrainz] AI-suggested MBID %s invalid or 404", suggested_mbid)
                except Exception as e:
                    logging.debug("[MusicBrainz] Bandcamp/web AI suggestion failed: %s", e)
            return (None, False)

        matching: List[tuple] = [
            x for x in all_fetched
            if not tracks or abs(len(tracks) - x[1].get('track_count', 0)) <= 1
        ]

        # Deterministic fast-path: if track titles were fetched for *all* candidates and exactly one is a perfect
        # tracklist match, accept it without AI (secure + cheaper). This only triggers when we have enough local
        # signal (track titles) and enough remote signal (track titles for every candidate).
        if tracks and len(matching) >= 2:
            try:
                local_titles = list(tracks)
                scored: list[tuple[float, dict]] = []
                missing_titles = 0
                for _rg, info in matching:
                    cand_titles = info.get("track_titles") if isinstance(info, dict) else None
                    if not cand_titles:
                        missing_titles += 1
                        continue
                    score = float(_crosscheck_tracklist_perfect(local_titles, list(cand_titles)))
                    scored.append((score, info))
                if scored and missing_titles == 0:
                    scored.sort(key=lambda x: -x[0])
                    best_score = float(scored[0][0])
                    best_count = sum(1 for s, _ in scored if abs(float(s) - best_score) < 1e-9)
                    if best_count == 1 and best_score >= 0.999:
                        picked = scored[0][1]
                        set_cached_mb_info(picked['id'], picked)
                        log_mb(
                            "Album %s ‚Äì \"%s\": accepted MB candidate %s (perfect tracklist match; skipped AI)",
                            artist,
                            title_raw or album_norm,
                            picked.get("id") or "?",
                        )
                        return (picked, False)
            except Exception:
                pass

        if use_ai_for_mb_verify and len(matching) >= 2:
            # AI verify is expensive (prompt can include tracklists). Only use it when there is real ambiguity.
            title_for_strict = (title_raw or album_norm or "").strip()
            strict_matches: list[tuple] = []
            if title_for_strict:
                for rg, info in all_fetched[:12]:
                    cand_title = str(info.get("mb_title") or rg.get("title") or "").strip()
                    cand_artists = info.get("mb_artist_names") or _extract_mb_artist_names(rg)
                    ok, _reason = _strict_identity_match_details(
                        local_artist=artist,
                        local_title=title_for_strict,
                        candidate_artist=cand_artists,
                        candidate_title=cand_title,
                    )
                    if ok:
                        strict_matches.append((rg, info))
            if len(strict_matches) == 1:
                # Unique strict match: no need to pay for an LLM disambiguation call.
                picked = strict_matches[0][1]
                set_cached_mb_info(picked['id'], picked)
                log_mb(
                    "Album %s ‚Äì \"%s\": accepted MB candidate %s (strict identity unique; skipped AI verify)",
                    artist,
                    title_for_strict or album_norm,
                    picked.get("id") or "?",
                )
                return (picked, False)

            # Phase 3: optional Discogs/Last.fm/Bandcamp for AI disambiguation (only when we actually call AI).
            extra_sources: List[dict] = []
            title_for_fetch = title_for_strict
            if title_for_fetch:
                extra_sources = list((_provider_fallbacks() or {}).get("extra_sources") or [])
            if extra_sources:
                lines = ["[Providers] %s ‚Äì %r: extra sources for AI disambiguation:" % (artist, title_for_fetch or "?")]
                for s in extra_sources:
                    src = s.get("source", "?")
                    title = s.get("title") or s.get("album") or "?"
                    artist_val = s.get("artist") or s.get("artist_name") or "?"
                    lines.append("  %s: %r by %s" % (src, title, artist_val))
                logging.info("\n".join(lines))

            chosen, _ = ai_verify_mb_match(
                artist, title_raw, album_norm,
                list(tracks) if tracks else None,
                len(tracks) if tracks else 0,
                all_fetched,
                has_cover=False,
                extra_sources=extra_sources or None,
            )
            if chosen:
                chosen_title = str(chosen[1].get("mb_title") or chosen[0].get("title") or "").strip()
                chosen_artists = chosen[1].get("mb_artist_names") or _extract_mb_artist_names(chosen[0])
                strict_ok, strict_reason = _strict_identity_match_details(
                    local_artist=artist,
                    local_title=title_raw or album_norm,
                    candidate_artist=chosen_artists,
                    candidate_title=chosen_title,
                )
                if not strict_ok:
                    log_mb(
                        "Album %s ‚Äì \"%s\": AI-verified candidate rejected (%s)",
                        artist,
                        title_raw or album_norm,
                        strict_reason,
                    )
                    chosen = None
            if chosen:
                # Partie 1: Optional vision check (local cover vs Cover Art Archive)
                # Vision cover comparison is expensive and can reject correct matches; keep it off by default.
                # We already guard identity via strict checks + tracklist evidence above.
                use_vision = False
                if use_vision:
                    folder_path = Path(album_folder) if not isinstance(album_folder, Path) else album_folder
                    local_data_uri = _get_local_cover_data_uri_for_vision(folder_path)
                    mb_cover_url = chosen[1].get("cover_url")
                    if local_data_uri and mb_cover_url:
                        try:
                            vision_model = getattr(sys.modules[__name__], "RESOLVED_MODEL", None) or getattr(sys.modules[__name__], "OPENAI_MODEL", "gpt-4o-mini")
                            sys_msg = "You reply with exactly Yes or No. If No, add in parentheses the reason, e.g. (artist photo) or (different cover). Optionally end with (confidence: N) where N is 0-100."
                            user_msg = (
                                "Image 1 is the local album cover. Image 2 is the MusicBrainz Cover Art Archive cover. "
                                "Do they represent the same album cover? Reply: Yes or No. "
                                "If No, add in parentheses why (e.g. 'No (artist photo)' or 'No (different album cover)'). "
                                "Optionally end with (confidence: N)."
                            )
                            log_cov(
                                "Vision: comparing local cover vs CAA for artist=%r album=%r (model=%s)",
                                artist,
                                album_norm or title_raw or "",
                                vision_model,
                            )
                            resp = call_ai_provider_vision(
                                getattr(sys.modules[__name__], "AI_PROVIDER", "openai"),
                                vision_model,
                                sys_msg,
                                user_msg,
                                image_urls=[mb_cover_url],
                                image_base64=[{"type": "image_url", "image_url": {"url": local_data_uri}}],
                                max_tokens=20,
                            )
                            verdict_clean, vision_confidence = parse_ai_confidence((resp or "").strip())
                            if vision_confidence is not None:
                                logging.info("[MusicBrainz Vision] Cover comparison confidence: %d", vision_confidence)
                            verdict = (verdict_clean or "").strip().upper()
                            # Log comparison result clearly: successful (same cover) or rejected (with reason if present)
                            if verdict and "YES" in verdict:
                                log_cov(
                                    "Vision comparison result: accepted ‚Äî same album cover (artist=%r album=%r)%s",
                                    artist, album_norm or title_raw or "",
                                    " confidence=%d" % vision_confidence if vision_confidence is not None else "",
                                )
                            else:
                                reason = (verdict_clean or "").strip()
                                if not reason:
                                    reason = "no verdict"
                                log_cov(
                                    "Vision comparison result: rejected ‚Äî %s (artist=%r album=%r)%s",
                                    reason, artist, album_norm or title_raw or "",
                                    " confidence=%d" % vision_confidence if vision_confidence is not None else "",
                                )
                            if verdict and "NO" in verdict:
                                chosen = None
                                log_cov(
                                    "Cover mismatch: rejecting AI-chosen MB match for artist=%r album=%r based on vision",
                                    artist,
                                    album_norm or title_raw or "",
                                )
                        except Exception as e:
                            logging.debug("[MusicBrainz Vision] Vision check failed: %s", e)
                if chosen:
                    set_cached_mb_info(chosen[1]['id'], chosen[1])
                    logging.info("[MusicBrainz Verify] Match verified by AI for artist=%r album=%r", artist, album_norm)
                    return (chosen[1], True)
            # Partie 2 Cas 2: AI said NONE but we have candidates; try web search + one more AI call
            if (
                chosen is None
                and all_fetched
                and USE_WEB_SEARCH_FOR_MB
                and SERPER_API_KEY.strip()
                and (not fast_fallback_mode)
                and (not _mb_budget_exceeded())
            ):
                title_for_web = (title_raw or album_norm or "").strip()
                if title_for_web:
                    q = f"{artist} {title_for_web} album"
                    web_results = _web_search_serper(q, num=10)
                    if web_results:
                        lines = ["[Providers] %s ‚Äì %r: web search (AI said NONE, retry with web):" % (artist, title_for_web), "  Query: %s ‚Äî %d result(s)" % (q, len(web_results))]
                        for i, r in enumerate(web_results[:8]):
                            lines.append("  %d. %s" % (i + 1, (r.get("title") or "?")[:70]))
                            if r.get("snippet"):
                                lines.append("     %s" % ((r.get("snippet") or "")[:100].replace("\n", " ")))
                        if len(web_results) > 8:
                            lines.append("  ... and %d more" % (len(web_results) - 8))
                        logging.info("\n".join(lines))
                        web_snippets = "\n".join([f"Web {i+1}) {r.get('title')} ‚Äî {r.get('snippet')} ({r.get('link')})" for i, r in enumerate(web_results[:10])])
                        letters = "ABCDEFGHIJKLMNOPQRSTUVWXYZ"
                        choices = [f"{letters[i]}: {all_fetched[i][0].get('title', '?')} (MBID: {all_fetched[i][1].get('id')})" for i in range(min(len(all_fetched), 10))]
                        prompt = f"Artist: {artist}. Album: {title_for_web}. We had MusicBrainz candidates but none matched. Web results:\n{web_snippets}\n\nCandidates: " + " | ".join(choices)
                        prompt += "\nReply with the letter (A/B/...) of the correct candidate, or an MBID (UUID) if you find one in web results, or NONE. Optionally end with (confidence: N) where N is 0-100."
                        try:
                            provider = getattr(sys.modules[__name__], "AI_PROVIDER", "openai")
                            model = getattr(sys.modules[__name__], "RESOLVED_MODEL", None) or getattr(sys.modules[__name__], "OPENAI_MODEL", "gpt-4o-mini")
                            reply = call_ai_provider(provider, model, "You reply with a single letter, or an MBID (UUID), or NONE. Optionally end with (confidence: N).", prompt, max_tokens=70)
                            reply_clean, ai_confidence = parse_ai_confidence((reply or "").strip())
                            if ai_confidence is not None:
                                logging.info("[MusicBrainz Verify] Web+AI choice confidence: %d", ai_confidence)
                            reply = reply_clean.upper()
                            if reply and reply != "NONE":
                                letter = reply[:1]
                                idx = letters.find(letter)
                                if 0 <= idx < len(all_fetched):
                                    picked = all_fetched[idx][1]
                                    strict_ok, strict_reason = _strict_identity_match_details(
                                        local_artist=artist,
                                        local_title=title_for_web or album_norm,
                                        candidate_artist=picked.get("mb_artist_names") or _extract_mb_artist_names(all_fetched[idx][0]),
                                        candidate_title=picked.get("mb_title") or all_fetched[idx][0].get("title") or "",
                                    )
                                    if strict_ok:
                                        set_cached_mb_info(picked['id'], picked)
                                        logging.info("[MusicBrainz Verify] Web+AI chose candidate %s for artist=%r album=%r", letter, artist, album_norm)
                                        return (picked, True)
                                    log_mb(
                                        "Album %s ‚Äì \"%s\": web+AI candidate %s rejected (%s)",
                                        artist,
                                        title_for_web or album_norm,
                                        picked.get("id") or "?",
                                        strict_reason,
                                    )
                                mbid_match = re.search(r"[0-9A-Fa-f]{8}-[0-9A-Fa-f]{4}-[0-9A-Fa-f]{4}-[0-9A-Fa-f]{4}-[0-9A-Fa-f]{12}", reply)
                                if mbid_match:
                                    suggested_mbid = mbid_match.group(0)
                                    try:
                                        info = _fetch_rg_details(suggested_mbid)
                                        mb_track_count = _mb_track_count_from_rg_info(info)
                                        formats = set()
                                        for release in info.get("release-list", []):
                                            for medium in release.get("medium-list", []):
                                                fmt, qty = medium.get("format"), medium.get("track-count", 1)
                                                if fmt:
                                                    formats.add(f"{qty}√ó{fmt}")
                                        rg_id_final = info.get("id", suggested_mbid)
                                        mb_title = str(info.get("title") or "").strip()
                                        mb_artists = _extract_mb_artist_names(info)
                                        strict_ok, strict_reason = _strict_identity_match_details(
                                            local_artist=artist,
                                            local_title=title_for_web or album_norm,
                                            candidate_artist=mb_artists,
                                            candidate_title=mb_title,
                                        )
                                        if not strict_ok:
                                            log_mb(
                                                "Album %s ‚Äì \"%s\": web+AI MBID %s rejected (%s)",
                                                artist,
                                                title_for_web or album_norm,
                                                rg_id_final,
                                                strict_reason,
                                            )
                                            raise RuntimeError("strict identity mismatch")
                                        result_dict = {
                                            "primary_type": info.get("primary-type", ""),
                                            "secondary_types": info.get("secondary-types", []),
                                            "format_summary": ", ".join(sorted(formats)),
                                            "id": rg_id_final,
                                            "track_count": mb_track_count,
                                            "cover_url": f"https://coverartarchive.org/release-group/{rg_id_final}/front",
                                            "mb_title": mb_title,
                                            "mb_artist_names": mb_artists,
                                        }
                                        set_cached_mb_info(rg_id_final, result_dict)
                                        logging.info("[MusicBrainz] Web+AI suggested MBID %s for artist=%r album=%r", rg_id_final, artist, album_norm)
                                        return (result_dict, True)
                                    except RuntimeError:
                                        pass
                                    except musicbrainzngs.WebServiceError:
                                        pass
                        except Exception as e:
                            logging.debug("[MusicBrainz] Web+AI second call failed: %s", e)
            if not matching:
                if candidates:
                    detail = ", ".join(f"{rg.get('title', '?')} ({rg.get('id', '')})" for rg in candidates[:10])
                    if len(candidates) > 10:
                        detail += f" ... and {len(candidates) - 10} more"
                    logging.debug(
                        "[MusicBrainz Search] artist=%r release=%r: AI said NONE or failed, no track-count match: %s",
                        artist, album_norm, detail,
                    )
                return (None, False)

        if not matching:
            if candidates:
                detail = ", ".join(f"{rg.get('title', '?')} ({rg.get('id', '')})" for rg in candidates[:10])
                if len(candidates) > 10:
                    detail += f" ... and {len(candidates) - 10} more"
                logging.debug(
                    "[MusicBrainz Search] artist=%r release=%r: %d candidate(s) but none matched (track count or fetch failed): %s",
                    artist, album_norm, len(candidates), detail,
                )
            return (None, False)
        if len(matching) == 1:
            set_cached_mb_info(matching[0][1]['id'], matching[0][1])
            log_mb(
                "Album %s ‚Äì \"%s\": accepted MB candidate %s (strict identity ok; single track-count match)",
                artist,
                title_raw or album_norm,
                matching[0][1].get("id") or "?",
            )
            return (matching[0][1], False)

        if len(matching) >= 2:
            # Try deterministic provider evidence (no LLM cost) before asking AI.
            title_for_fetch = (title_raw or album_norm or "").strip()
            extra_sources: List[dict] = []
            if title_for_fetch:
                try:
                    extra_sources = list((_provider_fallbacks() or {}).get("extra_sources") or [])
                except Exception:
                    extra_sources = []
            if extra_sources:
                votes = [0] * len(matching)
                for i, (rg, info) in enumerate(matching[:12]):
                    cand_title = str(info.get("mb_title") or rg.get("title") or "").strip()
                    cand_artists = info.get("mb_artist_names") or _extract_mb_artist_names(rg)
                    for s in extra_sources:
                        src_title = str(s.get("title") or s.get("album") or "").strip()
                        src_artist = str(s.get("artist") or s.get("artist_name") or "").strip()
                        if not src_title or not src_artist:
                            continue
                        ok, _reason = _strict_identity_match_details(
                            local_artist=src_artist,
                            local_title=src_title,
                            candidate_artist=cand_artists,
                            candidate_title=cand_title,
                        )
                        if ok:
                            votes[i] += 1
                max_votes = max(votes) if votes else 0
                if max_votes > 0 and votes.count(max_votes) == 1:
                    best_idx = votes.index(max_votes)
                    picked = matching[best_idx][1]
                    set_cached_mb_info(picked['id'], picked)
                    log_mb(
                        "Album %s ‚Äì \"%s\": accepted MB candidate %s (provider strict match; skipped AI)",
                        artist,
                        title_raw or album_norm,
                        picked.get("id") or "?",
                    )
                    return (picked, False)

            # Still ambiguous: ask AI to pick among matching titles only (cheap).
            if use_ai_for_mb_match and getattr(sys.modules[__name__], "ai_provider_ready", False):
                letters = "ABCDEFGHIJKLMNOPQRSTUVWXYZ"
                choices: list[str] = []
                for i, m in enumerate(matching[:10]):
                    rg, info = m
                    mid = str(info.get("id") or rg.get("id") or "").strip()
                    choices.append(f"{letters[i]}: {rg.get('title', 'Unknown')} (id={mid})")
                prompt = (
                    f"Artist: {artist}. Album: {title_raw or album_norm}. "
                    "Which MusicBrainz release-group is the same release? "
                    "Reply with only the letter (A/B/...) or NONE. Optionally end with (confidence: N).\n"
                    + "\n".join(choices)
                )
                if extra_sources:
                    other_lines = []
                    for s in extra_sources:
                        src = s.get("source", "?")
                        t = s.get("title") or s.get("album") or "?"
                        a = s.get("artist") or s.get("artist_name") or "?"
                        other_lines.append(f"  {src}: {t!r} (artist: {a!r})")
                    prompt += "\n\nOther sources:\n" + "\n".join(other_lines)
                try:
                    provider = getattr(sys.modules[__name__], "AI_PROVIDER", "openai")
                    model = getattr(sys.modules[__name__], "RESOLVED_MODEL", None) or getattr(sys.modules[__name__], "OPENAI_MODEL", "gpt-4o-mini")
                    reply = call_ai_provider(
                        provider,
                        model,
                        "Reply with a single letter (A/B/...) or NONE. Optionally end with (confidence: N).",
                        prompt,
                        max_tokens=30,
                    )
                    reply_clean, ai_confidence = parse_ai_confidence((reply or "").strip())
                    if ai_confidence is not None:
                        logging.info("[MusicBrainz Search] AI pick among matching candidates confidence: %d", ai_confidence)
                    letter = (reply_clean or "").strip().upper()[:1]
                    idx = letters.find(letter)
                    if 0 <= idx < len(matching):
                        picked = matching[idx][1]
                        # Optional vision check (OpenAI only): local cover vs CAA.
                        # Vision cover comparison is expensive and can reject correct matches; keep it off by default.
                        use_vision = False
                        if use_vision:
                            try:
                                folder_path = Path(album_folder) if not isinstance(album_folder, Path) else album_folder
                                local_data_uri = _get_local_cover_data_uri_for_vision(folder_path)
                                mb_cover_url = picked.get("cover_url")
                                if local_data_uri and mb_cover_url:
                                    sys_msg = "You reply with exactly Yes or No. If No, add in parentheses the reason. Optionally end with (confidence: N)."
                                    user_msg = (
                                        "Image 1 is the local album cover. Image 2 is the MusicBrainz Cover Art Archive cover. "
                                        "Do they represent the same album cover? Reply: Yes or No."
                                    )
                                    resp = call_ai_provider_vision(
                                        provider,
                                        model,
                                        sys_msg,
                                        user_msg,
                                        image_urls=[mb_cover_url],
                                        image_base64=[{"type": "image_url", "image_url": {"url": local_data_uri}}],
                                        max_tokens=20,
                                    )
                                    verdict_clean, _vision_conf = parse_ai_confidence((resp or "").strip())
                                    verdict = (verdict_clean or "").strip().upper()
                                    if verdict and "NO" in verdict:
                                        log_cov(
                                            "Cover mismatch: rejecting AI-chosen MB match for artist=%r album=%r based on vision",
                                            artist,
                                            album_norm or title_raw or "",
                                        )
                                        return (None, False)
                            except Exception as e:
                                logging.debug("[MusicBrainz Vision] Vision check failed: %s", e)

                        set_cached_mb_info(picked['id'], picked)
                        log_mb(
                            "Album %s ‚Äì \"%s\": accepted MB candidate %s (strict identity ok; AI tie-break among matches)",
                            artist,
                            title_raw or album_norm,
                            picked.get("id") or "?",
                        )
                        return (picked, True)
                except Exception as e:
                    logging.debug("[MusicBrainz Search] AI pick failed: %s", e)

            # Ambiguous and no deterministic winner: do not guess.
            log_mb(
                "Album %s ‚Äì \"%s\": multiple MusicBrainz candidates remain (strict identity ok) but no reliable disambiguation; leaving unmatched",
                artist,
                title_raw or album_norm,
            )
            return (None, False)
    except Exception as e:
        logging.debug("[MusicBrainz Search Groups] failed for '%s' / '%s': %s", artist, album_norm, e)
    return (None, False)


def process_ai_groups_batch(ai_groups: List[dict], max_workers: int = None) -> List[dict]:
    """
    Process multiple groups requiring AI in parallel using choose_best().
    Returns list of completed group dicts with 'best' and 'losers' set.

    This function is deliberately tolerant: when AI fails for a group, the error
    is recorded in state["scan_ai_errors"] but the scan continues for other groups.
    """
    if not ai_groups:
        return []

    from concurrent.futures import ThreadPoolExecutor, as_completed

    total = len(ai_groups)
    workers = max_workers or min(10, total)
    results: List[dict] = []

    def _process_one(group: dict) -> Optional[dict]:
        artist = group.get("artist") or ""
        editions = group.get("editions") or []
        title = ""
        if editions:
            first = editions[0]
            title = first.get("title_raw") or first.get("album_norm") or ""
        group_label = f"{artist} ‚Äì {title}" if artist or title else "unknown group"
        try:
            best = choose_best(editions, defer_ai=False)
            if not best:
                return None
            losers = [e for e in editions if e.get("album_id") != best.get("album_id")]

            # Preserve group-level evidence/flags for explainability + safe pipeline behavior.
            dupe_evidence = group.get("dupe_evidence")
            if dupe_evidence and isinstance(best, dict):
                try:
                    best["dupe_evidence"] = list(dupe_evidence)
                except Exception:
                    pass

            res = {
                "artist": artist,
                "album_id": best.get("album_id"),
                "best": best,
                "losers": losers,
                "fuzzy": group.get("fuzzy", False),
                "needs_ai": False,
            }
            for k in ("dupe_signal", "dupe_evidence", "same_folder", "no_move", "manual_review"):
                if k in group:
                    res[k] = group.get(k)
            return res
        except Exception as e:
            logging.error("[AI Batch] Error processing group for %s: %s", group_label, e)
            try:
                with lock:
                    state.setdefault("scan_ai_errors", []).append(
                        {"message": str(e), "group": group_label}
                    )
                    if len(state["scan_ai_errors"]) > 100:
                        state["scan_ai_errors"] = state["scan_ai_errors"][-80:]
            except Exception:
                pass
            return None

    processed = 0
    with ThreadPoolExecutor(max_workers=workers) as executor:
        future_to_group = {executor.submit(_process_one, g): g for g in ai_groups}
        for future in as_completed(future_to_group):
            res = future.result()
            processed += 1
            try:
                with lock:
                    state["scan_ai_batch_processed"] = processed
            except Exception:
                pass
            if res:
                results.append(res)

    return results


def ai_suggest_artist_roles(
    album_artist_name: str,
    track_credits: list[dict],
    album_title: str | None,
    release_group_id: str | None,
    use_ai: bool,
) -> Optional[dict]:
    """
    Optional AI helper to classify main vs featuring artists per track from MusicBrainz credits.

    Parameters
    ----------
    album_artist_name: canonical album artist name (from MB release-group or Plex).
    track_credits: list of {index, title, credit} where credit is a string from MB artist-credit.
    album_title: album title (for context in the prompt).
    release_group_id: MusicBrainz release-group ID (for logging / diagnostics only).
    use_ai: whether AI is enabled in settings; when False, this returns None immediately.

    Returns
    -------
    A dict like:
      {
        "main_album_artist": "Ochre",
        "featuring_by_track": {
          "3": ["Keef Baker"],
          "5": ["Global Goon"]
        }
      }
    or None when AI is disabled or an error occurs.
    """
    if not use_ai or not track_credits:
        return None

    try:
        # Build a compact but explicit prompt for the provider
        credits_lines = []
        for tc in track_credits[:40]:
            idx = tc.get("index")
            title = tc.get("title") or ""
            credit = tc.get("credit") or ""
            credits_lines.append(f"Track {idx}: {title} ‚Äî {credit}")
        credits_text = "\n".join(credits_lines)

        system_prompt = (
            "You are a tagging assistant for a music library manager. "
            "Your job is to identify the main album artist versus featuring/guest artists per track. "
            "Never split one album into multiple albums: there is always exactly one main album artist."
        )
        user_prompt = (
            f"Album title: {album_title or ''}\n"
            f"Album artist (from user / Plex): {album_artist_name}\n"
            f"Release group ID (MusicBrainz): {release_group_id or ''}\n\n"
            "Here are the per-track artist credits:\n"
            f"{credits_text}\n\n"
            "Decide:\n"
            "1) The single main album artist (string).\n"
            "2) For each track index, a list of featuring/guest artists only (exclude the main album artist).\n"
            "Respond strictly as JSON with keys 'main_album_artist' (string) and 'featuring_by_track' (object mapping track index to list of strings)."
        )

        reply = call_ai_provider(
            AI_PROVIDER,
            RESOLVED_MODEL or OPENAI_MODEL,
            system_prompt,
            user_prompt,
            max_tokens=400,
        )
        if not reply:
            return None
        try:
            data = json.loads(reply)
        except Exception:
            # Try to extract JSON substring if the model wrapped it
            start = reply.find("{")
            end = reply.rfind("}")
            if start != -1 and end != -1 and end > start:
                data = json.loads(reply[start : end + 1])
            else:
                raise

        main_album_artist = str(data.get("main_album_artist") or album_artist_name or "").strip()
        featuring_by_track = data.get("featuring_by_track") or {}
        if not isinstance(featuring_by_track, dict):
            featuring_by_track = {}

        # Normalise track indices to strings and values to list[str]
        norm_map: dict[str, list[str]] = {}
        for k, v in featuring_by_track.items():
            key = str(k)
            if isinstance(v, str):
                vals = [v]
            elif isinstance(v, list):
                vals = [str(x) for x in v if str(x).strip()]
            else:
                continue
            vals = [s.strip() for s in vals if s.strip()]
            if vals:
                norm_map[key] = vals

        return {
            "main_album_artist": main_album_artist or album_artist_name,
            "featuring_by_track": norm_map,
        }
    except Exception as e:
        # Log to scan_ai_errors but never fail the scan
        msg = f"ai_suggest_artist_roles failed for RG {release_group_id or '?'}: {e}"
        logging.debug(msg)
        try:
            state.setdefault("scan_ai_errors", []).append({"message": msg, "group": f"artist_roles:{release_group_id or ''}"})
            if len(state["scan_ai_errors"]) > 100:
                state["scan_ai_errors"] = state["scan_ai_errors"][-80:]
        except Exception:
            pass
        return None

def choose_best(editions: List[dict], defer_ai: bool = False) -> dict | None:
    """Select the best edition for a duplicate group.

    Strategy:
    - Reuse stable AI cache when the exact same folder set appears again (dupe_ai_cache).
    - Prefer a conservative deterministic heuristic when the pick is obvious.
    - Otherwise call the AI provider (unless defer_ai=True), and cache the result.
    - On AI failure, fall back to the heuristic instead of dropping the group.
    """
    if not editions:
        return None

    # 0) Filter out broken albums if there are non-broken alternatives.
    non_broken = [e for e in editions if not e.get('is_broken', False)]
    broken = [e for e in editions if e.get('is_broken', False)]
    if non_broken and broken:
        try:
            log_dupes(
                "Filtering out %d broken album(s) in favor of %d non-broken album(s) for artist=%s",
                len(broken),
                len(non_broken),
                editions[0].get('artist', '?'),
            )
        except Exception:
            pass
        editions = non_broken

    artist = str((editions[0] or {}).get('artist') or '').strip()
    group_key = _dupe_group_key_from_editions(editions)

    # 1) AI cache reuse.
    cached = _dupe_ai_cache_get(artist, group_key)
    if cached and cached.get('best_folder'):
        best_folder_key = _dupe_folder_key_str(cached.get('best_folder'))
        best_cached = next(
            (e for e in editions if _dupe_folder_key_str((e or {}).get('folder')) == best_folder_key),
            None,
        )
        if best_cached is not None:
            best_cached['rationale'] = cached.get('rationale') or 'AI cache'
            best_cached['merge_list'] = cached.get('merge_list') or []
            best_cached['used_ai'] = True
            best_cached['ai_provider'] = cached.get('ai_provider') or ''
            best_cached['ai_model'] = cached.get('ai_model') or ''
            if cached.get('confidence') is not None:
                best_cached['ai_confidence'] = cached.get('confidence')
            return best_cached

    # 2) Heuristic fast path.
    h_best, h_rationale, h_merge, h_confident = _dupe_choose_best_heuristic(editions)
    if h_best is not None and (h_confident or not ai_provider_ready):
        h_best['rationale'] = h_rationale
        h_best['merge_list'] = h_merge or []
        h_best['used_ai'] = False
        h_best['ai_provider'] = ''
        h_best['ai_model'] = ''
        return h_best

    # 3) AI path for ambiguous groups.
    if ai_provider_ready:
        if defer_ai:
            return None

        template = AI_PROMPT_FILE.read_text(encoding='utf-8')
        user_msg = template + "\nCandidate editions:\n"
        for idx, e in enumerate(editions):
            track_count = len(e.get('tracks', []))
            folder_path = path_for_fs_access(Path(e['folder'])) if e.get('folder') else None
            size_mb = (safe_folder_size(folder_path) // (1024 * 1024)) if folder_path else 0
            user_msg += (
                f"{idx}: fmt_score={e.get('fmt_score', 0)}, bitdepth={e.get('bd', 0)}, "
                f"tracks={track_count}, track_count={track_count}, size_mb={size_mb}, files={e.get('file_count', 0)}, "
                f"bitrate={e.get('br', 0)}, samplerate={e.get('sr', 0)}, duration={e.get('dur', 0)}"
            )
            meta = e.get('meta') or {}
            year = meta.get('date') or meta.get('originaldate') or ''
            mbid = meta.get('musicbrainz_albumid', '')
            user_msg += f" year={year} mbid={mbid}\n"

        rg_info = None
        for e in editions:
            if e.get('rg_info'):
                rg_info = e['rg_info']
                break
        if rg_info:
            user_msg += (
                'Release group info: '
                f"primary_type={rg_info.get('primary_type', '')}, "
                f"formats={rg_info.get('format_summary', '')}\n"
            )

        system_msg = (
            'You are an expert digital-music librarian.\n'
            'OUTPUT RULES (must follow exactly):\n'
            "- Return ONE single line only.\n"
            "- The line must contain EXACTLY two '|' characters.\n"
            "- Format: <index>|<brief rationale>|<comma-separated extra tracks>\n"
            "- Index is 0-based: 0 = first edition, 1 = second, etc. (candidates are listed as 0:, 1:, ...).\n"
            "- If there are no extra tracks, still include the final pipe but leave it empty.\n"
            "- Do not add any other text.\n"
            'Optionally end the rationale with (confidence: N) where N is 0-100.\n'
        )

        mod = sys.modules[__name__]
        model_to_use = getattr(mod, 'RESOLVED_MODEL', None) or OPENAI_MODEL or 'gpt-4o-mini'
        model_display = getattr(mod, 'RESOLVED_MODEL', None) or OPENAI_MODEL or model_to_use

        ai_confidence = None
        try:
            txt = call_ai_provider(AI_PROVIDER, model_to_use, system_msg, user_msg, max_tokens=256)
            lines = [l.strip() for l in (txt or '').replace('```', '').splitlines() if l.strip()]
            txt = lines[0] if lines else (txt or '')
            txt = re.sub(r'^(answer|r√©ponse)\s*:\s*', '', txt, flags=re.IGNORECASE).strip()

            m = re.match(r'^(\d+)\s*\|\s*(.*?)\s*\|\s*(.*)$', txt)
            if m:
                idx = int(m.group(1))
                if idx == len(editions) and len(editions) > 1:
                    idx -= 1
                idx = max(0, min(len(editions) - 1, idx))
                rationale = m.group(2).strip()
                extras_raw = m.group(3).strip()
                merge_list = [t.strip() for t in extras_raw.split(',') if t.strip()]
                m_conf = re.search(r'\s*\(confidence:\s*(\d+)\)\s*$', rationale, re.I)
                if m_conf:
                    ai_confidence = min(100, max(0, int(m_conf.group(1))))
                    rationale = rationale[: m_conf.start()].strip()
            else:
                m_num = re.search(r'(\d+)', txt)
                if not m_num:
                    raise ValueError(f'Invalid AI response format (no index found) ‚Äì got: {txt!r}')
                idx = int(m_num.group(1))
                if idx == len(editions) and len(editions) > 1:
                    idx -= 1
                idx = max(0, min(len(editions) - 1, idx))
                rationale = 'minimal AI reply; fallback parser used'
                merge_list = []

            best = editions[idx]
            best.update({
                'rationale': rationale,
                'merge_list': merge_list,
                'used_ai': True,
                'ai_provider': (AI_PROVIDER or ''),
                'ai_model': (model_display or ''),
            })
            if ai_confidence is not None:
                best['ai_confidence'] = ai_confidence

            _dupe_ai_cache_put(
                artist=artist,
                group_key=group_key,
                best_folder=_dupe_folder_key_str(best.get('folder')),
                rationale=rationale,
                merge_list=merge_list,
                ai_provider=(AI_PROVIDER or ''),
                ai_model=(model_display or ''),
                confidence=ai_confidence,
            )
            return best
        except Exception as e:
            group_label = f"{artist} ‚Äì {editions[0].get('title_raw', editions[0].get('album_norm', ''))}" if editions else (artist or '')
            with lock:
                state.setdefault('scan_ai_errors', []).append({'message': str(e), 'group': group_label})
                if len(state['scan_ai_errors']) > 100:
                    state['scan_ai_errors'] = state['scan_ai_errors'][-80:]
            logging.warning('AI failed for dupe group (%s): %s; falling back to heuristic', group_label, e)

    # 4) Final fallback: heuristic even if not confident.
    if h_best is not None:
        h_best['rationale'] = h_rationale
        h_best['merge_list'] = h_merge or []
        h_best['used_ai'] = False
        h_best['ai_provider'] = ''
        h_best['ai_model'] = ''
        return h_best

    return None

def scan_artist_duplicates(args):
    """
    ThreadPool worker: scan one artist for duplicate albums.
    args: (artist_id, artist_name) or (artist_id, artist_name, album_ids).
    When album_ids is provided (e.g. merged from multiple Plex artist entries with same name),
    use it so duplicates across folders (e.g. Ochre vs Ochre2) are detected.
    Returns (artist_name, list_of_groups, album_count, stats_dict).
    stats_dict contains: {"ai_used": count, "mb_used": count, "timing": {...}}
    """
    artist_id = args[0]
    artist_name = args[1]
    album_ids = args[2] if len(args) >= 3 else None
    artist_start_time = time.perf_counter()
    timing_stats = {
        "db_query_time": 0.0,
        "audio_analysis_time": 0.0,
        "mb_lookup_time": 0.0,
        "ai_processing_time": 0.0,
        "total_time": 0.0,
    }
    try:
        if scan_should_stop.is_set():
            return (artist_name, [], 0, {"ai_used": 0, "mb_used": 0, "timing": timing_stats}, [])
        while scan_is_paused.is_set() and not scan_should_stop.is_set():
            time.sleep(0.5)
        logging.info("Processing artist: %s", artist_name)
        mode = _get_library_mode()
        db_conn = None
        prebuilt_editions = None

        if mode == "files":
            # Build editions from state populated by _build_scan_plan (files backend).
            with lock:
                files_editions = state.get("files_editions_by_album_id") or {}
            if album_ids is None:
                album_ids = []
            editions_for_artist = []
            for idx, aid in enumerate(album_ids):
                fe = files_editions.get(aid)
                if not fe:
                    continue
                folder = fe.get("folder")
                if not folder:
                    continue
                with lock:
                    if artist_name in state.get("scan_active_artists", {}):
                        state["scan_active_artists"][artist_name]["albums_processed"] = idx
                        state["scan_active_artists"][artist_name]["current_album"] = {
                            "album_id": aid,
                            "album_title": fe.get("album_title") or f"Album {aid}",
                            "status": "analyzing_format",
                            "status_details": "",
                            "step_summary": "Running FFprobe‚Ä¶",
                            "step_response": "",
                        }
                fmt_score_val, br, sr, bd, audio_cache_hit = analyse_format(folder)
                tr = fe.get("tracks") or []
                meta_tags = fe.get("tags") or {}
                title_raw = _sanitize_album_title_display(fe.get("album_title") or folder.name.replace("_", " "))
                normalize_parenthetical = bool(_parse_bool(_get_config_from_db("NORMALIZE_PARENTHETICAL_FOR_DEDUPE") or "true"))
                album_norm_value = fe.get("album_norm") or norm_album_for_dedup(title_raw, normalize_parenthetical)
                plex_norm_value = album_norm_value
                # Files mode: deterministic broken detection from track indices.
                # For local files, *any* gap in a sane numbering sequence is a strong signal of missing tracks.
                is_broken = False
                expected_track_count = None
                actual_track_count = len(tr)
                missing_indices: list[int] = []
                try:
                    idxs = [int(getattr(t, "idx", 0) or 0) for t in (tr or []) if int(getattr(t, "idx", 0) or 0) > 0]
                    idxs = sorted(set(idxs))
                    if idxs and actual_track_count >= 4:
                        max_idx = max(idxs)
                        coverage = (actual_track_count / max_idx) if max_idx else 1.0
                        # Skip when numbering is obviously corrupt (prevents huge false positives).
                        if not (max_idx > max(120, actual_track_count * 3) and coverage < 0.5):
                            # Leading gap
                            if idxs[0] > 1:
                                missing_indices.extend(list(range(1, int(idxs[0]))))
                            # Internal gaps
                            for a, b in zip(idxs, idxs[1:]):
                                if int(b) - int(a) > 1:
                                    missing_indices.extend(list(range(int(a) + 1, int(b))))
                            if missing_indices:
                                is_broken = True
                                expected_track_count = max_idx
                                if len(missing_indices) > 5000:
                                    missing_indices = missing_indices[:5000]
                except Exception:
                    is_broken = False
                    expected_track_count = None
                    missing_indices = []
                editions_for_artist.append({
                    "album_id": aid,
                    "title_raw": title_raw,
                    "album_norm": album_norm_value,
                    "plex_norm": plex_norm_value,
                    "artist": artist_name,
                    "folder": folder,
                    "tracks": tr,
                    "file_count": fe.get("file_count") or len(tr),
                    "sig": signature(tr),
                    "titles": {t.title for t in tr},
                    "dur": sum(t.dur for t in tr),
                    "fmt_score": fmt_score_val,
                    "br": br,
                    "sr": sr,
                    "bd": bd,
                    "discs": len({t.disc for t in tr}) if tr else 1,
                    "meta": meta_tags,
                    "invalid": False,
                    "title_source": "tag:album",
                    "plex_title": title_raw,
                    "audio_cache_hit": audio_cache_hit,
                    "ordered_paths": fe.get("ordered_paths") or [],
                    "fingerprint": fe.get("fingerprint"),
                    "skip_heavy_processing": bool(fe.get("skip_heavy_processing")),
                    "has_cover": bool(fe.get("has_cover")),
                    "has_artist_image": bool(fe.get("has_artist_image")),
                    "missing_required_tags": list(fe.get("missing_required_tags") or []),
                    "has_mbid": bool(fe.get("has_mbid")),
                    "musicbrainz_id": (fe.get("musicbrainz_id") or "").strip(),
                    "is_broken": bool(is_broken),
                    "expected_track_count": expected_track_count,
                    "actual_track_count": int(actual_track_count),
                    "missing_indices": missing_indices,
                    # Provider identity (used by Dupe Detection v2 hard grouping)
                    "discogs_release_id": str(fe.get("discogs_release_id") or meta_tags.get("discogs_release_id") or "").strip(),
                    "lastfm_album_mbid": str(fe.get("lastfm_album_mbid") or meta_tags.get("lastfm_album_mbid") or "").strip(),
                    "bandcamp_album_url": str(fe.get("bandcamp_album_url") or meta_tags.get("bandcamp_album_url") or "").strip(),
                    "metadata_source": str(fe.get("metadata_source") or fe.get("identity_provider") or meta_tags.get("primary_metadata_source") or "").strip(),
                })
            prebuilt_editions = editions_for_artist
            # Changed-only scans only build editions for changed folders. For dupe detection we still
            # need to compare new/changed albums against the existing local catalog, otherwise we
            # miss dupes introduced in later runs. We pull lightweight context editions from the
            # published cache in state.db (no heavy provider work).
            try:
                with lock:
                    scan_type_hint = str(state.get("scan_type") or "full").strip().lower()
            except Exception:
                scan_type_hint = "full"
            if scan_type_hint == "changed_only" and editions_for_artist:
                try:
                    existing_folders: set[str] = set()
                    for e in editions_for_artist:
                        try:
                            existing_folders.add(str(Path(e.get("folder")).resolve()))
                        except Exception:
                            existing_folders.add(str(e.get("folder") or ""))
                    _ctx_artists, ctx_albums, _ctx_count = _load_files_library_published_payload_for_artist(artist_name)
                    # Allocate stable per-run ids for context editions (avoid collisions with scan-local ids)
                    next_ctx_id = (max([int(x) for x in album_ids if x is not None] or [0]) + 1) if album_ids else (max([int(e.get("album_id") or 0) for e in editions_for_artist] or [0]) + 1)
                    added = 0
                    for alb in (ctx_albums or []):
                        folder_path = str(alb.get("folder_path") or "").strip()
                        if not folder_path:
                            continue
                        try:
                            folder_obj = Path(folder_path)
                        except Exception:
                            continue
                        try:
                            folder_key = str(folder_obj.resolve())
                        except Exception:
                            folder_key = str(folder_obj)
                        if not folder_key or folder_key in existing_folders:
                            continue
                        # Convert published track dicts into Track tuples expected by dupe logic.
                        tracks_in: list = list(alb.get("tracks") or [])
                        tracks: list[Track] = []
                        br_guess = 0
                        sr_guess = 0
                        bd_guess = 0
                        for i, t in enumerate(tracks_in):
                            if not isinstance(t, dict):
                                continue
                            title = (t.get("title") or "").strip() or f"Track {i + 1}"
                            disc_num = _parse_int_loose(t.get("disc_num") or t.get("disc") or 1, 1) or 1
                            track_num = _parse_int_loose(t.get("track_num") or t.get("idx") or t.get("track") or (i + 1), i + 1) or (i + 1)
                            dur_sec = _parse_int_loose(t.get("duration_sec") or 0, 0) or 0
                            tracks.append(Track(title=title, idx=track_num, disc=disc_num, dur=int(dur_sec) * 1000))
                            br_guess = max(br_guess, int(_parse_int_loose(t.get("bitrate") or 0, 0) or 0))
                            sr_guess = max(sr_guess, int(_parse_int_loose(t.get("sample_rate") or 0, 0) or 0))
                            bd_guess = max(bd_guess, int(_parse_int_loose(t.get("bit_depth") or 0, 0) or 0))

                        meta_tags = {}
                        try:
                            meta_tags = json.loads(alb.get("primary_tags_json") or "{}") if alb.get("primary_tags_json") else {}
                            if not isinstance(meta_tags, dict):
                                meta_tags = {}
                        except Exception:
                            meta_tags = {}
                        try:
                            missing_required = json.loads(alb.get("missing_required_tags_json") or "[]") if alb.get("missing_required_tags_json") else []
                            if not isinstance(missing_required, list):
                                missing_required = []
                        except Exception:
                            missing_required = []

                        normalize_parenthetical = bool(_parse_bool(_get_config_from_db("NORMALIZE_PARENTHETICAL_FOR_DEDUPE") or "true"))
                        title_raw = (alb.get("title") or "").strip() or folder_obj.name.replace("_", " ").strip() or "Unknown Album"
                        album_norm_value = norm_album_for_dedup(title_raw, normalize_parenthetical)
                        fmt_txt = (alb.get("format") or "").strip().upper()
                        fmt_score_val = score_format(fmt_txt.lower()) if fmt_txt else 0

                        editions_for_artist.append(
                            {
                                "album_id": next_ctx_id,
                                "title_raw": title_raw,
                                "album_norm": album_norm_value,
                                "plex_norm": album_norm_value,
                                "artist": artist_name,
                                "folder": folder_obj,
                                "tracks": tracks,
                                "file_count": int(alb.get("track_count") or len(tracks) or 0),
                                "sig": signature(tracks),
                                "titles": {t.title for t in tracks},
                                "dur": sum(t.dur for t in tracks),
                                "fmt_score": fmt_score_val,
                                "br": br_guess,
                                "sr": sr_guess,
                                "bd": bd_guess,
                                "discs": len({t.disc for t in tracks}) if tracks else 1,
                                "meta": meta_tags,
                                "invalid": False,
                                "title_source": "published_cache",
                                "plex_title": title_raw,
                                "audio_cache_hit": True,
                                "ordered_paths": [Path(str(t.get("file_path"))) for t in tracks_in if isinstance(t, dict) and t.get("file_path")] if tracks_in else [],
                                "fingerprint": (alb.get("fingerprint") or "").strip() if isinstance(alb.get("fingerprint"), str) else alb.get("fingerprint"),
                                "skip_heavy_processing": True,
                                "has_cover": bool(alb.get("has_cover")),
                                "has_artist_image": bool(alb.get("has_artist_image")),
                                "missing_required_tags": missing_required,
                                "has_mbid": bool(alb.get("mb_identified")),
                                "musicbrainz_id": str(alb.get("musicbrainz_release_group_id") or "").strip(),
                                "discogs_release_id": str(alb.get("discogs_release_id") or "").strip(),
                                "lastfm_album_mbid": str(alb.get("lastfm_album_mbid") or "").strip(),
                                "bandcamp_album_url": str(alb.get("bandcamp_album_url") or "").strip(),
                                "metadata_source": str(alb.get("metadata_source") or "").strip(),
                                # Marker so scan stats can exclude this if needed.
                                "context_only": True,
                            }
                        )
                        existing_folders.add(folder_key)
                        next_ctx_id += 1
                        added += 1
                        if added >= 220:
                            break
                    if added:
                        prebuilt_editions = editions_for_artist
                        logging.debug("Files changed-only: added %d context edition(s) from published cache for artist %s", added, artist_name)
                except Exception:
                    logging.debug("Files changed-only: failed to add published context editions for artist %s", artist_name, exc_info=True)
        else:
            db_conn = plex_connect()

        if album_ids is None and db_conn is not None:
            logging.debug("[Artist %s (ID %s)] Fetching album IDs from Plex DB", artist_name, artist_id)
            db_query_start = time.perf_counter()
            placeholders = ",".join("?" for _ in SECTION_IDS)
            if CROSS_LIBRARY_DEDUPE:
                section_filter = ""
                section_args = []
            else:
                section_filter = f"AND alb.library_section_id IN ({placeholders})"
                section_args = list(SECTION_IDS)

            cursor = db_conn.execute(
                f"""
                SELECT alb.id
                FROM metadata_items alb
                JOIN metadata_items tr  ON tr.parent_id      = alb.id
                JOIN media_items      mi ON mi.metadata_item_id = tr.id
                JOIN media_parts      mp ON mp.media_item_id = mi.id
                WHERE alb.metadata_type = 9
                  AND alb.parent_id = ?
                  {section_filter}
                GROUP BY alb.id
                """,
                (artist_id, *section_args)
            )
            album_ids = [row[0] for row in cursor.fetchall()]
            timing_stats["db_query_time"] = time.perf_counter() - db_query_start
        logging.debug("[Artist %s (ID %s)] Album list for scan: %d albums", artist_name, artist_id, len(album_ids))

        # Update total_albums in active tracking
        with lock:
            if artist_name in state.get("scan_active_artists", {}):
                state["scan_active_artists"][artist_name]["total_albums"] = len(album_ids)

        groups = []
        stats = {"ai_used": 0, "mb_used": 0, "timing": {}}
        all_editions_for_stats = []
        if album_ids or prebuilt_editions:
            groups, stats, all_editions_for_stats = scan_duplicates(
                db_conn, artist_name, album_ids or [], prebuilt_editions=prebuilt_editions
            )
            # Merge timing stats
            if "timing" in stats:
                timing_stats.update(stats["timing"])
        if db_conn is not None:
            db_conn.close()
        
        timing_stats["total_time"] = time.perf_counter() - artist_start_time
        stats["timing"] = timing_stats

        logging.debug(
            "scan_artist_duplicates(): done Artist %s (ID %s) ‚Äì %d groups, %d albums, AI=%d, MB=%d, "
            "timing: total=%.2fs, db=%.2fs, audio=%.2fs, mb=%.2fs, ai=%.2fs",
            artist_name, artist_id, len(groups), len(album_ids), 
            stats.get("ai_used", 0), stats.get("mb_used", 0),
            timing_stats["total_time"], timing_stats["db_query_time"],
            timing_stats["audio_analysis_time"], timing_stats["mb_lookup_time"],
            timing_stats["ai_processing_time"]
        )
        return (artist_name, groups, len(album_ids), stats, all_editions_for_stats)
    except Exception as e:
        logging.error("Unexpected error scanning artist %s: %s", artist_name, e, exc_info=True)
        # On error, return no groups and zero albums so scan can continue
        timing_stats["total_time"] = time.perf_counter() - artist_start_time
        return (artist_name, [], 0, {"ai_used": 0, "mb_used": 0, "timing": timing_stats}, [])


def scan_duplicates(
    db_conn, artist: str, album_ids: List[int], *, prebuilt_editions: list | None = None
) -> tuple[List[dict], dict, list]:
    global no_file_streak_global, popup_displayed, gui
    scan_start_time = time.perf_counter()
    logging.debug("[Artist %s] Starting duplicate scan for album IDs: %s", artist, album_ids)
    logging.debug("Verbose SKIP_FOLDERS: %s", SKIP_FOLDERS)
    skip_count = 0
    editions = []
    total_albums = len(album_ids)
    processed_albums = 0
    PROGRESS_STATE["total"] = total_albums
    # Track folders and all album_ids pointing to each (for same-folder duplicate detection)
    seen_folders: dict[str, list[int]] = {}  # folder_path_resolved -> [album_id, ...]
    # Performance timing
    audio_analysis_time = 0.0
    mb_lookup_time = 0.0
    ai_processing_time = 0.0

    if prebuilt_editions is not None:
        # Files backend: use editions built by scan_artist_duplicates from state["files_editions_by_album_id"].
        editions = prebuilt_editions
        for e in editions:
            folder = e.get("folder")
            if folder is not None:
                try:
                    folder_str_resolved = str(Path(folder).resolve())
                    seen_folders.setdefault(folder_str_resolved, []).append(e["album_id"])
                except (OSError, RuntimeError):
                    seen_folders.setdefault(str(folder), []).append(e["album_id"])
        # Continue to MB enrichment and grouping below (skip the Plex for-loop).
    else:
        for aid in album_ids:
            processed_albums += 1
            PROGRESS_STATE["current"] = processed_albums
            # Periodic progress update every 100 albums
            if processed_albums % 100 == 0:
                logging.info("[Artist %s] processed %d/%d albums (skipped %d so far)", artist, processed_albums, total_albums, skip_count)
            try:
                if scan_should_stop.is_set():
                    break
                while scan_is_paused.is_set() and not scan_should_stop.is_set():
                    time.sleep(0.5)
                
                # Update current album tracking and albums_processed so UI shows progress during long artist scan
                with lock:
                    if artist in state.get("scan_active_artists", {}):
                        state["scan_active_artists"][artist]["albums_processed"] = processed_albums
                        album_title_str = album_title(db_conn, aid) or f"Album {aid}"
                        state["scan_active_artists"][artist]["current_album"] = {
                            "album_id": aid,
                            "album_title": album_title_str,
                            "status": "fetching_tracks",
                            "status_details": "",
                            "step_summary": "",
                            "step_response": ""
                        }
                
                tr = get_tracks(db_conn, aid)
                if not tr:
                    continue
                
                # Update: analyzing format
                with lock:
                    if artist in state.get("scan_active_artists", {}):
                        state["scan_active_artists"][artist]["current_album"]["status"] = "analyzing_format"
                        state["scan_active_artists"][artist]["current_album"]["status_details"] = "analyzing audio format"
                        state["scan_active_artists"][artist]["current_album"]["step_summary"] = "Running FFprobe‚Ä¶"
                
                folder = first_part_path(db_conn, aid)
                if not folder:
                    continue
                # Skip albums in configured skip folders (path-aware)
                logging.debug("Checking album %s at folder %s against skip prefixes %s", aid, folder, SKIP_FOLDERS)
                folder_resolved = Path(folder).resolve()
                folder_str_resolved = str(folder_resolved)
                
                # Track same-folder duplicates: multiple Plex album entries pointing to the same folder
                if folder_str_resolved in seen_folders:
                    seen_folders[folder_str_resolved].append(aid)
                    logging.warning(
                        "[Artist %s] Album ID %d points to the same folder as album ID(s) %s: %s. "
                        "Same-folder duplicate (Plex metadata). Will report as duplicate group.",
                        artist, aid, seen_folders[folder_str_resolved], folder_str_resolved
                    )
                    skip_count += 1
                    continue

                # First time we see this folder: record and process
                seen_folders[folder_str_resolved] = [aid]
                
                if SKIP_FOLDERS and any(folder_resolved.is_relative_to(Path(s).resolve()) for s in SKIP_FOLDERS):
                    skip_count += 1
                    logging.info("Skipping album %s since folder %s matches skip prefixes %s", aid, folder_resolved, SKIP_FOLDERS)
                    continue
                # count audio files once ‚Äì we re‚Äëuse it later
                file_count = sum(1 for f in folder.rglob("*") if AUDIO_RE.search(f.name))

                # consider edition invalid when technical data are all zero OR no files found

                # ‚îÄ‚îÄ‚îÄ audio‚Äëformat inspection ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
                audio_start = time.perf_counter()
                fmt_score, br, sr, bd, audio_cache_hit = analyse_format(folder)
                audio_analysis_time += time.perf_counter() - audio_start

                # --- metadata tags (first track only) -----------------------------
                first_audio = next((p for p in folder.rglob("*") if AUDIO_RE.search(p.name)), None)
                meta_tags = extract_tags(first_audio) if first_audio else {}

                # Mark as invalid if file_count == 0 OR all tech data are zero
                is_invalid = (file_count == 0) or (br == 0 and sr == 0 and bd == 0)

                # --- Quick retry before purging to avoid false negatives -------------
                if is_invalid:
                    time.sleep(0.5)
                    fmt_score_retry, br_retry, sr_retry, bd_retry, audio_cache_hit_retry = analyse_format(folder)
                    file_count_retry = file_count or sum(1 for f in folder.rglob("*") if AUDIO_RE.search(f.name))
                    if (file_count_retry == 0) or (br_retry == 0 and sr_retry == 0 and bd_retry == 0):
                        _purge_invalid_edition({
                            "folder":   folder,
                            "artist":   artist,
                            "title_raw": album_title(db_conn, aid),
                            "album_id": aid
                        })
                        continue            # do NOT add to the editions list
                    else:
                        fmt_score, br, sr, bd, audio_cache_hit = fmt_score_retry, br_retry, sr_retry, bd_retry, audio_cache_hit_retry
                        is_invalid = False

                plex_title = album_title(db_conn, aid)
                title_raw, title_source = derive_album_title(plex_title, meta_tags, folder, aid)
                normalize_parenthetical = bool(_parse_bool(_get_config_from_db("NORMALIZE_PARENTHETICAL_FOR_DEDUPE") or "true"))
                album_norm_value = norm_album_for_dedup(title_raw, normalize_parenthetical)
                
                # Update: album title + FFprobe result (low-level summary for UI)
                with lock:
                    if artist in state.get("scan_active_artists", {}) and state["scan_active_artists"][artist].get("current_album", {}).get("album_id") == aid:
                        state["scan_active_artists"][artist]["current_album"]["album_title"] = title_raw or plex_title or f"Album {aid}"
                        fmt_ext = first_audio.suffix.upper().lstrip(".") if first_audio else "?"
                        br_k = (br // 1000) if br >= 1000 else br
                        state["scan_active_artists"][artist]["current_album"]["step_summary"] = (
                            f"FFprobe: {fmt_ext} ¬∑ {br_k} kbps ¬∑ {sr} Hz ¬∑ {bd}-bit"
                            + (" (cached)" if audio_cache_hit else "")
                        )
                        state["scan_active_artists"][artist]["current_album"]["step_response"] = (
                            f"FFprobe: format {fmt_ext}, {br_k} kbps, {sr} Hz, {bd}-bit"
                            + (" (from cache)" if audio_cache_hit else "")
                        )
                        state["scan_format_done_count"] = state.get("scan_format_done_count", 0) + 1
                        state["scan_step_progress"] = state.get("scan_step_progress", 0) + 1

                # Plex-normalized title: same key as get_duplicate_groups_from_library so scan groups match library
                plex_norm_value = norm_album_for_dedup(plex_title or "", normalize_parenthetical) if plex_title else album_norm_value
                editions.append({
                    'album_id':  aid,
                    'title_raw': title_raw,
                    'album_norm': album_norm_value,
                    'plex_norm': plex_norm_value,  # For grouping: align with library (norm_album(plex title))
                    'artist':    artist,
                    'folder':    folder,
                    'tracks':    tr,
                    'file_count': file_count,
                    'sig':       signature(tr),
                    'titles':    {t.title for t in tr},
                    'dur':       sum(t.dur for t in tr),
                    'fmt_score': fmt_score,
                    'br':        br,
                    'sr':        sr,
                    'bd':        bd,
                    'discs':     len({t.disc for t in tr}),
                    'meta':      meta_tags,
                    'invalid':   False,
                    'title_source': title_source,
                    'plex_title': plex_title or "",
                    'audio_cache_hit': audio_cache_hit  # Track if this album used cache
                })

                # Store AcousticID fingerprints in cache during scan when USE_ACOUSTID is on
                if getattr(sys.modules[__name__], "USE_ACOUSTID", False):
                    try:
                        _store_acoustid_fingerprints_for_folder(path_for_fs_access(folder))
                    except Exception:
                        pass

                # Mark album as done if it's not part of any duplicate group (single edition)
                # This will be updated later if it becomes part of a group
                with lock:
                    if artist in state.get("scan_active_artists", {}) and state["scan_active_artists"][artist].get("current_album", {}).get("album_id") == aid:
                        # Don't mark as done yet - wait to see if it's part of a group
                        pass
            except Exception as e:
                logging.error("Error processing album %s for artist %s: %s", aid, artist, e, exc_info=True)
                # Mark as done even on error
                with lock:
                    if artist in state.get("scan_active_artists", {}) and state["scan_active_artists"][artist].get("current_album", {}).get("album_id") == aid:
                        state["scan_active_artists"][artist]["current_album"]["status"] = "done"
                        state["scan_active_artists"][artist]["current_album"]["status_details"] = ""
                        state["scan_active_artists"][artist]["current_album"]["step_summary"] = ""
                        state["scan_active_artists"][artist]["current_album"]["step_response"] = ""
                continue

    logging.debug("[Artist %s] Computed stats for %d valid editions: %s", artist, len(editions), [e['album_id'] for e in editions])

    if not USE_MUSICBRAINZ:
        logging.debug("[Artist %s] Skipping MusicBrainz enrichment (USE_MUSICBRAINZ=False).", artist)
    else:
        # ‚îÄ‚îÄ‚îÄ MusicBrainz enrichment & Box Set handling ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        mb_start = time.perf_counter()
        # Per-album MB lookup only (no batch fetch). Batch fetch caused queue pile-up and 300s
        # timeouts when multiple scan threads each submitted a long-running fetch_rg_* job.
        artist_mb_rg_index = None
        if MB_DISABLE_CACHE:
            log_mb(
                "Artist %s: FULL MusicBrainz rescan ‚Äì ignoring existing MBID tags and album lookup cache for this run",
                artist,
            )
        # Enrich using any available MusicBrainz ID tags (in priority order)
        id_tags = [
            'musicbrainz_releasegroupid',
            'musicbrainz_releaseid',
            'musicbrainz_originalreleaseid',
            'musicbrainz_albumid'
        ]
        for e in editions:
            # Set once per edition so all branches can use it (used before the later "album_norm = e['album_norm']")
            album_norm = e.get("album_norm") or e.get("title_raw") or ""
            tracks_edition = {t for t in (getattr(x, "title", None) for x in e.get("tracks", [])) if t}
            # Update current_album to this edition so UI shows MusicBrainz step for every album (not only the last one)
            with lock:
                if artist in state.get("scan_active_artists", {}):
                    state["scan_active_artists"][artist]["current_album"] = {
                        "album_id": e["album_id"],
                        "album_title": e.get("title_raw") or e.get("plex_title") or f"Album {e['album_id']}",
                        "status": "fetching_mb_id",
                        "status_details": "fetching MusicBrainz ID",
                        "step_summary": "Looking up release group from tags‚Ä¶",
                        "step_response": "",
                    }
            if e.get("skip_heavy_processing"):
                mbid_quick = (e.get("musicbrainz_id") or _extract_musicbrainz_id_from_meta(e.get("meta") or {}) or "").strip()
                if mbid_quick:
                    e["musicbrainz_id"] = mbid_quick
                    e["musicbrainz_type"] = "release-group"
                e["rg_info_source"] = "incremental_skip"
                e["is_broken"] = False
                with lock:
                    if artist in state.get("scan_active_artists", {}) and e["album_id"] == state["scan_active_artists"][artist].get("current_album", {}).get("album_id"):
                        state["scan_active_artists"][artist]["current_album"]["status"] = "searching_mb"
                        state["scan_active_artists"][artist]["current_album"]["status_details"] = "unchanged and complete (fast-skip)"
                        state["scan_active_artists"][artist]["current_album"]["step_summary"] = "Skipped heavy MB/provider lookup"
                        state["scan_active_artists"][artist]["current_album"]["step_response"] = "Incremental fast-skip: unchanged album already complete."
                    state["scan_mb_done_count"] = state.get("scan_mb_done_count", 0) + 1
                    state["scan_step_progress"] = state.get("scan_step_progress", 0) + 1
                continue
            skip_live_mb = getattr(sys.modules[__name__], "SKIP_MB_FOR_LIVE_ALBUMS", True) and _is_likely_live_album(
                e.get("folder"), e.get("title_raw") or e.get("plex_title")
            )
            if skip_live_mb:
                rg_info = None
                with lock:
                    if artist in state.get("scan_active_artists", {}) and e["album_id"] == state["scan_active_artists"][artist].get("current_album", {}).get("album_id"):
                        state["scan_active_artists"][artist]["current_album"]["status"] = "searching_mb"
                        state["scan_active_artists"][artist]["current_album"]["step_summary"] = "Skipped (live album; MB disabled for live albums)"
                        state["scan_active_artists"][artist]["current_album"]["step_response"] = "Skipped: live album; MusicBrainz lookup disabled. Fallback providers (Discogs/Last.fm/Bandcamp) will still be tried."
                logging.info(
                    "[Artist %s] Edition %s \"%s\": skipped MB (live album, SKIP_MB_FOR_LIVE_ALBUMS=true)",
                    artist, e.get("album_id"), e.get("title_raw") or e.get("plex_title") or album_norm,
                )
            else:
                meta = e.get('meta', {})
                rg_info = None
                mbid_found = None
                mbid_type = None
                tag_used = None

                # AcousticID first (for all albums when enabled, unless album already has MBID in tags and we skip to save API)
                existing_rgid_tag = meta.get("musicbrainz_releasegroupid")
                run_acoustid_first = (
                    USE_ACOUSTID and ACOUSTID_API_KEY
                    and (USE_ACOUSTID_WHEN_TAGGED or not existing_rgid_tag)
                )
                if not run_acoustid_first and USE_ACOUSTID and ACOUSTID_API_KEY and existing_rgid_tag:
                    logging.debug(
                        "[Artist %s] Edition %s: skip AcousticID (album has MBID in tags, USE_ACOUSTID_WHEN_TAGGED=false)",
                        artist, e.get("album_id"),
                    )
                if run_acoustid_first:
                    folder_ac = e.get("folder")
                    if folder_ac and Path(folder_ac).exists():
                        acoustid_rg_info, match_verified_by_ai = _identify_album_by_acoustic_id(
                            Path(folder_ac), artist or "Unknown", e.get("album_norm") or e.get("title_raw") or "Unknown"
                        )
                        if acoustid_rg_info:
                            e["acoustid_rg_info"] = acoustid_rg_info
                            e["acoustid_rg_id"] = acoustid_rg_info.get("id")
                            e["match_verified_by_ai"] = bool(match_verified_by_ai)
                            with lock:
                                if artist in state.get("scan_active_artists", {}) and e["album_id"] == state["scan_active_artists"][artist].get("current_album", {}).get("album_id"):
                                    state["scan_active_artists"][artist]["current_album"]["status_details"] = "AcousticID: identified (will verify with providers)"
                                    state["scan_active_artists"][artist]["current_album"]["step_summary"] = (
                                        "AcousticID: release group %s" % (e["acoustid_rg_id"] or "")
                                    )
                            logging.debug("[Artist %s] Edition %s AcousticID first: rg_id=%s", artist, e["album_id"], e.get("acoustid_rg_id"))

                # Already identified: album has release-group ID in tags ‚Äî use cache only, no API
                existing_rgid = meta.get('musicbrainz_releasegroupid')
                if existing_rgid and not MB_DISABLE_CACHE:
                    rg_info = get_cached_mb_info(existing_rgid)
                    mb_cache_hit = rg_info is not None
                    mbid_found = existing_rgid
                    mbid_type = 'release-group'
                    tag_used = 'musicbrainz_releasegroupid'
                    e['mb_cache_hit'] = mb_cache_hit
                    e['rg_info_source'] = tag_used
                    if rg_info:
                        e['rg_info'] = rg_info
                    e['musicbrainz_id'] = mbid_found
                    e['musicbrainz_type'] = mbid_type
                    with lock:
                        if artist in state.get("scan_active_artists", {}) and e['album_id'] == state["scan_active_artists"][artist].get("current_album", {}).get("album_id"):
                            cache_text = " (cached)" if mb_cache_hit else " (from tags, no API)"
                            state["scan_active_artists"][artist]["current_album"]["status"] = "searching_mb"
                            state["scan_active_artists"][artist]["current_album"]["status_details"] = f"MusicBrainz ID from tags{cache_text}"
                            rg_title = (rg_info.get("title") or "") if isinstance(rg_info, dict) else ""
                            state["scan_active_artists"][artist]["current_album"]["step_summary"] = (
                                f"MusicBrainz: release group \"{rg_title}\" (id: {mbid_found}){cache_text}"
                            )
                            state["scan_active_artists"][artist]["current_album"]["step_response"] = (
                                f"MusicBrainz: from tags. Release group \"{rg_title}\" (id: {mbid_found}){cache_text}"
                            )
                    album_title_log = e.get("title_raw") or e.get("plex_title") or album_norm
                    # AcousticID verification: if we have AcousticID result and it disagrees with tags, prefer AcousticID
                    if e.get("acoustid_rg_id") and rg_info and rg_info.get("id") != e["acoustid_rg_id"]:
                        log_mb(
                            "Album %s ‚Äì \"%s\": tags say RG %s but AcousticID says %s; using AcousticID",
                            artist, album_title_log, rg_info.get("id"), e["acoustid_rg_id"],
                        )
                        rg_info = e["acoustid_rg_info"]
                        e["rg_info"] = rg_info
                        e["musicbrainz_id"] = e["acoustid_rg_id"]
                        e["rg_info_source"] = "acoustid_verify"
                        mbid_found = e["acoustid_rg_id"]
                        with lock:
                            if artist in state.get("scan_active_artists", {}) and e["album_id"] == state["scan_active_artists"][artist].get("current_album", {}).get("album_id"):
                                state["scan_active_artists"][artist]["current_album"]["status_details"] = "AcousticID verified (overrode tags)"
                                state["scan_active_artists"][artist]["current_album"]["step_summary"] = (
                                    "AcousticID: release group %s (overrode tags)" % e["acoustid_rg_id"]
                                )

                    # High-level log per album when MBID comes directly from tags
                    if rg_info:
                        log_mb(
                            "Album %s ‚Äì \"%s\": MusicBrainz release-group %s from tags (%s)%s",
                            artist,
                            album_title_log,
                            mbid_found,
                            tag_used,
                            " (cached)" if mb_cache_hit else "",
                        )
                    logging.debug("[Artist %s] Edition %s already has MBID in tags, cache_hit=%s", artist, e['album_id'], mb_cache_hit)
                else:
                    # No release-group ID in tags (or full rescan requested): try other ID tags (release ID etc.) or search
                    if not MB_DISABLE_CACHE:
                        for tag in id_tags:
                            mbid = meta.get(tag)
                            if not mbid:
                                continue
                            try:
                                mb_cache_hit = False
                                if tag == 'musicbrainz_releasegroupid':
                                    rg_info, mb_cache_hit = fetch_mb_release_group_info(mbid)
                                    mbid_found = mbid
                                    mbid_type = 'release-group'
                                else:
                                    def _fetch_release():
                                        return musicbrainzngs.get_release_by_id(mbid, includes=["release-groups"])["release"]
                                    if MB_QUEUE_ENABLED and USE_MUSICBRAINZ:
                                        rel = get_mb_queue().submit(f"rel_{mbid}", _fetch_release)
                                    else:
                                        rel = _fetch_release()
                                    rgid = rel['release-group']['id']
                                    rg_info, mb_cache_hit = fetch_mb_release_group_info(rgid)
                                    mbid_found = rgid
                                    mbid_type = 'release-group'
                                e['mb_cache_hit'] = mb_cache_hit
                                tag_used = tag
                                with lock:
                                    if artist in state.get("scan_active_artists", {}) and e['album_id'] == state["scan_active_artists"][artist].get("current_album", {}).get("album_id"):
                                        cache_text = " (cached)" if mb_cache_hit else ""
                                        state["scan_active_artists"][artist]["current_album"]["status"] = "searching_mb"
                                        state["scan_active_artists"][artist]["current_album"]["status_details"] = f"MusicBrainz ID fetched{cache_text}"
                                        rg_title = (rg_info.get("title") or "") if isinstance(rg_info, dict) else ""
                                        state["scan_active_artists"][artist]["current_album"]["step_summary"] = (
                                            f"MusicBrainz: release group \"{rg_title}\" (id: {mbid_found}){cache_text}"
                                        )
                                        state["scan_active_artists"][artist]["current_album"]["step_response"] = (
                                            f"MusicBrainz: from tags ({tag}). Release group \"{rg_title}\" (id: {mbid_found}){cache_text}"
                                        )
                                album_title_log = e.get("title_raw") or e.get("plex_title") or album_norm
                                if rg_info:
                                    log_mb(
                                        "Album %s ‚Äì \"%s\": MusicBrainz release-group %s fetched via tag %s%s",
                                        artist,
                                        album_title_log,
                                        mbid_found,
                                        tag,
                                        " (cached)" if mb_cache_hit else "",
                                    )
                                logging.debug("[Artist %s] Edition %s RG info (via %s %s): %s", artist, e['album_id'], tag, mbid, rg_info)
                                break
                            except Exception as exc:
                                logging.debug("[Artist %s] MusicBrainz lookup failed for %s (%s): %s", artist, tag, mbid, exc)
                        if rg_info:
                            e['rg_info_source'] = tag_used
                            e['rg_info'] = rg_info
                            if mbid_found:
                                e['musicbrainz_id'] = mbid_found
                                e['musicbrainz_type'] = mbid_type

                # Fallback: search by metadata if no ID tag yielded results
                album_norm = e['album_norm']
                tracks = {t.title for t in e['tracks']}
                artist_norm_key = artist.lower().strip()
                if not rg_info:
                    # Check cache for "artist+album -> MBID".
                    cached_mbid = cached_rg_info = None
                    if not MB_DISABLE_CACHE:
                        cached_mbid, cached_rg_info = get_cached_mb_album_lookup(artist_norm_key, album_norm)
                    cached_not_found = bool(cached_mbid == "")
                    if cached_mbid and cached_mbid != "":
                        # Cached: found MBID for this artist+album ‚Äî use it, no API search
                        rg_info = cached_rg_info or get_cached_mb_info(cached_mbid)
                        if rg_info:
                            e['rg_info_source'] = 'fallback'
                            e['rg_info'] = rg_info
                            e['musicbrainz_id'] = cached_mbid
                            e['musicbrainz_type'] = 'release-group'
                            e['mb_cache_hit'] = True
                            with lock:
                                if artist in state.get("scan_active_artists", {}) and e['album_id'] == state["scan_active_artists"][artist].get("current_album", {}).get("album_id"):
                                    rg_title = (rg_info.get("title") or "") if isinstance(rg_info, dict) else ""
                                    state["scan_active_artists"][artist]["current_album"]["status_details"] = "MusicBrainz found (cached)"
                                    state["scan_active_artists"][artist]["current_album"]["step_summary"] = (
                                        f"MusicBrainz: found \"{rg_title}\" (id: {cached_mbid}) (cached)"
                                    )
                                    state["scan_active_artists"][artist]["current_album"]["step_response"] = (
                                        f"MusicBrainz: found release group \"{rg_title}\" (id: {cached_mbid}) (cached)"
                                    )
                            album_title_log = e.get("title_raw") or e.get("plex_title") or album_norm
                            log_mb(
                                "Album %s ‚Äì \"%s\": MusicBrainz release-group %s from album lookup cache",
                                artist,
                                album_title_log,
                                cached_mbid,
                            )
                            logging.debug("[Artist %s] Edition %s MBID from album lookup cache: %s", artist, e['album_id'], cached_mbid)
                    elif cached_not_found and not MB_RETRY_NOT_FOUND:
                        e["mb_cache_hit"] = True
                        e["rg_info_source"] = "mb_not_found_cached"
                        with lock:
                            if artist in state.get("scan_active_artists", {}) and e['album_id'] == state["scan_active_artists"][artist].get("current_album", {}).get("album_id"):
                                state["scan_active_artists"][artist]["current_album"]["status_details"] = "MusicBrainz skipped (cached not found)"
                                state["scan_active_artists"][artist]["current_album"]["step_summary"] = (
                                    "MusicBrainz: cached not-found (provider fallback first)"
                                )
                                state["scan_active_artists"][artist]["current_album"]["step_response"] = (
                                    "MusicBrainz: cached not-found, skipping re-query and using provider fallback."
                                )
                        album_title_log = e.get("title_raw") or e.get("plex_title") or album_norm
                        log_mb(
                            "Album %s ‚Äì \"%s\": skipping MusicBrainz re-query (cached not-found, MB_RETRY_NOT_FOUND=false)",
                            artist,
                            album_title_log,
                        )
                    if not rg_info and not (cached_not_found and not MB_RETRY_NOT_FOUND):
                        # Not in cache (or cache intentionally bypassed): try pre-fetched artist index first, then search
                        with lock:
                            if artist in state.get("scan_active_artists", {}) and e['album_id'] == state["scan_active_artists"][artist].get("current_album", {}).get("album_id"):
                                state["scan_active_artists"][artist]["current_album"]["status"] = "searching_mb"
                                state["scan_active_artists"][artist]["current_album"]["status_details"] = "searching MusicBrainz"
                                state["scan_active_artists"][artist]["current_album"]["step_summary"] = "Searching by artist + album name‚Ä¶"
                                state["scan_active_artists"][artist]["current_album"]["step_response"] = "MusicBrainz: querying by artist + album name‚Ä¶"
                        title_raw_mb = e.get("title_raw") or e.get("plex_title") or album_norm
                        album_folder_arg = e.get("folder")
                        rg_info = None
                        match_verified_by_ai = False
                        if artist_mb_rg_index is not None:
                            candidates = _match_album_norm_to_mb_index(album_norm, artist_mb_rg_index)
                            if candidates:
                                letters = "ABCDEFGHIJKLMNOPQRSTUVWXYZ"
                                lines = [
                                    "[MusicBrainz] %s ‚Äì %r: %d candidate(s) from artist index"
                                    % (artist, title_raw_mb or album_norm or "?", len(candidates)),
                                    *["  %s) %s (id=%s)" % (letters[i], (rg.get("title") or "?"), rg.get("id") or "?") for i, rg in enumerate(candidates[:20])],
                                ]
                                if len(candidates) > 20:
                                    lines.append("  ... and %d more" % (len(candidates) - 20))
                                logging.info("\n".join(lines))
                                chosen_rg_id = None
                                if len(candidates) == 1:
                                    chosen_rg_id = candidates[0].get("id")
                                elif len(candidates) > 1 and getattr(sys.modules[__name__], "ai_provider_ready", False):
                                    letters = "ABCDEFGHIJKLMNOPQRSTUVWXYZ"
                                    choices = [f"{letters[i]}) {rg.get('title', '?')} (id={rg.get('id', '')})" for i, rg in enumerate(candidates[:20])]
                                    track_list_str = ", ".join((list(tracks)[:30] if tracks else [])) if tracks else "(none)"
                                    if tracks and len(tracks) > 30:
                                        track_list_str += ", ..."
                                    prompt = (
                                        f"Our album: artist={artist!r}, title_raw={title_raw_mb or album_norm!r}, normalized={album_norm!r}. "
                                        f"Our track titles: [{track_list_str}].\n\n"
                                        "MusicBrainz candidates (id + title only):\n" + "\n".join(choices) + "\n\n"
                                        "Which candidate matches our album? Reply with only the letter (A, B, ...) or the MBID (UUID) or NONE if no match. "
                                        "Optionally end with (confidence: N) where N is 0-100."
                                    )
                                    system_msg = "You reply with a single letter (A, B, C, ...) or an MBID (UUID) or the word NONE. Optionally end with (confidence: N). No other explanation."
                                    try:
                                        provider = getattr(sys.modules[__name__], "AI_PROVIDER", "openai")
                                        model = getattr(sys.modules[__name__], "RESOLVED_MODEL", None) or getattr(sys.modules[__name__], "OPENAI_MODEL", "gpt-4o-mini")
                                        reply = call_ai_provider(provider, model, system_msg, prompt, max_tokens=40)
                                        reply_clean, ai_confidence = parse_ai_confidence((reply or "").strip())
                                        if ai_confidence is not None:
                                            logging.info("[Artist %s] MusicBrainz index AI choice confidence: %d", artist, ai_confidence)
                                        reply = reply_clean.upper()
                                        if reply and reply != "NONE":
                                            letter = reply[:1]
                                            idx = letters.find(letter)
                                            if 0 <= idx < len(candidates):
                                                chosen_rg_id = candidates[idx].get("id")
                                            if not chosen_rg_id:
                                                mbid_match = re.search(r"[0-9A-Fa-f]{8}-[0-9A-Fa-f]{4}-[0-9A-Fa-f]{4}-[0-9A-Fa-f]{4}-[0-9A-Fa-f]{12}", reply)
                                                if mbid_match:
                                                    chosen_rg_id = mbid_match.group(0)
                                        if chosen_rg_id:
                                            match_verified_by_ai = True
                                    except Exception:
                                        chosen_rg_id = candidates[0].get("id") if candidates else None
                                else:
                                    chosen_rg_id = candidates[0].get("id") if candidates else None
                                if chosen_rg_id:
                                    try:
                                        rg_info, _ = fetch_mb_release_group_info(chosen_rg_id)
                                    except Exception as _err:
                                        logging.debug("[Artist %s] fetch_mb_release_group_info failed for %s (from index): %s", artist, chosen_rg_id, _err)
                                        rg_info = None
                        if not rg_info:
                            # Fast-path: if we already have a cached strict Bandcamp identity with perfect tracklist,
                            # skip expensive MB search for this album and rely on provider arbitration below.
                            if bool(getattr(sys.modules[__name__], "MB_FAST_FALLBACK_MODE", True)) and not SCAN_DISABLE_CACHE:
                                try:
                                    bandcamp_status, bandcamp_cached = get_cached_provider_album_lookup(
                                        "bandcamp",
                                        artist,
                                        title_raw_mb or album_norm,
                                    )
                                except Exception:
                                    bandcamp_status, bandcamp_cached = (None, None)
                                if bandcamp_status == "found" and isinstance(bandcamp_cached, dict):
                                    provider_payloads_prefetched = {
                                        "discogs": None,
                                        "lastfm": None,
                                        "bandcamp": bandcamp_cached,
                                        "extra_sources": [],
                                    }
                                    local_titles_prefetch = list(tracks_edition) if tracks_edition else list(tracks or [])
                                    prefetched_arbitration = _arbitrate_provider_identity(
                                        artist_name=artist,
                                        album_title=title_raw_mb or album_norm,
                                        local_track_titles=local_titles_prefetch,
                                        provider_payloads=provider_payloads_prefetched,
                                    )
                                    if prefetched_arbitration and str(prefetched_arbitration.get("provider") or "").strip().lower() == "bandcamp":
                                        track_score_prefetch = float(prefetched_arbitration.get("track_score") or 0.0)
                                        title_score_prefetch = float(prefetched_arbitration.get("title_score") or 0.0)
                                        artist_score_prefetch = float(prefetched_arbitration.get("artist_score") or 0.0)
                                        if (
                                            track_score_prefetch >= 0.999
                                            and title_score_prefetch >= 0.999
                                            and artist_score_prefetch >= 0.999
                                        ):
                                            e["_provider_payloads_prefetched"] = provider_payloads_prefetched
                                            e["_provider_arbitration_prefetched"] = prefetched_arbitration
                                            e["_provider_fastpath_reason"] = "bandcamp_cached_strict_1.00"
                                            log_mb(
                                                "Album %s ‚Äì \"%s\": skipping MusicBrainz search (cached strict Bandcamp match, tracklist=1.00)",
                                                artist,
                                                title_raw_mb or album_norm,
                                            )

                            fastpath_reason = e.pop("_provider_fastpath_reason", None)
                            if not fastpath_reason:
                                rg_info, match_verified_by_ai = search_mb_release_group_by_metadata(
                                    artist,
                                    album_norm,
                                    tracks,
                                    title_raw=title_raw_mb,
                                    album_folder=album_folder_arg,
                                )
                        if rg_info and rg_info.get("track_titles") and tracks:
                            track_min = getattr(sys.modules[__name__], "TRACKLIST_MATCH_MIN", 0.8)
                            score = _crosscheck_tracklist(list(tracks), rg_info["track_titles"])
                            if score < track_min:
                                logging.info("[Artist %s] Edition %s: MB candidate rejected (tracklist match %.2f < %.2f)", artist, e.get("album_id"), score, track_min)
                                rg_info = None
                        if rg_info:
                            set_cached_mb_album_lookup(artist_norm_key, album_norm, rg_info.get('id') or "", rg_info)
                        else:
                            set_cached_mb_album_lookup(artist_norm_key, album_norm, "", None)
                        if rg_info:
                            e['rg_info_source'] = 'fallback'
                            e['rg_info'] = rg_info
                            e['match_verified_by_ai'] = bool(match_verified_by_ai)
                            if isinstance(rg_info, dict) and 'id' in rg_info:
                                e['musicbrainz_id'] = rg_info['id']
                                e['musicbrainz_type'] = 'release-group'
                                mbid = rg_info['id']
                                cached_mb = get_cached_mb_info(mbid)
                                e['mb_cache_hit'] = cached_mb is not None
                                with lock:
                                    if artist in state.get("scan_active_artists", {}) and e['album_id'] == state["scan_active_artists"][artist].get("current_album", {}).get("album_id"):
                                        cache_text = " (cached)" if cached_mb else ""
                                        ai_text = " (match verified by AI)" if match_verified_by_ai else ""
                                        state["scan_active_artists"][artist]["current_album"]["status_details"] = f"MusicBrainz found{cache_text}{ai_text}"
                                        rg_title = (rg_info.get("title") or "") if isinstance(rg_info, dict) else ""
                                        state["scan_active_artists"][artist]["current_album"]["step_summary"] = (
                                            f"MusicBrainz: found \"{rg_title}\" (id: {mbid}){cache_text}{ai_text}"
                                        )
                                        state["scan_active_artists"][artist]["current_album"]["step_response"] = (
                                            f"MusicBrainz: found release group \"{rg_title}\" (id: {mbid}){cache_text}{ai_text}"
                                        )
                            album_title_log = e.get("title_raw") or e.get("plex_title") or album_norm
                            log_mb(
                                "Album %s ‚Äì \"%s\": MusicBrainz search/fallback matched release-group %s%s",
                                artist,
                                album_title_log,
                                e.get("musicbrainz_id", "?"),
                                " (verified by AI)" if match_verified_by_ai else "",
                            )
                            logging.debug("[Artist %s] Edition %s RG info (search fallback)%s: %s", artist, e['album_id'], " (verified by AI)" if match_verified_by_ai else "", rg_info)
                        else:
                            logging.debug("[Artist %s] No RG info found via search for '%s'", artist, album_norm)
                            e['mb_cache_hit'] = False
                            with lock:
                                if artist in state.get("scan_active_artists", {}) and e['album_id'] == state["scan_active_artists"][artist].get("current_album", {}).get("album_id"):
                                    state["scan_active_artists"][artist]["current_album"]["step_summary"] = (
                                        f"MusicBrainz: no release group found for \"{album_norm}\""
                                    )
                                    state["scan_active_artists"][artist]["current_album"]["step_response"] = (
                                        f"MusicBrainz: no release group found for \"{album_norm}\""
                                    )
                            # Use stored AcousticID result when no RG from search (AcousticID was run first for all albums)
                            if not rg_info and e.get("acoustid_rg_info"):
                                rg_info = e["acoustid_rg_info"]
                                e["rg_info"] = rg_info
                                e["musicbrainz_id"] = e["acoustid_rg_id"]
                                e["rg_info_source"] = "acoustid"
                                rg_title = (rg_info.get("title") or "") if isinstance(rg_info, dict) else ""
                                mbid = rg_info.get("id", "?")
                                with lock:
                                    if artist in state.get("scan_active_artists", {}) and e["album_id"] == state["scan_active_artists"][artist].get("current_album", {}).get("album_id"):
                                        state["scan_active_artists"][artist]["current_album"]["status_details"] = "AcousticID: identified"
                                        state["scan_active_artists"][artist]["current_album"]["step_summary"] = (
                                            f"AcousticID: identified as \"{rg_title}\" (id: {mbid})"
                                        )
                                        state["scan_active_artists"][artist]["current_album"]["step_response"] = (
                                            f"AcousticID: identified as release group \"{rg_title}\" (id: {mbid})"
                                        )
                                log_mb("Album %s ‚Äì folder: AcousticID matched release-group %s (%s)", artist, mbid, rg_title)

                # AcousticID verification: if we have RG from search/other but it disagrees with AcousticID, prefer AcousticID
                if e.get("acoustid_rg_id") and e.get("rg_info") and e["rg_info"].get("id") != e["acoustid_rg_id"]:
                    log_mb(
                        "Album %s ‚Äì \"%s\": provider said RG %s but AcousticID says %s; using AcousticID",
                        artist, e.get("title_raw") or e.get("plex_title") or album_norm, e["rg_info"].get("id"), e["acoustid_rg_id"],
                    )
                    e["rg_info"] = e["acoustid_rg_info"]
                    e["musicbrainz_id"] = e["acoustid_rg_id"]
                    e["rg_info_source"] = "acoustid_verify"
                    with lock:
                        if artist in state.get("scan_active_artists", {}) and e["album_id"] == state["scan_active_artists"][artist].get("current_album", {}).get("album_id"):
                            state["scan_active_artists"][artist]["current_album"]["status_details"] = "AcousticID verified (overrode provider)"
                            state["scan_active_artists"][artist]["current_album"]["step_summary"] = (
                                "AcousticID: release group %s (overrode provider)" % e["acoustid_rg_id"]
                            )
                # LIVE_ALBUMS_MB_STRICT: for detected live albums, only keep MB assign if RG is type Live
                if getattr(sys.modules[__name__], "LIVE_ALBUMS_MB_STRICT", False) and e.get("rg_info") and _is_likely_live_album(e.get("folder"), e.get("title_raw") or e.get("plex_title")):
                    sec = (e["rg_info"].get("secondary_types") or []) if isinstance(e["rg_info"], dict) else []
                    if "Live" not in sec:
                        e["rg_info"] = None
                        e["musicbrainz_id"] = None
                        rg_info = None
                        e.pop("rg_info_source", None)
                        logging.info("[Artist %s] Edition %s: cleared MB assign (live album but RG is not type Live)", artist, e.get("album_id"))

            # Fallback when MusicBrainz found nothing: try Discogs, Last.fm, Bandcamp for metadata (and optionally MB ID from Last.fm)
            title_raw = e.get("title_raw") or e.get("plex_title") or album_norm
            if not rg_info:
                fallback_sources = []
                provider_payloads = e.pop("_provider_payloads_prefetched", None)
                if not isinstance(provider_payloads, dict) or not provider_payloads:
                    provider_payloads = {}
                    try:
                        provider_payloads = _fetch_album_provider_fallbacks_parallel(artist, title_raw)
                    except Exception as provider_exc:
                        logging.debug("[Artist %s] provider fallback fetch failed for %s: %s", artist, title_raw, provider_exc)
                        provider_payloads = {}

                discogs_info = provider_payloads.get("discogs")
                if isinstance(discogs_info, dict):
                    e["fallback_discogs"] = discogs_info
                    fallback_sources.append("Discogs")

                lastfm_info = provider_payloads.get("lastfm")
                if isinstance(lastfm_info, dict):
                    e["fallback_lastfm"] = lastfm_info
                    fallback_sources.append("Last.fm")
                    lfm_mbid = (lastfm_info.get("mbid") or "").strip()
                    if lfm_mbid:
                        try:
                            rg_info, _ = fetch_mb_release_group_info(lfm_mbid)
                            if rg_info:
                                strict_ok_src, strict_reason_src = _strict_identity_match_details(
                                    local_artist=artist,
                                    local_title=title_raw,
                                    candidate_artist=lastfm_info.get("artist") or lastfm_info.get("artist_name") or "",
                                    candidate_title=lastfm_info.get("title") or lastfm_info.get("album") or "",
                                )
                                strict_ok_mb, strict_reason_mb = _strict_identity_match_details(
                                    local_artist=artist,
                                    local_title=title_raw,
                                    candidate_artist=_extract_mb_artist_names(rg_info),
                                    candidate_title=rg_info.get("title") or "",
                                )
                                if not strict_ok_src or not strict_ok_mb:
                                    log_mb(
                                        "Album %s ‚Äì \"%s\": Last.fm MBID %s rejected (source=%s; mb=%s)",
                                        artist,
                                        title_raw,
                                        lfm_mbid,
                                        strict_reason_src,
                                        strict_reason_mb,
                                    )
                                    raise RuntimeError("strict identity mismatch")
                                e["rg_info"] = rg_info
                                e["musicbrainz_id"] = lfm_mbid
                                e["rg_info_source"] = "lastfm_fallback"
                                e["primary_metadata_source"] = "lastfm"
                                e["lastfm_album_mbid"] = lfm_mbid
                                if not isinstance(e.get("meta"), dict):
                                    e["meta"] = {}
                                e["meta"]["primary_metadata_source"] = "lastfm"
                                e["meta"][PMDA_MATCH_PROVIDER_TAG] = "lastfm"
                                e["meta"]["lastfm_album_mbid"] = lfm_mbid
                                with lock:
                                    state["scan_lastfm_matched"] = state.get("scan_lastfm_matched", 0) + 1
                                    if artist in state.get("scan_active_artists", {}) and e["album_id"] == state["scan_active_artists"][artist].get("current_album", {}).get("album_id"):
                                        state["scan_active_artists"][artist]["current_album"]["status_details"] = "MusicBrainz ID from Last.fm"
                                        state["scan_active_artists"][artist]["current_album"]["step_summary"] = (
                                            f"Last.fm: MusicBrainz release group (id: {lfm_mbid})"
                                        )
                                logging.debug("[Artist %s] Edition %s MB ID from Last.fm mbid: %s", artist, e["album_id"], lfm_mbid)
                        except RuntimeError:
                            pass
                        except Exception:
                            pass

                bandcamp_info = provider_payloads.get("bandcamp")
                if isinstance(bandcamp_info, dict):
                    e["fallback_bandcamp"] = bandcamp_info
                    fallback_sources.append("Bandcamp")

                if fallback_sources and artist in state.get("scan_active_artists", {}) and e["album_id"] == state["scan_active_artists"][artist].get("current_album", {}).get("album_id"):
                    with lock:
                        state["scan_active_artists"][artist]["current_album"]["step_response"] = (
                            state["scan_active_artists"][artist]["current_album"].get("step_response", "")
                            + ("; fallback: " + ", ".join(fallback_sources) if fallback_sources else "")
                        )

                # Strict identity arbitration across providers when MB is unavailable.
                if not rg_info:
                    local_titles = list(tracks_edition) if tracks_edition else []
                    arbitration_prefetched = e.pop("_provider_arbitration_prefetched", None)
                    if isinstance(arbitration_prefetched, dict):
                        arbitration = arbitration_prefetched
                    else:
                        arbitration = _arbitrate_provider_identity(
                            artist_name=artist,
                            album_title=title_raw,
                            local_track_titles=local_titles,
                            provider_payloads=provider_payloads,
                        )
                    if arbitration:
                        chosen_provider = str(arbitration.get("provider") or "").strip().lower()
                        chosen_payload = arbitration.get("payload") if isinstance(arbitration.get("payload"), dict) else {}
                        e["primary_metadata_source"] = chosen_provider
                        if not isinstance(e.get("meta"), dict):
                            e["meta"] = {}
                        e["meta"]["primary_metadata_source"] = chosen_provider
                        e["meta"][PMDA_MATCH_PROVIDER_TAG] = chosen_provider
                        if chosen_provider == "discogs":
                            e["discogs_release_id"] = str(
                                chosen_payload.get("release_id") or chosen_payload.get("master_id") or e.get("discogs_release_id") or ""
                            ).strip()
                            if e.get("discogs_release_id"):
                                e["meta"]["discogs_release_id"] = e["discogs_release_id"]
                            with lock:
                                state["scan_discogs_matched"] = state.get("scan_discogs_matched", 0) + 1
                        elif chosen_provider == "lastfm":
                            e["lastfm_album_mbid"] = str(
                                chosen_payload.get("mbid") or e.get("lastfm_album_mbid") or ""
                            ).strip()
                            if e.get("lastfm_album_mbid"):
                                e["meta"]["lastfm_album_mbid"] = e["lastfm_album_mbid"]
                            with lock:
                                state["scan_lastfm_matched"] = state.get("scan_lastfm_matched", 0) + 1
                        elif chosen_provider == "bandcamp":
                            e["bandcamp_album_url"] = str(
                                chosen_payload.get("album_url") or chosen_payload.get("url") or e.get("bandcamp_album_url") or ""
                            ).strip()
                            if e.get("bandcamp_album_url"):
                                e["meta"]["bandcamp_album_url"] = e["bandcamp_album_url"]
                            with lock:
                                state["scan_bandcamp_matched"] = state.get("scan_bandcamp_matched", 0) + 1

                        provider_tracklist = chosen_payload.get("tracklist") if isinstance(chosen_payload, dict) else None
                        # Track-count hint for incomplete detection when MB isn't available.
                        # This allows detecting tail-truncated albums even when there are no index gaps.
                        if isinstance(provider_tracklist, list) and provider_tracklist:
                            try:
                                e["_expected_track_count"] = int(len(provider_tracklist))
                            except Exception:
                                pass
                        if local_titles and isinstance(provider_tracklist, list) and provider_tracklist:
                            track_min = getattr(sys.modules[__name__], "TRACKLIST_MATCH_MIN", 0.8)
                            score = _crosscheck_tracklist(local_titles, provider_tracklist)
                            if score >= track_min:
                                e["mb_submission_payload"] = _prepare_mb_submission_payload(
                                    chosen_payload.get("artist_name") or chosen_payload.get("artist") or artist,
                                    chosen_payload.get("title") or chosen_payload.get("album") or "",
                                    chosen_payload.get("year") or "",
                                    provider_tracklist,
                                    chosen_provider,
                                )
                        logging.info(
                            "[Artist %s] Edition %s: provider identity accepted via %s (confidence=%.2f, source=%s, title=%.2f artist=%.2f track=%.2f)",
                            artist,
                            e.get("album_id"),
                            chosen_provider,
                            float(arbitration.get("confidence") or 0.0),
                            arbitration.get("confidence_source") or "heuristic",
                            float(arbitration.get("title_score") or 0.0),
                            float(arbitration.get("artist_score") or 0.0),
                            float(arbitration.get("track_score") or 0.0),
                        )
            # Increment MB-done count for this edition (whether we found rg_info or not)
            with lock:
                state["scan_mb_done_count"] = state.get("scan_mb_done_count", 0) + 1
                state["scan_step_progress"] = state.get("scan_step_progress", 0) + 1
            # Also store MBID from rg_info if not already set
            if e.get('rg_info') and 'musicbrainz_id' not in e and isinstance(e['rg_info'], dict) and 'id' in e['rg_info']:
                e['musicbrainz_id'] = e['rg_info']['id']
                e['musicbrainz_type'] = 'release-group'
            
            # Detect broken album (missing tracks)
            # Detect broken album (missing tracks).
            # Prefer MB track_count when present; otherwise fall back to provider tracklist size when available.
            mb_hint = e.get("rg_info")
            if not mb_hint:
                try:
                    exp = int(e.get("_expected_track_count") or 0)
                except Exception:
                    exp = 0
                if exp > 0:
                    mb_hint = {"track_count": exp}
            is_broken, expected_count, actual_count, missing_indices = detect_broken_album(
                db_conn,
                e["album_id"],
                e["tracks"],
                mb_hint,
            )
            e['is_broken'] = is_broken
            if is_broken:
                e['expected_track_count'] = expected_count
                e['actual_track_count'] = actual_count
                e['missing_indices'] = missing_indices
                logging.warning(
                    "[Artist %s] Album %s (%s) is broken: %d tracks found, expected %s, gaps: %s",
                    artist, e['album_id'], e.get('title_raw', ''), actual_count, expected_count or 'unknown', missing_indices
                )
        mb_lookup_time = time.perf_counter() - mb_start
        # --- MusicBrainz enrichment summary ---
        direct = sum(1 for e in editions if 'rg_info' in e and e.get('rg_info_source') in id_tags)
        fallback = sum(1 for e in editions if 'rg_info' in e and e.get('rg_info_source') == 'fallback')
        incremental_skipped = sum(1 for e in editions if e.get("rg_info_source") == "incremental_skip")
        missing = sum(1 for e in editions if 'rg_info' not in e)
        log_mb(
            "[Artist %s] enrichment summary: direct=%d, fallback=%d, fast-skip=%d, missing=%d "
            "(elapsed %.2fs)",
            artist,
            direct,
            fallback,
            incremental_skipped,
            missing,
            mb_lookup_time,
        )

    # Detect and collapse Box Set discs (skip as duplicates)
    from collections import defaultdict
    box_set_groups = defaultdict(list)
    for e in editions:
        sec_types = e.get('rg_info', {}).get('secondary_types', [])
        if 'Box Set' in sec_types:
            parent_folder = e['folder'].parent
            box_set_groups[parent_folder].append(e)

    if box_set_groups:
        for parent_folder, items in box_set_groups.items():
            log_mb(
                "Box Set detected at %s with %d disc(s) ‚Äì skipping duplicate detection for these discs.",
                parent_folder,
                len(items),
            )
        # Exclude all Box Set disc folders from further duplicate grouping
        editions = [e for e in editions if e['folder'].parent not in box_set_groups]
    # --- NO FILES HANDLING ---
    if editions:
        # Reset streak on success
        no_file_streak_global = 0
        ok_msg = (
            f"[Artist {artist}] FOUND {len(editions)} valid file editions on filesystem "
            f"for {len(album_ids)} albums. PATH_MAP and volume bindings appear correct!"
        )
        log_path(ok_msg)
    else:
        # No valid editions found
        no_file_streak_global += 1
        if skip_count == len(album_ids):
            logging.info(f"[Artist {artist}] All {skip_count} albums skipped due to SKIP_FOLDERS {SKIP_FOLDERS}")
            return [], {"ai_used": 0, "mb_used": 0}, []
        else:
            logger = logging.getLogger()
            logger.error(f"[Artist {artist}] FOUND 0 valid file editions on filesystem! Checked SKIP_FOLDERS: {SKIP_FOLDERS}")
            notify_discord = globals().get("notify_discord", None)
            if notify_discord:
                notify_discord(f"No files found for {artist}.")
            global popup_displayed
            if no_file_streak_global >= NO_FILE_THRESHOLD:
                if not popup_displayed:
                    gui.display_popup(
                        f"PMDA didn't find any files for {NO_FILE_THRESHOLD} artists in a row. "
                        "Aborting scan. Files appear unreachable from inside the container; "
                        "please check your volume bindings."
                    )
                    popup_displayed = True
                scan_should_stop.set()
                return [], {"ai_used": 0, "mb_used": 0}, []
            # Below threshold, do not show repeated popups -- let scan continue or fail silently
            return [], {"ai_used": 0, "mb_used": 0}, []
    for e in editions:
        logging.debug(
            f"[Artist {artist}] Edition {e['album_id']}: "
            f"norm='{e['album_norm']}', tracks={len(e['tracks'])}, dur_ms={e['dur']}, "
            f"files={e['file_count']}, fmt_score={e['fmt_score']}, "
            f"br={e['br']}, sr={e['sr']}, bd={e['bd']}"
        )
    # Map resolved folder path -> edition (for same-folder duplicate groups later)
    folder_to_edition: dict[str, dict] = {}
    for e in editions:
        try:
            k = str(Path(e["folder"]).resolve())
        except Exception:
            k = str(e["folder"])
        folder_to_edition[k] = e
    # album_id -> title (for same-folder loser labels when db_conn is None, e.g. Files backend)
    album_id_to_title: dict[int, str] = {
        e["album_id"]: (e.get("title_raw") or e.get("plex_title") or f"Album {e['album_id']}")
        for e in editions
    }
    # --- Dupe Detection v2 grouping: provider IDs + signatures + loose title + similarity ---
    from collections import defaultdict

    # In changed-only scans, we may inject context-only editions from the published cache
    # to detect dupes against older albums. Those context editions should not affect scan
    # health stats nor be persisted into scan_editions (they use run-local album_id).
    all_editions_for_stats = [e for e in editions if not bool(e.get("context_only"))]

    dupe_report: dict = {
        "version": 2,
        "groups_total": 0,
        "groups_needs_ai": 0,
        "groups_by_signal": {},
        "rejected_by_reason": {},
    }

    def _dr_inc(bucket: str, key: str, n: int = 1) -> None:
        if not key:
            return
        d = dupe_report.get(bucket)
        if not isinstance(d, dict):
            d = {}
            dupe_report[bucket] = d
        k = str(key)
        d[k] = int(d.get(k) or 0) + int(n or 0)

    def _folder_key(e: dict) -> str:
        folder = e.get("folder")
        if not folder:
            return ""
        try:
            return str(Path(folder).resolve())
        except Exception:
            return str(folder)

    def _all_same_folder(ed_list: list[dict]) -> bool:
        keys = {_folder_key(e) for e in ed_list if e.get("folder")}
        keys = {k for k in keys if k}
        return len(keys) == 1 and bool(keys)

    def _ensure_track_set(e: dict) -> set[str]:
        ts = e.get("_dupe_track_title_set")
        if isinstance(ts, set):
            return ts
        ts = _dupe_track_title_set(e.get("tracks") or [])
        e["_dupe_track_title_set"] = ts
        return ts

    def _is_classical(ed_list: list[dict]) -> bool:
        genres: list[str] = []
        for e in ed_list:
            g = ""
            try:
                g = (e.get("meta") or {}).get("genre") or ""
            except Exception:
                g = ""
            if g:
                genres.append(str(g).lower())
        return bool(genres) and all("classical" in g for g in genres)

    def _split_classical(ed_list: list[dict]) -> list[list[dict]]:
        subgroups: list[dict] = []
        for e in ed_list:
            meta = e.get("meta", {}) or {}
            year = meta.get("date") or meta.get("originaldate") or ""
            dur0 = e["tracks"][0].dur if e.get("tracks") else 0
            placed = False
            for sg in subgroups:
                if sg["year"] == year and abs(int(sg["dur0"]) - int(dur0)) <= 10000:
                    sg["editions"].append(e)
                    placed = True
                    break
            if not placed:
                subgroups.append({"year": year, "dur0": dur0, "editions": [e]})
        return [sg["editions"] for sg in subgroups if sg.get("editions")]

    def _group_pair_metrics(ed_list: list[dict]) -> tuple[float, float]:
        # Return (max_jaccard, min_track_ratio) across all pairs in this group.
        n = len(ed_list)
        if n < 2:
            return (1.0, 1.0)
        sets = [_ensure_track_set(e) for e in ed_list]
        tracks = [e.get("tracks") or [] for e in ed_list]
        max_j = 0.0
        min_ratio = 1.0
        for i in range(n):
            for j in range(i + 1, n):
                ratio = _dupe_track_count_ratio(tracks[i], tracks[j])
                if ratio < min_ratio:
                    min_ratio = ratio
                jac = _dupe_jaccard(sets[i], sets[j])
                if jac > max_j:
                    max_j = jac
        return (max_j, min_ratio)

    def _maybe_audio_overlap_max(ed_list: list[dict]) -> float:
        # Expensive: compute max chromaprint overlap across any pair in group.
        n = len(ed_list)
        if n < 2:
            return 0.0
        fps: list[set[str]] = []
        for e in ed_list:
            try:
                fps.append(_dupe_audio_fp_set_for_edition(e))
            except Exception:
                fps.append(set())
        max_ov = 0.0
        for i in range(n):
            for j in range(i + 1, n):
                a = fps[i]
                b = fps[j]
                if not a or not b:
                    continue
                ov = (len(a & b) / max(len(a), len(b))) if max(len(a), len(b)) else 0.0
                if ov > max_ov:
                    max_ov = ov
        return max_ov

    def _live_safety_skip(ed_list: list[dict]) -> bool:
        if LIVE_DEDUPE_MODE != "safe" or len(ed_list) < 2:
            return False
        live_flags: list[bool] = []
        track_sets: list[set[str]] = []
        track_counts: list[int] = []
        for e in ed_list:
            meta = e.get("meta", {}) or {}
            title_lower = (e.get("plex_title") or e.get("title_raw") or "").lower()
            sec_types = (meta.get("musicbrainz_secondarytypes") or "").lower()
            is_live = (
                "live" in sec_types
                or " live " in f" {title_lower} "
                or " live at " in title_lower
                or " live in " in title_lower
            )
            live_flags.append(is_live)
            ts = _ensure_track_set(e)
            if ts:
                track_sets.append(ts)
            track_counts.append(len(e.get("tracks") or []))
        if not any(live_flags):
            return False
        if not all(live_flags):
            _dr_inc("rejected_by_reason", "mixed_live_nonlive")
            return True
        if len(track_sets) >= 2:
            union = set().union(*track_sets)
            inter = set.intersection(*track_sets) if track_sets else set()
            jaccard = (len(inter) / len(union)) if union else 1.0
        else:
            jaccard = 1.0
        min_tracks = min(track_counts) if track_counts else 0
        max_tracks = max(track_counts) if track_counts else 0
        ratio = (min_tracks / max_tracks) if max_tracks else 1.0
        if jaccard < 0.8 or ratio < 0.75:
            _dr_inc("rejected_by_reason", "live_low_similarity")
            return True
        return False

    # Precompute v2 identity + loose title norms for grouping.
    for e in editions:
        title_src = (e.get("title_raw") or e.get("plex_title") or e.get("album_norm") or "").strip()
        e["_dupe_title_norm_loose"] = norm_album_for_dedup_loose(title_src)
        e["_dupe_edition_tokens"] = _dupe_extract_edition_tokens(title_src)
        e["_dupe_mb_rg"] = _dupe_get_mb_release_group_id(e)
        e["_dupe_mb_rel"] = _dupe_get_mb_release_id(e)
        e["_dupe_discogs"] = _dupe_get_discogs_id(e)
        e["_dupe_lastfm"] = _dupe_get_lastfm_mbid(e)
        e["_dupe_bandcamp"] = _dupe_get_bandcamp_url(e)

    out: list[dict] = []
    used_ids: set[int] = set()
    feedback_pairs = _dupe_load_feedback_pairs_for_artist(artist)

    # Track statistics
    mb_used_count = 0  # Editions enriched with provider identity
    ai_used_count = 0  # Groups where a previous AI decision was reused immediately
    audio_cache_hits = sum(1 for e in editions if e.get("audio_cache_hit", False))
    audio_cache_misses = len(editions) - audio_cache_hits
    mb_cache_hits = 0
    mb_cache_misses = 0

    if USE_MUSICBRAINZ:
        mb_used_count = sum(
            1
            for e in editions
            if e.get("musicbrainz_id") or (isinstance(e.get("rg_info"), dict) and e["rg_info"].get("id"))
        )
        mb_cache_hits = sum(1 for e in editions if (("rg_info" in e) and bool(e.get("mb_cache_hit", False))))
        mb_cache_misses = sum(1 for e in editions if (("rg_info" in e) and (not bool(e.get("mb_cache_hit", False)))))

    def _append_group(
        ed_list: list[dict],
        *,
        fuzzy: bool,
        signal: str,
        evidence: list[str] | None = None,
    ) -> None:
        nonlocal ai_used_count
        if not ed_list or len(ed_list) < 2:
            return
        # Never regroup already-used editions.
        ed_list = [e for e in ed_list if e.get("album_id") not in used_ids]
        if len(ed_list) < 2:
            return
        if _all_same_folder(ed_list):
            _dr_inc("rejected_by_reason", "same_folder")
            return
        if _live_safety_skip(ed_list):
            return

        # Classical safety: split by year + first-track duration threshold.
        subgroups = _split_classical(ed_list) if _is_classical(ed_list) else [ed_list]

        for sg in subgroups:
            if not sg or len(sg) < 2:
                continue
            sg = [e for e in sg if e.get("album_id") not in used_ids]
            if len(sg) < 2:
                continue

            if signal not in {"provider_id", "track_sig", "audio_fp", "user_label"} and not editions_share_confident_signal(sg):
                _dr_inc("rejected_by_reason", "low_confidence")
                continue

            chips = list(evidence or [])
            max_jac, min_ratio = _group_pair_metrics(sg)
            chips.append(f"TRACKS_JACCARD_MAX:{max_jac:.2f}")
            chips.append(f"TRACKS_RATIO_MIN:{min_ratio:.2f}")

            tok_union: set[str] = set()
            for e in sg:
                toks = e.get("_dupe_edition_tokens") or []
                if isinstance(toks, list):
                    for t in toks:
                        if t:
                            tok_union.add(str(t))
            if tok_union:
                chips.append("TOKENS:" + ",".join(sorted(tok_union))[:200])

            no_move = False
            manual_review = False

            # Feedback loop: if the user explicitly marked any pair as NOT a dupe, never auto-move.
            if feedback_pairs:
                try:
                    folders = [_folder_key(e) for e in sg]
                except Exception:
                    folders = []
                user_dupe = False
                user_not_dupe = False
                for i in range(len(folders)):
                    for j in range(i + 1, len(folders)):
                        key = _dupe_feedback_pair_key(folders[i], folders[j])
                        lab = feedback_pairs.get(key)
                        if lab == "dupe":
                            user_dupe = True
                        elif lab in {"not_dupe", "notdupe", "no"}:
                            user_not_dupe = True
                if user_dupe:
                    chips.append("USER_LABEL:DUPE")
                if user_not_dupe:
                    chips.append("USER_LABEL:NOT_DUPE")
                    manual_review = True
                    no_move = True
                    _dr_inc("rejected_by_reason", "user_not_dupe_conflict")
            if signal == "provider_id":
                # Provider IDs are strong, but wrong tags can collide. If coherence is extremely low,
                # keep for review but do not auto-move.
                if max_jac < 0.22 and min_ratio < 0.35:
                    audio_ov = 0.0
                    try:
                        audio_ov = _maybe_audio_overlap_max(sg)
                    except Exception:
                        audio_ov = 0.0
                    if audio_ov:
                        chips.append(f"AUDIO_FP_MAX:{audio_ov:.2f}")
                    if audio_ov < 0.87:
                        no_move = True
                        manual_review = True
                        chips.append("COHERENCE_LOW")
                        _dr_inc("rejected_by_reason", "provider_low_coherence")

            best = choose_best(sg, defer_ai=True)
            if best is None:
                out.append(
                    {
                        "artist": artist,
                        "album_id": None,
                        "editions": sg,
                        "fuzzy": bool(fuzzy),
                        "needs_ai": True,
                        "dupe_signal": signal,
                        "dupe_evidence": chips,
                        "no_move": bool(no_move),
                        "manual_review": bool(manual_review),
                    }
                )
                used_ids.update(e.get("album_id") for e in sg if e.get("album_id") is not None)
                dupe_report["groups_total"] = int(dupe_report.get("groups_total") or 0) + 1
                dupe_report["groups_needs_ai"] = int(dupe_report.get("groups_needs_ai") or 0) + 1
                _dr_inc("groups_by_signal", signal)
                continue

            losers = [e for e in sg if e.get("album_id") != best.get("album_id")]
            if not losers:
                used_ids.update(e.get("album_id") for e in sg if e.get("album_id") is not None)
                continue

            best["dupe_evidence"] = chips
            if best.get("used_ai", False):
                ai_used_count += 1

            out.append(
                {
                    "artist": artist,
                    "album_id": best.get("album_id"),
                    "best": best,
                    "losers": losers,
                    "fuzzy": bool(fuzzy),
                    "needs_ai": False,
                    "dupe_signal": signal,
                    "dupe_evidence": chips,
                    "no_move": bool(no_move),
                    "manual_review": bool(manual_review),
                }
            )
            used_ids.update(e.get("album_id") for e in sg if e.get("album_id") is not None)
            dupe_report["groups_total"] = int(dupe_report.get("groups_total") or 0) + 1
            _dr_inc("groups_by_signal", signal)

    # Phase 0: user feedback labels (force dupe grouping).
    if feedback_pairs and len(editions) >= 2:
        try:
            n = len(editions)
            parent = list(range(n))
            rank = [0] * n

            def _find_fb(i: int) -> int:
                while parent[i] != i:
                    parent[i] = parent[parent[i]]
                    i = parent[i]
                return i

            def _union_fb(i: int, j: int) -> None:
                ri = _find_fb(i)
                rj = _find_fb(j)
                if ri == rj:
                    return
                if rank[ri] < rank[rj]:
                    parent[ri] = rj
                elif rank[ri] > rank[rj]:
                    parent[rj] = ri
                else:
                    parent[rj] = ri
                    rank[ri] += 1

            folder_to_idxs: dict[str, list[int]] = defaultdict(list)
            for i, e in enumerate(editions):
                fk = _folder_key(e)
                if fk:
                    folder_to_idxs[fk].append(i)

            for (fa, fb), lab in (feedback_pairs or {}).items():
                if str(lab or "").strip().lower() != "dupe":
                    continue
                a = (fa or "").strip()
                b = (fb or "").strip()
                if not a or not b:
                    continue
                for ia in folder_to_idxs.get(a, [])[:5]:
                    for ib in folder_to_idxs.get(b, [])[:5]:
                        _union_fb(ia, ib)

            comps: dict[int, list[int]] = defaultdict(list)
            for i in range(n):
                comps[_find_fb(i)].append(i)
            for idxs in comps.values():
                if len(idxs) < 2:
                    continue
                grp = [editions[i] for i in idxs]
                _append_group(grp, fuzzy=False, signal="user_label", evidence=["USER_DUPE_LABEL"])
        except Exception:
            logging.debug("[Artist %s] user-label dupe grouping failed", artist, exc_info=True)

    # Phase 2: group by provider IDs (high precision).
    if len(editions) >= 2:
        n = len(editions)
        parent = list(range(n))
        rank = [0] * n

        def _find(i: int) -> int:
            while parent[i] != i:
                parent[i] = parent[parent[i]]
                i = parent[i]
            return i

        def _union(i: int, j: int) -> None:
            ri = _find(i)
            rj = _find(j)
            if ri == rj:
                return
            if rank[ri] < rank[rj]:
                parent[ri] = rj
            elif rank[ri] > rank[rj]:
                parent[rj] = ri
            else:
                parent[rj] = ri
                rank[ri] += 1

        id_to_idxs: dict[str, list[int]] = defaultdict(list)
        for i, e in enumerate(editions):
            mb_rg = (e.get("_dupe_mb_rg") or "").strip()
            if mb_rg:
                id_to_idxs[f"mb_rg:{mb_rg}"].append(i)
            mb_rel = (e.get("_dupe_mb_rel") or "").strip()
            if mb_rel:
                id_to_idxs[f"mb_rel:{mb_rel}"].append(i)
            discogs = (e.get("_dupe_discogs") or "").strip()
            if discogs:
                id_to_idxs[f"discogs:{discogs}"].append(i)
            lastfm = (e.get("_dupe_lastfm") or "").strip()
            if lastfm:
                id_to_idxs[f"lastfm:{lastfm}"].append(i)
            bandcamp = (e.get("_dupe_bandcamp") or "").strip()
            if bandcamp:
                id_to_idxs[f"bandcamp:{bandcamp}"].append(i)

        for _k, idxs in id_to_idxs.items():
            if len(idxs) < 2:
                continue
            base = idxs[0]
            for j in idxs[1:]:
                _union(base, j)

        comps: dict[int, list[int]] = defaultdict(list)
        for i in range(n):
            comps[_find(i)].append(i)

        for idxs in comps.values():
            if len(idxs) < 2:
                continue
            grp = [editions[i] for i in idxs]
            if len(grp) > 10:
                by_loose: dict[str, list[dict]] = defaultdict(list)
                for e in grp:
                    by_loose[str(e.get("_dupe_title_norm_loose") or "")].append(e)
                for _lk, sub in by_loose.items():
                    if len(sub) < 2:
                        continue
                    mb_rg_ids = {e.get("_dupe_mb_rg") for e in sub if e.get("_dupe_mb_rg")}
                    discogs_ids = {e.get("_dupe_discogs") for e in sub if e.get("_dupe_discogs")}
                    lastfm_ids = {e.get("_dupe_lastfm") for e in sub if e.get("_dupe_lastfm")}
                    bandcamp_urls = {e.get("_dupe_bandcamp") for e in sub if e.get("_dupe_bandcamp")}
                    evidence: list[str] = []
                    if len(mb_rg_ids) == 1:
                        evidence.append(f"MB_RG:{next(iter(mb_rg_ids))}")
                    if len(discogs_ids) == 1:
                        evidence.append(f"DISCOGS:{next(iter(discogs_ids))}")
                    if len(lastfm_ids) == 1:
                        evidence.append(f"LASTFM_MBID:{next(iter(lastfm_ids))}")
                    if len(bandcamp_urls) == 1:
                        evidence.append("BANDCAMP:" + str(next(iter(bandcamp_urls)))[:120])
                    _append_group(sub, fuzzy=False, signal="provider_id", evidence=evidence)
            else:
                mb_rg_ids = {e.get("_dupe_mb_rg") for e in grp if e.get("_dupe_mb_rg")}
                discogs_ids = {e.get("_dupe_discogs") for e in grp if e.get("_dupe_discogs")}
                lastfm_ids = {e.get("_dupe_lastfm") for e in grp if e.get("_dupe_lastfm")}
                bandcamp_urls = {e.get("_dupe_bandcamp") for e in grp if e.get("_dupe_bandcamp")}
                evidence: list[str] = []
                if len(mb_rg_ids) == 1:
                    evidence.append(f"MB_RG:{next(iter(mb_rg_ids))}")
                if len(discogs_ids) == 1:
                    evidence.append(f"DISCOGS:{next(iter(discogs_ids))}")
                if len(lastfm_ids) == 1:
                    evidence.append(f"LASTFM_MBID:{next(iter(lastfm_ids))}")
                if len(bandcamp_urls) == 1:
                    evidence.append("BANDCAMP:" + str(next(iter(bandcamp_urls)))[:120])
                _append_group(grp, fuzzy=False, signal="provider_id", evidence=evidence)

    # Phase 3: exact grouping by track signature (strong evidence; catches bad titles).
    remaining = [e for e in editions if e.get("album_id") not in used_ids]
    sig_groups: dict[tuple, list[dict]] = defaultdict(list)
    for e in remaining:
        sig = e.get("sig")
        if sig:
            sig_groups[sig].append(e)
    for ed_list in sig_groups.values():
        if len(ed_list) < 2:
            continue
        _append_group(ed_list, fuzzy=False, signal="track_sig", evidence=["SIG_MATCH"])

    # Phase 3.5: exact grouping by audio fingerprint signature (cached-only, high precision).
    remaining = [e for e in editions if e.get("album_id") not in used_ids]
    audio_groups: dict[str, list[dict]] = defaultdict(list)
    for e in remaining:
        try:
            sig = _dupe_audio_sig_for_edition(e, max_tracks=10, min_fps=3, compute_missing=False)
        except Exception:
            sig = ""
        if sig:
            audio_groups[sig].append(e)
    for sig, ed_list in audio_groups.items():
        if len(ed_list) < 2:
            continue
        _append_group(ed_list, fuzzy=False, signal="audio_fp", evidence=[f"AUDIO_SIG:{sig[:12]}"])

    # Phase 1/3: loose title grouping + similarity-based splitting.
    remaining = [e for e in editions if e.get("album_id") not in used_ids]
    loose_groups: dict[str, list[dict]] = defaultdict(list)
    for e in remaining:
        key = (e.get("_dupe_title_norm_loose") or "").strip() or (e.get("album_norm") or "").strip()
        loose_groups[key].append(e)
    for key, ed_list in loose_groups.items():
        if len(ed_list) < 2:
            continue
        clusters = _dupe_split_editions_by_similarity(ed_list, min_jaccard=0.82, min_ratio=0.75, allow_audio_fp=True)
        for c in clusters:
            if len(c) < 2:
                continue
            _append_group(c, fuzzy=True, signal="title_loose", evidence=[f"TITLE_LOOSE:{key}"])

    # Fallback: strict title grouping for remaining editions.
    remaining = [e for e in editions if e.get("album_id") not in used_ids]
    strict_groups: dict[str, list[dict]] = defaultdict(list)
    for e in remaining:
        group_key = (e.get("plex_norm") or e.get("album_norm") or "").strip()
        if not group_key or group_key.startswith("__untitled__"):
            group_key = (e.get("album_norm") or "").strip()
        strict_groups[group_key].append(e)
    for key, ed_list in strict_groups.items():
        if len(ed_list) < 2:
            continue
        _append_group(ed_list, fuzzy=True, signal="title_strict", evidence=[f"TITLE_STRICT:{key}"])

    # --- Same-folder duplicate groups: multiple Plex album entries pointing to one folder ---
    for folder_str, album_ids in seen_folders.items():
        if len(album_ids) < 2:
            continue
        best_edition = folder_to_edition.get(folder_str)
        if not best_edition:
            continue
        losers = []
        for aid in album_ids:
            if aid == best_edition["album_id"]:
                continue
            pt = album_id_to_title.get(aid, f"Album {aid}") if db_conn is None else (album_title(db_conn, aid) or f"Album {aid}")
            losers.append({
                "album_id": aid,
                "title_raw": pt,
                "folder": best_edition["folder"],
                "meta": {},
                "plex_title": pt,
                "br": 0,
                "sr": 0,
                "bd": 0,
            })
        if not losers:
            continue
        chips = ["SAME_FOLDER_DUPLICATE"]
        try:
            best_edition["dupe_evidence"] = chips
        except Exception:
            pass
        logging.info(
            "[Artist %s] Same-folder duplicate group: '%s' has %d Plex entries (best=%s, losers=%s)",
            artist, best_edition.get("title_raw", ""), len(album_ids),
            best_edition["album_id"], [l["album_id"] for l in losers]
        )
        out.append({
            "artist": artist,
            "album_id": best_edition["album_id"],
            "best": best_edition,
            "losers": losers,
            "fuzzy": False,
            "needs_ai": False,
            "dupe_signal": "same_folder",
            "dupe_evidence": chips,
            # Same-folder groups are Plex metadata duplicates; they require manual cleanup, not moves.
            "no_move": True,
            "manual_review": True,
            "same_folder": True,
        })
        dupe_report["groups_total"] = int(dupe_report.get("groups_total") or 0) + 1
        _dr_inc("groups_by_signal", "same_folder")

    logging.info(
        "[Artist %s] dupe_v2: groups=%d (needs_ai=%d) signals=%s rejected=%s",
        artist,
        int(dupe_report.get("groups_total") or 0),
        int(dupe_report.get("groups_needs_ai") or 0),
        dupe_report.get("groups_by_signal") or {},
        dupe_report.get("rejected_by_reason") or {},
    )
    # Keep groups that have losers (resolved) or need AI (will be resolved in batch).
    # Do not drop needs_ai groups: they have "editions" but no "losers" yet.
    out = [g for g in out if g.get("losers") or g.get("needs_ai")]
    
    # Calculate total scan time
    scan_total_time = time.perf_counter() - scan_start_time
    
    # Compile stats with timing (use all_editions_for_stats for per-edition counts)
    stats_editions = list(all_editions_for_stats)
    audio_cache_hits_stats = sum(1 for e in stats_editions if e.get("audio_cache_hit", False))
    audio_cache_misses_stats = max(0, len(stats_editions) - audio_cache_hits_stats)
    mb_cache_hits_stats = sum(
        1 for e in stats_editions if ("rg_info" in e) and bool(e.get("mb_cache_hit", False))
    )
    mb_cache_misses_stats = sum(
        1 for e in stats_editions if ("rg_info" in e) and (not bool(e.get("mb_cache_hit", False)))
    )
    mb_used_stats = sum(
        1
        for e in stats_editions
        if e.get("musicbrainz_id") or (isinstance(e.get("rg_info"), dict) and e["rg_info"].get("id"))
    )
    mb_verified_by_ai = sum(1 for e in stats_editions if e.get("match_verified_by_ai"))
    duplicate_groups_count = len(out)
    total_duplicates_count = sum(len(g.get("losers", [])) for g in out)
    broken_albums_count = sum(1 for e in stats_editions if e.get("is_broken", False))
    albums_without_mb_id = 0
    albums_without_artist_mb_id = 0
    albums_without_complete_tags = 0
    albums_without_artist_image = 0
    albums_without_album_image = 0

    for e in stats_editions:
        meta = e.get("meta", {}) or {}
        if (
            not meta.get("musicbrainz_releasegroupid")
            and not meta.get("musicbrainz_releaseid")
            and not e.get("musicbrainz_id")
        ):
            albums_without_mb_id += 1
        if not meta.get("musicbrainz_albumartistid") and not meta.get("musicbrainz_artistid"):
            albums_without_artist_mb_id += 1

        # REQUIRED_TAGS from settings = single source of truth
        missing_required = []
        try:
            missing_required = _check_required_tags(meta, REQUIRED_TAGS, edition=e)
        except Exception:
            missing_required = []
        if missing_required:
            albums_without_complete_tags += 1

        # Album cover (prefer cached boolean when available).
        has_cover = None
        if "has_cover" in e:
            has_cover = bool(e.get("has_cover"))
        else:
            folder = e.get("folder")
            folder_path = None
            if folder:
                folder_path = folder if isinstance(folder, Path) else Path(str(folder))
            if folder_path and folder_path.exists():
                cover_patterns = ["cover.*", "folder.*", "album.*", "artwork.*", "front.*"]
                for pattern in cover_patterns:
                    try:
                        matches = list(folder_path.glob(pattern))
                    except Exception:
                        matches = []
                    image_matches = [f for f in matches if f.suffix.lower() in [".jpg", ".jpeg", ".png", ".webp", ".gif"]]
                    if image_matches:
                        has_cover = True
                        break
                if has_cover is None:
                    has_cover = False
        if has_cover is False:
            albums_without_album_image += 1

        # Artist image is tracked downstream (per-artist), but keep the counter for completeness.
        if "has_artist_image" in e and (not bool(e.get("has_artist_image"))):
            albums_without_artist_image += 1

    stats = {
        "ai_used": ai_used_count,
        "mb_used": mb_used_stats,
        "mb_verified_by_ai": mb_verified_by_ai,
        "audio_cache_hits": audio_cache_hits_stats,
        "audio_cache_misses": audio_cache_misses_stats,
        "mb_cache_hits": mb_cache_hits_stats,
        "mb_cache_misses": mb_cache_misses_stats,
        "duplicate_groups_count": duplicate_groups_count,
        "total_duplicates_count": total_duplicates_count,
        "broken_albums_count": broken_albums_count,
        "albums_without_mb_id": albums_without_mb_id,
        "albums_without_artist_mb_id": albums_without_artist_mb_id,
        "albums_without_complete_tags": albums_without_complete_tags,
        "albums_without_album_image": albums_without_album_image,
        "albums_without_artist_image": albums_without_artist_image,
        "dupe_report": dupe_report,
        "timing": {
            "audio_analysis_time": audio_analysis_time,
            "mb_lookup_time": mb_lookup_time,
            "ai_processing_time": ai_processing_time,
            "total_time": scan_total_time,
        },
    }
    
    # Mark all remaining albums as done (those not in any duplicate group)
    with lock:
        if artist in state.get("scan_active_artists", {}):
            current_album = state["scan_active_artists"][artist].get("current_album", {})
            current_album_id = current_album.get("album_id")
            # If current album is not in used_ids, it means it's a single edition (no duplicates)
            # Mark it as done
            if current_album_id and current_album_id not in used_ids:
                state["scan_active_artists"][artist]["current_album"]["status"] = "done"
                state["scan_active_artists"][artist]["current_album"]["status_details"] = ""
                state["scan_active_artists"][artist]["current_album"]["step_summary"] = ""
                state["scan_active_artists"][artist]["current_album"]["step_response"] = ""
    
    # Store broken albums in database
    import json
    con = sqlite3.connect(str(STATE_DB_FILE), timeout=30)
    cur = con.cursor()
    for e in all_editions_for_stats:
        if e.get('is_broken', False):
            missing_indices_json = json.dumps(e.get('missing_indices', []))
            cur.execute("""
                INSERT OR REPLACE INTO broken_albums 
                (artist, album_id, expected_track_count, actual_track_count, missing_indices, musicbrainz_release_group_id, detected_at)
                VALUES (?, ?, ?, ?, ?, ?, ?)
            """, (
                artist,
                e['album_id'],
                e.get('expected_track_count'),
                e.get('actual_track_count', len(e.get('tracks', []))),
                missing_indices_json,
                e.get('musicbrainz_id'),
                time.time()
            ))
            
            # Auto-send to Lidarr if enabled
            if AUTO_FIX_BROKEN_ALBUMS and LIDARR_URL and LIDARR_API_KEY and e.get('musicbrainz_id'):
                try:
                    add_broken_album_to_lidarr(artist, e['album_id'], e.get('musicbrainz_id'), e.get('title_raw', ''))
                    cur.execute("""
                        UPDATE broken_albums SET sent_to_lidarr = 1 
                        WHERE artist = ? AND album_id = ?
                    """, (artist, e['album_id']))
                except Exception as lidarr_err:
                    logging.warning("Failed to auto-send broken album %s to Lidarr: %s", e['album_id'], lidarr_err)
    con.commit()
    con.close()
    
    return out, stats, all_editions_for_stats

def save_scan_editions_to_db(scan_id: int, all_editions_by_artist: Dict[str, List[dict]]):
    """
    Persist per-edition scan data to scan_editions for Library and Tag Fixer to use.
    Call after a scan completes (or is stopped) so last_completed_scan_id can be used to read from this table.
    """
    import json
    mode = _get_library_mode()
    cache_map = _load_files_album_scan_cache_map() if mode == "files" else {}
    con = sqlite3.connect(str(STATE_DB_FILE), timeout=30)
    cur = con.cursor()
    cur.execute("DELETE FROM scan_editions WHERE scan_id = ?", (scan_id,))
    row_count = 0
    for artist, editions_list in all_editions_by_artist.items():
        for e in editions_list:
            folder = e.get("folder")
            meta = e.get("meta", {})
            # has_cover: same logic as stats loop
            has_cover = 0
            if folder:
                folder_path = Path(folder) if not isinstance(folder, Path) else folder
                cover_patterns = ["cover.*", "folder.*", "album.*", "artwork.*", "front.*"]
                for pattern in cover_patterns:
                    matches = list(folder_path.glob(pattern))
                    image_matches = [f for f in matches if f.suffix.lower() in ['.jpg', '.jpeg', '.png', '.webp', '.gif']]
                    if image_matches:
                        has_cover = 1
                        break
            ordered_paths: list[Path] = []
            if mode == "files" and folder:
                try:
                    folder_path = Path(folder) if not isinstance(folder, Path) else folder
                    ordered_paths = _files_collect_ordered_audio_paths(folder_path, e.get("ordered_paths") or [])
                except Exception:
                    ordered_paths = []
                # Refresh tags from live files so required-tags reflect post-fix state
                try:
                    meta = dict(meta or {})
                    if ordered_paths:
                        live_tags = extract_tags(ordered_paths[0]) or {}
                        if live_tags:
                            meta.update(live_tags)
                except Exception:
                    pass
            # missing_required_tags (REQUIRED_TAGS from Settings = source of truth)
            edition_for_required = e
            if mode == "files" and not (edition_for_required.get("tracks") or []):
                derived_tracks = [
                    {"title": p.stem or f"Track {i + 1}", "idx": i + 1}
                    for i, p in enumerate(ordered_paths)
                ]
                edition_for_required = dict(e)
                edition_for_required["tracks"] = derived_tracks
            missing_required = _check_required_tags(meta, REQUIRED_TAGS, edition=edition_for_required)
            missing_required_json = json.dumps(missing_required) if missing_required else None
            folder_str = str(folder) if folder else ""
            fmt_text = get_primary_format(Path(folder_str)) if folder_str else ""
            if mode == "files":
                folder_key = ""
                try:
                    if folder:
                        folder_key = _album_folder_cache_key(Path(folder))
                except Exception:
                    folder_key = ""
                cached = cache_map.get(folder_key) or {}
                identity_fields = _extract_files_identity_fields(tags=meta, edition=e, cached=cached)
                mbid = identity_fields["musicbrainz_id"]
                discogs_release_id = identity_fields["discogs_release_id"]
                lastfm_album_mbid = identity_fields["lastfm_album_mbid"]
                bandcamp_album_url = identity_fields["bandcamp_album_url"]
                metadata_source = identity_fields["metadata_source"]
            else:
                mbid = (e.get("musicbrainz_id") or meta.get("musicbrainz_releasegroupid") or meta.get("musicbrainz_id") or "")
                mbid = (mbid.strip() if isinstance(mbid, str) else str(mbid or "").strip()) or ""
                discogs_release_id = str(
                    e.get("discogs_release_id")
                    or meta.get("discogs_release_id")
                    or ""
                ).strip()
                lastfm_album_mbid = str(
                    e.get("lastfm_album_mbid")
                    or meta.get("lastfm_album_mbid")
                    or ""
                ).strip()
                bandcamp_album_url = str(
                    e.get("bandcamp_album_url")
                    or meta.get("bandcamp_album_url")
                    or ""
                ).strip()
                metadata_source = _normalize_identity_provider(
                    str(
                        e.get("primary_metadata_source")
                        or e.get("metadata_source")
                        or meta.get("primary_metadata_source")
                        or meta.get(PMDA_MATCH_PROVIDER_TAG)
                        or ""
                    )
                )
            cur.execute("""
                INSERT INTO scan_editions
                (scan_id, artist, album_id, title_raw, folder, fmt_text, br, sr, bd, meta_json, musicbrainz_id,
                 is_broken, expected_track_count, actual_track_count, missing_indices, has_cover, missing_required_tags,
                 discogs_release_id, lastfm_album_mbid, bandcamp_album_url, metadata_source)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                scan_id,
                artist,
                e.get("album_id"),
                e.get("title_raw", ""),
                folder_str,
                fmt_text,
                e.get("br") or 0,
                e.get("sr") or 0,
                e.get("bd") or 0,
                json.dumps(meta, default=str),
                mbid,
                1 if e.get("is_broken") else 0,
                e.get("expected_track_count"),
                e.get("actual_track_count") or len(e.get("tracks", [])),
                json.dumps(e.get("missing_indices", [])),
                has_cover,
                missing_required_json,
                discogs_release_id,
                lastfm_album_mbid,
                bandcamp_album_url,
                metadata_source,
            ))
            row_count += 1
    con.commit()
    con.close()
    logging.debug("save_scan_editions_to_db: scan_id=%s, %d edition rows", scan_id, row_count)


def save_scan_artist_to_db(artist_name: str, groups: List[dict]) -> int:
    """
    Insert one artist's duplicate groups into duplicates_best and duplicates_loser.
    Skips groups without best/losers (e.g. needs_ai not yet processed). Returns count of groups saved.
    """
    con = sqlite3.connect(str(STATE_DB_FILE), timeout=30)
    cur = con.cursor()
    saved_count = 0
    for g in groups:
        if "best" not in g or "losers" not in g:
            continue
        saved_count += 1
        best = g["best"]
        best_folder_path = path_for_fs_access(Path(best["folder"])) if best.get("folder") else None
        best_size_mb = (safe_folder_size(best_folder_path) // (1024 * 1024)) if best_folder_path else 0
        best_track_count = len(best.get("tracks", []))
        used_ai = bool(best.get("used_ai", False))
        ai_provider = best.get("ai_provider") or ""
        ai_model = best.get("ai_model") or ""
        if used_ai and (not ai_provider or not ai_model):
            mod = sys.modules[__name__]
            ai_provider = ai_provider or (getattr(mod, "AI_PROVIDER", None) or "")
            ai_model = ai_model or (getattr(mod, "RESOLVED_MODEL", None) or getattr(mod, "OPENAI_MODEL", None) or "")
        try:
            evidence_json = json.dumps(best.get("dupe_evidence", []))
        except Exception:
            evidence_json = "[]"
        cur.execute("""
              INSERT OR IGNORE INTO duplicates_best
                (artist, album_id, title_raw, album_norm, folder,
                 fmt_text, br, sr, bd, dur, discs, rationale, merge_list, ai_used, meta_json, ai_provider, ai_model, evidence_json, size_mb, track_count, match_verified_by_ai)
              VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
          """, (
              artist_name,
              best["album_id"],
              best["title_raw"],
              best["album_norm"],
              str(best["folder"]),
              get_primary_format(Path(best["folder"])),
              best["br"],
              best["sr"],
              best["bd"],
              best["dur"],
              best["discs"],
              best.get("rationale", ""),
              json.dumps(best.get("merge_list", [])),
              int(used_ai),
              json.dumps(best.get("meta", {})),
              ai_provider,
              ai_model,
              evidence_json,
              best_size_mb,
              best_track_count,
              1 if best.get("match_verified_by_ai") else 0,
          ))
        for e in g["losers"]:
            size_mb = folder_size(e["folder"]) // (1024 * 1024)
            cur.execute("""
                INSERT INTO duplicates_loser
                  (artist, album_id, loser_album_id, folder, fmt_text, br, sr, bd, size_mb)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                artist_name,
                best["album_id"],
                e.get("album_id"),
                str(e["folder"]),
                get_primary_format(e["folder"]),
                e["br"],
                e["sr"],
                e["bd"],
                size_mb,
            ))
    con.commit()
    con.close()
    return saved_count


def save_scan_editions_artist_to_db(scan_id: int, artist_name: str, editions_list: List[dict]) -> int:
    """
    Insert one artist's editions into scan_editions (no DELETE). Returns row count inserted.
    """
    con = sqlite3.connect(str(STATE_DB_FILE), timeout=30)
    cur = con.cursor()
    row_count = 0
    for e in editions_list:
        folder = e.get("folder")
        meta = dict(e.get("meta", {}))
        if e.get("primary_metadata_source"):
            meta["primary_metadata_source"] = e["primary_metadata_source"]
        if e.get("mb_submission_payload"):
            meta["mb_submission_payload"] = e["mb_submission_payload"]
        has_cover = 0
        if folder:
            folder_path = Path(folder) if not isinstance(folder, Path) else folder
            cover_patterns = ["cover.*", "folder.*", "album.*", "artwork.*", "front.*"]
            for pattern in cover_patterns:
                matches = list(folder_path.glob(pattern))
                image_matches = [f for f in matches if f.suffix.lower() in [".jpg", ".jpeg", ".png", ".webp", ".gif"]]
                if image_matches:
                    has_cover = 1
                    break
        missing_required = _check_required_tags(meta, REQUIRED_TAGS, edition=e)
        try:
            missing_required_json = json.dumps(missing_required, default=str) if missing_required else None
        except (TypeError, ValueError):
            missing_required_json = None
        folder_str = str(folder) if folder else ""
        fmt_text = get_primary_format(Path(folder_str)) if folder_str else ""
        try:
            meta_json_str = json.dumps(meta, default=str)
        except (TypeError, ValueError):
            meta_json_str = "{}"
        mbid = (e.get("musicbrainz_id") or meta.get("musicbrainz_releasegroupid") or meta.get("musicbrainz_id") or "")
        mbid = (mbid.strip() if isinstance(mbid, str) else str(mbid or "").strip()) or ""
        discogs_release_id = str(
            e.get("discogs_release_id")
            or meta.get("discogs_release_id")
            or ""
        ).strip()
        lastfm_album_mbid = str(
            e.get("lastfm_album_mbid")
            or meta.get("lastfm_album_mbid")
            or ""
        ).strip()
        bandcamp_album_url = str(
            e.get("bandcamp_album_url")
            or meta.get("bandcamp_album_url")
            or ""
        ).strip()
        metadata_source = _normalize_identity_provider(
            str(
                e.get("primary_metadata_source")
                or e.get("metadata_source")
                or meta.get("primary_metadata_source")
                or meta.get(PMDA_MATCH_PROVIDER_TAG)
                or ""
            )
        )
        cur.execute("""
            INSERT INTO scan_editions
            (scan_id, artist, album_id, title_raw, folder, fmt_text, br, sr, bd, meta_json, musicbrainz_id,
             is_broken, expected_track_count, actual_track_count, missing_indices, has_cover, missing_required_tags,
             discogs_release_id, lastfm_album_mbid, bandcamp_album_url, metadata_source)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        """, (
            scan_id,
            artist_name,
            e.get("album_id"),
            e.get("title_raw", ""),
            folder_str,
            fmt_text,
            e.get("br") or 0,
            e.get("sr") or 0,
            e.get("bd") or 0,
            meta_json_str,
            mbid,
            1 if e.get("is_broken") else 0,
            e.get("expected_track_count"),
            e.get("actual_track_count") or len(e.get("tracks", [])),
            json.dumps(e.get("missing_indices", [])),
            has_cover,
            missing_required_json,
            discogs_release_id,
            lastfm_album_mbid,
            bandcamp_album_url,
            metadata_source,
        ))
        row_count += 1
    con.commit()
    con.close()
    return row_count


def update_scan_history_incremental(
    scan_id: int,
    artists_processed: int,
    duplicates_found: int,
    duplicate_groups_count: int,
    total_duplicates_count: int,
    broken_albums_count: int,
    missing_albums_count: int = 0,
    albums_without_artist_image: int = 0,
    albums_without_album_image: int = 0,
    albums_without_complete_tags: int = 0,
    albums_without_mb_id: int = 0,
    albums_without_artist_mb_id: int = 0,
) -> None:
    """
    Update the running scan_history row with current counters so UI can show partial progress.
    Only updates rows with status = 'running'.
    """
    try:
        con = sqlite3.connect(str(STATE_DB_FILE), timeout=10)
        cur = con.cursor()
        cur.execute(
            """
            UPDATE scan_history
            SET artists_processed = ?,
                duplicates_found = ?,
                duplicate_groups_count = ?,
                total_duplicates_count = ?,
                broken_albums_count = ?,
                missing_albums_count = ?,
                albums_without_artist_image = ?,
                albums_without_album_image = ?,
                albums_without_complete_tags = ?,
                albums_without_mb_id = ?,
                albums_without_artist_mb_id = ?
            WHERE scan_id = ? AND status = 'running'
            """,
            (
                artists_processed,
                duplicates_found,
                duplicate_groups_count,
                total_duplicates_count,
                broken_albums_count,
                missing_albums_count,
                albums_without_artist_image,
                albums_without_album_image,
                albums_without_complete_tags,
                albums_without_mb_id,
                albums_without_artist_mb_id,
                scan_id,
            ),
        )
        con.commit()
        con.close()
    except Exception as e:
        logging.debug("update_scan_history_incremental failed: %s", e)


def save_scan_to_db(scan_results: Dict[str, List[dict]]):
    """
    Given a dict of { artist_name: [group_dicts...] }, clear duplicates tables and re‚Äêpopulate them.
    """
    import sqlite3, json

    # (Removed: filtering of invalid editions; already purged upstream)
    con = sqlite3.connect(str(STATE_DB_FILE))
    cur = con.cursor()

    # 1) Clear both duplicates tables
    cur.execute("DELETE FROM duplicates_loser")
    cur.execute("DELETE FROM duplicates_best")

    # 2) Re-insert all scan results (skip groups that have no best/losers, e.g. needs_ai not yet processed)
    saved_count = 0
    skipped_count = 0
    saved_with_ai = 0
    for artist, groups in scan_results.items():
        for g in groups:
            if "best" not in g or "losers" not in g:
                skipped_count += 1
                logging.debug("save_scan_to_db: skipping group without best/losers (artist=%s)", artist)
                continue
            saved_count += 1
            best = g["best"]
            if best.get("used_ai"):
                saved_with_ai += 1
            # Persist size_mb and track_count so Unduper shows them after reload
            best_folder_path = path_for_fs_access(Path(best["folder"])) if best.get("folder") else None
            best_size_mb = (safe_folder_size(best_folder_path) // (1024 * 1024)) if best_folder_path else 0
            best_track_count = len(best.get("tracks", []))
            # When used_ai, ensure ai_provider/ai_model are set (e.g. from cache they may be empty)
            used_ai = bool(best.get("used_ai", False))
            ai_provider = best.get("ai_provider") or ""
            ai_model = best.get("ai_model") or ""
            if used_ai and (not ai_provider or not ai_model):
                mod = sys.modules[__name__]
                ai_provider = ai_provider or (getattr(mod, "AI_PROVIDER", None) or "")
                ai_model = ai_model or (getattr(mod, "RESOLVED_MODEL", None) or getattr(mod, "OPENAI_MODEL", None) or "")
            try:
                evidence_json = json.dumps(best.get("dupe_evidence", []))
            except Exception:
                evidence_json = "[]"
            # Best edition
            cur.execute("""
                  INSERT OR IGNORE INTO duplicates_best
                    (artist, album_id, title_raw, album_norm, folder,
                     fmt_text, br, sr, bd, dur, discs, rationale, merge_list, ai_used, meta_json, ai_provider, ai_model, evidence_json, size_mb, track_count, match_verified_by_ai)
                  VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
              """, (
                  artist,
                  best['album_id'],
                  best['title_raw'],
                  best['album_norm'],
                  str(best['folder']),
                  get_primary_format(Path(best['folder'])),
                  best['br'],
                  best['sr'],
                  best['bd'],
                  best['dur'],
                  best['discs'],
                  best.get('rationale', ''),
                  json.dumps(best.get('merge_list', [])),
                  int(used_ai),
                  json.dumps(best.get('meta', {})),
                  ai_provider,
                  ai_model,
                  evidence_json,
                  best_size_mb,
                  best_track_count,
                  1 if best.get('match_verified_by_ai') else 0,
              ))

            # All "loser" editions
            for e in g['losers']:
                size_mb = folder_size(e['folder']) // (1024 * 1024)
                cur.execute("""
                    INSERT INTO duplicates_loser
                      (artist, album_id, loser_album_id, folder, fmt_text, br, sr, bd, size_mb)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    artist,
                    best['album_id'],
                    e.get('album_id'),
                    str(e['folder']),
                    get_primary_format(e['folder']),
                    e['br'],
                    e['sr'],
                    e['bd'],
                    size_mb
                ))

    # 3) Commit & close
    con.commit()
    con.close()
    if saved_count or skipped_count:
        logging.info(
            "save_scan_to_db: saved %d group(s) (%d with AI), skipped %d without best/losers",
            saved_count, saved_with_ai, skipped_count,
        )
        # Expose duplicate decision stats for summary_json
        try:
            with lock:
                state["scan_duplicate_groups_saved"] = int(saved_count)
                state["scan_duplicate_groups_ai_saved"] = int(saved_with_ai)
                state["scan_duplicate_groups_skipped"] = int(skipped_count)
        except Exception:
            # Telemetry only; never break scan on state update failure
            pass


def _remove_dedupe_group_from_db(artist: str, best_album_id: int, loser_album_ids: List[int]) -> None:
    """Remove one duplicate group from DB after it has been successfully moved to /dupes."""
    try:
        con = sqlite3.connect(str(STATE_DB_FILE))
        cur = con.cursor()
        cur.execute("DELETE FROM duplicates_best WHERE artist = ? AND album_id = ?", (artist, best_album_id))
        for aid in loser_album_ids:
            cur.execute("DELETE FROM duplicates_loser WHERE artist = ? AND album_id = ?", (artist, aid))
        con.commit()
        con.close()
    except Exception as e:
        logging.warning("_remove_dedupe_group_from_db failed for %s / %s: %s", artist, best_album_id, e)


def load_scan_from_db() -> Dict[str, List[dict]]:
    """
    Read the most-recent duplicate-scan from STATE_DB_FILE and rebuild the
    in-memory structure used by the Web UI.

    Returns
    -------
    dict
        { artist_name : [ group_dict, ... ] }
    """
    import json
    try:
        con = sqlite3.connect(str(STATE_DB_FILE))
        cur = con.cursor()

        # ---- 1) Best editions -------------------------------------------------
        cur.execute("PRAGMA table_info(duplicates_best)")
        best_cols = {r[1] for r in cur.fetchall()}
        has_match_verified = "match_verified_by_ai" in best_cols
        has_evidence_json = "evidence_json" in best_cols
        cur.execute(
            """
            SELECT artist, album_id, title_raw, album_norm, folder,
                   fmt_text, br, sr, bd, dur, discs, rationale, merge_list, ai_used, meta_json,
                   ai_provider, ai_model
            """ + (", evidence_json" if has_evidence_json else "") + """
                   , size_mb, track_count
            """ + (", match_verified_by_ai" if has_match_verified else "") + """
            FROM   duplicates_best
            """
        )
        best_rows = cur.fetchall()

        # ---- 2) Loser editions -----------------------------------------------
        cur.execute("PRAGMA table_info(duplicates_loser)")
        loser_cols = {r[1] for r in cur.fetchall()}
        has_loser_album_id = "loser_album_id" in loser_cols
        if has_loser_album_id:
            cur.execute(
                """
                SELECT artist, album_id, loser_album_id, folder, fmt_text, br, sr, bd, size_mb
                FROM   duplicates_loser
                """
            )
        else:
            cur.execute(
                """
                SELECT artist, album_id, folder, fmt_text, br, sr, bd, size_mb
                FROM   duplicates_loser
                """
            )
        loser_rows = cur.fetchall()
        con.close()
    except sqlite3.OperationalError as e:
        # If the user wiped state.db while the app is running, tables may be missing.
        # Recreate schema and return an empty result instead of 500.
        if "no such table" in str(e).lower():
            logging.warning("load_scan_from_db: missing table in state.db (%s); reinitializing DB and returning empty scan.", e)
            try:
                init_state_db()
            except Exception:
                logging.exception("init_state_db failed while handling missing tables in load_scan_from_db")
            return {}
        raise

    # Map losers by (artist, album_id) for quick lookup. album_id in table = best (group key).
    # loser_album_id = Plex metadata_item id of this edition (for tracks/title/path).
    loser_map: Dict[tuple, List[dict]] = defaultdict(list)
    for row in loser_rows:
        if has_loser_album_id:
            artist, aid, loser_aid, folder, fmt, br, sr, bd, size_mb = row[:9]
            edition_album_id = loser_aid if loser_aid is not None else aid
        else:
            artist, aid, folder, fmt, br, sr, bd, size_mb = row[:8]
            edition_album_id = aid
        loser_map[(artist, aid)].append(
            {
                "folder": Path(folder) if folder else None,
                "fmt": fmt,
                "br": br or 0,
                "sr": sr or 0,
                "bd": bd or 0,
                "size": size_mb,
                "album_id": edition_album_id,
                "artist": artist,
                "title_raw": None,
            }
        )

    results: Dict[str, List[dict]] = defaultdict(list)

    for row in best_rows:
        (artist, aid, title_raw, album_norm, folder, fmt_txt, br, sr, bd, dur, discs,
         rationale, merge_list_json, ai_used, meta_json) = row[:15]
        idx = 15
        ai_provider = (row[idx] or "") if len(row) > idx else ""
        idx += 1
        ai_model = (row[idx] or "") if len(row) > idx else ""
        idx += 1
        evidence_raw = None
        if has_evidence_json:
            evidence_raw = row[idx] if len(row) > idx else None
            idx += 1
        size_mb = row[idx] if len(row) > idx else None
        idx += 1
        track_count = row[idx] if len(row) > idx else None
        idx += 1
        match_verified_by_ai = bool(row[idx]) if has_match_verified and len(row) > idx else False
        try:
            dupe_evidence = json.loads(evidence_raw) if evidence_raw else []
            if not isinstance(dupe_evidence, list):
                dupe_evidence = []
        except Exception:
            dupe_evidence = []

        best_entry = {
            "album_id": aid,
            "title_raw": title_raw,
            "album_norm": album_norm,
            "folder": Path(folder),
            "fmt_text": fmt_txt,
            "br": br,
            "sr": sr,
            "bd": bd,
            "dur": dur,
            "discs": discs,
            "rationale": rationale,
            "merge_list": json.loads(merge_list_json) if merge_list_json else [],
            "used_ai": bool(ai_used),
            "meta": json.loads(meta_json or "{}"),
            "ai_provider": ai_provider,
            "ai_model": ai_model,
            "dupe_evidence": dupe_evidence,
            "size_mb": size_mb,
            "track_count": track_count,
            "match_verified_by_ai": match_verified_by_ai,
        }

        losers = loser_map.get((artist, aid), [])

        # Some loser rows still need the readable title; fetch from Plex by this edition's album_id.
        for l in losers:
            if l["title_raw"] is None:
                db_plx = plex_connect()
                title = album_title(db_plx, l["album_id"])
                l["title_raw"] = title or ""
                db_plx.close()

        results[artist].append(
            {
                "artist": artist,
                "album_id": aid,
                "best": best_entry,
                "losers": losers,
            }
        )

    return results

def clear_db_on_new_scan():
    """
    When a user triggers "Start New Scan," wipe prior duplicates from memory.
    The DB will be cleared and repopulated only once the scan completes.
    """
    with lock:
        state["duplicates"].clear()

def _get_library_mode() -> str:
    """
    Return the active library mode.
    PMDA now runs in Files mode only.
    """
    mode = (LIBRARY_MODE or "files").strip().lower()
    if mode != "files":
        logging.warning("LIBRARY_MODE '%s' ignored; forcing 'files' mode.", mode)
    return "files"


def _extract_musicbrainz_id_from_meta(meta: dict | None) -> str:
    """Return normalized MBID-like string from tag/meta dict (empty string when absent)."""
    m = meta or {}
    for key in (
        "musicbrainz_releasegroupid",
        "musicbrainz_release_group_id",
        "musicbrainz_releaseid",
        "musicbrainz_release_id",
        "musicbrainz_id",
        "musicbrainz_albumid",
    ):
        v = m.get(key)
        if v is None:
            continue
        s = str(v).strip()
        if s:
            return s
    return ""


def _extract_files_identity_fields(
    *,
    tags: dict | None = None,
    edition: dict | None = None,
    cached: dict | None = None,
) -> dict:
    """
    Resolve album identity in Files mode.
    Identity can come from:
    - MusicBrainz ID
    - Fallback provider match (Discogs / Last.fm / Bandcamp)
    """
    tags = dict(tags or {})
    edition = dict(edition or {})
    cached = dict(cached or {})

    mbid = (
        str(
            edition.get("musicbrainz_id")
            or _extract_musicbrainz_id_from_meta(tags)
            or cached.get("musicbrainz_id")
            or ""
        ).strip()
    )
    discogs_release_id = str(
        edition.get("discogs_release_id")
        or tags.get("discogs_release_id")
        or cached.get("discogs_release_id")
        or ""
    ).strip()
    lastfm_album_mbid = str(
        edition.get("lastfm_album_mbid")
        or tags.get("lastfm_album_mbid")
        or cached.get("lastfm_album_mbid")
        or ""
    ).strip()
    bandcamp_album_url = str(
        edition.get("bandcamp_album_url")
        or tags.get("bandcamp_album_url")
        or cached.get("bandcamp_album_url")
        or ""
    ).strip()
    metadata_source_raw = (
        edition.get("metadata_source")
        or edition.get("primary_metadata_source")
        or edition.get("provider_used")
        or edition.get("pmda_match_provider")
        or tags.get(PMDA_MATCH_PROVIDER_TAG)
        or cached.get("metadata_source")
        or cached.get("identity_provider")
        or ""
    )
    metadata_source = _normalize_identity_provider(str(metadata_source_raw or ""))
    identity_provider = ""
    if mbid:
        identity_provider = "musicbrainz"
    elif metadata_source in {"musicbrainz", "discogs", "lastfm", "bandcamp"}:
        identity_provider = metadata_source
    elif discogs_release_id:
        identity_provider = "discogs"
    elif lastfm_album_mbid:
        identity_provider = "lastfm"
    elif bandcamp_album_url:
        identity_provider = "bandcamp"
    elif bool(cached.get("has_identity")):
        identity_provider = _normalize_identity_provider(str(cached.get("identity_provider") or "")) or ""
    has_identity = bool(identity_provider)
    if not has_identity and bool(cached.get("has_identity")):
        has_identity = True
    return {
        "musicbrainz_id": mbid,
        "has_mbid": bool(mbid),
        "discogs_release_id": discogs_release_id,
        "lastfm_album_mbid": lastfm_album_mbid,
        "bandcamp_album_url": bandcamp_album_url,
        "metadata_source": metadata_source,
        "identity_provider": identity_provider,
        "has_identity": has_identity,
    }


def _album_folder_cache_key(folder: Path | str) -> str:
    """Stable key used by files_album_scan_cache for one album folder."""
    p = Path(folder)
    try:
        return str(p.resolve())
    except (OSError, RuntimeError):
        return str(p)


def _compute_album_fingerprint(paths: list[Path]) -> str:
    """
    Lightweight album fingerprint from path/name/size/mtime.
    Used by changed-only scans and fast incremental skip.
    """
    h = hashlib.blake2b(digest_size=20)
    for p in sorted(paths, key=lambda x: str(x)):
        try:
            st = p.stat()
            h.update(str(p.name).encode("utf-8", "replace"))
            h.update(b"|")
            h.update(str(int(st.st_size)).encode("ascii"))
            h.update(b"|")
            h.update(str(int(getattr(st, "st_mtime_ns", int(st.st_mtime * 1e9)))).encode("ascii"))
            h.update(b"\n")
        except OSError:
            h.update(str(p).encode("utf-8", "replace"))
            h.update(b"|missing\n")
    return h.hexdigest()


def _load_files_album_scan_cache_map() -> dict[str, dict]:
    """Load files album cache rows keyed by folder path."""
    out: dict[str, dict] = {}
    try:
        con = sqlite3.connect(str(STATE_DB_FILE), timeout=20)
        cur = con.cursor()
        cur.execute("PRAGMA table_info(files_album_scan_cache)")
        cols = {r[1] for r in cur.fetchall()}
        has_mbid_col = "musicbrainz_id" in cols
        has_identity_col = "has_identity" in cols
        has_identity_provider_col = "identity_provider" in cols
        has_discogs_col = "discogs_release_id" in cols
        has_lastfm_col = "lastfm_album_mbid" in cols
        has_bandcamp_col = "bandcamp_album_url" in cols
        has_metadata_source_col = "metadata_source" in cols

        cur.execute(
            f"""
            SELECT
                folder_path,
                fingerprint,
                has_cover,
                has_artist_image,
                has_complete_tags,
                has_mbid,
                {'musicbrainz_id' if has_mbid_col else "''"} AS musicbrainz_id,
                {'has_identity' if has_identity_col else '0'} AS has_identity,
                {'identity_provider' if has_identity_provider_col else "''"} AS identity_provider,
                {'discogs_release_id' if has_discogs_col else "''"} AS discogs_release_id,
                {'lastfm_album_mbid' if has_lastfm_col else "''"} AS lastfm_album_mbid,
                {'bandcamp_album_url' if has_bandcamp_col else "''"} AS bandcamp_album_url,
                {'metadata_source' if has_metadata_source_col else "''"} AS metadata_source,
                missing_required_tags,
                updated_at,
                artist_name,
                album_title
            FROM files_album_scan_cache
            """
        )
        for row in cur.fetchall():
            folder_path = row[0] or ""
            if not folder_path:
                continue
            try:
                missing_required = json.loads(row[13] or "[]")
                if not isinstance(missing_required, list):
                    missing_required = []
            except Exception:
                missing_required = []
            identity_provider = _normalize_identity_provider(str(row[8] or ""))
            metadata_source = _normalize_identity_provider(str(row[12] or ""))
            has_identity = bool(row[7]) or bool(identity_provider)
            out[folder_path] = {
                "fingerprint": row[1] or "",
                "has_cover": bool(row[2]),
                "has_artist_image": bool(row[3]),
                "has_complete_tags": bool(row[4]),
                "has_mbid": bool(row[5]),
                "musicbrainz_id": (row[6] or "").strip(),
                "has_identity": has_identity,
                "identity_provider": identity_provider,
                "discogs_release_id": (row[9] or "").strip(),
                "lastfm_album_mbid": (row[10] or "").strip(),
                "bandcamp_album_url": (row[11] or "").strip(),
                "metadata_source": metadata_source,
                "missing_required_tags": missing_required,
                "updated_at": float(row[14] or 0),
                "artist_name": row[15] or "",
                "album_title": row[16] or "",
            }
        con.close()
    except Exception:
        logging.debug("Failed to load files album scan cache", exc_info=True)
    return out


def _upsert_files_album_scan_cache_rows(rows: list[dict]) -> None:
    """Upsert rows into files_album_scan_cache."""
    if not rows:
        return
    try:
        con = sqlite3.connect(str(STATE_DB_FILE), timeout=30)
        cur = con.cursor()
        cur.executemany(
            """
            INSERT INTO files_album_scan_cache
            (folder_path, fingerprint, artist_name, album_title,
             has_cover, has_artist_image, has_complete_tags, has_mbid, has_identity,
             identity_provider, musicbrainz_id, discogs_release_id, lastfm_album_mbid,
             bandcamp_album_url, metadata_source,
             missing_required_tags, last_scan_id, updated_at)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ON CONFLICT(folder_path) DO UPDATE SET
              fingerprint=excluded.fingerprint,
              artist_name=excluded.artist_name,
              album_title=excluded.album_title,
              has_cover=excluded.has_cover,
              has_artist_image=excluded.has_artist_image,
              has_complete_tags=excluded.has_complete_tags,
              has_mbid=excluded.has_mbid,
              has_identity=excluded.has_identity,
              identity_provider=excluded.identity_provider,
              musicbrainz_id=excluded.musicbrainz_id,
              discogs_release_id=excluded.discogs_release_id,
              lastfm_album_mbid=excluded.lastfm_album_mbid,
              bandcamp_album_url=excluded.bandcamp_album_url,
              metadata_source=excluded.metadata_source,
              missing_required_tags=excluded.missing_required_tags,
              last_scan_id=excluded.last_scan_id,
              updated_at=excluded.updated_at
            """,
            [
                (
                    r.get("folder_path") or "",
                    r.get("fingerprint") or "",
                    r.get("artist_name") or "",
                    r.get("album_title") or "",
                    1 if r.get("has_cover") else 0,
                    1 if r.get("has_artist_image") else 0,
                    1 if r.get("has_complete_tags") else 0,
                    1 if r.get("has_mbid") else 0,
                    1 if r.get("has_identity") else 0,
                    _normalize_identity_provider(str(r.get("identity_provider") or "")),
                    r.get("musicbrainz_id") or "",
                    r.get("discogs_release_id") or "",
                    r.get("lastfm_album_mbid") or "",
                    r.get("bandcamp_album_url") or "",
                    _normalize_identity_provider(str(r.get("metadata_source") or "")),
                    json.dumps(r.get("missing_required_tags") or []),
                    r.get("last_scan_id"),
                    float(r.get("updated_at") or time.time()),
                )
                for r in rows
                if (r.get("folder_path") or "").strip()
            ],
        )
        con.commit()
        con.close()
    except Exception:
        logging.debug("Failed to upsert files album scan cache rows", exc_info=True)


def _refresh_files_album_scan_cache_from_editions(editions: list[dict], scan_id: int | None = None) -> None:
    """
    Refresh incremental files cache from a set of scanned editions.
    Called after each artist to keep changed-only scans accurate.
    """
    if not editions:
        return
    rows: list[dict] = []
    now = time.time()
    for e in editions:
        folder_raw = e.get("folder")
        if not folder_raw:
            continue
        folder = path_for_fs_access(Path(folder_raw))
        if not folder or not folder.exists():
            continue
        folder_key = _album_folder_cache_key(folder)
        ordered_paths = [Path(p) for p in (e.get("ordered_paths") or []) if Path(p).exists()]
        if not ordered_paths:
            try:
                ordered_paths = sorted(
                    [p for p in folder.rglob("*") if p.is_file() and AUDIO_RE.search(p.name)],
                    key=lambda x: str(x),
                )
            except Exception:
                ordered_paths = []
        # Always prefer live filesystem stats so the cache reflects the post-fix state
        # (mtime/size can change after tag/cover writes during improve step).
        computed_fingerprint = _compute_album_fingerprint(ordered_paths)
        fingerprint = computed_fingerprint or (e.get("fingerprint") or "").strip()
        tags = dict(e.get("meta") or {})
        if ordered_paths:
            try:
                live_tags = extract_tags(ordered_paths[0]) or {}
                if live_tags:
                    tags.update(live_tags)
            except Exception:
                if not tags:
                    tags = {}
        edition_for_required = e
        if not (edition_for_required.get("tracks") or []):
            # Keep incremental cache healthy when caller does not provide tracks:
            # derive a minimal track list from filesystem order.
            derived_tracks = [
                {"title": p.stem or f"Track {i + 1}", "idx": i + 1}
                for i, p in enumerate(ordered_paths)
            ]
            edition_for_required = dict(e)
            edition_for_required["tracks"] = derived_tracks
        missing_required = _check_required_tags(tags, REQUIRED_TAGS, edition=edition_for_required)
        has_cover = album_folder_has_cover(folder)
        has_artist_image = _artist_folder_has_image(folder.parent if folder.parent else folder)
        identity_fields = _extract_files_identity_fields(tags=tags, edition=e, cached={})
        mbid = identity_fields["musicbrainz_id"]
        rows.append(
            {
                "folder_path": folder_key,
                "fingerprint": fingerprint,
                "artist_name": e.get("artist") or e.get("artist_name") or "",
                "album_title": e.get("title_raw") or e.get("album_title") or folder.name,
                "has_cover": has_cover,
                "has_artist_image": has_artist_image,
                "has_complete_tags": len(missing_required) == 0,
                "has_mbid": bool(identity_fields["has_mbid"]),
                "has_identity": bool(identity_fields["has_identity"]),
                "identity_provider": identity_fields["identity_provider"],
                "musicbrainz_id": mbid,
                "discogs_release_id": identity_fields["discogs_release_id"],
                "lastfm_album_mbid": identity_fields["lastfm_album_mbid"],
                "bandcamp_album_url": identity_fields["bandcamp_album_url"],
                "metadata_source": identity_fields["metadata_source"],
                "missing_required_tags": missing_required,
                "last_scan_id": scan_id,
                "updated_at": now,
            }
        )
    _upsert_files_album_scan_cache_rows(rows)


def _files_collect_ordered_audio_paths(folder: Path, ordered_paths_raw: list | None = None) -> list[Path]:
    out: list[Path] = []
    seen: set[str] = set()
    for raw in (ordered_paths_raw or []):
        try:
            p = path_for_fs_access(Path(raw))
        except Exception:
            continue
        try:
            if p.exists() and p.is_file() and AUDIO_RE.search(p.name):
                sp = str(p.resolve())
                if sp in seen:
                    continue
                seen.add(sp)
                out.append(p)
        except Exception:
            continue
    if out:
        return out
    try:
        discovered = [p for p in folder.rglob("*") if p.is_file() and AUDIO_RE.search(p.name)]
    except Exception:
        discovered = []
    return sorted(discovered, key=lambda p: str(p))


def _files_track_value(track_obj, attr: str, fallback=None):
    if track_obj is None:
        return fallback
    if isinstance(track_obj, dict):
        if attr in track_obj:
            return track_obj.get(attr)
        if attr == "idx":
            return (
                track_obj.get("idx")
                or track_obj.get("index")
                or track_obj.get("track_num")
                or track_obj.get("track")
            )
        if attr == "disc":
            return track_obj.get("disc") or track_obj.get("disc_num")
        if attr == "dur":
            return (
                track_obj.get("dur")
                or track_obj.get("duration")
                or track_obj.get("duration_ms")
                or track_obj.get("duration_sec")
            )
        return fallback
    return getattr(track_obj, attr, fallback)


def _files_build_track_entries_from_item(item: dict, folder: Path) -> list[dict]:
    ordered_paths = _files_collect_ordered_audio_paths(folder, item.get("ordered_paths") or [])
    fallback_tracks = list(item.get("tracks") or [])
    br = _parse_int_loose(item.get("br"), 0)
    sr = _parse_int_loose(item.get("sr"), 0)
    bd = _parse_int_loose(item.get("bd"), 0)
    out: list[dict] = []
    for idx, p in enumerate(ordered_paths):
        track_obj = fallback_tracks[idx] if idx < len(fallback_tracks) else None
        title = (_files_track_value(track_obj, "title", "") or "").strip() or p.stem or f"Track {idx + 1}"
        disc_num = _parse_int_loose(_files_track_value(track_obj, "disc", 1), 1) or 1
        track_num = _parse_int_loose(_files_track_value(track_obj, "idx", idx + 1), idx + 1) or (idx + 1)
        raw_dur = _files_track_value(track_obj, "dur", 0)
        dur_sec = 0
        try:
            dur_num = float(raw_dur or 0)
            # Track helper uses milliseconds internally; accept seconds when obviously small.
            dur_sec = int(dur_num / 1000.0) if dur_num > 5000 else int(dur_num)
        except Exception:
            dur_sec = int(max(0.0, _parse_duration_seconds_loose(raw_dur, 0.0)))
        fmt = (p.suffix.lower().lstrip(".") or "UNKNOWN").upper()
        try:
            file_size = int(p.stat().st_size)
        except OSError:
            file_size = 0
        out.append(
            {
                "file_path": str(p),
                "title": title,
                "disc_num": disc_num,
                "track_num": track_num,
                "duration_sec": max(0, dur_sec),
                "format": fmt,
                "bitrate": br,
                "sample_rate": sr,
                "bit_depth": bd,
                "file_size_bytes": file_size,
            }
        )
    out.sort(key=lambda t: (int(t.get("disc_num") or 1), int(t.get("track_num") or 0), str(t.get("file_path") or "")))
    return out


def _clear_files_library_published_rows() -> int:
    try:
        con = sqlite3.connect(str(STATE_DB_FILE), timeout=20)
        cur = con.cursor()
        cur.execute("DELETE FROM files_library_published_albums")
        deleted = int(cur.rowcount or 0)
        con.commit()
        con.close()
        return deleted
    except Exception:
        logging.debug("Failed to clear files_library_published_albums", exc_info=True)
        return 0


def _reset_files_live_index_for_scan() -> None:
    """
    Start a fresh progressive Files library for a new full scan:
    - clear published rows in state.db
    - clear indexed artists/albums/tracks in PostgreSQL
    """
    deleted = _clear_files_library_published_rows()
    if not _files_pg_init_schema():
        logging.warning("Files live index reset: PostgreSQL schema unavailable")
        return
    acquired = files_index_lock.acquire(blocking=True)
    if not acquired:
        return
    try:
        conn = _files_pg_connect()
        if conn is None:
            logging.warning("Files live index reset: PostgreSQL unavailable")
            return
        try:
            with conn.transaction():
                with conn.cursor() as cur:
                    cur.execute("TRUNCATE TABLE files_tracks, files_albums, files_artists RESTART IDENTITY CASCADE")
                    _files_index_write_meta(cur, "artists", "0")
                    _files_index_write_meta(cur, "albums", "0")
                    _files_index_write_meta(cur, "tracks", "0")
                    _files_index_write_meta(cur, "last_reason", "scan_full_reset")
                    _files_index_write_meta(cur, "last_build_ts", str(int(time.time())))
        finally:
            conn.close()
        _files_cache_invalidate_all()
        _files_index_set_state(
            running=False,
            started_at=None,
            finished_at=time.time(),
            phase="idle",
            current_folder=None,
            folders_processed=0,
            total_folders=0,
            artists=0,
            albums=0,
            tracks=0,
            error=None,
        )
        logging.info("Files live index reset for new scan: cleared %d published row(s) and truncated PG index tables", deleted)
    except Exception as e:
        logging.warning("Files live index reset failed: %s", e)
    finally:
        files_index_lock.release()


def _upsert_files_library_published_rows(rows: list[dict]) -> int:
    if not rows:
        return 0
    filtered = [r for r in rows if (r.get("folder_path") or "").strip()]
    if not filtered:
        return 0
    try:
        con = sqlite3.connect(str(STATE_DB_FILE), timeout=30)
        cur = con.cursor()
        cur.executemany(
            """
            INSERT INTO files_library_published_albums (
                folder_path, scan_id, artist_name, artist_norm, album_title, title_norm,
                year, date_text, genre, label, tags_json, format, is_lossless,
                has_cover, cover_path, has_artist_image, artist_image_path,
                mb_identified, musicbrainz_release_group_id, discogs_release_id, lastfm_album_mbid,
                bandcamp_album_url, primary_metadata_source, track_count, total_duration_sec,
                is_broken, expected_track_count, actual_track_count, missing_indices_json,
                missing_required_tags_json, primary_tags_json, tracks_json, fingerprint, updated_at
            ) VALUES (
                ?, ?, ?, ?, ?, ?,
                ?, ?, ?, ?, ?, ?, ?,
                ?, ?, ?, ?,
                ?, ?, ?, ?,
                ?, ?, ?, ?,
                ?, ?, ?, ?,
                ?, ?, ?, ?, ?
            )
            ON CONFLICT(folder_path) DO UPDATE SET
                scan_id=excluded.scan_id,
                artist_name=excluded.artist_name,
                artist_norm=excluded.artist_norm,
                album_title=excluded.album_title,
                title_norm=excluded.title_norm,
                year=excluded.year,
                date_text=excluded.date_text,
                genre=excluded.genre,
                label=excluded.label,
                tags_json=excluded.tags_json,
                format=excluded.format,
                is_lossless=excluded.is_lossless,
                has_cover=excluded.has_cover,
                cover_path=excluded.cover_path,
                has_artist_image=excluded.has_artist_image,
                artist_image_path=excluded.artist_image_path,
                mb_identified=excluded.mb_identified,
                musicbrainz_release_group_id=excluded.musicbrainz_release_group_id,
                discogs_release_id=excluded.discogs_release_id,
                lastfm_album_mbid=excluded.lastfm_album_mbid,
                bandcamp_album_url=excluded.bandcamp_album_url,
                primary_metadata_source=excluded.primary_metadata_source,
                track_count=excluded.track_count,
                total_duration_sec=excluded.total_duration_sec,
                is_broken=excluded.is_broken,
                expected_track_count=excluded.expected_track_count,
                actual_track_count=excluded.actual_track_count,
                missing_indices_json=excluded.missing_indices_json,
                missing_required_tags_json=excluded.missing_required_tags_json,
                primary_tags_json=excluded.primary_tags_json,
                tracks_json=excluded.tracks_json,
                fingerprint=excluded.fingerprint,
                updated_at=excluded.updated_at
            """,
            [
                (
                    r.get("folder_path") or "",
                    r.get("scan_id"),
                    r.get("artist_name") or "Unknown Artist",
                    r.get("artist_norm") or "unknown artist",
                    r.get("album_title") or "Unknown Album",
                    r.get("title_norm") or "unknown album",
                    r.get("year"),
                    r.get("date_text") or "",
                    r.get("genre") or "",
                    r.get("label") or "",
                    r.get("tags_json") or "[]",
                    r.get("format") or "",
                    1 if r.get("is_lossless") else 0,
                    1 if r.get("has_cover") else 0,
                    r.get("cover_path") or "",
                    1 if r.get("has_artist_image") else 0,
                    r.get("artist_image_path") or "",
                    1 if r.get("mb_identified") else 0,
                    r.get("musicbrainz_release_group_id") or "",
                    r.get("discogs_release_id") or "",
                    r.get("lastfm_album_mbid") or "",
                    r.get("bandcamp_album_url") or "",
                    r.get("primary_metadata_source") or "",
                    int(r.get("track_count") or 0),
                    int(r.get("total_duration_sec") or 0),
                    1 if r.get("is_broken") else 0,
                    r.get("expected_track_count"),
                    int(r.get("actual_track_count") or 0),
                    r.get("missing_indices_json") or "[]",
                    r.get("missing_required_tags_json") or "[]",
                    r.get("primary_tags_json") or "{}",
                    r.get("tracks_json") or "[]",
                    r.get("fingerprint") or "",
                    float(r.get("updated_at") or time.time()),
                )
                for r in filtered
            ],
        )
        con.commit()
        con.close()
        return len(filtered)
    except Exception:
        logging.debug("Failed to upsert files_library_published_albums rows", exc_info=True)
        return 0


def _publish_files_library_artist_from_items(
    artist_name: str,
    items: list[dict],
    *,
    scan_id: int | None = None,
    results_by_album_id: dict[int, dict] | None = None,
) -> int:
    """
    Publish artist albums to files_library_published_albums after full per-artist flow.
    These rows are used as the source for progressive Files index rebuilds.
    """
    if not items:
        return 0
    rows: list[dict] = []
    now = time.time()
    results_by_album_id = results_by_album_id or {}
    for item in items:
        folder_raw = (item.get("folder") or "").strip()
        if not folder_raw:
            continue
        folder = path_for_fs_access(Path(folder_raw))
        if not folder.exists() or not folder.is_dir():
            continue
        track_entries = _files_build_track_entries_from_item(item, folder)
        if not track_entries:
            continue
        first_audio = None
        try:
            first_audio = path_for_fs_access(Path(track_entries[0]["file_path"]))
        except Exception:
            first_audio = None
        tags = dict(item.get("meta") or {})
        if first_audio and first_audio.exists():
            live_tags = extract_tags(first_audio) or {}
            if live_tags:
                tags.update(live_tags)
        title_fallback = (
            (item.get("album_title") or "").strip()
            or (item.get("title_raw") or "").strip()
            or folder.name.replace("_", " ").strip()
            or "Unknown Album"
        )
        artist_fallback = (item.get("artist") or "").strip() or (artist_name or "").strip() or "Unknown Artist"
        artist_resolved = _pick_album_artist_from_tag_dicts([tags], default=artist_fallback)
        album_resolved = _pick_album_title_from_tag_dicts([tags], fallback=title_fallback)
        album_resolved = _sanitize_album_title_display(album_resolved)
        label_resolved = _pick_album_label_from_tag_dicts([tags])
        artist_norm = " ".join((artist_resolved or "").split()).lower() or "unknown artist"
        title_norm = norm_album_for_dedup(album_resolved, normalize_parenthetical=True) or "unknown album"
        date_text = (tags.get("date") or tags.get("year") or "").strip()
        year = _parse_int_loose((date_text[:4] if date_text else tags.get("year")), 0) or None
        raw_genres = _split_genre_values(tags.get("genre") or "")
        inferred_genre = _infer_genre_from_bandcamp_tags(raw_genres) if raw_genres else None
        genre = inferred_genre if inferred_genre else ("; ".join(raw_genres[:6]) if raw_genres else "")
        fmt_counts: dict[str, int] = defaultdict(int)
        total_duration_sec = 0
        for tr in track_entries:
            fmt_counts[(tr.get("format") or "UNKNOWN").upper()] += 1
            total_duration_sec += int(tr.get("duration_sec") or 0)
        dominant_format = max(fmt_counts.items(), key=lambda x: x[1])[0] if fmt_counts else "UNKNOWN"
        cover_path = _first_cover_path(folder)
        has_cover = bool(cover_path and cover_path.is_file())
        artist_folder = _files_guess_artist_folder(folder, artist_resolved)
        artist_image_path = _first_artist_image_path(artist_folder) if artist_folder else None
        has_artist_image = bool(artist_image_path and artist_image_path.is_file())
        indices = [int(t.get("track_num") or 0) for t in track_entries if int(t.get("track_num") or 0) > 0]
        actual_track_count = len(track_entries)
        is_broken = False
        expected_track_count = None
        missing_indices: list[int] = []
        if indices:
            max_idx = max(indices)
            coverage = (actual_track_count / max_idx) if max_idx else 1.0
            # Skip broken detection when track numbering is obviously corrupt.
            if max_idx > max(120, actual_track_count * 3) and coverage < 0.5:
                is_broken = False
                expected_track_count = None
                missing_indices = []
            else:
                is_broken, _actual_count_from_indices, gaps = _detect_gaps_in_indices(indices)
                if is_broken:
                    expected_track_count = max_idx
                    for start_i, end_i in gaps:
                        if (end_i - start_i) > 2000:
                            continue
                        missing_indices.extend(list(range(start_i + 1, end_i)))
                        if len(missing_indices) > 5000:
                            missing_indices = missing_indices[:5000]
                            break
        mbid = (
            (item.get("musicbrainz_id") or "").strip()
            or _extract_musicbrainz_id_from_meta(tags)
            or ""
        )
        missing_required = _check_required_tags(
            tags,
            REQUIRED_TAGS,
            edition={"tracks": [{"title": t.get("title"), "index": t.get("track_num")} for t in track_entries]},
        )
        album_id = _parse_int_loose(item.get("album_id"), 0)
        result = results_by_album_id.get(album_id, {})
        pmda_provider = (tags.get(PMDA_MATCH_PROVIDER_TAG) or "").strip()
        rows.append(
            {
                "folder_path": _album_folder_cache_key(folder),
                "scan_id": scan_id,
                "artist_name": artist_resolved,
                "artist_norm": artist_norm,
                "album_title": album_resolved,
                "title_norm": title_norm,
                "year": year,
                "date_text": date_text[:32] if date_text else "",
                "genre": genre or "",
                "label": (label_resolved or "").strip(),
                "tags_json": json.dumps(raw_genres[:20]),
                "format": dominant_format,
                "is_lossless": dominant_format in _LOSSLESS_FORMATS,
                "has_cover": has_cover,
                "cover_path": str(cover_path) if cover_path else "",
                "has_artist_image": has_artist_image,
                "artist_image_path": str(artist_image_path) if artist_image_path else "",
                "mb_identified": bool(mbid),
                "musicbrainz_release_group_id": mbid,
                "discogs_release_id": (
                    result.get("discogs_release_id")
                    or item.get("discogs_release_id")
                    or ""
                ).strip(),
                "lastfm_album_mbid": (
                    result.get("lastfm_album_mbid")
                    or item.get("lastfm_album_mbid")
                    or ""
                ).strip(),
                "bandcamp_album_url": (
                    result.get("bandcamp_album_url")
                    or item.get("bandcamp_album_url")
                    or ""
                ).strip(),
                "primary_metadata_source": (
                    (
                        result.get("provider_used")
                        or result.get("pmda_match_provider")
                        or item.get("primary_metadata_source")
                        or item.get("metadata_source")
                        or pmda_provider
                        or ""
                    ).strip()
                ),
                "track_count": actual_track_count,
                "total_duration_sec": total_duration_sec,
                "is_broken": bool(is_broken),
                "expected_track_count": expected_track_count,
                "actual_track_count": actual_track_count,
                "missing_indices_json": json.dumps(missing_indices),
                "missing_required_tags_json": json.dumps(missing_required),
                "primary_tags_json": json.dumps(tags, default=str),
                "tracks_json": json.dumps(track_entries),
                "fingerprint": (item.get("fingerprint") or "").strip(),
                "updated_at": now,
            }
        )
    inserted = _upsert_files_library_published_rows(rows)
    if inserted:
        logging.debug("Published %d album(s) for artist '%s' to files_library_published_albums", inserted, artist_name)
    return inserted


def _rows_to_files_library_payload(rows: list[tuple]) -> tuple[dict[str, dict], list[dict], int]:
    artists_map: dict[str, dict] = {}
    albums_payload: list[dict] = []
    root_dirs = _files_root_dir_strings()
    for row in rows:
        folder_path = (row[0] or "").strip()
        if not folder_path:
            continue
        # Published rows can become stale when PMDA moves albums out of FILES_ROOTS (dupes/incomplete)
        # or when the user deletes folders. Never rebuild the live index with missing paths.
        try:
            album_folder_live = path_for_fs_access(Path(folder_path))
            if not album_folder_live.exists() or not album_folder_live.is_dir():
                continue
        except Exception:
            continue
        artist_name = (row[1] or "").strip() or "Unknown Artist"
        artist_norm = (row[2] or "").strip() or " ".join(artist_name.split()).lower() or "unknown artist"
        album_title = _sanitize_album_title_display((row[3] or "").strip()) or "Unknown Album"
        # Recompute local artist image from the actual folder structure to avoid
        # falsely assigning a single FILES_ROOT/artist.jpg to every artist in flat libraries.
        image_path = ""
        has_image = False
        try:
            album_folder = path_for_fs_access(Path(folder_path))
            artist_folder = _files_guess_artist_folder(album_folder, artist_name, root_dirs=root_dirs)
            local_img = _first_artist_image_path(artist_folder) if artist_folder else None
            if local_img and local_img.is_file():
                image_path = str(local_img)
                has_image = True
        except Exception:
            has_image = False
            image_path = ""
        if not has_image:
            # Fallback to the published stored path only when it exists and is not a FILES_ROOT-level image.
            stored_path = (row[15] or "").strip()
            stored_has = bool(row[14]) and bool(stored_path)
            if stored_has:
                try:
                    sp = path_for_fs_access(Path(stored_path))
                except Exception:
                    sp = Path(stored_path)
                try:
                    if sp.is_file() and not _files_is_files_root_dir(sp.parent, root_dirs=root_dirs):
                        # Extra guard: image should live under the album folder's ancestry.
                        try:
                            album_folder = path_for_fs_access(Path(folder_path))
                            if sp.parent == album_folder or sp.parent in album_folder.parents:
                                image_path = str(sp)
                                has_image = True
                        except Exception:
                            image_path = str(sp)
                            has_image = True
                except Exception:
                    pass
        if artist_norm not in artists_map:
            artists_map[artist_norm] = {
                "name": artist_name,
                "image_path": image_path or None,
                "has_image": has_image,
            }
        elif has_image and not artists_map[artist_norm].get("has_image"):
            artists_map[artist_norm]["image_path"] = image_path
            artists_map[artist_norm]["has_image"] = True
        try:
            tags_json = json.loads(row[9] or "[]") if row[9] else []
            if not isinstance(tags_json, list):
                tags_json = []
        except Exception:
            tags_json = []
        try:
            tracks = json.loads(row[26] or "[]") if row[26] else []
            if not isinstance(tracks, list):
                tracks = []
        except Exception:
            tracks = []
        albums_payload.append(
            {
                "artist_norm": artist_norm,
                "title": album_title,
                "title_norm": (row[4] or "").strip() or norm_album_for_dedup(album_title, normalize_parenthetical=True),
                "folder_path": folder_path,
                "year": row[5],
                "date_text": (row[6] or "").strip(),
                "genre": (row[7] or "").strip(),
                "label": (row[8] or "").strip(),
                "tags_json": json.dumps(tags_json),
                "format": (row[10] or "").strip(),
                "is_lossless": bool(row[11]),
                "has_cover": bool(row[12]),
                "cover_path": (row[13] or "").strip(),
                "mb_identified": bool(row[16]),
                "musicbrainz_release_group_id": (row[17] or "").strip(),
                "track_count": int(row[18] or 0),
                "total_duration_sec": int(row[19] or 0),
                "is_broken": bool(row[20]),
                "expected_track_count": row[21],
                "actual_track_count": int(row[22] or 0),
                "missing_indices_json": row[23] or "[]",
                "missing_required_tags_json": row[24] or "[]",
                "primary_tags_json": row[25] or "{}",
                "tracks": tracks,
                "discogs_release_id": (row[27] or "").strip(),
                "lastfm_album_mbid": (row[28] or "").strip(),
                "bandcamp_album_url": (row[29] or "").strip(),
                "metadata_source": _normalize_identity_provider((row[30] or "").strip()),
            }
        )
    return artists_map, albums_payload, len(albums_payload)


def _load_files_library_published_payload() -> tuple[dict[str, dict], list[dict], int]:
    """Load published albums from state.db as payload for Files PG index rebuild."""
    try:
        con = sqlite3.connect(str(STATE_DB_FILE), timeout=20)
        cur = con.cursor()
        cur.execute(
            """
            SELECT
                folder_path, artist_name, artist_norm, album_title, title_norm,
                year, date_text, genre, label, tags_json, format, is_lossless,
                has_cover, cover_path, has_artist_image, artist_image_path,
                mb_identified, musicbrainz_release_group_id, track_count, total_duration_sec,
                is_broken, expected_track_count, actual_track_count, missing_indices_json,
                missing_required_tags_json, primary_tags_json, tracks_json,
                discogs_release_id, lastfm_album_mbid, bandcamp_album_url, primary_metadata_source
            FROM files_library_published_albums
            ORDER BY lower(artist_name), lower(album_title), folder_path
            """
        )
        rows = cur.fetchall()
        con.close()
    except Exception:
        logging.debug("Failed to load files_library_published_albums", exc_info=True)
        return {}, [], 0
    return _rows_to_files_library_payload(rows)


def _load_files_library_published_payload_for_artist(artist_hint: str) -> tuple[dict[str, dict], list[dict], int]:
    """Load published payload for one artist (by normalized name)."""
    artist_name = str(artist_hint or "").strip()
    if not artist_name:
        return {}, [], 0
    artist_norm = " ".join(artist_name.split()).lower()
    artist_norm_alt = norm_album(artist_name or "") or artist_norm
    artist_like = "%" + " ".join(artist_name.lower().split()).replace("%", "").replace("_", "") + "%"
    try:
        con = sqlite3.connect(str(STATE_DB_FILE), timeout=20)
        cur = con.cursor()
        cur.execute(
            """
            SELECT
                folder_path, artist_name, artist_norm, album_title, title_norm,
                year, date_text, genre, label, tags_json, format, is_lossless,
                has_cover, cover_path, has_artist_image, artist_image_path,
                mb_identified, musicbrainz_release_group_id, track_count, total_duration_sec,
                is_broken, expected_track_count, actual_track_count, missing_indices_json,
                missing_required_tags_json, primary_tags_json, tracks_json,
                discogs_release_id, lastfm_album_mbid, bandcamp_album_url, primary_metadata_source
            FROM files_library_published_albums
            WHERE artist_norm = ?
               OR artist_norm = ?
               OR lower(artist_name) = lower(?)
               OR lower(artist_name) LIKE ?
            ORDER BY lower(album_title), folder_path
            """,
            (artist_norm, artist_norm_alt, artist_name, artist_like),
        )
        rows = cur.fetchall()
        con.close()
    except Exception:
        logging.debug("Failed to load files_library_published_albums for artist %s", artist_name, exc_info=True)
        return {}, [], 0
    return _rows_to_files_library_payload(rows)


def _compute_scan_source_signature(mode: str, scan_type: str) -> str:
    """Build a stable signature describing the scan source and scope."""
    if mode == "files":
        roots = []
        for r in (FILES_ROOTS or []):
            if not r:
                continue
            try:
                roots.append(str(Path(r).resolve()))
            except (OSError, RuntimeError):
                roots.append(str(r))
        skips = []
        for s in (SKIP_FOLDERS or []):
            if not s:
                continue
            try:
                skips.append(str(Path(s).resolve()))
            except (OSError, RuntimeError):
                skips.append(str(s))
        payload = {
            "mode": "files",
            "scan_type": scan_type,
            "roots": sorted(roots),
            "skip_folders": sorted(skips),
        }
    else:
        payload = {
            "mode": "plex",
            "scan_type": scan_type,
            "section_ids": sorted(int(x) for x in (SECTION_IDS or [])),
            "path_map": sorted((k, str(v)) for k, v in (PATH_MAP or {}).items()),
        }
    raw = json.dumps(payload, sort_keys=True, separators=(",", ":"))
    return hashlib.sha256(raw.encode("utf-8")).hexdigest()


def _compute_artist_signature(
    mode: str,
    artist_name: str,
    album_ids: list[int],
    files_editions_by_album_id: dict[int, dict] | None = None,
) -> str:
    """Compute artist signature so resume can detect new/changed albums."""
    parts: list[str] = []
    if mode == "files":
        files_map = files_editions_by_album_id or {}
        for aid in sorted(album_ids):
            fe = files_map.get(aid) or {}
            folder = fe.get("folder")
            folder_key = _album_folder_cache_key(folder) if folder else str(aid)
            fp = (fe.get("fingerprint") or "").strip()
            parts.append(f"{folder_key}|{fp}")
    else:
        parts = [str(int(a)) for a in sorted(album_ids)]
    payload = {
        "artist": (artist_name or "").strip().lower(),
        "album_count": len(album_ids),
        "parts": parts,
    }
    raw = json.dumps(payload, sort_keys=True, separators=(",", ":"))
    return hashlib.sha256(raw.encode("utf-8")).hexdigest()


def _prepare_resume_scan_artists(
    mode: str,
    scan_type: str,
    artists_merged: list[tuple[int, str, list[int]]],
    files_editions_by_album_id: dict[int, dict] | None = None,
) -> tuple[str, list[tuple[int, str, list[int]]], int, int]:
    """
    Create/reuse a persistent resume run and return artists that still need processing.
    Returns: (run_id, artists_to_scan, skipped_artists, skipped_albums).
    """
    now = time.time()
    source_signature = _compute_scan_source_signature(mode, scan_type)
    artist_signatures: dict[str, str] = {}
    for _aid, artist_name, album_ids in artists_merged:
        artist_signatures[artist_name] = _compute_artist_signature(
            mode,
            artist_name,
            album_ids,
            files_editions_by_album_id=files_editions_by_album_id,
        )

    run_id: str | None = None
    artists_to_scan: list[tuple[int, str, list[int]]] = []
    skipped_artists = 0
    skipped_albums = 0

    con = sqlite3.connect(str(STATE_DB_FILE), timeout=30)
    cur = con.cursor()
    cur.execute(
        """
        SELECT run_id, status
        FROM scan_resume_runs
        WHERE source_signature = ? AND mode = ? AND scan_type = ?
        ORDER BY updated_at DESC
        LIMIT 1
        """,
        (source_signature, mode, scan_type),
    )
    prev = cur.fetchone()
    if prev and (prev[1] or "").strip().lower() != "completed":
        run_id = prev[0]
        cur.execute(
            "SELECT artist_name, artist_signature, status FROM scan_resume_artists WHERE run_id = ?",
            (run_id,),
        )
        prev_artists = {
            (r[0] or ""): {
                "artist_signature": (r[1] or ""),
                "status": (r[2] or "pending").strip().lower(),
            }
            for r in cur.fetchall()
        }
        for artist_id, artist_name, album_ids in artists_merged:
            sig = artist_signatures.get(artist_name, "")
            prev_row = prev_artists.get(artist_name)
            is_done_same_signature = bool(
                prev_row
                and prev_row.get("status") == "done"
                and prev_row.get("artist_signature") == sig
            )
            if is_done_same_signature:
                skipped_artists += 1
                skipped_albums += len(album_ids)
                continue
            artists_to_scan.append((artist_id, artist_name, album_ids))
            cur.execute(
                """
                INSERT INTO scan_resume_artists
                (run_id, artist_name, artist_signature, status, album_count, updated_at, error)
                VALUES (?, ?, ?, 'pending', ?, ?, NULL)
                ON CONFLICT(run_id, artist_name) DO UPDATE SET
                  artist_signature=excluded.artist_signature,
                  status='pending',
                  album_count=excluded.album_count,
                  updated_at=excluded.updated_at,
                  error=NULL
                """,
                (run_id, artist_name, sig, len(album_ids), now),
            )
        cur.execute(
            "UPDATE scan_resume_runs SET updated_at = ?, status = 'running' WHERE run_id = ?",
            (now, run_id),
        )
    else:
        run_id = uuid.uuid4().hex
        cur.execute(
            """
            INSERT INTO scan_resume_runs
            (run_id, created_at, updated_at, mode, scan_type, source_signature, status, scan_id)
            VALUES (?, ?, ?, ?, ?, ?, 'running', NULL)
            """,
            (run_id, now, now, mode, scan_type, source_signature),
        )
        artists_to_scan = list(artists_merged)
        cur.executemany(
            """
            INSERT INTO scan_resume_artists
            (run_id, artist_name, artist_signature, status, album_count, updated_at, error)
            VALUES (?, ?, ?, 'pending', ?, ?, NULL)
            """,
            [
                (
                    run_id,
                    artist_name,
                    artist_signatures.get(artist_name, ""),
                    len(album_ids),
                    now,
                )
                for _artist_id, artist_name, album_ids in artists_merged
            ],
        )
    con.commit()
    con.close()
    return run_id, artists_to_scan, skipped_artists, skipped_albums


def _has_unfinished_resume_run(mode: str, scan_type: str) -> bool:
    """
    Return True if there is a non-completed resume run for the current source signature.
    Used to avoid clearing progressive Files index when the user is resuming an interrupted scan.
    """
    source_signature = _compute_scan_source_signature(mode, scan_type)
    try:
        con = sqlite3.connect(str(STATE_DB_FILE), timeout=15)
        cur = con.cursor()
        cur.execute(
            """
            SELECT status
            FROM scan_resume_runs
            WHERE source_signature = ? AND mode = ? AND scan_type = ?
            ORDER BY updated_at DESC
            LIMIT 1
            """,
            (source_signature, mode, scan_type),
        )
        row = cur.fetchone()
        con.close()
        if not row:
            return False
        return (row[0] or "").strip().lower() != "completed"
    except Exception:
        logging.debug("Failed to check unfinished resume runs", exc_info=True)
        return False


def _set_resume_artist_status(run_id: str | None, artist_name: str, status: str, error: str | None = None) -> None:
    """Update one artist status for a resume run."""
    if not run_id or not artist_name:
        return
    now = time.time()
    try:
        con = sqlite3.connect(str(STATE_DB_FILE), timeout=15)
        cur = con.cursor()
        cur.execute(
            """
            UPDATE scan_resume_artists
            SET status = ?, updated_at = ?, error = ?
            WHERE run_id = ? AND artist_name = ?
            """,
            ((status or "pending").strip().lower(), now, error, run_id, artist_name),
        )
        cur.execute(
            "UPDATE scan_resume_runs SET updated_at = ? WHERE run_id = ?",
            (now, run_id),
        )
        con.commit()
        con.close()
    except Exception:
        logging.debug("Failed to update resume artist status for %s", artist_name, exc_info=True)


def _set_resume_run_status(run_id: str | None, status: str, scan_id: int | None = None) -> None:
    """Finalize resume run status."""
    if not run_id:
        return
    now = time.time()
    try:
        con = sqlite3.connect(str(STATE_DB_FILE), timeout=15)
        cur = con.cursor()
        cur.execute(
            """
            UPDATE scan_resume_runs
            SET status = ?, updated_at = ?, scan_id = COALESCE(?, scan_id)
            WHERE run_id = ?
            """,
            ((status or "failed").strip().lower(), now, scan_id, run_id),
        )
        con.commit()
        con.close()
    except Exception:
        logging.debug("Failed to finalize resume run %s", run_id, exc_info=True)


def _build_files_editions(scan_type: str = "full") -> tuple[list[tuple[int, str, list[int]]], int, dict]:
    """
    Scan FILES_ROOTS, group audio files by parent folder (album candidate), infer
    artist/album/tracklist from tags, and return (artists_merged, total_albums, files_editions_by_album_id).
    Caller must store files_editions_by_album_id in state for workers (e.g. state["files_editions_by_album_id"]).
    """
    from collections import defaultdict

    scan_type = (scan_type or "full").strip().lower()
    if scan_type not in {"full", "changed_only", "incomplete_only"}:
        scan_type = "full"

    roots = [Path(r) for r in (FILES_ROOTS or []) if r]
    if not roots:
        with lock:
            state["scan_discovery_running"] = False
            state["scan_discovery_current_root"] = None
            state["scan_discovery_roots_done"] = 0
            state["scan_discovery_roots_total"] = 0
            state["scan_discovery_files_found"] = 0
            state["scan_discovery_folders_found"] = 0
            state["scan_discovery_albums_found"] = 0
            state["scan_discovery_artists_found"] = 0
        return [], 0, {}

    skip_list = list(SKIP_FOLDERS or [])
    cache_map = _load_files_album_scan_cache_map()
    changed_pending_folder_keys: list[str] = []
    changed_pending_deleted_folder_keys: list[str] = []
    if scan_type == "changed_only":
        seen_pending: set[str] = set()
        for row in _list_files_pending_changes(limit=50000):
            key = str(row.get("folder_path") or "").strip()
            if not key or key in seen_pending:
                continue
            seen_pending.add(key)
            changed_pending_folder_keys.append(key)
        if changed_pending_folder_keys:
            log_scan(
                "FILES changed-only: %d dirty album folder(s) queued by watcher.",
                len(changed_pending_folder_keys),
            )
        else:
            log_scan(
                "FILES changed-only: no watcher queue entries found; falling back to filesystem discovery + fast skip.",
            )
    with lock:
        state["scan_dirty_folders_pending_clear"] = list(changed_pending_folder_keys)
    heartbeat_interval_s = 10.0
    heartbeat_frames = ("|", "/", "-", "\\")
    heartbeat_idx = 0
    last_heartbeat_ts = 0.0

    def _emit_files_discovery_heartbeat(
        stage: str,
        *,
        root: str | None = None,
        roots_done: int | None = None,
        roots_total: int | None = None,
        files_found: int | None = None,
        entries_scanned: int | None = None,
        folders_done: int | None = None,
        folders_total: int | None = None,
        artists_found: int | None = None,
        albums_found: int | None = None,
        force: bool = False,
    ) -> None:
        nonlocal heartbeat_idx, last_heartbeat_ts
        now = time.monotonic()
        if not force and (now - last_heartbeat_ts) < heartbeat_interval_s:
            return
        last_heartbeat_ts = now
        frame = heartbeat_frames[heartbeat_idx % len(heartbeat_frames)]
        heartbeat_idx += 1
        parts: list[str] = []
        if roots_done is not None and roots_total is not None:
            parts.append(f"roots {int(roots_done)}/{int(roots_total)}")
        if root:
            parts.append(f"root={root}")
        if entries_scanned is not None:
            parts.append(f"visited={int(entries_scanned)}")
        if files_found is not None:
            parts.append(f"audio={int(files_found)}")
        if folders_done is not None and folders_total is not None:
            parts.append(f"folders {int(folders_done)}/{int(folders_total)}")
        if artists_found is not None:
            parts.append(f"artists={int(artists_found)}")
        if albums_found is not None:
            parts.append(f"albums={int(albums_found)}")
        suffix = " | ".join(parts)
        log_scan("FILES discovery %s %s%s", frame, stage, (f" | {suffix}" if suffix else ""))

    def _infer_disc_track_from_filename(path: Path, fallback_track: int) -> tuple[int, int]:
        """
        Fast filename-only track parser for cache-first scan path.
        Examples accepted: 01 ..., 1-05 ..., CD2-07 ..., A1 ...
        """
        stem = path.stem.strip()
        disc = 1
        track = fallback_track
        m = re.match(r"^\s*(?:cd|disc)\s*(\d{1,2})\s*[-_. ]\s*(\d{1,3})\b", stem, flags=re.IGNORECASE)
        if m:
            return (_parse_int_loose(m.group(1), 1) or 1, _parse_int_loose(m.group(2), fallback_track) or fallback_track)
        m = re.match(r"^\s*(\d{1,2})\s*[-_.]\s*(\d{1,3})\b", stem)
        if m:
            return (_parse_int_loose(m.group(1), 1) or 1, _parse_int_loose(m.group(2), fallback_track) or fallback_track)
        # Vinyl-side style: "A1", "B2", optionally with separators. Require digits so we don't mis-detect
        # normal names like "Ochre - ..." as side "O".
        m = re.match(r"^\s*([A-Z])\s*(?:[-_. ]?\s*(\d{1,3}))\b", stem, flags=re.IGNORECASE)
        if m:
            disc = (ord(m.group(1).upper()) - ord("A")) + 1
            track = _parse_int_loose(m.group(2), 1) or 1
            return (max(1, disc), max(1, track))
        m = re.match(r"^\s*(\d{1,3})\b", stem)
        if m:
            track = _parse_int_loose(m.group(1), fallback_track) or fallback_track
            return (disc, max(1, track))
        return (disc, max(1, track))

    def _title_from_filename(path: Path, fallback_index: int) -> str:
        stem = path.stem.strip()
        cleaned = re.sub(r"^\s*(?:cd|disc)\s*\d{1,2}\s*[-_. ]\s*\d{1,3}\s*[-_. ]*", "", stem, flags=re.IGNORECASE)
        cleaned = re.sub(r"^\s*\d{1,2}\s*[-_.]\s*\d{1,3}\s*[-_. ]*", "", cleaned)
        # Vinyl-side style: "A1 - Track", "B02_Track". Require digits so we don't strip leading letters
        # from normal names (e.g. "Ochre - ...").
        cleaned = re.sub(r"^\s*[A-Z]\s*(?:[-_. ]?\s*\d{1,3})\s*[-_. ]*", "", cleaned, flags=re.IGNORECASE)
        cleaned = re.sub(r"^\s*\d{1,3}\s*[-_. ]*", "", cleaned)
        cleaned = cleaned.strip(" -_.")
        return cleaned or stem or f"Track {fallback_index}"

    with lock:
        state["scan_discovery_running"] = True
        state["scan_discovery_current_root"] = None
        state["scan_discovery_roots_done"] = 0
        state["scan_discovery_roots_total"] = len(roots)
        state["scan_discovery_files_found"] = 0
        state["scan_discovery_folders_found"] = 0
        state["scan_discovery_albums_found"] = 0
        state["scan_discovery_artists_found"] = 0

    def _on_discovery_progress(payload: dict) -> None:
        try:
            with lock:
                state["scan_discovery_running"] = not bool(payload.get("done"))
                state["scan_discovery_current_root"] = payload.get("root")
                state["scan_discovery_roots_done"] = int(payload.get("roots_done") or 0)
                state["scan_discovery_roots_total"] = int(payload.get("roots_total") or len(roots))
                state["scan_discovery_files_found"] = int(payload.get("files_found") or 0)
        except Exception:
            pass
        _emit_files_discovery_heartbeat(
            "scanning filesystem",
            root=str(payload.get("root") or ""),
            roots_done=int(payload.get("roots_done") or 0),
            roots_total=int(payload.get("roots_total") or len(roots)),
            files_found=int(payload.get("files_found") or 0),
            entries_scanned=int(payload.get("entries_scanned") or 0),
        )

    _emit_files_discovery_heartbeat("scanning filesystem", roots_done=0, roots_total=len(roots), force=True)
    by_folder: dict[Path, list[Path]] = defaultdict(list)
    audio_files: list[Path] = []
    if scan_type == "changed_only" and changed_pending_folder_keys:
        folders_total_pending = len(changed_pending_folder_keys)
        for idx, folder_key in enumerate(changed_pending_folder_keys, start=1):
            folder_path = path_for_fs_access(Path(folder_key))
            if not folder_path.exists() or not folder_path.is_dir():
                changed_pending_deleted_folder_keys.append(folder_key)
                _emit_files_discovery_heartbeat(
                    "watcher queue scan",
                    folders_done=idx,
                    folders_total=folders_total_pending,
                    files_found=len(audio_files),
                    force=(idx == folders_total_pending),
                )
                continue
            try:
                album_files = sorted(
                    [p for p in folder_path.rglob("*") if p.is_file() and AUDIO_RE.search(p.name)],
                    key=lambda p: str(p),
                )
            except Exception:
                album_files = []
            if not album_files:
                changed_pending_deleted_folder_keys.append(folder_key)
            else:
                by_folder[folder_path].extend(album_files)
                audio_files.extend(album_files)
            _emit_files_discovery_heartbeat(
                "watcher queue scan",
                folders_done=idx,
                folders_total=folders_total_pending,
                files_found=len(audio_files),
                force=(idx == folders_total_pending),
            )
    else:
        audio_files = _iter_audio_files_under_roots(
            FILES_ROOTS,
            progress_cb=_on_discovery_progress,
            progress_every=250,
            heartbeat_seconds=5.0,
        )
        for p in audio_files:
            by_folder[p.parent].append(p)
    with lock:
        state["scan_discovery_files_found"] = len(audio_files)
        state["scan_discovery_folders_found"] = len(by_folder)
        state["scan_discovery_running"] = False
        state["scan_discovery_roots_done"] = len(roots)
    if changed_pending_deleted_folder_keys:
        removed_from_cache = 0
        removed_from_published = 0
        try:
            con = sqlite3.connect(str(STATE_DB_FILE), timeout=20)
            cur = con.cursor()
            cur.executemany(
                "DELETE FROM files_album_scan_cache WHERE folder_path = ?",
                [(k,) for k in changed_pending_deleted_folder_keys],
            )
            removed_from_cache = int(cur.rowcount or 0)
            cur.executemany(
                "DELETE FROM files_library_published_albums WHERE folder_path = ?",
                [(k,) for k in changed_pending_deleted_folder_keys],
            )
            removed_from_published = int(cur.rowcount or 0)
            con.commit()
            con.close()
        except Exception:
            logging.debug("Failed to remove deleted changed-only folders from caches", exc_info=True)
        if removed_from_cache or removed_from_published:
            log_scan(
                "FILES changed-only: removed %d deleted album folder(s) from cache (%d scan-cache, %d published rows).",
                len(changed_pending_deleted_folder_keys),
                removed_from_cache,
                removed_from_published,
            )
        else:
            log_scan(
                "FILES changed-only: %d dirty folder(s) no longer exist on disk.",
                len(changed_pending_deleted_folder_keys),
            )
    _emit_files_discovery_heartbeat(
        "grouped audio files",
        roots_done=len(roots),
        roots_total=len(roots),
        files_found=len(audio_files),
        folders_done=0,
        folders_total=len(by_folder),
        force=True,
    )

    files_editions_by_album_id: dict[int, dict] = {}
    artist_to_album_ids: dict[str, list[int]] = defaultdict(list)
    next_album_id = 1
    skipped_unchanged_complete = 0
    fast_skip_marked = 0
    fast_skip_full_cached = 0

    folders_total = len(by_folder)
    folders_done = 0
    for folder, paths in sorted(by_folder.items(), key=lambda x: str(x[0])):
        folders_done += 1
        try:
            folder_resolved = folder.resolve()
        except (OSError, RuntimeError):
            continue
        if skip_list:
            try:
                if any(folder_resolved.is_relative_to(Path(s).resolve()) for s in skip_list if s):
                    logging.debug("Skipping folder (SKIP_FOLDERS): %s", folder)
                    continue
            except (ValueError, OSError):
                pass

        ordered_paths = sorted(paths, key=lambda p: str(p))
        if not ordered_paths:
            continue

        # Cache-first fast path:
        # avoid expensive per-file mutagen reads when album fingerprint is unchanged and
        # last known quality/identity state is already healthy.
        fingerprint = _compute_album_fingerprint(ordered_paths)
        folder_key = _album_folder_cache_key(folder_resolved)
        cached = cache_map.get(folder_key) or {}
        cached_missing = cached.get("missing_required_tags") or []
        unchanged = bool(cached and (cached.get("fingerprint") == fingerprint))
        cached_has_cover = bool(cached.get("has_cover"))
        cached_has_artist_image = bool(cached.get("has_artist_image"))
        cached_healthy = bool(
            cached
            and cached_has_cover
            and cached_has_artist_image
            and cached.get("has_complete_tags")
            and cached.get("has_identity")
            and not cached_missing
        )
        has_cover_now = album_folder_has_cover(folder_resolved)
        has_artist_image_now = _artist_folder_has_image(folder_resolved.parent if folder_resolved.parent else folder_resolved)
        cached_fast_skip = bool(
            unchanged
            and cached_healthy
            and has_cover_now
            and has_artist_image_now
        )
        if cached_fast_skip and scan_type == "changed_only":
            skipped_unchanged_complete += 1
            continue
        if cached_fast_skip and scan_type in {"full", "incomplete_only"}:
            artist_name = (cached.get("artist_name") or folder_resolved.parent.name.replace("_", " ") or "Unknown Artist").strip() or "Unknown Artist"
            album_title_tag = (cached.get("album_title") or folder_resolved.name.replace("_", " ")).strip() or "Unknown Album"
            album_title_tag = _sanitize_album_title_display(album_title_tag)
            tracks: list[Track] = []
            for i, p in enumerate(ordered_paths):
                disc, trk = _infer_disc_track_from_filename(p, i + 1)
                tracks.append(Track(title=_title_from_filename(p, i + 1), idx=trk, disc=disc, dur=0))
            if not tracks:
                continue
            exts = [p.suffix.lower().lstrip(".") for p in ordered_paths]
            format_ext = max(set(exts), key=exts.count).upper() if exts else "UNKNOWN"
            normalize_parenthetical = bool(_parse_bool(_get_config_from_db("NORMALIZE_PARENTHETICAL_FOR_DEDUPE") or "true"))
            album_norm = norm_album_for_dedup(album_title_tag, normalize_parenthetical)
            identity_now = _extract_files_identity_fields(tags={}, edition={}, cached=cached)
            mbid_now = identity_now["musicbrainz_id"]
            has_mbid_now = bool(identity_now["has_mbid"])
            has_identity_now = bool(identity_now["has_identity"])
            identity_provider_now = identity_now["identity_provider"]
            album_id = next_album_id
            next_album_id += 1
            files_editions_by_album_id[album_id] = {
                "folder": folder,
                "artist_name": artist_name,
                "album_title": album_title_tag,
                "album_norm": album_norm,
                "tracks": tracks,
                "format": format_ext,
                "tags": {
                    "artist": artist_name,
                    "album": album_title_tag,
                    "musicbrainz_releasegroupid": mbid_now,
                    "musicbrainz_albumid": mbid_now,
                },
                "confidence_score": 0.9,
                "file_count": len(ordered_paths),
                "ordered_paths": ordered_paths,
                "fingerprint": fingerprint,
                "folder_key": folder_key,
                "missing_required_tags": list(cached_missing),
                "has_cover": has_cover_now,
                "has_artist_image": has_artist_image_now,
                "has_mbid": has_mbid_now,
                "has_identity": has_identity_now,
                "identity_provider": identity_provider_now,
                "musicbrainz_id": mbid_now,
                "discogs_release_id": identity_now["discogs_release_id"],
                "lastfm_album_mbid": identity_now["lastfm_album_mbid"],
                "bandcamp_album_url": identity_now["bandcamp_album_url"],
                "metadata_source": identity_now["metadata_source"],
                "skip_heavy_processing": True,
            }
            artist_to_album_ids[artist_name].append(album_id)
            fast_skip_marked += 1
            fast_skip_full_cached += 1
            if (album_id % 25) == 0:
                with lock:
                    state["scan_discovery_albums_found"] = next_album_id - 1
                    state["scan_discovery_artists_found"] = len(artist_to_album_ids)
            _emit_files_discovery_heartbeat(
                "building album candidates",
                files_found=len(audio_files),
                folders_done=folders_done,
                folders_total=folders_total,
                artists_found=len(artist_to_album_ids),
                albums_found=next_album_id - 1,
            )
            continue

        # Discovery hot path optimization:
        # parse tags only on first file, derive track list from filenames for the rest.
        first_tags = extract_tags(ordered_paths[0]) or {}
        artist_name = _pick_album_artist_from_tag_dicts([first_tags], default="Unknown Artist")
        album_title_tag = _pick_album_title_from_tag_dicts([first_tags], fallback=folder.name.replace("_", " "))
        album_title_tag = _sanitize_album_title_display(album_title_tag)
        # Untagged albums: use folder structure as fallback identity.
        if (artist_name or "").strip().lower() in {"unknown", "unknown artist", "various", "various artists"}:
            try:
                parent_name = (folder.parent.name or "").replace("_", " ").strip()
            except Exception:
                parent_name = ""
            if parent_name:
                artist_name = parent_name

        tracks: list[Track] = []
        first_disc, first_trk = _parse_disc_track_loose(first_tags, fallback_disc=1, fallback_track=1)
        first_title = (first_tags.get("title") or first_tags.get("name") or "").strip()
        for i, p in enumerate(ordered_paths):
            disc, trk = _infer_disc_track_from_filename(p, i + 1)
            title = _title_from_filename(p, i + 1)
            if i == 0:
                disc = first_disc or disc
                trk = first_trk or trk
                title = first_title or title
            tracks.append(Track(title=title, idx=trk, disc=disc, dur=0))

        if not tracks:
            continue

        # Dominant format and confidence
        exts = [p.suffix.lower().lstrip(".") for p in ordered_paths]
        format_ext = max(set(exts), key=exts.count).upper() if exts else "UNKNOWN"
        fmt_score = FMT_SCORE.get(format_ext.lower(), 0)  # kept for parity with legacy logic
        has_album_tag = bool(_normalize_meta_text(first_tags.get("album")))
        has_artist_tag = bool(_normalize_meta_text(first_tags.get("artist") or first_tags.get("albumartist")))
        confidence = 0.5 + (0.2 if has_album_tag else 0) + (0.1 if has_artist_tag else 0) + (0.2 if len(ordered_paths) >= 3 else 0)

        normalize_parenthetical = bool(_parse_bool(_get_config_from_db("NORMALIZE_PARENTHETICAL_FOR_DEDUPE") or "true"))
        album_norm = norm_album_for_dedup(album_title_tag, normalize_parenthetical)
        fingerprint = _compute_album_fingerprint(ordered_paths)
        folder_key = _album_folder_cache_key(folder_resolved)
        missing_required_now = _check_required_tags(first_tags, REQUIRED_TAGS, edition={"tracks": tracks})
        has_cover_now = album_folder_has_cover(folder_resolved)
        has_artist_image_now = _artist_folder_has_image(folder_resolved.parent if folder_resolved.parent else folder_resolved)
        cached = cache_map.get(folder_key) or {}
        identity_now = _extract_files_identity_fields(tags=first_tags, edition={}, cached=cached)
        mbid_now = identity_now["musicbrainz_id"]
        has_mbid_now = bool(identity_now["has_mbid"])
        has_identity_now = bool(identity_now["has_identity"])
        identity_provider_now = identity_now["identity_provider"]
        cached_missing = cached.get("missing_required_tags") or []
        cached_healthy = bool(
            cached
            and cached.get("has_cover")
            and cached.get("has_artist_image")
            and cached.get("has_complete_tags")
            and cached.get("has_identity")
            and not cached_missing
        )
        unchanged = bool(cached and (cached.get("fingerprint") == fingerprint))
        current_healthy = bool(
            has_cover_now
            and has_artist_image_now
            and has_identity_now
            and not missing_required_now
        )
        fast_skip_heavy = unchanged and cached_healthy and current_healthy

        if scan_type == "changed_only" and fast_skip_heavy:
            skipped_unchanged_complete += 1
            continue

        album_id = next_album_id
        next_album_id += 1

        files_editions_by_album_id[album_id] = {
            "folder": folder,
            "artist_name": artist_name,
            "album_title": album_title_tag,
            "album_norm": album_norm,
            "tracks": tracks,
            "format": format_ext,
            "tags": first_tags,
            "confidence_score": confidence,
            "file_count": len(ordered_paths),
            "ordered_paths": ordered_paths,
            "fingerprint": fingerprint,
            "folder_key": folder_key,
            "missing_required_tags": missing_required_now,
            "has_cover": has_cover_now,
            "has_artist_image": has_artist_image_now,
            "has_mbid": has_mbid_now,
            "has_identity": has_identity_now,
            "identity_provider": identity_provider_now,
            "musicbrainz_id": mbid_now,
            "discogs_release_id": identity_now["discogs_release_id"],
            "lastfm_album_mbid": identity_now["lastfm_album_mbid"],
            "bandcamp_album_url": identity_now["bandcamp_album_url"],
            "metadata_source": identity_now["metadata_source"],
            "skip_heavy_processing": fast_skip_heavy,
        }
        artist_to_album_ids[artist_name].append(album_id)
        if (album_id % 25) == 0:
            with lock:
                state["scan_discovery_albums_found"] = next_album_id - 1
                state["scan_discovery_artists_found"] = len(artist_to_album_ids)
        _emit_files_discovery_heartbeat(
            "building album candidates",
            files_found=len(audio_files),
            folders_done=folders_done,
            folders_total=folders_total,
            artists_found=len(artist_to_album_ids),
            albums_found=next_album_id - 1,
        )
        if fast_skip_heavy:
            fast_skip_marked += 1
    # Build artists_merged: (artist_id, artist_name, album_ids). For Files we use 0 as artist_id.
    artists_merged = [(0, name, ids) for name, ids in sorted(artist_to_album_ids.items(), key=lambda x: x[0].lower())]
    total_albums = next_album_id - 1
    with lock:
        state["scan_discovery_running"] = False
        state["scan_discovery_current_root"] = None
        state["scan_discovery_albums_found"] = total_albums
        state["scan_discovery_artists_found"] = len(artists_merged)
    _emit_files_discovery_heartbeat(
        "ready",
        files_found=len(audio_files),
        folders_done=folders_total,
        folders_total=folders_total,
        artists_found=len(artists_merged),
        albums_found=total_albums,
        force=True,
    )
    log_scan(
        "FILES backend: discovered %d artist(s), %d album(s) from %d audio file(s)%s%s%s",
        len(artists_merged),
        total_albums,
        len(audio_files),
        f"; changed-only skipped {skipped_unchanged_complete} unchanged+healthy album(s)" if scan_type == "changed_only" else "",
        f"; cache-first fast-skip {fast_skip_full_cached} album(s)" if fast_skip_full_cached else "",
        f"; fast-skip candidates {fast_skip_marked}" if fast_skip_marked else "",
    )
    return artists_merged, total_albums, files_editions_by_album_id


def _sanitize_export_component(s: str, max_len: int = 200) -> str:
    """Replace filesystem-illegal characters and truncate for use in export paths."""
    if not s:
        return "Unknown"
    # Strip and replace chars that are problematic on Windows/Unix
    bad = re.compile(r'[\x00/\\:*?"<>|]')
    out = bad.sub("_", str(s).strip())
    out = " ".join(out.split())
    return out[:max_len] if len(out) > max_len else out or "Unknown"


def build_export_path(
    edition: dict,
    track_index: int,
    source_path: Path,
    template: str,
    export_root: str,
) -> Path:
    """
    Build the target path for one track under EXPORT_ROOT using EXPORT_NAMING_TEMPLATE.
    Placeholders: {letter}, {artist}, {album}, {year}, {disc}, {track}, {title}, {format}, {ext}.
    """
    artist = _sanitize_export_component(edition.get("artist_name") or "Unknown Artist")
    album = _sanitize_export_component(edition.get("album_title") or "Unknown Album")
    tags = edition.get("tags") or {}
    year = (tags.get("date") or tags.get("year") or "").strip()[:4] or ""
    fmt = (edition.get("format") or "UNKNOWN").upper()
    ext = (source_path.suffix or "").lower()
    if not ext or ext not in [x.lower() for x in [".flac", ".mp3", ".m4a", ".ogg", ".opus", ".wav", ".aac"]]:
        ext = ".flac"
    tracks_list = edition.get("tracks") or []
    disc_num = 1
    track_num = track_index + 1
    title = f"Track {track_num}"
    if track_index < len(tracks_list):
        t = tracks_list[track_index]
        disc_num = getattr(t, "disc", 1) or 1
        track_num = getattr(t, "idx", track_index + 1) or (track_index + 1)
        title = _sanitize_export_component(getattr(t, "title", "") or title)
    # First letter of artist for folder (A-Z or 0-9)
    first = (artist or "U")[0].upper()
    if not first.isalnum():
        first = "0"
    # Simple placeholder substitution (no format spec like :02d in template for now)
    subs = {
        "letter": first,
        "artist": artist,
        "album": album,
        "year": year,
        "disc": str(disc_num),
        "track": f"{track_num:02d}",
        "title": title,
        "format": fmt,
        "ext": ext,
    }
    path_str = template
    for k, v in subs.items():
        path_str = path_str.replace("{" + k + "}", v)
    # Handle {track:02d} style if present
    path_str = re.sub(r"\{track:02d\}", f"{track_num:02d}", path_str)
    out = Path(export_root) / path_str.lstrip("/")
    return out


def analyse_directory_structure(roots: list[str], max_samples: int = 300) -> dict:
    """
    Sample audio files under roots, collect relative paths and tags, infer path patterns,
    and return templates + metrics for GET /api/files/structure/overview.
    """
    import random
    paths = _iter_audio_files_under_roots(roots or [])
    if not paths:
        return {"templates": [], "metrics": {}, "samples": [], "sample_count": 0}
    if len(paths) > max_samples:
        paths = random.sample(paths, max_samples)
    samples = []
    for p in paths:
        try:
            rel = p
            for r in roots:
                if r:
                    try:
                        rel = p.relative_to(Path(r))
                        break
                    except ValueError:
                        continue
            rel_str = str(rel)
        except Exception:
            rel_str = str(p)
        tags = extract_tags(p)
        samples.append({
            "path": rel_str,
            "artist": (tags.get("albumartist") or tags.get("artist") or "").strip(),
            "album": (tags.get("album") or "").strip(),
            "year": (tags.get("date") or tags.get("year") or "").strip()[:4],
            "ext": p.suffix.lower().lstrip("."),
        })
    # Simple pattern: count path depth and segment patterns
    depths = [s["path"].count("/") + s["path"].count("\\") for s in samples]
    avg_depth = sum(depths) / len(depths) if depths else 0
    # Dominant "template" as example: first path split into parts
    templates = []
    if samples:
        example = samples[0]["path"]
        templates.append({"name": "sampled", "example": example})
    metrics = {
        "sample_count": len(samples),
        "total_files_estimate": len(_iter_audio_files_under_roots(roots or [])),
        "average_path_depth": round(avg_depth, 1),
        "paths_with_artist_tag": sum(1 for s in samples if s.get("artist")),
        "paths_with_album_tag": sum(1 for s in samples if s.get("album")),
    }
    return {"templates": templates, "metrics": metrics, "samples": samples[:20], "sample_count": len(samples)}


def _run_export_library() -> None:
    """Background worker: build export library from Files editions (hardlinks/symlinks/copies/moves)."""
    export_root = (EXPORT_ROOT or "").strip()
    if not export_root:
        with lock:
            state["export_progress"] = {"running": False, "error": "EXPORT_ROOT is not configured", "tracks_done": 0, "total_tracks": 0, "albums_done": 0, "total_albums": 0}
        return
    template = (EXPORT_NAMING_TEMPLATE or "").strip()
    legacy_template = "{letter}/{artist}/{album} ({year}, {format})/{track}_{artist}_{title}{ext}"
    if not template or template == legacy_template:
        template = "{letter}/{artist}/{album} ({year}, {format})/{track} - {artist} - {title}{ext}"
    strategy = (EXPORT_LINK_STRATEGY or "hardlink").strip().lower()
    if strategy not in ("hardlink", "symlink", "copy", "move"):
        strategy = "hardlink"
    with lock:
        state["export_progress"] = {"running": True, "tracks_done": 0, "total_tracks": 0, "albums_done": 0, "total_albums": 0, "error": None}
    try:
        _, _, editions_by_id = _build_files_editions(scan_type="full")
        total_tracks = sum(len(e.get("ordered_paths") or []) for e in editions_by_id.values())
        total_albums = len(editions_by_id)
        with lock:
            state["export_progress"]["total_tracks"] = total_tracks
            state["export_progress"]["total_albums"] = total_albums
        tracks_done = 0
        albums_done = 0
        for album_id, edition in editions_by_id.items():
            ordered_paths = edition.get("ordered_paths") or []
            for i, src in enumerate(ordered_paths):
                if not src.exists():
                    continue
                tgt = build_export_path(edition, i, src, template, export_root)
                # If a previous run already created the correct hardlink, skip
                try:
                    if tgt.exists():
                        src_stat = src.stat()
                        tgt_stat = tgt.stat()
                        if src_stat.st_ino == tgt_stat.st_ino and src_stat.st_dev == tgt_stat.st_dev:
                            continue
                except OSError:
                    # Fall back to normal collision handling below
                    pass
                # Collision: if different file exists at target, add (1), (2), ...
                base = tgt.parent / tgt.stem
                ext = tgt.suffix
                n = 0
                while tgt.exists():
                    n += 1
                    tgt = tgt.parent / f"{base.name} ({n}){ext}"
                tgt.parent.mkdir(parents=True, exist_ok=True)
                try:
                    if strategy == "hardlink":
                        try:
                            tgt.hardlink_to(src)
                        except OSError:
                            import shutil
                            shutil.copy2(src, tgt)
                    elif strategy == "symlink":
                        tgt.symlink_to(src)
                    elif strategy == "move":
                        if src.resolve() != tgt.resolve():
                            shutil.move(str(src), str(tgt))
                    else:
                        import shutil
                        shutil.copy2(src, tgt)
                except Exception as e:
                    logging.warning("Export failed for %s -> %s: %s", src, tgt, e)
                tracks_done += 1
                with lock:
                    state["export_progress"]["tracks_done"] = tracks_done
            albums_done += 1
            with lock:
                state["export_progress"]["albums_done"] = albums_done
        with lock:
            state["export_progress"]["running"] = False
            state["export_progress"]["error"] = None
    except Exception as e:
        logging.exception("Export library failed: %s", e)
        with lock:
            state["export_progress"]["running"] = False
            state["export_progress"]["error"] = str(e)


def _build_scan_plan(scan_type: str = "full") -> tuple[list[tuple[int, str, list[int]]], int]:
    """
    Build the list of artists/albums to scan and return (artists_merged, total_albums).

    In Plex mode this wraps the existing Plex DB queries. In future this function
    will dispatch to a filesystem-based backend when LIBRARY_MODE == 'files'.
    """
    mode = _get_library_mode()
    scan_type = (scan_type or "full").strip().lower()
    if mode == "files":
        if not FILES_ROOTS:
            raise RuntimeError("FILES_ROOTS is empty ‚Äì configure at least one music root for files library mode.")
        log_scan("FILES mode source roots: %s", ", ".join(str(r) for r in FILES_ROOTS))
        artists_merged, total_albums, files_editions_by_album_id = _build_files_editions(scan_type=scan_type)
        with lock:
            state["files_editions_by_album_id"] = files_editions_by_album_id
        return artists_merged, total_albums

    if scan_type == "changed_only":
        logging.info("Scan type 'changed_only' is only optimized in Files mode; using full scan plan for Plex mode.")

    # Plex-backed scan plan (current behaviour)
    db_conn = plex_connect()
    placeholders = ",".join("?" for _ in SECTION_IDS)

    # Total albums for progress bar
    total_albums = db_conn.execute(
        f"SELECT COUNT(*) FROM metadata_items "
        f"WHERE metadata_type=9 AND library_section_id IN ({placeholders})",
        SECTION_IDS,
    ).fetchone()[0]

    # Fetch all artists in the selected sections
    artists_raw = db_conn.execute(
        f"SELECT id, title FROM metadata_items "
        f"WHERE metadata_type=8 AND library_section_id IN ({placeholders})",
        SECTION_IDS,
    ).fetchall()

    # Merge artists by normalized name so duplicates across Plex \"artist\" entries
    # (e.g. Ochre from folder A and Ochre from folder B) are scanned together.
    from collections import defaultdict

    artists_by_name: dict[str, list[tuple[int, str]]] = defaultdict(list)
    for artist_id, artist_name in artists_raw:
        name_norm = (artist_name or "").strip().lower()
        artists_by_name[name_norm].append((artist_id, artist_name))

    artists_merged: list[tuple[int, str, list[int]]] = []
    for _name_norm, id_name_list in artists_by_name.items():
        artist_ids = [aid for aid, _ in id_name_list]
        primary_id, primary_name = id_name_list[0]
        ph = ",".join("?" for _ in artist_ids)
        album_ids_for_name = [
            row[0]
            for row in db_conn.execute(
                f"SELECT id FROM metadata_items "
                f"WHERE metadata_type=9 AND parent_id IN ({ph})",
                artist_ids,
            ).fetchall()
        ]
        artists_merged.append((primary_id, primary_name, album_ids_for_name))
        if len(id_name_list) > 1:
            log_scan(
                "Merged %d Plex artist entries for '%s' into one scan (%d albums)",
                len(id_name_list),
                primary_name,
                len(album_ids_for_name),
            )

    db_conn.close()
    return artists_merged, total_albums


def _pipeline_flags_for_scan(scan_type: str, run_improve_after_requested: bool) -> dict[str, bool | str]:
    """Resolve effective pipeline flags for the current scan."""
    scan_kind = (scan_type or "full").strip().lower()
    scan_is_content = scan_kind in {"full", "changed_only"}
    # Pipeline flags are the canonical source; run_improve_after remains a one-shot override.
    run_match_fix = bool(scan_is_content and (PIPELINE_ENABLE_MATCH_FIX or run_improve_after_requested))
    # "dedupe" in pipeline flags means *automatic* deduplication/moves, not just detection.
    # It must track AUTO_MOVE_DUPES to avoid misleading scan summaries and scan_history rows.
    auto_move_dup = bool(getattr(sys.modules[__name__], "AUTO_MOVE_DUPES", False))
    run_dedupe = bool(scan_is_content and PIPELINE_ENABLE_DEDUPE and auto_move_dup)
    run_incomplete_move = bool(scan_is_content and PIPELINE_ENABLE_INCOMPLETE_MOVE)
    run_export = bool(scan_is_content and PIPELINE_ENABLE_EXPORT)
    run_player_sync = bool(scan_is_content and PIPELINE_ENABLE_PLAYER_SYNC)
    sync_target = _normalize_player_target(getattr(sys.modules[__name__], "PIPELINE_PLAYER_TARGET", "none"))
    if sync_target == "none":
        run_player_sync = False
    return {
        "match_fix": run_match_fix,
        "dedupe": run_dedupe,
        "incomplete_move": run_incomplete_move,
        "export": run_export,
        "player_sync": run_player_sync,
        "sync_target": sync_target,
    }


def _auto_move_incomplete_albums_for_scan(
    scan_id: int | None,
    editions_by_artist: dict[str, list[dict]] | None,
) -> dict:
    """
    Move broken/incomplete album folders to INCOMPLETE_ALBUMS_TARGET_DIR.
    Returns {'moved': int, 'size_mb': int, 'errors': int}.
    """
    result = {"moved": 0, "size_mb": 0, "errors": 0}
    if not scan_id or not editions_by_artist:
        return result

    target_dir = str(_get_config_from_db("INCOMPLETE_ALBUMS_TARGET_DIR") or "/dupes/incomplete_albums").strip()
    target_root = path_for_fs_access(Path(target_dir))
    try:
        target_root.mkdir(parents=True, exist_ok=True)
    except Exception as e:
        logging.warning("Auto-move incomplete: cannot create target dir %s: %s", target_root, e)
        result["errors"] += 1
        return result

    candidates: dict[str, dict] = {}
    for artist_name, editions in (editions_by_artist or {}).items():
        for e in (editions or []):
            if not e.get("is_broken"):
                continue
            folder = e.get("folder")
            if not folder:
                continue
            src_folder = path_for_fs_access(Path(str(folder)))
            key = str(src_folder)
            if key in candidates:
                continue
            candidates[key] = {
                "artist": str(artist_name or ""),
                "album_id": int(e.get("album_id") or 0),
                "title_raw": str(e.get("title_raw") or ""),
                "src": src_folder,
                "fmt_text": str(e.get("fmt_text") or get_primary_format(src_folder) or ""),
            }

    if not candidates:
        return result

    def _files_forget_album_folder(folder: Path | str) -> None:
        """Remove a moved/deleted album folder from Files-mode caches/index payload tables."""
        try:
            if _get_library_mode() != "files":
                return
        except Exception:
            return
        try:
            key = _album_folder_cache_key(folder)
        except Exception:
            key = str(folder)
        try:
            con2 = sqlite3.connect(str(STATE_DB_FILE), timeout=10)
            cur2 = con2.cursor()
            cur2.execute("DELETE FROM files_album_scan_cache WHERE folder_path = ?", (key,))
            cur2.execute("DELETE FROM files_library_published_albums WHERE folder_path = ?", (key,))
            con2.commit()
            con2.close()
        except Exception:
            logging.debug("Files cache/published cleanup failed for %s", key, exc_info=True)

    try:
        con = sqlite3.connect(str(STATE_DB_FILE), timeout=20)
        cur = con.cursor()
        cur.execute("PRAGMA table_info(scan_moves)")
        move_cols = [r[1] for r in cur.fetchall()]
        has_reason = "move_reason" in move_cols
    except Exception:
        con = None
        cur = None
        move_cols = []
        has_reason = False

    try:
        for item in candidates.values():
            src_folder = item["src"]
            if not src_folder.exists():
                continue
            # Keep artist namespace for readability when browsing incomplete quarantine.
            artist_dir = _sanitize_path_component(str(item["artist"] or "Unknown Artist"))
            dst = target_root / artist_dir / src_folder.name
            counter = 1
            while dst.exists():
                dst = target_root / artist_dir / f"{src_folder.name} ({counter})"
                counter += 1
            try:
                dst.parent.mkdir(parents=True, exist_ok=True)
                size_mb = int(folder_size(src_folder) // (1024 * 1024))
                try:
                    _files_watcher_suppress_folder(src_folder, seconds=180.0, reason="pmda_move_incomplete")
                except Exception:
                    pass
                safe_move(str(src_folder), str(dst))
                try:
                    _files_watcher_suppress_folder(dst, seconds=180.0, reason="pmda_move_incomplete")
                except Exception:
                    pass
                _files_forget_album_folder(src_folder)
                result["moved"] += 1
                result["size_mb"] += size_mb
                if cur is not None:
                    moved_at = time.time()
                    if "album_title" in move_cols and "fmt_text" in move_cols and has_reason:
                        cur.execute(
                            """
                            INSERT INTO scan_moves
                            (scan_id, artist, album_id, original_path, moved_to_path, size_mb, moved_at, album_title, fmt_text, move_reason)
                            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                            """,
                            (
                                scan_id,
                                item["artist"],
                                item["album_id"],
                                str(src_folder),
                                str(dst),
                                size_mb,
                                moved_at,
                                item["title_raw"],
                                item["fmt_text"],
                                "incomplete",
                            ),
                        )
                    elif "album_title" in move_cols and "fmt_text" in move_cols:
                        cur.execute(
                            """
                            INSERT INTO scan_moves
                            (scan_id, artist, album_id, original_path, moved_to_path, size_mb, moved_at, album_title, fmt_text)
                            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
                            """,
                            (
                                scan_id,
                                item["artist"],
                                item["album_id"],
                                str(src_folder),
                                str(dst),
                                size_mb,
                                moved_at,
                                item["title_raw"],
                                item["fmt_text"],
                            ),
                        )
                    else:
                        cur.execute(
                            """
                            INSERT INTO scan_moves
                            (scan_id, artist, album_id, original_path, moved_to_path, size_mb, moved_at)
                            VALUES (?, ?, ?, ?, ?, ?, ?)
                            """,
                            (
                                scan_id,
                                item["artist"],
                                item["album_id"],
                                str(src_folder),
                                str(dst),
                                size_mb,
                                moved_at,
                            ),
                        )
            except Exception as move_err:
                result["errors"] += 1
                logging.warning(
                    "Auto-move incomplete failed: %s -> %s: %s",
                    src_folder,
                    dst,
                    move_err,
                )
        if con is not None:
            con.commit()
    finally:
        if con is not None:
            con.close()
    return result


def _mark_broken_from_dupe_groups(
    all_results: dict,
    editions_by_artist: dict[str, list[dict]] | None,
    *,
    ratio_threshold: float = 0.90,
) -> int:
    """
    Heuristic: if a dupe group's best edition has notably more tracks than a loser,
    mark that loser as broken so the incomplete-move step can quarantine it.

    This catches tail-truncated albums even when provider/MB expected track count is unknown.
    """
    if not all_results or not editions_by_artist:
        return 0
    try:
        thr = float(ratio_threshold)
    except Exception:
        thr = 0.90
    thr = max(0.50, min(0.99, thr))

    by_id: dict[int, dict] = {}
    for _artist, eds in (editions_by_artist or {}).items():
        for e in (eds or []):
            try:
                aid = int(e.get("album_id") or 0)
            except Exception:
                aid = 0
            if aid > 0:
                by_id[aid] = e

    def _track_indices(edition: dict) -> list[int]:
        idxs: list[int] = []
        try:
            for t in (edition.get("tracks") or []):
                try:
                    v = int(getattr(t, "idx", None) or 0)
                except Exception:
                    try:
                        v = int((t or {}).get("idx") or 0) if isinstance(t, dict) else 0
                    except Exception:
                        v = 0
                if v > 0:
                    idxs.append(v)
        except Exception:
            pass
        idxs = sorted(set(idxs))
        if idxs:
            return idxs

        # Files-mode editions often do not carry rich Track objects through all codepaths.
        # Fall back to parsing track numbers from filenames inside the edition folder.
        cached = edition.get("_fs_track_indices")
        if isinstance(cached, list) and cached:
            try:
                return sorted(set(int(x) for x in cached if int(x) > 0))
            except Exception:
                pass

        folder_raw = edition.get("folder")
        if not folder_raw:
            return []
        try:
            folder_path = path_for_fs_access(Path(str(folder_raw)))
        except Exception:
            try:
                folder_path = Path(str(folder_raw))
            except Exception:
                folder_path = None
        if not folder_path or (not folder_path.exists()) or (not folder_path.is_dir()):
            return []

        def _parse_track_idx(name: str) -> int:
            base = os.path.basename(name or "")
            if not base:
                return 0
            stem = Path(base).stem
            s = stem.strip()
            if not s:
                return 0
            # Common patterns:
            # - "01 Title"
            # - "1-01 Title" (disc-track)
            # - "CD1 01 Title"
            m = re.match(r"^\s*(?:cd\s*\d+\s*)?(\d{1,3})\b", s, flags=re.IGNORECASE)
            if m:
                try:
                    return int(m.group(1))
                except Exception:
                    return 0
            m2 = re.match(r"^\s*(\d{1,2})\s*[-_. ]\s*(\d{1,2})\b", s)
            if m2:
                try:
                    return int(m2.group(2))
                except Exception:
                    return 0
            return 0

        found: list[int] = []
        try:
            for p in folder_path.rglob("*"):
                if not p.is_file():
                    continue
                if not AUDIO_RE.search(p.name):
                    continue
                idx = _parse_track_idx(p.name)
                if idx > 0:
                    found.append(idx)
        except Exception:
            found = []
        found = sorted(set(found))
        try:
            if found:
                edition["_fs_track_indices"] = found
        except Exception:
            pass
        return found

    marked = 0
    for _artist, groups in (all_results or {}).items():
        for g in (groups or []):
            best = g.get("best") if isinstance(g, dict) else None
            losers = (g.get("losers") or []) if isinstance(g, dict) else []
            if not isinstance(best, dict) or not losers:
                continue
            best_idxs = _track_indices(best)
            try:
                best_count = int(
                    best.get("actual_track_count")
                    or best.get("track_count")
                    or best.get("file_count")
                    or len(best.get("tracks") or [])
                )
            except Exception:
                best_count = 0
            if best_count < 3 and len(best_idxs) < 3:
                continue
            for loser in losers:
                if not isinstance(loser, dict):
                    continue
                try:
                    if loser.get("is_broken", False):
                        continue
                except Exception:
                    pass
                loser_idxs = _track_indices(loser)
                try:
                    loser_count = int(
                        loser.get("actual_track_count")
                        or loser.get("track_count")
                        or loser.get("file_count")
                        or len(loser.get("tracks") or [])
                    )
                except Exception:
                    loser_count = 0
                if loser_count <= 0:
                    continue

                # Prefer an index-based "missing in the middle" signal: missing indices <= max(loser_idx).
                missing_mid = []
                if best_idxs and loser_idxs:
                    try:
                        best_set = set(best_idxs)
                        loser_set = set(loser_idxs)
                        missing = sorted(best_set - loser_set)
                        if missing and (min(missing) <= max(loser_idxs)):
                            missing_mid = missing
                    except Exception:
                        missing_mid = []

                # Fallback: only mark as broken on track-count ratio when indices are unavailable.
                if (not missing_mid) and best_count > 0:
                    if (loser_count / best_count) >= thr:
                        continue
                # Mark loser (group dict) and the canonical edition dict (by album_id) so later steps agree.
                try:
                    loser["is_broken"] = True
                    loser["expected_track_count"] = int(max(best_idxs) if best_idxs else best_count)
                    loser["actual_track_count"] = loser_count
                    if missing_mid:
                        loser["missing_indices"] = missing_mid[:5000]
                    else:
                        loser.setdefault("missing_indices", [])
                except Exception:
                    pass
                try:
                    loser_id = int(loser.get("album_id") or 0)
                except Exception:
                    loser_id = 0
                if loser_id > 0 and loser_id in by_id:
                    try:
                        e = by_id[loser_id]
                        e["is_broken"] = True
                        e["expected_track_count"] = int(max(best_idxs) if best_idxs else best_count)
                        e["actual_track_count"] = loser_count
                        if missing_mid:
                            e["missing_indices"] = missing_mid[:5000]
                        else:
                            e.setdefault("missing_indices", [])
                    except Exception:
                        pass
                marked += 1
    return marked


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ BACKGROUND TASKS (WEB) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def background_scan():
    """
    Scan the entire library in parallel, persist results to SQLite,
    and update the in‚Äëmemory `state` for the Web UI.

    The function is now exception‚Äësafe: no single worker failure will abort
    the whole scan, and `state["scanning"]` is **always** cleared even when
    an unexpected error occurs, so the front‚Äëend never hangs in "running".
    """
    # Reload library backend settings (mode + files roots) and Plex selectors/path map
    # so scan always uses the latest saved sources from Settings.
    _reload_library_mode_and_files_roots_from_db()
    _reload_musicbrainz_settings_from_db()
    mode = _get_library_mode()
    if mode == "plex":
        _reload_section_ids_from_db()
        _reload_path_map_from_db()
    if mode == "plex" and not SECTION_IDS:
        logging.warning("background_scan(): SECTION_IDS is empty after reload; aborting scan (Plex mode)")
        with lock:
            state["scanning"] = False
        return
    if mode == "plex" and not PATH_MAP:
        logging.warning("background_scan(): PATH_MAP is empty after reload; albums will not resolve to container paths. Run Detect & verify in Settings.")
    if mode == "plex":
        logging.debug(f"background_scan(): SECTION_IDS=%s, PATH_MAP keys=%s, opening Plex DB at {PLEX_DB_FILE}", SECTION_IDS, list(PATH_MAP.keys()))
    else:
        logging.debug("background_scan(): FILES mode active; skipping Plex PATH_MAP reload/checks.")
    scan_perf_start = time.perf_counter()
    all_results: Dict[str, List[dict]] = {}  # Always defined so finally can persist
    all_editions_by_artist: Dict[str, List[dict]] = {}  # For scan_editions (Library, Tag Fixer)
    scan_incremental_queue = None
    scan_incremental_writer_thread = None
    files_live_index_last_trigger = 0.0
    files_live_index_interval_sec = 45.0
    scan_status = "failed"
    scan_type = "full"
    resume_run_id = None
    scan_post_queue = None
    scan_post_worker_thread = None
    run_improve_after_requested = False
    pipeline_flags = {
        "match_fix": False,
        "dedupe": False,
        "incomplete_move": False,
        "export": False,
        "player_sync": False,
        "sync_target": "none",
    }
    scan_stream_post_by_artist = False
    streamed_post_process_done = False
    files_live_index_precleared = False
    with lock:
        run_improve_after_requested = bool(state.get("run_improve_after", False))
        scan_type = (state.get("scan_type") or "full").strip().lower()
    if scan_type not in {"full", "changed_only"}:
        scan_type = "full"
    pipeline_flags = _pipeline_flags_for_scan(scan_type, run_improve_after_requested)

    # Mark scan as running immediately so UI can show early source-discovery activity.
    with lock:
        state["scanning"] = True
        state["scan_type"] = scan_type
        state["scan_progress"] = 0
        state["scan_total"] = 0
        state["scan_step_progress"] = 0
        state["scan_step_total"] = 0
        state["scan_artists_processed"] = 0
        state["scan_artists_total"] = 0
        state["scan_detected_artists_total"] = 0
        state["scan_detected_albums_total"] = 0
        state["scan_resume_skipped_artists"] = 0
        state["scan_resume_skipped_albums"] = 0
        state["scan_active_artists"] = {}
        state["scan_finalizing"] = False
        state["scan_discovery_running"] = (_get_library_mode() == "files")
        state["scan_discovery_current_root"] = None
        state["scan_discovery_roots_done"] = 0
        state["scan_discovery_roots_total"] = 0
        state["scan_discovery_files_found"] = 0
        state["scan_discovery_folders_found"] = 0
        state["scan_discovery_albums_found"] = 0
        state["scan_discovery_artists_found"] = 0
        state["scan_pipeline_flags"] = dict(pipeline_flags)
        state["scan_pipeline_sync_target"] = str(pipeline_flags.get("sync_target") or "none")

    try:
        if _get_library_mode() == "files" and scan_type == "full":
            if _has_unfinished_resume_run("files", scan_type):
                log_scan("Files full scan: unfinished resume run detected, keeping current live library index.")
            else:
                _reset_files_live_index_for_scan()
                files_live_index_precleared = True

        # Log cache behavior for this run so logs show whether existing cache is being used
        if SCAN_DISABLE_CACHE:
            log_scan(
                "Background scan started with SCAN_DISABLE_CACHE=True ‚Äì ignoring audio and metadata caches for this run"
            )
        else:
            log_scan(
                "Background scan started with SCAN_DISABLE_CACHE=False ‚Äì using audio and metadata caches when available"
            )

        # 1) Build the scan plan from the active library backend.
        artists_merged, total_albums = _build_scan_plan(scan_type=scan_type)
        detected_artists_total = len(artists_merged)
        detected_albums_total = total_albums
        total_artists = detected_artists_total
        files_editions_for_resume = {}
        if _get_library_mode() == "files":
            with lock:
                files_editions_for_resume = dict(state.get("files_editions_by_album_id") or {})
        resume_run_id, artists_merged, resume_skipped_artists, resume_skipped_albums = _prepare_resume_scan_artists(
            _get_library_mode(),
            scan_type,
            artists_merged,
            files_editions_by_album_id=files_editions_for_resume,
        )
        total_artists = len(artists_merged)
        total_albums = sum(len(ids) for _a, _n, ids in artists_merged)
        with lock:
            state["scan_resume_run_id"] = resume_run_id
            state["scan_detected_artists_total"] = int(detected_artists_total or 0)
            state["scan_detected_albums_total"] = int(detected_albums_total or 0)
            state["scan_resume_skipped_artists"] = int(resume_skipped_artists or 0)
            state["scan_resume_skipped_albums"] = int(resume_skipped_albums or 0)
        if resume_skipped_artists:
            log_scan(
                "Resume: skipped %d already-done artist(s), %d album(s) unchanged since last interrupted run.",
                resume_skipped_artists,
                resume_skipped_albums,
            )
        if _get_library_mode() == "files":
            if scan_type == "full" and resume_skipped_artists == 0 and not files_live_index_precleared:
                _reset_files_live_index_for_scan()
            files_live_index_last_trigger = time.time()
            if _trigger_files_index_rebuild_async(reason="scan_started"):
                files_live_index_last_trigger = time.time()
                logging.info("Files library live sync: initial index rebuild started")
        log_scan(
            "SCAN [%s] %d artist(s), %d album(s)%s",
            scan_type,
            total_artists,
            total_albums,
            f" ‚Äì Section ID(s): {SECTION_IDS}" if _get_library_mode() == "plex" else " (Files backend)",
        )

        # --- Discord: announce scan start ---
        notify_discord_embed(
            title="üîÑ PMDA scan started",
            description=(
                f"Scanning {len(artists_merged)} artists / {total_albums} albums‚Ä¶ "
                "Buckle up!"
            )
        )

        logging.debug(
            f"background_scan(): {len(artists_merged)} artists (merged by name), {total_albums} albums total"
        )

        # Reload AI config from DB and run functional check (probe/ladder for OpenAI)
        _reload_ai_config_and_reinit()
        logging.debug("Scan: AI reload/reinit done. ai_provider_ready=%s", ai_provider_ready)

        if not ai_provider_ready:
            logging.warning("background_scan(): AI not configured or functional check failed; aborting scan.")
            _set_resume_run_status(resume_run_id, "failed")
            with lock:
                state["scanning"] = False
                state["scan_ai_preflight_error"] = getattr(sys.modules[__name__], "AI_FUNCTIONAL_ERROR_MSG", None) or "Configure the AI provider in Settings."
                state["scan_resume_run_id"] = resume_run_id
            return

        # Reset live state. Step-based progress: total_steps = 3*albums + 2 (+1 if auto-move).
        scan_start_epoch = time.time()
        with lock:
            state.update(scanning=True, scan_progress=0, scan_total=total_albums + 2)
            state["scan_type"] = scan_type
            state["scan_resume_run_id"] = resume_run_id
            state["scan_total_albums"] = total_albums
            state["scan_detected_artists_total"] = int(detected_artists_total or 0)
            state["scan_detected_albums_total"] = int(detected_albums_total or 0)
            state["scan_resume_skipped_artists"] = int(resume_skipped_artists or 0)
            state["scan_resume_skipped_albums"] = int(resume_skipped_albums or 0)
            state["scan_step_progress"] = 0
            # scan_step_total set after _reload_auto_move_from_db() so AUTO_MOVE_DUPES is current
            state["duplicates"].clear()
            # Initialize scan details tracking
            state["scan_artists_processed"] = 0
            state["scan_artists_total"] = total_artists
            state["scan_ai_used_count"] = 0
            state["scan_mb_used_count"] = 0
            state["scan_ai_enabled"] = ai_provider_ready
            state["scan_mb_enabled"] = USE_MUSICBRAINZ
            # Initialize ETA tracking
            state["scan_start_time"] = scan_start_epoch
            state["scan_last_update_time"] = scan_start_epoch
            state["scan_last_progress"] = 0
            state["scan_format_done_count"] = 0
            state["scan_mb_done_count"] = 0
            state["scan_active_artists"] = {}
            # Initialize cache tracking
            state["scan_audio_cache_hits"] = 0
            state["scan_audio_cache_misses"] = 0
            state["scan_mb_cache_hits"] = 0
            state["scan_mb_cache_misses"] = 0
            # Initialize detailed statistics tracking
            state["scan_duplicate_groups_count"] = 0
            state["scan_total_duplicates_count"] = 0
            state["scan_broken_albums_count"] = 0
            state["scan_missing_albums_count"] = 0
            state["scan_albums_without_artist_image"] = 0
            state["scan_albums_without_album_image"] = 0
            state["scan_albums_without_complete_tags"] = 0
            state["scan_albums_without_mb_id"] = 0
            state["scan_albums_without_artist_mb_id"] = 0
            state["scan_mb_verified_by_ai_count"] = 0
            state["scan_discogs_matched"] = 0
            state["scan_lastfm_matched"] = 0
            state["scan_bandcamp_matched"] = 0
            # PMDA-level per-scan stats (albums processed/complete, with cover/artist image)
            state["scan_pmda_albums_processed"] = 0
            state["scan_pmda_albums_complete"] = 0
            state["scan_pmda_albums_with_cover"] = 0
            state["scan_pmda_albums_with_artist_image"] = 0
            state["scan_incomplete_moved_count"] = 0
            state["scan_incomplete_moved_mb"] = 0
            state["scan_player_sync_target"] = None
            state["scan_player_sync_ok"] = None
            state["scan_player_sync_message"] = ""
            state["scan_ai_errors"] = []
            state["scan_steps_log"] = []  # Per-step log for "what was done" (append after each artist)
            # Aggregate explainable dupe detection stats across the scan (Dupe Detection v2).
            state["scan_dupe_report"] = {
                "version": 2,
                "groups_total": 0,
                "groups_needs_ai": 0,
                "groups_by_signal": {},
                "rejected_by_reason": {},
            }
            state["scan_post_processing"] = False
            state["scan_post_total"] = 0
            state["scan_post_done"] = 0
            state["scan_post_current_artist"] = None
            state["scan_post_current_album"] = None
            state["scan_discovery_running"] = False
            state["scan_discovery_current_root"] = None
            state["scan_discovery_roots_done"] = 0
            state["scan_discovery_roots_total"] = 0
            state["scan_discovery_files_found"] = 0
            state["scan_discovery_folders_found"] = 0
            state["scan_discovery_albums_found"] = 0
            state["scan_discovery_artists_found"] = 0
            # Preflight: store MB and AI connection status for end-of-scan summary
            mb_ok, ai_ok = _run_preflight_checks()
            state["scan_mb_connection_ok"] = mb_ok
            state["scan_ai_connection_ok"] = ai_ok
        
        # Reload AUTO_MOVE_DUPES from DB so scan uses current setting (UI may have toggled it)
        _reload_auto_move_from_db()
        pipeline_flags = _pipeline_flags_for_scan(scan_type, run_improve_after_requested)
        # Files changed-only scans can legitimately produce 0 albums to process (e.g. watcher noise,
        # or a folder that is unchanged+healthy and gets fast-skipped). In that case, skip expensive
        # pipeline steps (export rebuild, player sync, etc.) so the auto background scan stays fast.
        if str(scan_type or "").strip().lower() == "changed_only" and int(total_albums or 0) == 0:
            pipeline_flags = dict(pipeline_flags)
            pipeline_flags.update(
                match_fix=False,
                dedupe=False,
                incomplete_move=False,
                export=False,
                player_sync=False,
                sync_target="none",
            )
        with lock:
            extra_steps = 0
            if pipeline_flags.get("dedupe"):
                extra_steps += 1
            if pipeline_flags.get("incomplete_move"):
                extra_steps += 1
            if pipeline_flags.get("export"):
                extra_steps += 1
            if pipeline_flags.get("player_sync"):
                extra_steps += 1
            # Step-based progress: 3 steps per album + AI step + finalize + optional pipeline extras.
            state["scan_step_total"] = 3 * total_albums + 2 + extra_steps
            state["scan_pipeline_flags"] = dict(pipeline_flags)
            state["scan_pipeline_sync_target"] = str(pipeline_flags.get("sync_target") or "none")

        # Create scan history entry
        con = sqlite3.connect(str(STATE_DB_FILE))
        cur = con.cursor()
        cur.execute("PRAGMA table_info(scan_history)")
        scan_cols = [r[1] for r in cur.fetchall()]
        if "entry_type" in scan_cols:
            cur.execute("""
                INSERT INTO scan_history 
                (start_time, albums_scanned, artists_total, ai_enabled, mb_enabled, auto_move_enabled, status,
                 duplicate_groups_count, total_duplicates_count, broken_albums_count, missing_albums_count,
                 albums_without_artist_image, albums_without_album_image, albums_without_complete_tags,
                 albums_without_mb_id, albums_without_artist_mb_id, entry_type)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, 'scan')
            """, (
                scan_start_epoch,
                total_albums,
                total_artists,
                1 if ai_provider_ready else 0,
                1 if USE_MUSICBRAINZ else 0,
                1 if bool(getattr(sys.modules[__name__], "AUTO_MOVE_DUPES", False)) else 0,
                'running',
                0, 0, 0, 0, 0, 0, 0, 0, 0  # Initialize all detailed stats to 0
            ))
        else:
            cur.execute("""
                INSERT INTO scan_history 
                (start_time, albums_scanned, artists_total, ai_enabled, mb_enabled, auto_move_enabled, status,
                 duplicate_groups_count, total_duplicates_count, broken_albums_count, missing_albums_count,
                 albums_without_artist_image, albums_without_album_image, albums_without_complete_tags,
                 albums_without_mb_id, albums_without_artist_mb_id)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                scan_start_epoch,
                total_albums,
                total_artists,
                1 if ai_provider_ready else 0,
                1 if USE_MUSICBRAINZ else 0,
                1 if bool(getattr(sys.modules[__name__], "AUTO_MOVE_DUPES", False)) else 0,
                'running',
                0, 0, 0, 0, 0, 0, 0, 0, 0  # Initialize all detailed stats to 0
            ))
        scan_id = cur.lastrowid
        con.commit()
        con.close()
        
        # Store scan_id in state for linking moves
        with lock:
            state["scan_id"] = scan_id
        _set_resume_run_status(resume_run_id, "running", scan_id=scan_id)

        # Clear scan_editions for this scan_id so only the latest run's data is stored
        con = sqlite3.connect(str(STATE_DB_FILE))
        con.execute("DELETE FROM scan_editions WHERE scan_id = ?", (scan_id,))
        con.execute("DELETE FROM duplicates_loser")
        con.execute("DELETE FROM duplicates_best")
        con.commit()
        con.close()

        clear_db_on_new_scan()  # wipe previous duplicate tables

        # Background writer for incremental persist (duplicates + scan_editions + scan_history)
        scan_incremental_queue = Queue()
        scan_incremental_writer_thread = None

        def _scan_incremental_writer():
            while True:
                item = scan_incremental_queue.get()
                if item is None:
                    break
                sid, aname, grps, eds = item
                try:
                    save_scan_artist_to_db(aname, grps)
                    save_scan_editions_artist_to_db(sid, aname, eds)
                    logging.debug(
                        "Incremental persist: %s (%d groups, %d editions)",
                        aname, len(grps), len(eds),
                    )
                    with lock:
                        update_scan_history_incremental(
                            sid,
                            artists_processed=state.get("scan_artists_processed", 0),
                            duplicates_found=sum(len(g) for g in state["duplicates"].values()),
                            duplicate_groups_count=state.get("scan_duplicate_groups_count", 0),
                            total_duplicates_count=state.get("scan_total_duplicates_count", 0),
                            broken_albums_count=state.get("scan_broken_albums_count", 0),
                            missing_albums_count=state.get("scan_missing_albums_count", 0),
                            albums_without_artist_image=state.get("scan_albums_without_artist_image", 0),
                            albums_without_album_image=state.get("scan_albums_without_album_image", 0),
                            albums_without_complete_tags=state.get("scan_albums_without_complete_tags", 0),
                            albums_without_mb_id=state.get("scan_albums_without_mb_id", 0),
                            albums_without_artist_mb_id=state.get("scan_albums_without_artist_mb_id", 0),
                        )
                except Exception as e:
                    logging.warning("Incremental scan persist failed for artist %s: %s", aname, e)

        scan_incremental_writer_thread = threading.Thread(target=_scan_incremental_writer, daemon=True)
        scan_incremental_writer_thread.start()

        should_request_improve = bool(pipeline_flags.get("match_fix"))
        if should_request_improve and _get_library_mode() == "files":
            # Files mode: run metadata fix/covers artist-by-artist during the scan so
            # Library updates are progressive instead of "all at end".
            scan_stream_post_by_artist = True
            streamed_post_process_done = True
            scan_post_queue = Queue()

            def _scan_postprocess_worker():
                providers = ["musicbrainz", "discogs", "lastfm", "bandcamp"]
                by_provider = {p: {"identified": 0, "covers": 0, "tags": 0} for p in providers}
                post_done = 0
                root_dirs = _files_root_dir_strings()
                def _update_edition_after_fix(artist_name: str, item: dict, result: dict) -> None:
                    folder_raw = (item.get("folder") or "").strip()
                    if not folder_raw:
                        return
                    try:
                        folder_path = Path(folder_raw)
                    except Exception:
                        return
                    ordered_paths = _files_collect_ordered_audio_paths(folder_path, item.get("ordered_paths") or [])
                    # Refresh tags from disk for accuracy.
                    tags = dict(item.get("meta") or {})
                    try:
                        if ordered_paths:
                            live_tags = extract_tags(ordered_paths[0]) or {}
                            if live_tags:
                                tags.update(live_tags)
                    except Exception:
                        pass

                    # Snapshot pre-fix health from the publish item so we can update scan counters by delta.
                    pre_missing_required = item.get("pre_missing_required_tags")
                    pre_has_cover = item.get("pre_has_cover")
                    pre_has_artist_image = item.get("pre_has_artist_image")
                    pre_has_mb_id = item.get("pre_has_mb_id")
                    pre_has_artist_mb_id = item.get("pre_has_artist_mb_id")

                    # Compute post-fix health.
                    edition_for_required: dict = {"tracks": list(item.get("tracks") or [])}
                    if not (edition_for_required.get("tracks") or []):
                        derived_tracks = [
                            {"title": p.stem or f"Track {i + 1}", "idx": i + 1}
                            for i, p in enumerate(ordered_paths)
                        ]
                        edition_for_required["tracks"] = derived_tracks
                    try:
                        missing_required_new = _check_required_tags(tags, REQUIRED_TAGS, edition=edition_for_required)
                    except Exception:
                        missing_required_new = []
                    try:
                        has_cover_new = bool(album_folder_has_cover(folder_path))
                    except Exception:
                        has_cover_new = False
                    try:
                        artist_folder = _files_guess_artist_folder(folder_path, artist_name, root_dirs=root_dirs)
                        has_artist_image_new = bool(_artist_folder_has_image(artist_folder)) if artist_folder else False
                    except Exception:
                        has_artist_image_new = False
                    has_mb_id_new = bool(
                        tags.get("musicbrainz_releasegroupid")
                        or tags.get("musicbrainz_releaseid")
                        or result.get("musicbrainz_id")
                        or result.get("release_mbid")
                    )
                    has_artist_mb_id_new = bool(
                        tags.get("musicbrainz_albumartistid")
                        or tags.get("musicbrainz_artistid")
                        or tags.get("musicbrainz_albumartist_id")
                        or tags.get("musicbrainz_artist_id")
                    )
                    with lock:
                        editions = all_editions_by_artist.get(artist_name) or []
                        for e in editions:
                            if int(e.get("album_id") or 0) != int(item.get("album_id") or 0) and str(e.get("folder") or "") != folder_raw:
                                continue
                            e["meta"] = tags
                            if ordered_paths:
                                e["ordered_paths"] = [str(p) for p in ordered_paths]
                            if result.get("musicbrainz_id"):
                                e["musicbrainz_id"] = result.get("musicbrainz_id")
                            if result.get("provider_used"):
                                e["primary_metadata_source"] = result.get("provider_used")
                                e["metadata_source"] = result.get("provider_used")
                            if result.get("discogs_release_id"):
                                e["discogs_release_id"] = result.get("discogs_release_id")
                            if result.get("lastfm_album_mbid"):
                                e["lastfm_album_mbid"] = result.get("lastfm_album_mbid")
                            if result.get("bandcamp_album_url"):
                                e["bandcamp_album_url"] = result.get("bandcamp_album_url")
                            e["missing_required_tags"] = list(missing_required_new or [])
                            e["has_cover"] = bool(has_cover_new)
                            e["has_artist_image"] = bool(has_artist_image_new)

                            # Delta-adjust scan health counters so live stats reflect post-fix state.
                            try:
                                if pre_missing_required is not None:
                                    old_missing = bool(pre_missing_required)
                                    new_missing = bool(missing_required_new)
                                    if old_missing != new_missing:
                                        state["scan_albums_without_complete_tags"] = max(
                                            0,
                                            int(state.get("scan_albums_without_complete_tags", 0)) + (-1 if old_missing else 1),
                                        )
                                if pre_has_cover is not None:
                                    old_without = not bool(pre_has_cover)
                                    new_without = not bool(has_cover_new)
                                    if old_without != new_without:
                                        state["scan_albums_without_album_image"] = max(
                                            0,
                                            int(state.get("scan_albums_without_album_image", 0)) + (-1 if old_without else 1),
                                        )
                                if pre_has_artist_image is not None:
                                    old_without = not bool(pre_has_artist_image)
                                    new_without = not bool(has_artist_image_new)
                                    if old_without != new_without:
                                        state["scan_albums_without_artist_image"] = max(
                                            0,
                                            int(state.get("scan_albums_without_artist_image", 0)) + (-1 if old_without else 1),
                                        )
                                if pre_has_mb_id is not None:
                                    old_without = not bool(pre_has_mb_id)
                                    new_without = not bool(has_mb_id_new)
                                    if old_without != new_without:
                                        state["scan_albums_without_mb_id"] = max(
                                            0,
                                            int(state.get("scan_albums_without_mb_id", 0)) + (-1 if old_without else 1),
                                        )
                                if pre_has_artist_mb_id is not None:
                                    old_without = not bool(pre_has_artist_mb_id)
                                    new_without = not bool(has_artist_mb_id_new)
                                    if old_without != new_without:
                                        state["scan_albums_without_artist_mb_id"] = max(
                                            0,
                                            int(state.get("scan_albums_without_artist_mb_id", 0)) + (-1 if old_without else 1),
                                        )
                            except Exception:
                                logging.debug("Post-process counter delta update failed", exc_info=True)
                            break
                with lock:
                    state["scan_post_processing"] = True
                    state["scan_post_total"] = total_albums
                    state["scan_post_done"] = 0
                    state["scan_post_current_artist"] = None
                    state["scan_post_current_album"] = None
                while True:
                    payload = scan_post_queue.get()
                    if payload is None:
                        scan_post_queue.task_done()
                        break
                    artist_name_for_batch, items = payload
                    batch_results: dict[int, dict] = {}
                    try:
                        for item in items:
                            if scan_should_stop.is_set():
                                break
                            try:
                                _idx, album_id, album_title, artist_name, result, _steps = _improve_one_album_item(item)
                            except Exception as post_err:
                                logging.warning("Post-process worker failed for artist %s item: %s", artist_name_for_batch, post_err)
                                continue
                            try:
                                _update_edition_after_fix(artist_name, item, result or {})
                            except Exception:
                                logging.debug("Post-process edition refresh failed for %s", artist_name, exc_info=True)
                            try:
                                batch_results[int(album_id)] = dict(result or {})
                            except Exception:
                                pass

                            prov = result.get("provider_used") or "musicbrainz"
                            if prov in by_provider:
                                if result.get("tags_updated") or result.get("cover_saved"):
                                    by_provider[prov]["identified"] += 1
                                if result.get("cover_saved"):
                                    by_provider[prov]["covers"] += 1
                                if result.get("tags_updated"):
                                    by_provider[prov]["tags"] += 1

                            post_done += 1
                            with lock:
                                state["scan_post_done"] = post_done
                                state["scan_post_current_artist"] = artist_name
                                state["scan_post_current_album"] = album_title
                                if result.get("pmda_matched") or result.get("pmda_cover") or result.get("pmda_artist_image"):
                                    state["scan_pmda_albums_processed"] = state.get("scan_pmda_albums_processed", 0) + 1
                                if result.get("pmda_cover"):
                                    state["scan_pmda_albums_with_cover"] = state.get("scan_pmda_albums_with_cover", 0) + 1
                                if result.get("pmda_artist_image"):
                                    state["scan_pmda_albums_with_artist_image"] = state.get("scan_pmda_albums_with_artist_image", 0) + 1
                                if result.get("pmda_complete"):
                                    state["scan_pmda_albums_complete"] = state.get("scan_pmda_albums_complete", 0) + 1
                                step_log = state.get("scan_steps_log") or []
                                step_log.append(
                                    f"[post] {artist_name} ‚Äî {album_title}: "
                                    f"{'tags' if result.get('tags_updated') else 'no-tags'} / "
                                    f"{'cover' if result.get('cover_saved') else 'no-cover'}"
                                )
                                if len(step_log) > 200:
                                    step_log = step_log[-200:]
                                state["scan_steps_log"] = step_log
                            if _get_library_mode() == "files":
                                _refresh_files_album_scan_cache_from_editions(
                                    [
                                        {
                                            "folder": item.get("folder"),
                                            "artist": artist_name,
                                            "artist_name": artist_name,
                                            "title_raw": album_title,
                                            "album_title": album_title,
                                            "musicbrainz_id": item.get("musicbrainz_id") or result.get("musicbrainz_id"),
                                            "meta": item.get("meta") or {},
                                            "tracks": item.get("tracks") or [],
                                            "ordered_paths": item.get("ordered_paths") or [],
                                            "fingerprint": item.get("fingerprint") or "",
                                            "provider_used": result.get("provider_used"),
                                            "metadata_source": (
                                                result.get("provider_used")
                                                or result.get("pmda_match_provider")
                                                or item.get("metadata_source")
                                                or item.get("primary_metadata_source")
                                                or ""
                                            ),
                                            "identity_provider": (
                                                result.get("provider_used")
                                                or result.get("pmda_match_provider")
                                                or item.get("metadata_source")
                                                or item.get("primary_metadata_source")
                                                or ""
                                            ),
                                            "discogs_release_id": (
                                                result.get("discogs_release_id")
                                                or item.get("discogs_release_id")
                                                or ""
                                            ),
                                            "lastfm_album_mbid": (
                                                result.get("lastfm_album_mbid")
                                                or item.get("lastfm_album_mbid")
                                                or ""
                                            ),
                                            "bandcamp_album_url": (
                                                result.get("bandcamp_album_url")
                                                or item.get("bandcamp_album_url")
                                                or ""
                                            ),
                                        }
                                    ],
                                    scan_id=scan_id,
                                )
                    finally:
                        if _get_library_mode() == "files":
                            try:
                                _publish_files_library_artist_from_items(
                                    artist_name_for_batch,
                                    items,
                                    scan_id=scan_id,
                                    results_by_album_id=batch_results,
                                )
                            except Exception:
                                logging.debug(
                                    "Files publication failed for artist %s in post worker",
                                    artist_name_for_batch,
                                    exc_info=True,
                                )
                            rebuild_reason = f"scan_artist_ready_{artist_name_for_batch}"
                            try:
                                res = _rebuild_files_library_index_for_artist(
                                    artist_name_for_batch,
                                    reason=rebuild_reason,
                                    wait_if_running=True,
                                )
                                if not res.get("ok"):
                                    logging.debug(
                                        "Files artist index sync returned non-ok for %s in post worker: %s",
                                        artist_name_for_batch,
                                        res.get("error") or res,
                                    )
                            except Exception:
                                logging.debug(
                                    "Files artist index sync failed for %s",
                                    artist_name_for_batch,
                                    exc_info=True,
                                )
                        scan_post_queue.task_done()

                with lock:
                    state["scan_post_processing"] = False
                    state["scan_post_current_artist"] = None
                    state["scan_post_current_album"] = None
                    state["last_fix_all_by_provider"] = by_provider
                    state["last_fix_all_total_albums"] = post_done

            scan_post_worker_thread = threading.Thread(target=_scan_postprocess_worker, daemon=True)
            scan_post_worker_thread.start()

        futures = []
        import concurrent.futures
        future_to_albums: dict[concurrent.futures.Future, int] = {}
        future_to_artist: dict[concurrent.futures.Future, str] = {}
        with ThreadPoolExecutor(max_workers=SCAN_THREADS) as executor:
            for primary_id, artist_name, album_ids_list in artists_merged:
                album_cnt = len(album_ids_list)
                # Track artist before submitting
                with lock:
                    state["scan_active_artists"][artist_name] = {
                        "start_time": time.time(),
                        "total_albums": album_cnt,
                        "albums_processed": 0
                    }
                _set_resume_artist_status(resume_run_id, artist_name, "running")
                # Pass (artist_id, artist_name, album_ids) so worker uses combined albums (merged by name)
                fut = executor.submit(scan_artist_duplicates, (primary_id, artist_name, album_ids_list))
                futures.append(fut)
                future_to_albums[fut] = album_cnt
                future_to_artist[fut] = artist_name

            artists_processed = 0
            for future in as_completed(futures):
                # Allow stop/pause mid‚Äëscan
                if scan_should_stop.is_set():
                    break
                album_cnt = future_to_albums.get(future, 0)
                artist_name = future_to_artist.get(future, "<unknown>")
                stats = {"ai_used": 0, "mb_used": 0}
                artist_failed = False
                artist_error = None
                try:
                    result = future.result()
                    if len(result) == 5:
                        artist_name, groups, _, stats, all_editions = result
                        all_editions_by_artist[artist_name] = all_editions
                    elif len(result) == 4:
                        artist_name, groups, _, stats = result
                        all_editions_by_artist[artist_name] = []
                    else:
                        # Backward compatibility: old format without stats
                        artist_name, groups, _ = result
                        stats = {"ai_used": 0, "mb_used": 0}
                        all_editions_by_artist[artist_name] = []
                except Exception as e:
                    logging.exception("Worker crash for artist %s: %s", artist_name, e)
                    worker_errors.put((artist_name, str(e)))
                    artist_failed = True
                    artist_error = str(e)
                    groups = []
                    stats = {"ai_used": 0, "mb_used": 0}
                    all_editions_by_artist[artist_name] = []
                finally:
                    with lock:
                        state["scan_progress"] += album_cnt
                        state["scan_step_progress"] = state.get("scan_step_progress", 0) + album_cnt  # compare step: 1 per album
                        state["scan_artists_processed"] += 1
                        state["scan_ai_used_count"] += stats.get("ai_used", 0)
                        state["scan_mb_used_count"] += stats.get("mb_used", 0)
                        state["scan_audio_cache_hits"] += stats.get("audio_cache_hits", 0)
                        state["scan_audio_cache_misses"] += stats.get("audio_cache_misses", 0)
                        state["scan_mb_cache_hits"] += stats.get("mb_cache_hits", 0)
                        state["scan_mb_cache_misses"] += stats.get("mb_cache_misses", 0)
                        # Aggregate detailed statistics
                        state["scan_duplicate_groups_count"] += stats.get("duplicate_groups_count", 0)
                        state["scan_total_duplicates_count"] += stats.get("total_duplicates_count", 0)
                        state["scan_broken_albums_count"] += stats.get("broken_albums_count", 0)
                        state["scan_albums_without_mb_id"] += stats.get("albums_without_mb_id", 0)
                        state["scan_albums_without_artist_mb_id"] += stats.get("albums_without_artist_mb_id", 0)
                        state["scan_mb_verified_by_ai_count"] += stats.get("mb_verified_by_ai", 0)
                        state["scan_albums_without_complete_tags"] += stats.get("albums_without_complete_tags", 0)
                        state["scan_albums_without_album_image"] += stats.get("albums_without_album_image", 0)
                        state["scan_albums_without_artist_image"] += stats.get("albums_without_artist_image", 0)
                        # Merge dupe_report (per-artist) into a scan-level report for comparisons between builds.
                        dr = stats.get("dupe_report")
                        if isinstance(dr, dict):
                            agg = state.get("scan_dupe_report")
                            if not isinstance(agg, dict):
                                agg = {
                                    "version": 2,
                                    "groups_total": 0,
                                    "groups_needs_ai": 0,
                                    "groups_by_signal": {},
                                    "rejected_by_reason": {},
                                }
                            try:
                                agg["version"] = int(agg.get("version") or dr.get("version") or 2)
                            except Exception:
                                agg["version"] = 2
                            agg["groups_total"] = int(agg.get("groups_total") or 0) + int(dr.get("groups_total") or 0)
                            agg["groups_needs_ai"] = int(agg.get("groups_needs_ai") or 0) + int(dr.get("groups_needs_ai") or 0)
                            for bucket in ("groups_by_signal", "rejected_by_reason"):
                                dst = agg.get(bucket)
                                if not isinstance(dst, dict):
                                    dst = {}
                                    agg[bucket] = dst
                                src = dr.get(bucket) or {}
                                if isinstance(src, dict):
                                    for k, v in src.items():
                                        key = str(k)
                                        try:
                                            dst[key] = int(dst.get(key) or 0) + int(v or 0)
                                        except Exception:
                                            continue
                            state["scan_dupe_report"] = agg
                        # Remove artist from active tracking when done
                        if artist_name in state.get("scan_active_artists", {}):
                            del state["scan_active_artists"][artist_name]
                        # Append one line to steps log for this artist (bounded to avoid unbounded growth)
                        step_log = state.get("scan_steps_log") or []
                        n_albums = album_cnt
                        n_grps = stats.get("duplicate_groups_count", 0)
                        n_broken = stats.get("broken_albums_count", 0)
                        n_mb = max(0, n_albums - stats.get("albums_without_mb_id", 0))
                        step_log.append(
                            f"{artist_name}: {n_albums} albums, MB identified {n_mb}, broken {n_broken}, duplicate groups {n_grps}"
                        )
                        # Keep only the latest 200 entries so JSON payloads and DB rows stay small
                        if len(step_log) > 200:
                            step_log = step_log[-200:]
                        state["scan_steps_log"] = step_log
                        if groups:
                            all_results[artist_name] = groups
                            state["duplicates"][artist_name] = groups
                        # Enqueue for incremental persist (duplicates + scan_editions + scan_history)
                        scan_incremental_queue.put((scan_id, artist_name, groups, all_editions_by_artist.get(artist_name, [])))
                    if _get_library_mode() == "files":
                        _refresh_files_album_scan_cache_from_editions(
                            all_editions_by_artist.get(artist_name, []),
                            scan_id=scan_id,
                        )
                    publish_items: list[dict] = []
                    if _get_library_mode() == "files":
                        publish_items = _build_improve_items_from_editions(
                            artist_name,
                            all_editions_by_artist.get(artist_name, []),
                        )
                    _set_resume_artist_status(
                        resume_run_id,
                        artist_name,
                        "failed" if artist_failed else "done",
                        error=artist_error,
                    )
                    if scan_stream_post_by_artist and scan_post_queue is not None:
                        if publish_items:
                            scan_post_queue.put((artist_name, publish_items))
                    elif _get_library_mode() == "files" and publish_items:
                        try:
                            _publish_files_library_artist_from_items(
                                artist_name,
                                publish_items,
                                scan_id=scan_id,
                                results_by_album_id={},
                            )
                        except Exception:
                            logging.debug("Files publication failed for artist %s", artist_name, exc_info=True)
                        rebuild_reason = f"scan_artist_ready_{artist_name}"
                        try:
                            res = _rebuild_files_library_index_for_artist(
                                artist_name,
                                reason=rebuild_reason,
                                wait_if_running=True,
                            )
                            if not res.get("ok"):
                                logging.debug(
                                    "Files artist index sync returned non-ok for %s: %s",
                                    artist_name,
                                    res.get("error") or res,
                                )
                        except Exception:
                            logging.debug("Files artist index sync failed for %s", artist_name, exc_info=True)
                    artists_processed += 1
                    if _get_library_mode() == "files":
                        now_ts = time.time()
                        if (now_ts - files_live_index_last_trigger) >= files_live_index_interval_sec:
                            files_live_index_last_trigger = now_ts
                            if _trigger_files_index_rebuild_async(reason=f"scan_live_sync_{artists_processed}"):
                                logging.debug(
                                    "Files library live sync: triggered rebuild after %d/%d artist(s)",
                                    artists_processed,
                                    total_artists,
                                )
                    # Log scan progress every 10 artists or if debug/verbose, using a tree-style line
                    if artists_processed % 10 == 0 or logging.getLogger().isEnabledFor(logging.DEBUG):
                        log_scan(
                            "‚îú‚îÄ Artist %s (%d/%d processed, %d duplicate group(s))",
                            artist_name,
                            artists_processed,
                            total_artists,
                            n_grps,
                        )

        # Collect all groups requiring AI processing (must be kept in scan_duplicates output, not filtered by "losers")
        ai_groups_to_process = []
        ai_group_positions = {}  # Track position of each AI group for replacement (key = artist + sorted edition ids)
        for artist_name, groups in all_results.items():
            for i, group in enumerate(groups):
                if group.get("needs_ai", False):
                    ai_groups_to_process.append(group)
                    # Normalize to sorted ints so key matches when merging ai_result (same type/order)
                    key = (artist_name, tuple(sorted(int(e['album_id']) for e in group.get("editions", []))))
                    ai_group_positions[key] = (artist_name, i)
        log_ai(
            "‚îú‚îÄ [AI] Duplicate selection: %d group(s) requiring AI from %d artist(s) with duplicates",
            len(ai_groups_to_process),
            sum(1 for a, grps in all_results.items() if any(g.get("needs_ai") for g in grps)),
        )
        
        # Process AI groups in parallel batch
        if ai_groups_to_process and ai_provider_ready:
            log_ai(
                "‚îÇ  ‚îú‚îÄ Processing %d group(s) requiring AI in parallel batch (max %d concurrent)‚Ä¶",
                len(ai_groups_to_process),
                AI_BATCH_SIZE,
            )
            with lock:
                state["scan_active_artists"]["_ai_batch"] = {
                    "start_time": time.time(),
                    "total_groups": len(ai_groups_to_process),
                    "groups_processed": 0
                }
                state["scan_ai_batch_total"] = len(ai_groups_to_process)
                state["scan_ai_batch_processed"] = 0
                state["scan_ai_current_label"] = None
            
            ai_results = process_ai_groups_batch(ai_groups_to_process, max_workers=AI_BATCH_SIZE)
            log_ai(
                "‚îÇ  ‚îî‚îÄ AI batch returned %d result(s) for %d group(s)",
                len(ai_results),
                len(ai_groups_to_process),
            )
            
            # Update all_results with AI-processed groups; ensure used_ai/ai_provider/ai_model so Unduper shows "AI"
            mod = sys.modules[__name__]
            for ai_result in ai_results:
                try:
                    artist_name = ai_result.get("artist")
                    best = ai_result.get("best")
                    losers = ai_result.get("losers")
                    if not artist_name or best is None or not isinstance(losers, list):
                        continue
                    best.setdefault("used_ai", True)
                    if not best.get("ai_provider"):
                        best["ai_provider"] = getattr(mod, "AI_PROVIDER", None) or ""
                    if not best.get("ai_model"):
                        best["ai_model"] = getattr(mod, "RESOLVED_MODEL", None) or getattr(mod, "OPENAI_MODEL", None) or ""
                    # Normalize to sorted ints to match ai_group_positions key
                    result_edition_ids = tuple(sorted(int(e.get("album_id") or 0) for e in [best] + losers))
                    key = (artist_name, result_edition_ids)
                    
                    if key in ai_group_positions:
                        target_artist, target_index = ai_group_positions[key]
                        if target_artist in all_results and target_index < len(all_results[target_artist]):
                            all_results[target_artist][target_index] = ai_result
                        else:
                            if target_artist not in all_results:
                                all_results[target_artist] = []
                            all_results[target_artist].append(ai_result)
                    else:
                        if artist_name not in all_results:
                            all_results[artist_name] = []
                        all_results[artist_name].append(ai_result)
                except Exception as merge_err:
                    logging.warning("Skipping malformed AI result: %s", merge_err)
            
            # Update state (start_time was stored with time.time(), so use time.time() for duration)
            _ai_batch = state.get("scan_active_artists", {}).get("_ai_batch", {})
            ai_batch_start = _ai_batch.get("start_time", time.time())
            ai_batch_time = time.time() - ai_batch_start
            with lock:
                if "_ai_batch" in state.get("scan_active_artists", {}):
                    del state["scan_active_artists"]["_ai_batch"]
                state.pop("scan_ai_batch_total", None)
                state.pop("scan_ai_batch_processed", None)
                state.pop("scan_ai_current_label", None)
                state["scan_ai_used_count"] += len(ai_results)
                # Update duplicates in state
                state["duplicates"] = all_results
                # Progress: step 2/3 done (albums + AI batch); bar shows e.g. 39/40 until finalize
                state["scan_progress"] = state["scan_total"] - 1
                state["scan_step_progress"] = state.get("scan_step_progress", 0) + 1  # AI batch step done
            logging.info(
                f"AI batch processing completed: {len(ai_results)}/{len(ai_groups_to_process)} groups processed successfully "
                f"in {ai_batch_time:.2f}s (avg {ai_batch_time/max(len(ai_groups_to_process), 1):.2f}s per group)"
            )
        else:
            # No AI batch run (no groups or no provider): still count the AI step so progress reaches step_total
            with lock:
                state["scan_step_progress"] = state.get("scan_step_progress", 0) + 1
        
        # Final pass: any group still with needs_ai and no best/losers -> run AI (no heuristic fallback)
        fallback_count = 0
        for artist_name, groups in list(all_results.items()):
            for i, g in enumerate(groups):
                if g.get("needs_ai", False) and "best" not in g and "editions" in g:
                    editions = g["editions"]
                    if len(editions) >= 2:
                        best = choose_best(editions, defer_ai=False)
                        if best:
                            losers = [e for e in editions if e["album_id"] != best["album_id"]]
                            all_results[artist_name][i] = {
                                "artist": artist_name,
                                "album_id": best["album_id"],
                                "best": best,
                                "losers": losers,
                                "fuzzy": g.get("fuzzy", False),
                                "needs_ai": False,
                            }
                            fallback_count += 1
        if fallback_count:
            logging.info("Final pass: applied AI selection to %d group(s) that had no best/losers.", fallback_count)
            with lock:
                state["scan_ai_used_count"] = state.get("scan_ai_used_count", 0) + fallback_count
                state["duplicates"] = all_results

        # Recompute dupe counts from final all_results (after AI merge + final pass),
        # so scan_history stats match what actually happened (even when groups were unresolved during per-artist threads).
        try:
            final_groups = 0
            final_losers = 0
            final_needs_ai = 0
            for _a, _grps in (all_results or {}).items():
                for _g in (_grps or []):
                    if isinstance(_g, dict) and _g.get("needs_ai", False):
                        final_needs_ai += 1
                    if not isinstance(_g, dict):
                        continue
                    best = _g.get("best")
                    losers = _g.get("losers")
                    if best is not None and isinstance(losers, list):
                        final_groups += 1
                        final_losers += len(losers or [])
            with lock:
                state["scan_duplicate_groups_count"] = int(final_groups)
                state["scan_total_duplicates_count"] = int(final_losers)
                state["scan_dupe_groups_needs_ai"] = int(final_needs_ai)
        except Exception:
            logging.debug("Final dupe count recompute failed", exc_info=True)
        
        # Calculate missing albums count (compare Plex albums with MusicBrainz)
        # This is a simplified version - for now, we'll set it to 0 as calculating it requires
        # MusicBrainz API calls for each artist which could be slow during scan
        # This can be implemented later as a post-scan analysis or background task
        missing_albums_total = 0
        with lock:
            state["scan_missing_albums_count"] = missing_albums_total

        # Drain incremental writer queue so all artist data is persisted before final save_scan_to_db
        if scan_incremental_writer_thread is not None and scan_incremental_queue is not None:
            scan_incremental_queue.put(None)
            scan_incremental_writer_thread.join(timeout=120)

        # Drain post-processing queue (Files mode streamed improve) before finalizing.
        if scan_post_worker_thread is not None and scan_post_queue is not None:
            scan_post_queue.put(None)
            scan_post_queue.join()
            scan_post_worker_thread.join(timeout=600)

        # Persist is done in finally so we always save (even on stop/exception); auto-move runs synchronously in finally after save.

    finally:
        # "Finalizing": persist to DB and update scan_history before marking scan done.
        # UI shows "Finalizing" until this is complete; only then scanning=False and stats appear.
        with lock:
            state["scan_finalizing"] = True
            state["last_dedupe_moved_count"] = 0
            state["last_dedupe_saved_mb"] = 0
            # Progress: ensure we're at step 2/3 (e.g. 39/40) during finalize; if there was no AI batch we were still at 38
            state["scan_progress"] = max(state["scan_progress"], state["scan_total"] - 1)
            state["scan_step_progress"] = state.get("scan_step_progress", 0) + 1  # finalize step started
        if scan_post_worker_thread is not None and scan_post_queue is not None and scan_post_worker_thread.is_alive():
            # If we arrived here through an exception path, stop and drain safely.
            try:
                scan_post_queue.put(None)
                scan_post_queue.join()
                scan_post_worker_thread.join(timeout=600)
            except Exception:
                logging.debug("Post-process queue shutdown in finally failed", exc_info=True)
        try:
            save_scan_to_db(all_results)
            with lock:
                state["duplicates"] = all_results
        except Exception as e:
            logging.warning("save_scan_to_db in finally failed: %s", e)
        try:
            _scan_id = state.get("scan_id")
            if _scan_id and all_editions_by_artist is not None:
                save_scan_editions_to_db(_scan_id, all_editions_by_artist)
        except Exception as e:
            logging.warning("save_scan_editions_to_db in finally failed: %s", e)
        # Safety rule: do not infer "broken/incomplete" state from duplicate groups.
        # Incomplete moves must only use deterministic broken detection computed per-edition
        # (track index gaps, etc.). This avoids large false positives and unintended mass moves.
        # Pipeline step: move incomplete albums to configured quarantine folder.
        # Run this *before* dedupe so "broken" variants don't get moved as dupes.
        incomplete_move_result = {"moved": 0, "size_mb": 0, "errors": 0}
        if pipeline_flags.get("incomplete_move"):
            logging.info("Pipeline step incomplete-move: scanning broken albums from current run...")
            try:
                incomplete_move_result = _auto_move_incomplete_albums_for_scan(
                    int(state.get("scan_id") or 0),
                    all_editions_by_artist,
                )
                logging.info(
                    "Pipeline step incomplete-move: moved %d album(s), %d MB, errors=%d",
                    int(incomplete_move_result.get("moved") or 0),
                    int(incomplete_move_result.get("size_mb") or 0),
                    int(incomplete_move_result.get("errors") or 0),
                )
            except Exception:
                logging.exception("Pipeline step incomplete-move failed")
            with lock:
                state["scan_incomplete_moved_count"] = int(incomplete_move_result.get("moved") or 0)
                state["scan_incomplete_moved_mb"] = int(incomplete_move_result.get("size_mb") or 0)
                state["scan_step_progress"] = state.get("scan_step_progress", 0) + 1

        # Pipeline step: dedupe (synchronous so scan summary includes moved counts).
        # Guarded by AUTO_MOVE_DUPES: when disabled, we must never move anything automatically.
        # Magic mode implies automatic dedupe moves (users expect "fully automatic" behavior).
        auto_move_dup = bool(getattr(sys.modules[__name__], "AUTO_MOVE_DUPES", False) or getattr(sys.modules[__name__], "MAGIC_MODE", False))
        if pipeline_flags.get("dedupe") and all_results and auto_move_dup:
            flat_groups = [g for groups in all_results.values() for g in groups]
            # One group per unique (artist, set of editions) so we never move both editions of the same album
            seen_group_keys = set()
            deduped_flat = []
            for g in flat_groups:
                # Safety: never auto-move groups explicitly marked as no-move/manual-review.
                if g.get("no_move") or g.get("manual_review") or g.get("same_folder"):
                    continue
                best = g.get("best")
                losers = g.get("losers", [])
                if not best or not losers:
                    continue
                # Never auto-move broken/incomplete editions as dupes; those belong in the incomplete quarantine step.
                filtered_losers = []
                for e in losers:
                    try:
                        if e.get("is_broken", False):
                            continue
                    except Exception:
                        pass
                    try:
                        folder = path_for_fs_access(Path(str(e.get("folder") or "")))
                    except Exception:
                        folder = None
                    if not folder or (not folder.exists()):
                        continue
                    filtered_losers.append(e)
                if not filtered_losers:
                    continue
                g2 = dict(g)
                g2["losers"] = filtered_losers
                edition_ids = [int(best.get("album_id") or 0)]
                for e in filtered_losers:
                    edition_ids.append(int(e.get("album_id") or 0))
                key = (g2.get("artist") or "", tuple(sorted(edition_ids)))
                if key in seen_group_keys:
                    continue
                seen_group_keys.add(key)
                deduped_flat.append(g2)
            flat_groups = deduped_flat
            if flat_groups:
                logging.info("Pipeline step dedupe: running automatic deduplication (%d group(s))...", len(flat_groups))
                try:
                    background_dedupe(flat_groups)
                except Exception as e:
                    logging.warning("background_dedupe in finally failed: %s", e)
                    with lock:
                        state["deduping"] = False
                        state["dedupe_progress"] = 0
                        state["dedupe_total"] = 0
                        state["dedupe_start_time"] = None
                        state["dedupe_current_group"] = None
                        state["dedupe_last_write"] = None
                        state["dedupe_saved_this_run"] = 0
                logging.info("Pipeline step dedupe: done")
            with lock:
                state["scan_step_progress"] = state.get("scan_step_progress", 0) + 1
        elif pipeline_flags.get("dedupe") and all_results and (not auto_move_dup):
            logging.info("Pipeline step dedupe: AUTO_MOVE_DUPES is disabled; skipping automatic deduplication")
            with lock:
                state["scan_step_progress"] = state.get("scan_step_progress", 0) + 1

        # Pipeline step: export library (Files mode only).
        if pipeline_flags.get("export"):
            if _get_library_mode() == "files":
                logging.info("Pipeline step export: rebuilding Files export library...")
                try:
                    with lock:
                        state["scan_post_processing"] = True
                        state["scan_post_current_artist"] = "Library"
                        state["scan_post_current_album"] = "Export rebuild"
                    _run_export_library()
                except Exception:
                    logging.exception("Pipeline step export failed")
                finally:
                    with lock:
                        state["scan_post_processing"] = False
                        state["scan_post_current_artist"] = None
                        state["scan_post_current_album"] = None
            else:
                logging.info("Pipeline step export skipped: not in Files mode")
            with lock:
                state["scan_step_progress"] = state.get("scan_step_progress", 0) + 1

        # Pipeline step: sync external player library.
        sync_result_ok = None
        sync_result_msg = ""
        sync_target = str(pipeline_flags.get("sync_target") or "none")
        if pipeline_flags.get("player_sync"):
            logging.info("Pipeline step player-sync: target=%s", sync_target)
            try:
                sync_result_ok, sync_result_msg = _trigger_player_refresh_by_target(sync_target)
                if sync_result_ok:
                    logging.info("Pipeline step player-sync: %s", sync_result_msg)
                else:
                    logging.warning("Pipeline step player-sync failed: %s", sync_result_msg)
            except Exception:
                sync_result_ok = False
                sync_result_msg = "Unexpected error while triggering player refresh"
                logging.exception("Pipeline step player-sync failed")
            with lock:
                state["scan_player_sync_target"] = sync_target
                state["scan_player_sync_ok"] = bool(sync_result_ok)
                state["scan_player_sync_message"] = sync_result_msg
                state["scan_step_progress"] = state.get("scan_step_progress", 0) + 1
        end_time = time.time()
        scan_id = None
        with lock:
            state["scan_progress"] = state["scan_total"]  # force 100 % before stopping
            state["scan_step_progress"] = state.get("scan_step_total", state["scan_step_progress"])  # ensure 100% for steps
            scan_id = state.get("scan_id")
            scan_start_epoch = state.get("scan_start_time") or end_time
        
        # Update scan history entry (summary_json etc.); only then mark scan done
        if scan_id:
            con = sqlite3.connect(str(STATE_DB_FILE))
            cur = con.cursor()
            duration = int(end_time - scan_start_epoch) if scan_start_epoch else None
            with lock:
                duplicates_found = sum(len(groups) for groups in all_results.values())
                artists_processed = state.get("scan_artists_processed", 0)
                ai_used_count = state.get("scan_ai_used_count", 0)
                mb_used_count = state.get("scan_mb_used_count", 0)
                space_saved = get_stat("space_saved")
                # Get detailed statistics
                duplicate_groups_count = state.get("scan_duplicate_groups_count", 0)
                total_duplicates_count = state.get("scan_total_duplicates_count", 0)
                broken_albums_count = state.get("scan_broken_albums_count", 0)
                missing_albums_count = state.get("scan_missing_albums_count", 0)  # Will be calculated separately
                albums_without_artist_image = state.get("scan_albums_without_artist_image", 0)
                albums_without_album_image = state.get("scan_albums_without_album_image", 0)
                albums_without_complete_tags = state.get("scan_albums_without_complete_tags", 0)
                albums_without_mb_id = state.get("scan_albums_without_mb_id", 0)
                albums_without_artist_mb_id = state.get("scan_albums_without_artist_mb_id", 0)
                # When auto-move ran this scan (in finally), these are set by background_dedupe
                dupes_moved_this_scan = state.get("last_dedupe_moved_count", 0)
                space_saved_mb_this_scan = state.get("last_dedupe_saved_mb", 0)
                albums_moved_this_scan = state.get("last_dedupe_moved_count", 0)
                incomplete_moved_this_scan = state.get("scan_incomplete_moved_count", 0)
                incomplete_moved_mb_this_scan = state.get("scan_incomplete_moved_mb", 0)
                player_sync_target = state.get("scan_player_sync_target")
                player_sync_ok = state.get("scan_player_sync_ok")
                player_sync_message = state.get("scan_player_sync_message")
            
            scan_status = 'cancelled' if scan_should_stop.is_set() else 'completed'
            # Build summary_json for end-of-scan summary (FFmpeg formats, MB, AI)
            mb_conn_ok = state.get("scan_mb_connection_ok", False)
            ai_conn_ok = state.get("scan_ai_connection_ok", False)
            mb_done = state.get("scan_mb_done_count", 0)
            mb_used = state.get("scan_mb_used_count", 0)
            ai_groups = state.get("scan_ai_used_count", 0)
            mb_verified_by_ai = state.get("scan_mb_verified_by_ai_count", 0)
            cur.execute(
                "SELECT fmt_text, COUNT(*) FROM scan_editions WHERE scan_id = ? GROUP BY fmt_text",
                (scan_id,),
            )
            ffmpeg_formats = {row[0] or "?": row[1] for row in cur.fetchall()}
            ai_errors_raw = state.get("scan_ai_errors", [])
            # Deduplicate by message, keep last occurrence; limit to 20 for summary
            # Exclude recovered "AI index out of range; clamped" so UI does not show them as errors
            seen_msg = set()
            ai_errors_dedup = []
            for entry in reversed(ai_errors_raw):
                msg = entry.get("message", "")
                if not msg or msg in seen_msg:
                    continue
                if "AI index out of range" in msg and "clamped" in msg:
                    continue
                seen_msg.add(msg)
                ai_errors_dedup.append(entry)
                if len(ai_errors_dedup) >= 20:
                    break
            ai_errors_dedup.reverse()
            ai_errors_total = len(ai_errors_dedup)
            # Duplicate decision stats from save_scan_to_db (fallback to counters when missing)
            duplicate_groups_saved = int(state.get("scan_duplicate_groups_saved", duplicate_groups_count))
            duplicate_groups_ai_saved = int(state.get("scan_duplicate_groups_ai_saved", 0))
            duplicate_groups_skipped = int(state.get("scan_duplicate_groups_skipped", 0))
            # Heuristic approximation of AI error recovery vs unresolved:
            # - groups recovered: min(errors, AI-decided groups)
            # - unresolved: remaining errors beyond recovered
            ai_failed_then_recovered = min(ai_errors_total, duplicate_groups_ai_saved)
            ai_failed_unresolved = max(ai_errors_total - ai_failed_then_recovered, 0)
            # Last-scan-only stats for "Last scan summary" UI (only this scan's numbers)
            artists_total = state.get("scan_artists_total", 0)
            albums_scanned = state.get("scan_total_albums", state.get("scan_total", 0))
            scan_discogs_matched = state.get("scan_discogs_matched", 0)
            scan_lastfm_matched = state.get("scan_lastfm_matched", 0)
            scan_bandcamp_matched = state.get("scan_bandcamp_matched", 0)
            audio_hits = state.get("scan_audio_cache_hits", 0)
            audio_misses = state.get("scan_audio_cache_misses", 0)
            mb_hits = state.get("scan_mb_cache_hits", 0)
            mb_misses = state.get("scan_mb_cache_misses", 0)
            albums_without_mb = state.get("scan_albums_without_mb_id", 0)
            albums_with_mb = max(0, albums_scanned - albums_without_mb) if albums_scanned else mb_used
            # Lossy vs lossless from ffmpeg_formats (lossless: FLAC, ALAC, WAV, AIFF, etc.)
            lossless_keys = {"FLAC", "ALAC", "WAV", "AIFF", "APE", "WV", "TAK"}
            lossless_count = sum(c for fmt, c in ffmpeg_formats.items() if (fmt or "").upper().strip() in lossless_keys)
            lossy_count = max(0, sum(ffmpeg_formats.values()) - lossless_count)
            # Cover provenance during scan (optional, may be missing on older runs)
            cover_from_mb = state.get("scan_cover_from_mb", 0)
            cover_from_discogs = state.get("scan_cover_from_discogs", 0)
            cover_from_lastfm = state.get("scan_cover_from_lastfm", 0)
            cover_from_bandcamp = state.get("scan_cover_from_bandcamp", 0)
            cover_from_web = state.get("scan_cover_from_web", 0)
            # PMDA album-level stats from improve-all (may be zero when Magic mode not run)
            pmda_albums_processed = state.get("scan_pmda_albums_processed", 0)
            pmda_albums_complete = state.get("scan_pmda_albums_complete", 0)
            pmda_albums_with_cover = state.get("scan_pmda_albums_with_cover", 0)
            pmda_albums_with_artist_image = state.get("scan_pmda_albums_with_artist_image", 0)

            summary = {
                "ffmpeg_formats": ffmpeg_formats,
                "mb_connection_ok": mb_conn_ok,
                "mb_albums_verified": mb_done,
                "mb_albums_identified": mb_used,
                "ai_connection_ok": ai_conn_ok,
                "ai_groups_count": ai_groups,
                "mb_verified_by_ai": mb_verified_by_ai,
                "ai_errors": ai_errors_dedup,
                # Duplicate decision stats
                "duplicate_groups_total": duplicate_groups_count,
                "duplicate_groups_saved": duplicate_groups_saved,
                "duplicate_groups_ai_decided": duplicate_groups_ai_saved,
                "duplicate_groups_skipped": duplicate_groups_skipped,
                "duplicate_groups_ai_failed_total": ai_errors_total,
                "duplicate_groups_ai_failed_then_recovered": ai_failed_then_recovered,
                "duplicate_groups_ai_failed_unresolved": ai_failed_unresolved,
                # Last-scan-only stats for "Last scan summary" UI
                "duration_seconds": duration,
                "artists_total": artists_total,
                "albums_scanned": albums_scanned,
                "duplicate_groups_count": duplicate_groups_count,
                "total_duplicates_count": total_duplicates_count,
                "broken_albums_count": broken_albums_count,
                "missing_albums_count": missing_albums_count,
                "albums_without_artist_image": albums_without_artist_image,
                "albums_without_album_image": albums_without_album_image,
                "albums_without_complete_tags": albums_without_complete_tags,
                "albums_without_mb_id": albums_without_mb_id,
                "albums_without_artist_mb_id": albums_without_artist_mb_id,
                "audio_cache_hits": audio_hits,
                "audio_cache_misses": audio_misses,
                "mb_cache_hits": mb_hits,
                "mb_cache_misses": mb_misses,
                "lossy_count": lossy_count,
                "lossless_count": lossless_count,
                "albums_with_mb_id": albums_with_mb,
                "albums_without_mb_id": albums_without_mb,
                # When auto-move ran this scan
                "dupes_moved_this_scan": dupes_moved_this_scan,
                "space_saved_mb_this_scan": space_saved_mb_this_scan,
                "incomplete_moved_this_scan": incomplete_moved_this_scan,
                "incomplete_moved_mb_this_scan": incomplete_moved_mb_this_scan,
                "player_sync_target": player_sync_target,
                "player_sync_ok": player_sync_ok,
                "player_sync_message": player_sync_message,
                # Fallback sources during scan (when MusicBrainz found nothing)
                "scan_discogs_matched": scan_discogs_matched,
                "scan_lastfm_matched": scan_lastfm_matched,
                "scan_bandcamp_matched": scan_bandcamp_matched,
                # Cover provenance (when Improve Album / fallbacks fetched covers)
                "cover_from_mb": cover_from_mb,
                "cover_from_discogs": cover_from_discogs,
                "cover_from_lastfm": cover_from_lastfm,
                "cover_from_bandcamp": cover_from_bandcamp,
                "cover_from_web": cover_from_web,
                # PMDA tags-based album health (albums touched by PMDA during this run)
                "pmda_albums_processed": pmda_albums_processed,
                "pmda_albums_complete": pmda_albums_complete,
                "pmda_albums_with_cover": pmda_albums_with_cover,
                "pmda_albums_with_artist_image": pmda_albums_with_artist_image,
            }
            summary["dupe_report"] = state.get("scan_dupe_report") or {}
            # Build human-readable list of steps executed (for History > Scan Details)
            steps_executed = []
            steps_executed.append("1. Format analysis (FFprobe): all albums analyzed for format/bitrate/sample rate.")
            if USE_MUSICBRAINZ:
                steps_executed.append(
                    f"2. MusicBrainz lookup: {mb_used} identified ({mb_done} release groups processed). "
                    f"Re-check not found: {'yes' if getattr(sys.modules[__name__], 'MB_RETRY_NOT_FOUND', False) else 'no'}."
                )
            else:
                steps_executed.append("2. MusicBrainz lookup: disabled.")
            steps_executed.append(
                f"3. AI to choose among MB candidates: {'enabled, ' + str(mb_verified_by_ai) + ' matches chosen' if getattr(sys.modules[__name__], 'USE_AI_FOR_MB_MATCH', False) else 'disabled'}."
            )
            steps_executed.append(
                f"4. AI to verify MB match: {'enabled, ' + str(mb_verified_by_ai) + ' verified' if getattr(sys.modules[__name__], 'USE_AI_FOR_MB_VERIFY', False) else 'disabled'}."
            )
            steps_executed.append(
                f"5. AI vision for cover comparison: {'enabled' if getattr(sys.modules[__name__], 'USE_AI_VISION_FOR_COVER', False) else 'disabled'}."
            )
            steps_executed.append(
                f"6. Web search (Serper) for MusicBrainz: {'enabled' if getattr(sys.modules[__name__], 'USE_WEB_SEARCH_FOR_MB', False) else 'disabled'}."
            )
            steps_executed.append(
                f"7. Discogs fallback: {'enabled, ' + str(scan_discogs_matched) + ' matched' if getattr(sys.modules[__name__], 'USE_DISCOGS', False) else 'disabled'}."
            )
            steps_executed.append(
                f"8. Last.fm fallback: {'enabled, ' + str(scan_lastfm_matched) + ' matched' if getattr(sys.modules[__name__], 'USE_LASTFM', False) else 'disabled'}."
            )
            steps_executed.append(
                f"9. Bandcamp fallback: {'enabled, ' + str(scan_bandcamp_matched) + ' matched' if getattr(sys.modules[__name__], 'USE_BANDCAMP', False) else 'disabled'}."
            )
            # REQUIRED_TAGS from settings = single source of truth
            req_tags = getattr(sys.modules[__name__], "REQUIRED_TAGS", ["artist", "album", "genre", "year"])
            tags_str = ", ".join((t or "").strip() or "?" for t in req_tags)
            steps_executed.append(
                f"10. Incomplete album definition (required tags: {tags_str}): {albums_without_complete_tags} without complete tags, {broken_albums_count} broken (missing tracks)."
            )
            steps_executed.append(
                f"11. Duplicate detection: {duplicate_groups_count} groups, {total_duplicates_count} total duplicate editions."
            )
            steps_executed.append(
                "12. Pipeline steps: "
                f"match_fix={'on' if pipeline_flags.get('match_fix') else 'off'}, "
                f"dedupe={'on' if pipeline_flags.get('dedupe') else 'off'}, "
                f"incomplete_move={'on' if pipeline_flags.get('incomplete_move') else 'off'}, "
                f"export={'on' if pipeline_flags.get('export') else 'off'}, "
                f"player_sync={'on' if pipeline_flags.get('player_sync') else 'off'}"
                + (f" (target={pipeline_flags.get('sync_target')})" if pipeline_flags.get("player_sync") else "")
            )
            summary["steps_executed"] = steps_executed
            summary["scan_steps_log"] = state.get("scan_steps_log") or []

            # Emit a compact end-of-scan stats block in logs for quick troubleshooting.
            def _pct(n: int, d: int) -> str:
                return "n/a" if d <= 0 else f"{(100.0 * float(n) / float(d)):.1f}%"

            albums_with_complete_tags = max(0, albums_scanned - albums_without_complete_tags)
            albums_with_cover = max(0, albums_scanned - albums_without_album_image)
            albums_with_artist_image = max(0, albums_scanned - albums_without_artist_image)
            bar = "‚îÄ" * 85
            logging.info("%s", bar)
            logging.info("SCAN SUMMARY [scan_id=%s, status=%s]", scan_id, scan_status)
            logging.info("Artists processed        : %s / %s", artists_processed, artists_total)
            logging.info("Albums scanned           : %s", albums_scanned)
            logging.info(
                "Albums with all tags     : %s / %s (%s)",
                albums_with_complete_tags, albums_scanned, _pct(albums_with_complete_tags, albums_scanned)
            )
            logging.info(
                "Albums with cover art    : %s / %s (%s)",
                albums_with_cover, albums_scanned, _pct(albums_with_cover, albums_scanned)
            )
            logging.info(
                "Albums with artist image : %s / %s (%s)",
                albums_with_artist_image, albums_scanned, _pct(albums_with_artist_image, albums_scanned)
            )
            logging.info(
                "MB coverage              : %s / %s (%s)",
                albums_with_mb, albums_scanned, _pct(albums_with_mb, albums_scanned)
            )
            logging.info(
                "Fallback matches         : Discogs=%s Last.fm=%s Bandcamp=%s",
                scan_discogs_matched, scan_lastfm_matched, scan_bandcamp_matched
            )
            logging.info(
                "Duplicates               : groups=%s editions=%s moved=%s",
                duplicate_groups_count, total_duplicates_count, albums_moved_this_scan
            )
            logging.info(
                "Incomplete moved         : %s album(s), %s MB",
                incomplete_moved_this_scan,
                incomplete_moved_mb_this_scan,
            )
            if player_sync_target:
                logging.info(
                    "Player sync             : target=%s status=%s",
                    player_sync_target,
                    "ok" if player_sync_ok else f"failed ({player_sync_message or 'unknown'})",
                )
            logging.info("Space saved (this scan)  : %s MB", space_saved_mb_this_scan)
            logging.info(
                "Cover sources            : MB=%s Discogs=%s Last.fm=%s Bandcamp=%s Web=%s",
                cover_from_mb, cover_from_discogs, cover_from_lastfm, cover_from_bandcamp, cover_from_web
            )
            logging.info(
                "Cache hit/miss           : audio=%s/%s MB=%s/%s",
                audio_hits, audio_misses, mb_hits, mb_misses
            )
            logging.info("Duration                 : %ss", duration if duration is not None else "n/a")
            logging.info("%s", bar)

            summary_json_str = json.dumps(summary)
            cur.execute("""
                UPDATE scan_history
                SET end_time = ?,
                    duration_seconds = ?,
                    duplicates_found = ?,
                    artists_processed = ?,
                    ai_used_count = ?,
                    mb_used_count = ?,
                    space_saved_mb = ?,
                    albums_moved = ?,
                    status = ?,
                    duplicate_groups_count = ?,
                    total_duplicates_count = ?,
                    broken_albums_count = ?,
                    missing_albums_count = ?,
                    albums_without_artist_image = ?,
                    albums_without_album_image = ?,
                    albums_without_complete_tags = ?,
                    albums_without_mb_id = ?,
                    albums_without_artist_mb_id = ?,
                    summary_json = ?
                WHERE scan_id = ?
            """, (
                end_time,
                duration,
                duplicates_found,
                artists_processed,
                ai_used_count,
                mb_used_count,
                space_saved_mb_this_scan,
                albums_moved_this_scan,
                scan_status,
                duplicate_groups_count,
                total_duplicates_count,
                broken_albums_count,
                missing_albums_count,
                albums_without_artist_image,
                albums_without_album_image,
                albums_without_complete_tags,
                albums_without_mb_id,
                albums_without_artist_mb_id,
                summary_json_str,
                scan_id
            ))
            if scan_status == 'completed':
                cur.execute(
                    "INSERT OR REPLACE INTO settings (key, value) VALUES ('last_completed_scan_id', ?)",
                    (str(scan_id),),
                )
            con.commit()
            con.close()
            _set_resume_run_status(
                resume_run_id,
                "completed" if scan_status == "completed" else ("cancelled" if scan_status == "cancelled" else "failed"),
                scan_id=scan_id,
            )
        
        if not scan_id:
            _set_resume_run_status(
                resume_run_id,
                "completed" if scan_status == "completed" else ("cancelled" if scan_status == "cancelled" else "failed"),
                scan_id=None,
            )

        if _get_library_mode() == "files" and scan_status == "completed":
            pending_to_clear: list[str] = []
            if scan_type == "full":
                pending_to_clear = [
                    str(r.get("folder_path") or "").strip()
                    for r in _list_files_pending_changes(limit=50000)
                    if str(r.get("folder_path") or "").strip()
                ]
            else:
                with lock:
                    pending_to_clear = [
                        str(p).strip()
                        for p in (state.get("scan_dirty_folders_pending_clear") or [])
                        if str(p).strip()
                    ]
            if pending_to_clear:
                cleared = _clear_files_pending_changes(pending_to_clear)
                logging.info(
                    "FILES %s scan: cleared %d pending change row(s) after successful run.",
                    scan_type,
                    cleared,
                )
        with lock:
            state["scan_dirty_folders_pending_clear"] = []

        with lock:
            state["scan_finalizing"] = False
            run_improve_after = state.pop("run_improve_after", False)
        # Magic / run-improve-after: run improve-all only for a completed scan.
        should_request_improve = bool(run_improve_after or pipeline_flags.get("match_fix"))
        if should_request_improve and scan_status != "completed":
            logging.info(
                "Skipping post-scan improve-all because current scan status is '%s' (only 'completed' is eligible).",
                scan_status,
            )
        elif should_request_improve and scan_status == "completed" and streamed_post_process_done:
            logging.info("Post-processing was streamed artist-by-artist during scan (Files mode).")

        if should_request_improve and scan_status == "completed" and not streamed_post_process_done:
            best_albums = []
            seen_ids = set()
            seen_group_keys = set()  # One best per (artist, set of edition ids) when from duplicate groups
            if all_results:
                for artist_name, groups in all_results.items():
                    for g in groups:
                        best = g.get("best")
                        losers = g.get("losers", [])
                        if not best:
                            continue
                        edition_ids = [int(best.get("album_id") or 0)]
                        for e in losers:
                            edition_ids.append(int(e.get("album_id") or 0))
                        key = (artist_name or "", tuple(sorted(edition_ids)))
                        if key in seen_group_keys:
                            continue
                        seen_group_keys.add(key)
                        if best.get("album_id") in seen_ids:
                            continue
                        seen_ids.add(best["album_id"])
                        mbid = (best.get("musicbrainz_id") or (best.get("meta") or {}).get("musicbrainz_releasegroupid") or (best.get("meta") or {}).get("musicbrainz_id") or "")
                        mbid = (mbid if isinstance(mbid, str) else str(mbid)).strip()
                        best_albums.append({
                            "album_id": best["album_id"],
                            "artist": artist_name,
                            "title_raw": best.get("title_raw", ""),
                            "album_title": best.get("title_raw") or best.get("album_norm") or f"Album {best['album_id']}",
                            "musicbrainz_id": mbid or "",
                            "folder": (best.get("folder") or "").strip(),
                        })
            # Include albums from the CURRENT scan only (never from previous scans).
            scan_id_for_improve = scan_id
            if scan_id_for_improve is not None:
                try:
                    con = sqlite3.connect(str(STATE_DB_FILE))
                    cur = con.cursor()
                    # Include all albums from the current scan so improve-all can enrich tags/covers
                    # even when there is no MusicBrainz ID yet (e.g. Bandcamp/Last.fm-only matches, or new REQUIRED_TAGS like "genre").
                    cur.execute(
                        "SELECT artist, album_id, title_raw, musicbrainz_id, folder FROM scan_editions WHERE scan_id = ?",
                        (scan_id_for_improve,),
                    )
                    for row in cur.fetchall():
                        artist_name, album_id, title_raw, mbid, folder = row[0], row[1], row[2] or "", (row[3] or "").strip(), row[4] or ""
                        if album_id in seen_ids:
                            continue
                        seen_ids.add(album_id)
                        best_albums.append({
                            "artist": artist_name,
                            "album_id": album_id,
                            "title_raw": (title_raw or "").strip(),
                            "album_title": (title_raw or "").strip() or f"Album {album_id}",
                            "musicbrainz_id": mbid or "",
                            "folder": (folder or "").strip(),
                        })
                    con.close()
                except Exception as e:
                    logging.debug("Post-scan improve: could not load current scan_editions for extra albums: %s", e)
            current_mode = _get_library_mode()
            # Fallback to duplicates_best only in Plex mode.
            if not best_albums and current_mode != "files":
                try:
                    con = sqlite3.connect(str(STATE_DB_FILE))
                    cur = con.cursor()
                    cur.execute(
                        "SELECT artist, album_id, title_raw, album_norm, folder, meta_json FROM duplicates_best"
                    )
                    for row in cur.fetchall():
                        artist_name, album_id, title_raw, album_norm, folder, meta_json = row[0], row[1], row[2] or "", row[3] or "", row[4] or "", row[5] or ""
                        if album_id in seen_ids:
                            continue
                        seen_ids.add(album_id)
                        mbid = None
                        if meta_json:
                            try:
                                meta = json.loads(meta_json)
                                mbid = meta.get("musicbrainz_releasegroupid") or meta.get("musicbrainz_id")
                            except Exception:
                                pass
                        best_albums.append({
                            "artist": artist_name,
                            "album_id": album_id,
                            "title_raw": (title_raw or "").strip(),
                            "album_title": (title_raw or "").strip() or (album_norm or "").strip() or f"Album {album_id}",
                            "musicbrainz_id": mbid or "",
                            "folder": (folder or "").strip(),
                        })
                    con.close()
                    if best_albums:
                        logging.debug("Post-scan improve: built best_albums from duplicates_best (%d albums)", len(best_albums))
                except Exception as e:
                    logging.debug("Post-scan improve: could not load duplicates_best fallback: %s", e)
            if best_albums:
                if MAGIC_MODE:
                    logging.info("Magic mode: dedupe done, starting improve-all (%d albums) ‚Äì tags, covers, artist images", len(best_albums))
                else:
                    logging.info("Run improve after scan: starting improve-all for %d albums ‚Äì tags, covers, artist images", len(best_albums))
                with lock:
                    state["scan_post_processing"] = True
                    state["scan_post_total"] = len(best_albums)
                    state["scan_post_done"] = 0
                    state["scan_post_current_artist"] = None
                    state["scan_post_current_album"] = None
                _run_improve_all_albums_global(best_albums)
                if _get_library_mode() == "files":
                    _refresh_files_album_scan_cache_from_editions(best_albums, scan_id=scan_id)
            else:
                if MAGIC_MODE:
                    logging.info("Magic mode: dedupe done; no albums to improve from current scan.")
                else:
                    logging.info("Run improve after scan: no albums to improve from current scan.")
        if scan_stream_post_by_artist and scan_status == "completed":
            try:
                if _get_library_mode() == "files" and getattr(sys.modules[__name__], "AUTO_EXPORT_LIBRARY", False):
                    logging.info("Auto-export: rebuilding Files library after streamed post-processing.")
                    with lock:
                        state["scan_post_processing"] = True
                        state["scan_post_current_artist"] = "Library"
                        state["scan_post_current_album"] = "Export rebuild"
                    _run_export_library()
            except Exception as e:
                logging.exception("Auto-export library after streamed post-processing failed: %s", e)
        with lock:
            state["scan_post_processing"] = False
            state["scan_post_current_artist"] = None
            state["scan_post_current_album"] = None
            state["scan_discovery_running"] = False
            state["scan_discovery_current_root"] = None
            state["scan_resume_run_id"] = None
            state["scanning"] = False
        logging.debug("background_scan(): finished (flag cleared)")
        duration = time.perf_counter() - scan_perf_start
        groups_found = sum(len(v) for v in all_results.values()) if 'all_results' in locals() else 0
        removed_dupes = get_stat("removed_dupes")
        space_saved   = get_stat("space_saved")
        total_artists = len(artists_merged) if 'artists_merged' in locals() else 0
        err_count = worker_errors.qsize()
        if err_count:
            errs = []
            while not worker_errors.empty():
                errs.append(worker_errors.get())
            err_file = CONFIG_DIR / f"scan_errors_{int(time.time())}.log"
            with err_file.open("w", encoding="utf-8") as fh:
                for art, msg in errs:
                    fh.write(f"{art}: {msg}\n")
            logging.warning("‚ö†Ô∏è  %d worker errors ‚Äì details in %s", err_count, err_file)
            notify_discord(
                f"‚ö†Ô∏è  PMDA scan finished with {err_count} errors. "
                f"See {err_file.name} for details."
            )
        notify_discord(
            "üü¢ PMDA scan completed in "
            f"{duration:.1f}s\n"
            f"Artists: {total_artists}\n"
            f"Albums: {total_albums if 'total_albums' in locals() else 0}\n"
            f"Duplicate groups found: {groups_found}\n"
            f"Duplicates removed so far: {removed_dupes}\n"
            f"Space saved: {space_saved}  MB"
        )
        if _get_library_mode() == "files":
            # Keep files-library browsing in sync after each completed scan.
            _trigger_files_index_rebuild_async(reason="scan_completed")

def background_dedupe(all_groups: List[dict]):
    """
    Processes deduplication of all groups in a background thread.
    Updates stats in DB and in-memory state.
    Sets dedupe_current_group so the UI can show artist, album, winner, losers, destination.
    """
    ensure_dedupe_scan_id()
    dupe_root = getattr(sys.modules[__name__], "DUPE_ROOT", Path("/dupes"))
    with lock:
        state.update(
            deduping=True,
            dedupe_progress=0,
            dedupe_total=len(all_groups),
            dedupe_start_time=time.time(),
            dedupe_saved_this_run=0,
            dedupe_current_group=None,
            dedupe_last_write=None,
        )

    total_moved = 0
    removed_count = 0
    artists_to_refresh = set()

    # Never move a folder that is any group's best (avoids moving both editions when duplicate groups slip through)
    best_folders = set()
    for g in all_groups:
        best = g.get("best")
        if best and best.get("folder"):
            p = path_for_fs_access(Path(best["folder"]))
            if p:
                best_folders.add(str(p))

    for g in all_groups:
        best = g.get("best", {})
        losers = g.get("losers", [])
        artist = g["artist"]
        album_title = best.get("title_raw", "")
        num_dupes = 1 + len(losers)
        # Ensure folder values are str for JSON (edition dict may store Path)
        current_group = {
            "artist": artist,
            "album": album_title,
            "num_dupes": num_dupes,
            "winner": {
                "title_raw": best.get("title_raw", ""),
                "album_id": best.get("album_id"),
                "folder": str(best.get("folder") or ""),
            },
            "losers": [
                {"title_raw": e.get("title_raw", ""), "album_id": e.get("album_id"), "folder": str(e.get("folder") or "")}
                for e in losers
            ],
            "destination": str(dupe_root),
            "status": "moving",
        }
        with lock:
            state["dedupe_current_group"] = current_group

        moved = perform_dedupe(g, best_folders=best_folders)
        removed_count += len(moved)
        group_saved = sum(item["size"] for item in moved)
        total_moved += group_saved
        artists_to_refresh.add(g["artist"])

        with lock:
            state["dedupe_progress"] += 1
            state["dedupe_saved_this_run"] = state.get("dedupe_saved_this_run", 0) + group_saved
            state["dedupe_current_group"] = None
            logging.debug(f"background_dedupe(): processed group for '{artist}|{album_title}', dedupe_progress={state['dedupe_progress']}/{state['dedupe_total']}")
            # Remove this group from in-memory state so the list shrinks on next /api/duplicates
            # Only remove if still present (same ref can appear twice from AI merge, avoid ValueError)
            if artist in state["duplicates"]:
                lst = state["duplicates"][artist]
                if g in lst:
                    lst.remove(g)
                if not state["duplicates"][artist]:
                    del state["duplicates"][artist]
        # Remove from DB so /api/duplicates (and reload) shows shrinking list
        best_album_id = best.get("album_id")
        loser_album_ids = [e.get("album_id") for e in losers if e.get("album_id") is not None]
        if best_album_id is not None:
            _remove_dedupe_group_from_db(artist, best_album_id, loser_album_ids)

    # Update stats in DB
    increment_stat("space_saved", total_moved)
    increment_stat("removed_dupes", removed_count)
    notify_discord(
        f"üü¢ Deduplication finished: {removed_count} duplicate folders moved, "
        f"{total_moved}  MB reclaimed."
    )
    logging.debug(f"background_dedupe(): updated stats: space_saved += {total_moved}, removed_dupes += {removed_count}")

    # Refresh Plex for all affected artists (each section in SECTION_IDS)
    section_ids = getattr(sys.modules[__name__], "SECTION_IDS", []) or []
    for artist in artists_to_refresh:
        letter = quote_plus(artist[0].upper())
        art_enc = quote_plus(artist)
        for sid in section_ids:
            try:
                logging.info(
                    "background_dedupe(): requesting Plex refresh for artist '%s' in section %s (path=/music/matched/%s/%s)",
                    artist,
                    sid,
                    letter,
                    art_enc,
                )
                plex_api(f"/library/sections/{sid}/refresh?path=/music/matched/{letter}/{art_enc}", method="GET")
                plex_api(f"/library/sections/{sid}/emptyTrash", method="PUT")
            except Exception as e:
                logging.warning(f"background_dedupe(): plex refresh/emptyTrash failed for artist={artist} section={sid}: {e}")

    with lock:
        scan_id = state.get("scan_id")
        state["deduping"] = False
        state["dedupe_current_group"] = None
        state["dedupe_last_write"] = None
        state["dedupe_start_time"] = None
        state["dedupe_saved_this_run"] = 0
        # For "Last scan summary": dupes moved and space saved in this run (when auto-move was used)
        state["last_dedupe_moved_count"] = removed_count
        state["last_dedupe_saved_mb"] = total_moved
    if scan_id is not None:
        update_dedupe_scan_summary(scan_id, total_moved, removed_count)
    logging.debug("background_dedupe(): deduping completed")

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ SUPPORT FUNCTIONS ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def fetch_cover_as_base64(album_id: int) -> Optional[str]:
    """
    Fetch album thumb from Plex as a base64 data‚ÄêURI.
    Returns None on failure.
    """
    try:
        resp = plex_api(f"/library/metadata/{album_id}/thumb")
        if resp.status_code == 200:
            b64 = base64.b64encode(resp.content).decode("utf-8")
            return f"data:image/jpeg;base64,{b64}"
    except Exception:
        pass
    return None

def perform_dedupe(group: dict, best_folders: set = None) -> List[dict]:
    """
    Move each "loser" folder out to DUPE_ROOT, delete metadata in Plex,
    and return a list of dicts describing each moved item.
    Cover is fetched after moves so the first group does not block on Plex API (avoids stuck 1/N).
    If best_folders is provided, never move a loser whose folder is in that set (keeps one edition per album).
    """
    moved_items: List[dict] = []
    artist = group["artist"]
    best_title = group["best"]["title_raw"]
    if best_folders is None:
        best_folders = set()

    def _files_forget_album_folder(folder: Path | str) -> None:
        """Remove a moved/deleted album folder from Files-mode caches/index payload tables."""
        try:
            if _get_library_mode() != "files":
                return
        except Exception:
            return
        try:
            key = _album_folder_cache_key(folder)
        except Exception:
            key = str(folder)
        try:
            con = sqlite3.connect(str(STATE_DB_FILE), timeout=10)
            cur = con.cursor()
            cur.execute("DELETE FROM files_album_scan_cache WHERE folder_path = ?", (key,))
            cur.execute("DELETE FROM files_library_published_albums WHERE folder_path = ?", (key,))
            con.commit()
            con.close()
        except Exception:
            logging.debug("Files cache/published cleanup failed for %s", key, exc_info=True)

    num_losers = len(group["losers"])
    for idx, loser in enumerate(group["losers"], 1):
        src_folder = Path(loser["folder"])
        # Never move a folder that is any group's best (safeguard when duplicate groups exist)
        src_resolved = path_for_fs_access(src_folder)
        if best_folders and src_resolved and str(src_resolved) in best_folders:
            logging.warning("perform_dedupe(): skipping loser (folder is another group's best) ‚Äì %s", src_folder)
            continue
        # Skip if the source folder is absent (e.g. already moved or path mapping issue)
        if not src_folder.exists():
            logging.warning(f"perform_dedupe(): source folder missing ‚Äì {src_folder}; skipping.")
            continue
        base_dst = build_dupe_destination(src_folder)
        dst = base_dst
        counter = 1
        while dst.exists():
            candidate = base_dst.parent / f"{base_dst.name} ({counter})"
            if not candidate.exists():
                dst = candidate
                break
            counter += 1
        dst.parent.mkdir(parents=True, exist_ok=True)

        logging.info("Moving dupe %s/%s: %s  ‚Üí  %s", idx, num_losers, src_folder, dst)
        logging.debug("perform_dedupe(): moving %s ‚Üí %s", src_folder, dst)
        try:
            try:
                _files_watcher_suppress_folder(src_folder, seconds=180.0, reason="pmda_move_dedupe")
            except Exception:
                pass
            safe_move(str(src_folder), str(dst))
            try:
                _files_watcher_suppress_folder(dst, seconds=180.0, reason="pmda_move_dedupe")
            except Exception:
                pass
            # Keep Files-mode browsing consistent: the loser is now outside FILES_ROOTS.
            _files_forget_album_folder(src_folder)
            with lock:
                state["dedupe_last_write"] = {"path": str(dst), "at": time.time()}
            logging.info("Moved to /dupes: %s", dst)
        except Exception as move_err:
            logging.error("perform_dedupe(): move failed for %s ‚Üí %s ‚Äì %s",
                          src_folder, dst, move_err)
            continue

        # warn if something prevented full deletion (e.g. Thumbs.db)
        if src_folder.exists():
            logging.warning("perform_dedupe(): %s was not fully removed (left‚Äëover non‚Äëaudio files?)", src_folder)
            notify_discord(f"‚ö† Folder **{src_folder.name}** could not be fully removed (non‚Äëaudio files locked?). Check manually.")

        size_mb = folder_size(dst) // (1024 * 1024)
        fmt_text = loser.get("fmt_text", loser.get("fmt", ""))
        br_kbps = loser["br"] // 1000
        sr = loser["sr"]
        bd = loser["bd"]

        loser_id = loser["album_id"]
        try:
            plex_api(f"/library/metadata/{loser_id}/trash", method="PUT")
            time.sleep(0.3)
            plex_api(f"/library/metadata/{loser_id}", method="DELETE")
        except Exception as e:
            logging.warning(f"perform_dedupe(): failed to delete Plex metadata for {loser_id}: {e}")

        # Record move in scan_moves table
        moved_at = time.time()
        scan_id = None
        with lock:
            scan_id = state.get("scan_id")
        
        if scan_id:
            try:
                con = sqlite3.connect(str(STATE_DB_FILE))
                cur = con.cursor()
                cur.execute("PRAGMA table_info(scan_moves)")
                move_cols = [r[1] for r in cur.fetchall()]
                if "album_title" in move_cols and "fmt_text" in move_cols and "move_reason" in move_cols:
                    cur.execute("""
                        INSERT INTO scan_moves
                        (scan_id, artist, album_id, original_path, moved_to_path, size_mb, moved_at, album_title, fmt_text, move_reason)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                    """, (
                        scan_id,
                        artist,
                        loser_id,
                        str(src_folder),
                        str(dst),
                        size_mb,
                        moved_at,
                        best_title or "",
                        fmt_text or "",
                        "dedupe",
                    ))
                elif "album_title" in move_cols and "fmt_text" in move_cols:
                    cur.execute("""
                        INSERT INTO scan_moves
                        (scan_id, artist, album_id, original_path, moved_to_path, size_mb, moved_at, album_title, fmt_text)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
                    """, (
                        scan_id,
                        artist,
                        loser_id,
                        str(src_folder),
                        str(dst),
                        size_mb,
                        moved_at,
                        best_title or "",
                        fmt_text or ""
                    ))
                else:
                    cur.execute("""
                        INSERT INTO scan_moves
                        (scan_id, artist, album_id, original_path, moved_to_path, size_mb, moved_at)
                        VALUES (?, ?, ?, ?, ?, ?, ?)
                    """, (
                        scan_id,
                        artist,
                        loser_id,
                        str(src_folder),
                        str(dst),
                        size_mb,
                        moved_at
                    ))
                con.commit()
                con.close()
            except Exception as e:
                logging.warning(f"perform_dedupe(): failed to record move in scan_moves: {e}")

        moved_items.append({
            "artist":    artist,
            "title_raw": best_title,
            "size":      size_mb,
            "fmt":       fmt_text,
            "br":        br_kbps,
            "sr":        sr,
            "bd":        bd,
            "thumb_data": None
        })

    # Fetch cover after moves so we do not block the first group on Plex API (fixes stuck 1/N dedupe).
    cover_data = fetch_cover_as_base64(group["best"]["album_id"]) if group.get("best", {}).get("album_id") else None
    for m in moved_items:
        m["thumb_data"] = cover_data

    return moved_items


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ UI card helper ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def _build_card_list(dup_dict) -> list[dict]:
    """
    Convert the nested `state["duplicates"]` dict into the flat list of
    cards expected by both the main page and /api/duplicates.
    """
    cards = []
    db_conn = None
    try:
        db_conn = plex_connect()
    except Exception:
        pass
    for artist, groups in dup_dict.items():
        for g in groups:
            if "best" not in g or "losers" not in g:
                continue
            best = g["best"]
            # Only consider losers whose folder still exists (not yet moved to dupes)
            losers = g["losers"]
            existing_losers = []
            for loser in losers:
                loser_folder = path_for_fs_access(Path(loser["folder"])) if loser.get("folder") else None
                if loser_folder and loser_folder.exists():
                    existing_losers.append(loser)
            # Skip group if no duplicate left to show (all losers moved or group had none)
            if not existing_losers:
                continue
            folder_path = path_for_fs_access(Path(best["folder"]))
            if not folder_path.exists():
                continue
            best_fmt = best.get("fmt_text", get_primary_format(folder_path))
            formats = [best_fmt] + [
                loser.get("fmt", get_primary_format(path_for_fs_access(Path(loser["folder"]))))
                for loser in existing_losers
            ]
            display_title = best["album_norm"].title()
            # Ensure used_ai groups have provider/model for METHOD column (backfill from globals if missing)
            used_ai = best.get("used_ai", False)
            ai_provider = best.get("ai_provider") or ""
            ai_model = best.get("ai_model") or ""
            if used_ai and (not ai_provider or not ai_model):
                mod = sys.modules[__name__]
                ai_provider = ai_provider or (getattr(mod, "AI_PROVIDER", None) or "")
                ai_model = ai_model or (getattr(mod, "RESOLVED_MODEL", None) or getattr(mod, "OPENAI_MODEL", None) or "")
            # Use persisted size_mb/track_count when available (so Unduper shows data after reload)
            if best.get("size_mb") is not None:
                size_mb = int(best["size_mb"])
                size_bytes = size_mb * (1024 * 1024)
            else:
                size_bytes = safe_folder_size(folder_path)
                size_mb = size_bytes // (1024 * 1024)
            if best.get("track_count") is not None:
                track_count = int(best["track_count"])
            else:
                track_count = 0
                if db_conn:
                    try:
                        track_count = len(get_tracks(db_conn, best["album_id"]))
                    except Exception:
                        pass
            cards.append({
                "artist_key": artist.replace(" ", "_"),
                "artist": artist,
                "album_id": best["album_id"],
                "n": len(existing_losers) + 1,
                "best_thumb": thumb_url(best["album_id"]),
                "best_title": display_title,
                "best_fmt": best_fmt,
                "formats": formats,
                "used_ai": used_ai,
                "ai_provider": ai_provider,
                "ai_model": ai_model,
                "size": size_bytes,
                "size_mb": size_mb,
                "track_count": track_count,
                "path": str(folder_path),
                "no_move": False,
                "match_verified_by_ai": bool(best.get("match_verified_by_ai", False)),
            })
    if db_conn:
        try:
            db_conn.close()
        except Exception:
            pass
    return cards


# --- New scan control endpoints ---
from flask import Response

def _requires_config():
    """Return 503 response when required backend config is missing for current mode."""
    _reload_library_mode_and_files_roots_from_db()
    if not FILES_ROOTS:
        return jsonify({"error": "No source folders configured. Add your music folders in Settings.", "requiresConfig": True}), 503
    return None

def start_background_scan():
    if _get_library_mode() == "files":
        _restart_files_watcher_if_needed()
    with lock:
        if not state["scanning"]:
            state.update(scanning=True, scan_progress=0, scan_total=0, scan_step_progress=0, scan_step_total=0)
            state["scan_ai_enabled"] = bool(ai_provider_ready)
            state["scan_mb_enabled"] = USE_MUSICBRAINZ
            logging.debug("start_scan(): launching background_scan() thread")
            threading.Thread(target=background_scan, daemon=True).start()

def _run_preflight_checks():
    """Run MusicBrainz and AI connectivity checks. Returns (mb_ok: bool, ai_ok: bool).
    For AI, ai_ok reflects ai_provider_ready (functional check is done by _reload_ai_config_and_reinit)."""
    mb_ok = False
    if USE_MUSICBRAINZ:
        try:
            test_mbid = "9162580e-5df4-32de-80cc-f45a8d8a9b1d"
            musicbrainzngs.get_release_group_by_id(test_mbid, includes=[])
            mb_ok = True
        except Exception:
            pass
    ai_ok = bool(ai_provider_ready)
    return mb_ok, ai_ok


def _run_discogs_preflight() -> tuple[bool, str]:
    """Test Discogs API connectivity. Returns (ok, message)."""
    if not USE_DISCOGS or not (getattr(sys.modules[__name__], "DISCOGS_USER_TOKEN", "") or "").strip():
        return False, "Disabled (no token)"
    try:
        import discogs_client
        token = (getattr(sys.modules[__name__], "DISCOGS_USER_TOKEN", "") or "").strip()
        d = discogs_client.Client("PMDA/0.6.6", user_token=token)
        d.identity()
        return True, "Discogs reachable"
    except Exception as e:
        return False, f"Discogs unreachable: {e}"


def _run_lastfm_preflight() -> tuple[bool, str]:
    """Test Last.fm API connectivity. Returns (ok, message)."""
    if not USE_LASTFM or not (getattr(sys.modules[__name__], "LASTFM_API_KEY", "") or "").strip():
        return False, "Disabled (no API key)"
    try:
        api_key = (getattr(sys.modules[__name__], "LASTFM_API_KEY", "") or "").strip()
        resp = requests.get(
            "https://ws.audioscrobbler.com/2.0/",
            params={"method": "album.getInfo", "artist": "Cher", "album": "Believe", "api_key": api_key, "format": "json"},
            timeout=10,
        )
        if resp.status_code == 200:
            data = resp.json()
            if "error" in data and data.get("error") != 0:
                return False, f"Last.fm API error: {data.get('message', 'Unknown')}"
            return True, "Last.fm reachable"
        return False, f"Last.fm HTTP {resp.status_code}"
    except Exception as e:
        return False, f"Last.fm unreachable: {e}"


def _run_serper_preflight() -> tuple[bool, str]:
    """Check Serper (web search) API key and optionally connectivity. Returns (ok, message)."""
    key = (getattr(sys.modules[__name__], "SERPER_API_KEY", "") or "").strip()
    if not key:
        return False, "No API key (web search disabled)"
    use_web = getattr(sys.modules[__name__], "USE_WEB_SEARCH_FOR_MB", False)
    if not use_web:
        return True, "API key set (web search for MB disabled in settings)"
    try:
        resp = requests.post(
            "https://google.serper.dev/search",
            headers={"X-API-KEY": key, "Content-Type": "application/json"},
            json={"q": "test", "num": 1},
            timeout=8,
        )
        if resp.status_code == 200:
            return True, "Serper reachable"
        if resp.status_code == 429:
            return False, "Rate limit or no credits"
        return False, f"HTTP {resp.status_code}"
    except Exception as e:
        return False, f"Serper unreachable: {e}"


def _serper_web_search(query: str, num: int = 5) -> list[dict]:
    """Thin wrapper around Serper search with basic error handling. Returns list of {title,link,snippet}."""
    key = getattr(sys.modules[__name__], "SERPER_API_KEY", "") or ""
    if not key.strip():
        return []
    url = "https://google.serper.dev/search"
    headers = {"X-API-KEY": key, "Content-Type": "application/json"}
    body = {"q": query, "num": num}
    try:
        resp = requests.post(url, json=body, headers=headers, timeout=10)
        if resp.status_code != 200:
            if resp.status_code in (401, 403, 429):
                logging.warning(
                    "[Serper] Rate limit or no credits (HTTP %s) ‚Äì web search skipped. %s",
                    resp.status_code,
                    resp.text[:200],
                )
            else:
                logging.debug("[Serper] HTTP %s: %s", resp.status_code, resp.text[:200])
            return []
        data = resp.json()
        organic = data.get("organic") or []
        out: list[dict] = []
        for item in organic:
            if isinstance(item, dict):
                out.append(
                    {
                        "title": item.get("title") or "",
                        "link": item.get("link") or "",
                        "snippet": item.get("snippet") or "",
                    }
                )
        return out
    except Exception as e:
        logging.debug("[Serper] Request failed: %s", e)
        return []


def _fetch_cover_from_web(artist_name: str, album_title: str) -> Optional[tuple[bytes, str]]:
    """
    Try to find an album cover via web search (Serper + OpenGraph image).
    Returns (content_bytes, mime) or None on failure.
    """
    key = (getattr(sys.modules[__name__], "SERPER_API_KEY", "") or "").strip()
    if not key:
        return None
    query = f"{artist_name} {album_title} album cover".strip()
    results = _serper_web_search(query, num=5)
    if not results:
        return None
    for item in results:
        link = item.get("link") or ""
        if not link:
            continue
        low = link.lower()
        # Avoid Wikipedia/Wikimedia results; they are often unrelated photos and create false positives.
        if "wikipedia.org/" in low or "wikimedia.org/" in low:
            continue
        try:
            resp = requests.get(link, timeout=8, allow_redirects=True)
        except Exception as e:
            logging.debug("[WebCover] Failed to fetch %s: %s", link, e)
            continue
        if resp.status_code != 200:
            continue
        ct = (resp.headers.get("content-type") or "").split(";")[0].strip().lower()
        # Direct image
        if ct.startswith("image/") and resp.content:
            return resp.content, ct or "image/jpeg"
        # HTML page ‚Äì try to extract og:image and fetch it
        if "text/html" in ct and resp.text:
            try:
                m = re.search(r'<meta\s+property=["\']og:image["\']\s+content=["\']([^"\']+)["\']', resp.text, re.IGNORECASE)
                if not m:
                    m = re.search(r'<meta\s+content=["\']([^"\']+)["\']\s+property=["\']og:image["\']', resp.text, re.IGNORECASE)
                if not m:
                    continue
                img_url = m.group(1).strip()
                if not img_url:
                    continue
                img_resp = requests.get(img_url, timeout=8, allow_redirects=True)
                if img_resp.status_code == 200 and img_resp.content:
                    img_ct = (
                        (img_resp.headers.get("content-type") or "")
                        .split(";")[0]
                        .strip()
                        .lower()
                    )
                    if not img_ct.startswith("image/"):
                        img_ct = "image/jpeg"
                    return img_resp.content, img_ct
            except Exception as e:
                logging.debug("[WebCover] Failed to extract og:image from %s: %s", link, e)
                continue
    return None


def _run_acoustid_preflight() -> tuple[bool, str]:
    """Check AcousticID: enabled, API key, and pyacoustid available. Returns (ok, message)."""
    if not getattr(sys.modules[__name__], "USE_ACOUSTID", False):
        return False, "Disabled"
    key = (getattr(sys.modules[__name__], "ACOUSTID_API_KEY", "") or "").strip()
    if not key:
        return False, "No API key"
    try:
        import acoustid  # noqa: F401
        return True, "AcousticID configured"
    except ImportError as e:
        return False, f"pyacoustid not installed: {e}"


def _fetch_discogs_release(artist_name: str, album_title: str) -> Optional[dict]:
    """
    Search Discogs for a release by artist + album. Returns dict with title, year, cover_url, artist_name, or None.
    Requires USE_DISCOGS and DISCOGS_USER_TOKEN.
    Search can return Masters first; d.release(master_id) would 404. We collect candidate release IDs (from
    type=release or from master.main_release). main_release is a Release object in discogs_client, so we must
    use its .id for d.release(). We try each candidate until one succeeds.
    """
    if not USE_DISCOGS:
        return None
    token = (getattr(sys.modules[__name__], "DISCOGS_USER_TOKEN", "") or "").strip()
    if not token:
        return None
    def _page_to_candidate_ids(d_client, page_list: list) -> list:
        out = []
        for item in page_list or []:
            item_id = getattr(item, "id", None) or (getattr(item, "data", None) or {}).get("id")
            if item_id is None:
                continue
            item_type = getattr(item, "type", None) or (getattr(item, "data", None) or {}).get("type", "release")
            if item_type == "release":
                rid = getattr(item_id, "id", item_id) if not isinstance(item_id, int) else item_id
                out.append(int(rid))
            elif item_type == "master":
                try:
                    master = d_client.master(int(item_id))
                    main = getattr(master, "main_release", None) or (getattr(master, "data", None) or {}).get("main_release")
                    if main is not None:
                        rid = getattr(main, "id", main) if not isinstance(main, int) else main
                        out.append(int(rid))
                except Exception:
                    continue
        return out

    try:
        import discogs_client
        from discogs_client.exceptions import HTTPError
        d = discogs_client.Client("PMDA/0.6.6", user_token=token)
        album_norm = norm_album(album_title) or album_title
        results = d.search(album_norm, artist=artist_name, type="release")
        page = results.page(1)
        candidate_ids = _page_to_candidate_ids(d, page)
        if not candidate_ids:
            combined = f"{artist_name} {album_title}".strip()
            if combined:
                results = d.search(combined, type="release")
                page = results.page(1)
                candidate_ids = _page_to_candidate_ids(d, page)
        seen = set()
        unique_ids = [x for x in candidate_ids if x not in seen and not seen.add(x)]
        for release_id in unique_ids:
            try:
                full_release = d.release(release_id)
                title = getattr(full_release, "title", None) or (full_release.data.get("title") if getattr(full_release, "data", None) else album_title)
                year = getattr(full_release, "year", None) or (full_release.data.get("year") if getattr(full_release, "data", None) else "")
                cover_url = None
                if getattr(full_release, "images", None) and len(full_release.images) > 0:
                    img = full_release.images[0]
                    cover_url = img.get("uri") or img.get("resource_url")
                artist_str = artist_name
                if getattr(full_release, "artists", None) and full_release.artists:
                    artist_str = getattr(full_release.artists[0], "name", None) or artist_name
                tracklist = []
                for tr in getattr(full_release, "tracklist", []) or []:
                    t_title = getattr(tr, "title", None) or (getattr(tr, "data", None) or {}).get("title")
                    if t_title:
                        tracklist.append(str(t_title))
                try:
                    release_id = str(getattr(full_release, "id", "") or "")
                except Exception:
                    release_id = ""
                master_id = ""
                try:
                    master_obj = getattr(full_release, "master", None)
                    if master_obj is not None:
                        master_id = str(getattr(master_obj, "id", "") or "")
                except Exception:
                    master_id = ""
                return {
                    "title": title,
                    "year": str(year) if year else "",
                    "cover_url": cover_url,
                    "artist_name": artist_str,
                    "tracklist": tracklist,
                    "release_id": release_id,
                    "master_id": master_id,
                }
            except HTTPError as he:
                if getattr(he, "status_code", None) == 404:
                    continue
                raise
        return None
    except Exception as e:
        logging.warning("Discogs fetch failed for %s / %s: %s", artist_name, album_title, e)
        return None


def _fetch_lastfm_album_info(artist_name: str, album_title: str, mbid: Optional[str] = None) -> Optional[dict]:
    """
    Call Last.fm album.getInfo. Returns dict with cover_url, toptags (list of str), title, artist, or None.
    """
    if not USE_LASTFM:
        return None
    api_key = (getattr(sys.modules[__name__], "LASTFM_API_KEY", "") or "").strip()
    if not api_key:
        return None
    try:
        params = {"method": "album.getInfo", "artist": artist_name, "album": album_title, "api_key": api_key, "format": "json"}
        if mbid:
            params["mbid"] = mbid
        resp = requests.get("https://ws.audioscrobbler.com/2.0/", params=params, timeout=10)
        if resp.status_code != 200:
            return None
        data = resp.json()
        if data.get("error") and data.get("error") != 0:
            search_params = {"method": "album.search", "album": f"{artist_name} {album_title}".strip(), "api_key": api_key, "format": "json"}
            search_resp = requests.get("https://ws.audioscrobbler.com/2.0/", params=search_params, timeout=10)
            if search_resp.status_code == 200:
                search_data = search_resp.json()
                matches = (search_data.get("results") or {}).get("albummatches") or {}
                album_list = matches.get("album") or []
                if isinstance(album_list, dict):
                    album_list = [album_list]
                if album_list:
                    first = album_list[0]
                    search_artist = (first.get("artist") or "").strip() or artist_name
                    search_album = (first.get("name") or "").strip() or album_title
                    params2 = {"method": "album.getInfo", "artist": search_artist, "album": search_album, "api_key": api_key, "format": "json"}
                    resp2 = requests.get("https://ws.audioscrobbler.com/2.0/", params=params2, timeout=10)
                    if resp2.status_code == 200:
                        data = resp2.json()
                        if not (data.get("error") and data.get("error") != 0):
                            album_title = search_album
                            artist_name = search_artist
            if data.get("error") and data.get("error") != 0:
                return None
        album = data.get("album") or {}
        images = album.get("image") or []
        cover_url = None
        size_rank = {
            "small": 1,
            "medium": 2,
            "large": 3,
            "extralarge": 4,
            "mega": 5,
        }
        best_rank = -1
        for img in images:
            if not isinstance(img, dict):
                continue
            url = (img.get("#text") or "").strip()
            if not url:
                continue
            rank = size_rank.get((img.get("size") or "").strip().lower(), 0)
            if rank >= best_rank:
                best_rank = rank
                cover_url = url
        if not cover_url and images:
            try:
                cover_url = (images[-1].get("#text") or "").strip() if isinstance(images[-1], dict) else None
            except Exception:
                cover_url = None
        toptags = []
        for t in (album.get("toptags", {}).get("tag") or []):
            name = t.get("name") if isinstance(t, dict) else str(t)
            if name:
                toptags.append(name)
        wiki = album.get("wiki") or {}
        wiki_summary = wiki.get("summary") if isinstance(wiki, dict) else ""
        wiki_content = wiki.get("content") if isinstance(wiki, dict) else ""
        mbid = (album.get("mbid") or "").strip()
        return {
            "cover_url": cover_url,
            "toptags": toptags,
            "title": album.get("name") or album_title,
            "artist": album.get("artist", {}).get("name") if isinstance(album.get("artist"), dict) else artist_name,
            "mbid": mbid,
            "wiki_summary": wiki_summary or "",
            "wiki_content": wiki_content or "",
        }
    except Exception as e:
        logging.warning("Last.fm fetch failed for %s / %s: %s", artist_name, album_title, e)
        return None


def _strip_html_text(value: str) -> str:
    txt = str(value or "")
    if not txt:
        return ""
    txt = re.sub(r"<[^>]+>", " ", txt)
    txt = txt.replace("&amp;", "&").replace("&quot;", '"').replace("&#39;", "'")
    txt = re.sub(r"\s+", " ", txt).strip()
    return txt


def _truncate_text(value: str, max_chars: int = 420) -> str:
    txt = _strip_html_text(value)
    if len(txt) <= max_chars:
        return txt
    clipped = txt[:max_chars].rsplit(" ", 1)[0].strip()
    return f"{clipped}..." if clipped else txt[:max_chars]


def _norm_artist_key(name: str) -> str:
    """Stable key for artist-name lookups (DB keys, external image cache keys)."""
    return " ".join((name or "").split()).lower().strip()


def _word_count(text: str) -> int:
    try:
        return len(re.findall(r"[A-Za-z0-9]+(?:'[A-Za-z0-9]+)?", (text or "")))
    except Exception:
        return 0


_LASTFM_BIO_GARBAGE_PATTERNS = (
    r"read more on last\.?fm\.?$",
    r"read more on last\.?fm.*$",
    r"user-contributed text is available under the creative commons.*$",
    r"additional terms may apply.*$",
)


def _cleanup_lastfm_bio_text(text: str) -> str:
    """Strip common Last.fm boilerplate from the end of bios/summaries."""
    t = _strip_html_text(text or "")
    if not t:
        return ""
    lowered = t.lower()
    # Fast path: remove common suffixes.
    for pat in _LASTFM_BIO_GARBAGE_PATTERNS:
        try:
            if re.search(pat, lowered, flags=re.IGNORECASE):
                t = re.sub(pat, "", t, flags=re.IGNORECASE).strip()
                lowered = t.lower()
        except Exception:
            continue
    # Remove any trailing "Read more on Last.fm" segment even when preceded by ellipsis.
    m = re.search(r"\bread more on last\.?fm\b", lowered)
    if m:
        t = t[: m.start()].strip(" .¬∑\u2026")
    return t.strip()


def _is_garbage_bio(text: str) -> bool:
    t = (text or "").strip()
    if not t:
        return True
    low = t.lower()
    if "read more on last.fm" in low or "read more on lastfm" in low:
        return True
    if low.startswith("read more on last.fm"):
        return True
    if "user-contributed text is available under" in low:
        return True
    # A handful of tokens is effectively useless as a bio.
    if _word_count(t) < 12:
        return True
    return False


def _is_acceptable_original_bio(text: str) -> bool:
    # Spec target: accept original sources when >= 100 words.
    return _word_count(text or "") >= 100


def _fetch_lastfm_artist_info(artist_name: str) -> Optional[dict]:
    """
    Call Last.fm artist.getInfo. Returns dict with bio/tags/similar artists or None.
    """
    if not USE_LASTFM:
        return None
    api_key = (getattr(sys.modules[__name__], "LASTFM_API_KEY", "") or "").strip()
    if not api_key:
        return None
    try:
        def _pick_best_lastfm_image_url(images) -> str:
            if isinstance(images, dict):
                images = [images]
            if not isinstance(images, list):
                return ""
            rank = {
                "mega": 60,
                "extralarge": 50,
                "large": 40,
                "medium": 30,
                "small": 20,
                "": 10,
            }
            best_url = ""
            best_rank = -1
            for im in images:
                if not isinstance(im, dict):
                    continue
                url = (im.get("#text") or im.get("text") or "").strip()
                if not url:
                    continue
                # Last.fm frequently returns a "missing image" placeholder that looks like a music note.
                # We treat those as absent so the UI can fall back to Wikipedia or local/DB images.
                try:
                    if _is_probably_placeholder_artist_image_url(url):
                        continue
                except Exception:
                    pass
                r = rank.get(str(im.get("size") or "").strip().lower(), 0)
                if r > best_rank:
                    best_rank = r
                    best_url = url
            return best_url

        data = None
        artist = {}
        # Avoid Last.fm autocorrect mapping obscure artists to wrong entities.
        for autocorrect in (0, 1):
            params = {"method": "artist.getInfo", "artist": artist_name, "api_key": api_key, "format": "json", "autocorrect": autocorrect}
            resp = requests.get("https://ws.audioscrobbler.com/2.0/", params=params, timeout=10)
            if resp.status_code != 200:
                continue
            data = resp.json()
            if (data or {}).get("error") and (data or {}).get("error") != 0:
                continue
            artist = (data or {}).get("artist") or {}
            returned_name = (artist.get("name") or "").strip()
            if returned_name:
                score = _provider_identity_text_score(artist_name or "", returned_name)
                if score < 0.78:
                    # Try without/with autocorrect; otherwise reject to prevent nonsense bios/similar.
                    continue
            break
        if not artist:
            return None
        bio = artist.get("bio") or {}
        summary = _cleanup_lastfm_bio_text((bio.get("summary") if isinstance(bio, dict) else "") or "")
        content = _cleanup_lastfm_bio_text((bio.get("content") if isinstance(bio, dict) else "") or "")
        tags_raw = (artist.get("tags") or {}).get("tag") or []
        if isinstance(tags_raw, dict):
            tags_raw = [tags_raw]
        tags = []
        for t in tags_raw:
            name = (t.get("name") if isinstance(t, dict) else str(t) or "").strip()
            if name:
                tags.append(name)
        similar_raw = (artist.get("similar") or {}).get("artist") or []
        if isinstance(similar_raw, dict):
            similar_raw = [similar_raw]
        similar = []
        for item in similar_raw:
            if not isinstance(item, dict):
                continue
            name = (item.get("name") or "").strip()
            mbid = (item.get("mbid") or "").strip()
            if not name:
                continue
            sim_img = _pick_best_lastfm_image_url(item.get("image"))
            entry = {"name": name, "mbid": mbid, "type": "Last.fm"}
            if sim_img:
                entry["image_url"] = sim_img
            similar.append(entry)
        artist_image_url = _pick_best_lastfm_image_url(artist.get("image"))
        best_bio = content or summary
        if _is_garbage_bio(best_bio):
            best_bio = ""
        best_short = summary or content
        best_short = "" if _is_garbage_bio(best_short) else best_short
        return {
            "bio": best_bio,
            "short_bio": _truncate_text(best_short, max_chars=460) if best_short else "",
            "tags": _dedupe_keep_order(tags)[:20],
            "similar": similar[:20],
            "image_url": artist_image_url or "",
            "source": "lastfm",
        }
    except Exception as e:
        logging.debug("Last.fm artist profile fetch failed for %s: %s", artist_name, e)
        return None


def _fetch_wikipedia_pageimage(title: str, lang: str = "en", thumb_px: int = 640) -> str:
    """
    Return a Wikipedia lead thumbnail URL for a page title, or "" on failure.
    Uses MediaWiki `pageimages` (no API key required).
    """
    t = (title or "").strip()
    l = (lang or "en").strip().lower() or "en"
    thumb_px = max(64, min(1600, int(thumb_px or 640)))
    if not t:
        return ""
    api_url = f"https://{l}.wikipedia.org/w/api.php"
    headers = {"User-Agent": "PMDA/0.7.5 (self-hosted music library; https://github.com/silkyclouds/PMDA)"}
    try:
        resp = requests.get(
            api_url,
            params={
                "action": "query",
                "prop": "pageimages",
                "piprop": "thumbnail",
                "pithumbsize": thumb_px,
                "titles": t,
                "redirects": 1,
                "format": "json",
                "utf8": 1,
            },
            headers=headers,
            timeout=10,
        )
        if resp.status_code != 200:
            return ""
        data = resp.json()
        pages = ((data.get("query") or {}).get("pages") or {}) if isinstance(data, dict) else {}
        for _pid, page in (pages or {}).items():
            if not isinstance(page, dict):
                continue
            thumb = page.get("thumbnail") or {}
            if isinstance(thumb, dict):
                src = (thumb.get("source") or "").strip()
                if src:
                    return src
        return ""
    except Exception:
        return ""


def _fetch_wikipedia_intro_extract(title: str, lang: str = "en") -> tuple[str, str]:
    """
    Return (extract, page_url) for a Wikipedia page title, or ("","") on failure.
    Uses the MediaWiki API with plaintext extracts.
    """
    t = (title or "").strip()
    l = (lang or "en").strip().lower() or "en"
    if not t:
        return "", ""
    api_url = f"https://{l}.wikipedia.org/w/api.php"
    headers = {"User-Agent": "PMDA/0.7.5 (self-hosted music library; https://github.com/silkyclouds/PMDA)"}
    try:
        resp = requests.get(
            api_url,
            params={
                "action": "query",
                "prop": "extracts",
                "explaintext": 1,
                "exintro": 1,
                "redirects": 1,
                "titles": t,
                "format": "json",
                "utf8": 1,
            },
            headers=headers,
            timeout=10,
        )
        if resp.status_code != 200:
            return "", ""
        data = resp.json()
        pages = ((data.get("query") or {}).get("pages") or {}) if isinstance(data, dict) else {}
        extract = ""
        canonical_title = t
        for _pid, page in (pages or {}).items():
            if not isinstance(page, dict):
                continue
            canonical_title = (page.get("title") or canonical_title).strip()
            extract = (page.get("extract") or "").strip()
            break
        extract = re.sub(r"\s+", " ", extract).strip()
        if not extract:
            return "", ""
        # Avoid disambiguation intros (common false positives).
        low = extract.lower()
        if "may refer to" in low and len(extract) < 220:
            return "", ""
        page_url = f"https://{l}.wikipedia.org/wiki/{quote(canonical_title.replace(' ', '_'), safe='')}"
        return extract, page_url
    except Exception:
        return "", ""


def _fetch_wikipedia_artist_bio(artist_name: str, lang: str = "en") -> Optional[dict]:
    """
    Best-effort Wikipedia intro for an artist name.
    Returns {"bio","short_bio","source","url","lang"} or None.
    """
    name = (artist_name or "").strip()
    l = (lang or "en").strip().lower() or "en"
    if not name:
        return None
    api_url = f"https://{l}.wikipedia.org/w/api.php"
    headers = {"User-Agent": "PMDA/0.7.5 (self-hosted music library; https://github.com/silkyclouds/PMDA)"}
    try:
        def _looks_music_related(extract: str) -> bool:
            low = (extract or "").lower()
            if not low:
                return False
            good = (
                "musician", "band", "singer", "songwriter", "composer", "producer", "dj",
                "album", "record", "label", "single", "track", "genre", "formed", "born",
            )
            bad = (
                "pigment", "clay", "oxide", "mineral", "earth pigment", "paint", "soil",
                "ferric", "hematite", "kaolin",
            )
            score = 0
            for kw in good:
                if kw in low:
                    score += 1
            for kw in bad:
                if kw in low:
                    score -= 2
            return score >= 1

        # Use slightly biased queries to avoid common false positives (e.g. "Ochre" the pigment).
        for q in (f"{name} musician", f"{name} band", name):
            resp = requests.get(
                api_url,
                params={
                    "action": "query",
                    "list": "search",
                    "srsearch": q,
                    "srlimit": 5,
                    "format": "json",
                    "utf8": 1,
                },
                headers=headers,
                timeout=10,
            )
            if resp.status_code != 200:
                continue
            data = resp.json()
            results = ((data.get("query") or {}).get("search") or []) if isinstance(data, dict) else []
            if not isinstance(results, list) or not results:
                continue
            # Try a few candidates to dodge disambiguation pages and non-music pages.
            for item in results[:5]:
                if not isinstance(item, dict):
                    continue
                title = (item.get("title") or "").strip()
                if not title:
                    continue
                extract, page_url = _fetch_wikipedia_intro_extract(title, lang=l)
                if not extract or not _looks_music_related(extract):
                    continue
                img_url = _fetch_wikipedia_pageimage(title, lang=l, thumb_px=720) or ""
                short = _truncate_text(extract, max_chars=460)
                return {
                    "bio": extract,
                    "short_bio": short,
                    "source": f"wikipedia:{l}",
                    "url": page_url,
                    "lang": l,
                    "image_url": img_url,
                }
    except Exception:
        return None
    return None


_last_bandcamp_request = 0.0
_bandcamp_lock = threading.Lock()


def _dedupe_keep_order(items: List[str]) -> List[str]:
    out: List[str] = []
    seen = set()
    for item in items:
        key = (item or "").strip()
        if not key:
            continue
        if key in seen:
            continue
        seen.add(key)
        out.append(key)
    return out


def _bandcamp_cover_url_candidates(cover_url: str) -> List[str]:
    """
    Expand a Bandcamp cover URL into likely higher-resolution candidates.
    Bandcamp URLs usually look like:
      https://f4.bcbits.com/img/a1234567890_16.jpg
    Where `_0` is typically the highest available resolution.
    """
    base = (cover_url or "").strip()
    if not base:
        return []
    candidates = [base]
    m = re.match(r"^(https?://[^/]+/img/[ab]\d+)_([0-9]+)(\.[a-zA-Z0-9]+)(\?.*)?$", base)
    if m:
        prefix, _size, ext, query = m.groups()
        q = query or ""
        for sz in ("0", "1024", "700", "350", "16"):
            candidates.append(f"{prefix}_{sz}{ext}{q}")
    return _dedupe_keep_order(candidates)


def _lastfm_cover_url_candidates(cover_url: str) -> List[str]:
    """
    Expand a Last.fm image URL to likely larger variants.
    Last.fm often uses path chunks like /34s/, /64s/, /300x300/.
    """
    base = (cover_url or "").strip()
    if not base:
        return []
    candidates = [base]
    if re.search(r"/\d+s/", base):
        candidates.extend([
            re.sub(r"/\d+s/", "/300x300/", base),
            re.sub(r"/\d+s/", "/600x600/", base),
            re.sub(r"/\d+s/", "/1000x1000/", base),
        ])
    elif re.search(r"/\d+x\d+/", base):
        candidates.extend([
            re.sub(r"/\d+x\d+/", "/600x600/", base),
            re.sub(r"/\d+x\d+/", "/1000x1000/", base),
        ])
    return _dedupe_keep_order(candidates)


def _download_best_cover_image(
    source: str,
    cover_url: Optional[str],
    *,
    cover_candidates: Optional[List[str]] = None,
    headers: Optional[dict] = None,
    timeout: int = 12,
) -> Optional[Tuple[bytes, str, str]]:
    """
    Download the best (largest byte size) image among available candidates.
    Returns tuple: (content, mime, url_used) or None.
    """
    urls: List[str] = []
    if cover_candidates:
        urls.extend([u for u in cover_candidates if isinstance(u, str)])
    if cover_url:
        urls.append(cover_url)
    urls = _dedupe_keep_order(urls)
    if not urls:
        return None

    source_l = (source or "").strip().lower()
    expanded: List[str] = []
    for u in urls:
        expanded.append(u)
        if "bandcamp" in source_l or "bcbits.com/img/" in u:
            expanded.extend(_bandcamp_cover_url_candidates(u))
        if "last.fm" in source_l or "lastfm" in source_l:
            expanded.extend(_lastfm_cover_url_candidates(u))
    candidates = _dedupe_keep_order(expanded)

    best: Optional[Tuple[bytes, str, str, int]] = None
    req_headers = dict(headers or {})
    # Many hosts (notably Wikimedia/Wikipedia) expect a real UA; keep it consistent across fetches.
    req_headers.setdefault("User-Agent", "PMDA/0.7.5 (self-hosted music library; https://github.com/silkyclouds/PMDA)")
    for u in candidates:
        try:
            resp = requests.get(u, headers=req_headers, timeout=timeout, allow_redirects=True)
            if resp.status_code != 200:
                continue
            mime = (resp.headers.get("content-type") or "").split(";")[0].strip().lower()
            if not mime.startswith("image/"):
                continue
            content = resp.content or b""
            size = len(content)
            if size <= 0:
                continue
            if best is None or size > best[3]:
                best = (content, mime or "image/jpeg", u, size)
        except Exception:
            continue

    if best is None:
        return None
    logging.info("[Cover] source=%s selected=%s bytes=%d", source, best[2], best[3])
    return best[0], best[1], best[2]


def _infer_genre_from_bandcamp_tags(tags: List[str]) -> Optional[str]:
    """
    Infer a genre string from Bandcamp tag list.
    Strategy:
      - Normalize tags to lowercase.
      - Prefer tags that look like music genres (filter out obvious locations such as 'new mexico').
      - Return a semicolon-separated string of 1‚Äì3 best tags, or None if nothing usable.
    """
    if not tags:
        return None

    # Keep insertion order, dedupe case-insensitive
    cleaned_raw: List[str] = []
    seen_lower = set()
    for t in tags:
        raw = (t or "").strip()
        if not raw:
            continue
        normalized = re.sub(r"\s+", " ", raw).strip()
        low = normalized.lower()
        if low in seen_lower:
            continue
        seen_lower.add(low)
        cleaned_raw.append(normalized)

    if not cleaned_raw:
        return None

    # Known non-genre tokens (locations + generic marketplace tags)
    blacklist_exact = {
        "new mexico", "usa", "us", "uk", "united states", "united kingdom",
        "france", "germany", "italy", "spain", "japan", "canada",
        "london", "paris", "berlin", "new york", "los angeles",
        "vinyl", "cassette", "cd", "digital", "download", "album", "ep", "lp", "single",
    }
    genre_candidates: List[str] = []
    for raw in cleaned_raw:
        low = raw.lower()
        if low in blacklist_exact:
            continue
        if len(low) < 2:
            continue
        if re.fullmatch(r"[0-9\W_]+", low):
            continue
        genre_candidates.append(low)

    # Fallback: if everything was filtered, keep original cleaned tags
    if not genre_candidates:
        genre_candidates = [x.lower() for x in cleaned_raw]

    # Keep up to 6 genres so releases like:
    # "electronic idm acid breakcore experimental techno" are fully preserved.
    selected = genre_candidates[:6]
    return "; ".join(selected)


def _fetch_bandcamp_album_info(artist_name: str, album_title: str) -> Optional[dict]:
    """
    Search Bandcamp for an album by artist + title. Returns dict with title, artist_name, year, cover_url, tracklist, tags, or None.
    Uses scraping; no public API. Rate-limited (5s between calls). User-Agent identifies PMDA.
    Use at your own risk regarding Bandcamp ToS.
    """
    if not USE_BANDCAMP:
        return None
    global _last_bandcamp_request
    with _bandcamp_lock:
        now = time.time()
        wait = 5.0 - (now - _last_bandcamp_request)
        if wait > 0:
            time.sleep(wait)
        _last_bandcamp_request = time.time()
    headers = {"User-Agent": "PMDA/1.0 (metadata fallback; https://github.com/silkyclouds/PMDA)"}

    def _split_bandcamp_keywords(value: str) -> list[str]:
        raw = (value or "").strip()
        if not raw:
            return []
        # Bandcamp JSON-LD keywords is sometimes a comma-separated string.
        # Split conservatively and keep insertion order.
        parts: list[str] = []
        for part in re.split(r"[,\n\r]+", raw):
            txt = re.sub(r"\s+", " ", (part or "").strip())
            if txt:
                parts.append(txt)
        return parts

    def _parse_album_page(album_url: str) -> Optional[dict]:
        album_resp = requests.get(album_url, headers=headers, timeout=15)
        if album_resp.status_code != 200:
            return None
        page = album_resp.text
        title = None
        cover_url = None
        cover_candidates: List[str] = []
        og_title = re.search(r'<meta\s+property="og:title"\s+content="([^"]+)"', page)
        if og_title:
            title = html.unescape(og_title.group(1)).strip()
        og_image = re.search(r'<meta\s+property="og:image"\s+content="([^"]+)"', page)
        if og_image:
            cover_url = og_image.group(1).strip()
            cover_candidates.append(cover_url)
        page_cover_matches = re.findall(
            r"(https?://f\d+\.bcbits\.com/img/[ab]\d+_[0-9]+\.[a-zA-Z0-9]+(?:\?[^\"]*)?)",
            page,
        )
        if page_cover_matches:
            cover_candidates.extend(page_cover_matches)
        art_id_match = re.search(r'"art_id"\s*:\s*(\d+)', page)
        if art_id_match:
            art_id = art_id_match.group(1)
            cover_candidates.append(f"https://f4.bcbits.com/img/a{art_id}_0.jpg")
        cover_candidates = _dedupe_keep_order(cover_candidates)
        if cover_candidates:
            prioritized = []
            for u in cover_candidates:
                m_size = re.search(r"_([0-9]+)\.[a-zA-Z0-9]+(?:\?|$)", u)
                if m_size:
                    val = int(m_size.group(1))
                    score = 10_000_000 if val == 0 else val
                else:
                    score = 0
                prioritized.append((score, u))
            prioritized.sort(key=lambda x: x[0], reverse=True)
            cover_url = prioritized[0][1]

        artist_str = artist_name
        if title and " by " in title:
            # Bandcamp uses "Album, by Artist". Album titles can themselves contain "by",
            # so split on the *last* occurrence to avoid mis-parsing titles like "Destroyed by Fire".
            parts = title.rsplit(" by ", 1)
            title = parts[0].strip()
            if len(parts) > 1:
                artist_str = parts[1].strip()
        if not title:
            title = album_title

        year = ""
        year_m = re.search(r'released\s+(\w+\s+\d{1,2},?\s+\d{4})', page)
        if year_m:
            year = year_m.group(1)

        tracklist = []
        track_title_spans = re.findall(r'class="track-title"[^>]*>([^<]+)</span>', page)
        if track_title_spans:
            tracklist = [html.unescape(t).strip() for t in track_title_spans if t.strip()]
        if not tracklist:
            track_title_data = re.findall(r'data-item-title="([^"]+)"', page)
            if track_title_data:
                tracklist = [html.unescape(t).strip() for t in track_title_data if t.strip()]

        tags: List[str] = []
        try:
            # Some Bandcamp pages do not use rel="tag" anymore. Keep both patterns.
            tag_matches = re.findall(r'rel=[\'"]tag[\'"][^>]*>([^<]+)</a>', page, flags=re.IGNORECASE)
            if not tag_matches:
                tag_matches = re.findall(
                    r'<a[^>]*class=[\'"][^\'"]*\btag\b[^\'"]*[\'"][^>]*>([^<]+)</a>',
                    page,
                    flags=re.IGNORECASE,
                )
            tags = [html.unescape(t).strip() for t in tag_matches if isinstance(t, str) and t.strip()]
            if not tags:
                jsonld_blocks = re.findall(
                    r'<script[^>]+type="application/ld\+json"[^>]*>\s*(.*?)\s*</script>',
                    page,
                    flags=re.IGNORECASE | re.DOTALL,
                )
                for block in jsonld_blocks:
                    try:
                        data = json.loads(block)
                    except Exception:
                        continue
                    # JSON-LD can be a dict or a list of dicts.
                    objects = []
                    if isinstance(data, dict):
                        objects = [data]
                    elif isinstance(data, list):
                        objects = [o for o in data if isinstance(o, dict)]
                    for obj in objects:
                        keywords = obj.get("keywords")
                        if isinstance(keywords, list):
                            tags.extend([str(x).strip() for x in keywords if str(x).strip()])
                        elif isinstance(keywords, str):
                            tags.extend(_split_bandcamp_keywords(keywords))
                    if tags:
                        break
            if not tags:
                # Some pages embed JSON in the data-tralbum attribute.
                m_tralbum = re.search(r'data-tralbum="([^"]+)"', page, flags=re.IGNORECASE)
                if m_tralbum:
                    try:
                        blob = html.unescape(m_tralbum.group(1))
                        data = json.loads(blob)
                        if isinstance(data, dict):
                            raw_tags = (
                                data.get("tags")
                                or (data.get("current") or {}).get("tags")
                                or (data.get("album") or {}).get("tags")
                            )
                            if isinstance(raw_tags, list):
                                tags.extend([str(x).strip() for x in raw_tags if str(x).strip()])
                            elif isinstance(raw_tags, str):
                                tags.extend(_split_bandcamp_keywords(raw_tags))
                    except Exception:
                        pass
            tags = _dedupe_keep_order([t for t in tags if isinstance(t, str) and t.strip()])
        except Exception:
            tags = []

        return {
            "title": title,
            "artist_name": artist_str,
            "year": year,
            "cover_url": cover_url,
            "cover_candidates": cover_candidates,
            "tracklist": tracklist,
            "tags": tags,
            "album_url": album_url,
        }

    try:
        q = quote_plus(f"{artist_name} {album_title}".strip())
        search_url = f"https://bandcamp.com/search?q={q}"
        resp = requests.get(search_url, headers=headers, timeout=15)
        if resp.status_code != 200:
            return None
        search_page = resp.text

        artist_norm_compact = _normalize_identity_text_strict(artist_name).replace(" ", "")
        album_norm_compact = _normalize_identity_album_strict(album_title).replace(" ", "")

        candidates: List[Tuple[float, str]] = []
        matches = re.findall(
            r'href="(https?://[^"]*bandcamp\.com/album/([^"?#]+))',
            search_page,
            flags=re.IGNORECASE,
        )
        for idx, (full_url, slug) in enumerate(matches):
            url = full_url.split("?")[0].split("#")[0].strip()
            if not url:
                continue
            score = 0.0
            slug_norm = _normalize_identity_album_strict(slug).replace(" ", "")
            if album_norm_compact and slug_norm:
                if album_norm_compact == slug_norm:
                    score += 2.0
                elif album_norm_compact in slug_norm or slug_norm in album_norm_compact:
                    score += 1.0

            host = re.sub(r"^https?://", "", url).split("/")[0]
            host_artist = host.split(".")[0]
            host_artist_norm = _normalize_identity_text_strict(host_artist).replace(" ", "")
            if artist_norm_compact and host_artist_norm:
                if artist_norm_compact == host_artist_norm:
                    score += 1.5
                elif artist_norm_compact in host_artist_norm or host_artist_norm in artist_norm_compact:
                    score += 0.75

            score += max(0.0, 0.25 - (idx * 0.01))
            candidates.append((score, url))

        if not candidates:
            return None

        ranked: Dict[str, float] = {}
        for score, url in candidates:
            ranked[url] = max(score, ranked.get(url, float("-inf")))
        ranked_urls = [u for u, _s in sorted(ranked.items(), key=lambda item: item[1], reverse=True)]

        strict_payload: Optional[dict] = None
        best_payload: Optional[dict] = None
        best_score = -1.0
        for album_url in ranked_urls[:4]:
            payload = _parse_album_page(album_url)
            if not payload:
                continue
            strict_ok, _strict_reason = _strict_identity_match_details(
                local_artist=artist_name,
                local_title=album_title,
                candidate_artist=payload.get("artist_name") or "",
                candidate_title=payload.get("title") or "",
            )
            if strict_ok:
                strict_payload = payload
                break
            title_score = _provider_identity_text_score(album_title, str(payload.get("title") or ""))
            artist_score = _provider_identity_text_score(artist_name, str(payload.get("artist_name") or ""))
            combined = (title_score * 0.6) + (artist_score * 0.4)
            if combined > best_score:
                best_score = combined
                best_payload = payload

        if strict_payload is not None:
            return strict_payload
        return best_payload
    except Exception as e:
        logging.warning("Bandcamp fetch failed for %s / %s: %s", artist_name, album_title, e)
        return None


def _web_search_serper(query: str, num: int = 10) -> List[dict]:
    """
    Run a web search via Serper.dev API. Returns list of {"title", "link", "snippet"}.
    Empty list if no API key, or on network/API error.
    """
    key = getattr(sys.modules[__name__], "SERPER_API_KEY", "") or ""
    if not key.strip():
        return []
    url = "https://google.serper.dev/search"
    headers = {"X-API-KEY": key, "Content-Type": "application/json"}
    body = {"q": query, "num": num}
    try:
        resp = requests.post(url, json=body, headers=headers, timeout=10)
        if resp.status_code != 200:
            if resp.status_code in (401, 403, 429):
                logging.warning(
                    "[Serper] Rate limit or no credits (HTTP %s) ‚Äì web search skipped. %s",
                    resp.status_code, resp.text[:200]
                )
            else:
                logging.debug("[Serper] HTTP %s: %s", resp.status_code, resp.text[:200])
            return []
        data = resp.json()
        organic = data.get("organic") or []
        out = []
        for item in organic:
            if isinstance(item, dict):
                out.append({
                    "title": item.get("title") or "",
                    "link": item.get("link") or "",
                    "snippet": item.get("snippet") or "",
                })
        return out
    except Exception as e:
        logging.debug("[Serper] Request failed: %s", e)
        return []


@app.get("/api/scan/preflight")
def scan_preflight():
    """Check MusicBrainz, AI, Discogs, Last.fm and Bandcamp. Returns clear ok/error for UI."""
    _reload_ai_config_and_reinit()
    mb_ok, ai_ok = _run_preflight_checks()
    provider_name = getattr(sys.modules[__name__], "AI_PROVIDER", None) or "OpenAI"
    resolved_model = getattr(sys.modules[__name__], "RESOLVED_MODEL", None)
    ai_func_err = getattr(sys.modules[__name__], "AI_FUNCTIONAL_ERROR_MSG", None)
    if not USE_MUSICBRAINZ:
        mb_msg = "MusicBrainz disabled in settings"
    elif mb_ok:
        mb_msg = "MusicBrainz reachable"
    else:
        mb_msg = "MusicBrainz unreachable"
    musicbrainz = {"ok": mb_ok, "message": mb_msg}
    if ai_ok:
        ai_msg = f"{provider_name} reachable" + (f", model {resolved_model} (params verified)" if resolved_model else "") if (OPENAI_API_KEY and openai_client) else f"{provider_name} configured"
    else:
        ai_msg = ai_func_err or "No API key or provider configured"
    ai_provider = {"ok": ai_ok, "message": ai_msg, "provider": provider_name}

    discogs_ok, discogs_msg = _run_discogs_preflight()
    discogs = {"ok": discogs_ok, "message": discogs_msg}
    lastfm_ok, lastfm_msg = _run_lastfm_preflight()
    lastfm = {"ok": lastfm_ok, "message": lastfm_msg}
    if USE_BANDCAMP:
        bandcamp = {"ok": True, "message": "Configured (fallback ultime, no connection test)"}
    else:
        bandcamp = {"ok": False, "message": "Disabled"}

    serper_ok, serper_msg = _run_serper_preflight()
    serper = {"ok": serper_ok, "message": serper_msg}
    acoustid_ok, acoustid_msg = _run_acoustid_preflight()
    acoustid = {"ok": acoustid_ok, "message": acoustid_msg}
    paths = _paths_rw_status()
    return jsonify(musicbrainz=musicbrainz, ai=ai_provider, discogs=discogs, lastfm=lastfm, bandcamp=bandcamp, serper=serper, acoustid=acoustid, paths=paths)


@app.route("/scan/start", methods=["POST"])
def start_scan():
    r = _requires_config()
    if r is not None:
        return r
    data = request.get_json(silent=True) or {}
    scan_type = (data.get("scan_type") or "full").strip().lower()
    run_improve_after = bool(data.get("run_improve_after", False))
    if scan_type not in {"full", "changed_only", "incomplete_only"}:
        return jsonify({"error": f"Invalid scan_type: {scan_type}"}), 400

    if scan_type == "incomplete_only":
        with lock:
            if state.get("incomplete_scan") and state["incomplete_scan"].get("running"):
                return jsonify({"error": "Incomplete scan already running"}), 409
        threading.Thread(target=_run_incomplete_albums_scan, daemon=True).start()
        return jsonify({"status": "ok", "scan_type": "incomplete_only"})

    _reload_ai_config_and_reinit()
    # Scans must be allowed even when AI is not ready: PMDA will use deterministic heuristics,
    # and only attempt AI calls when ai_provider_ready is True.
    ai_warning = None
    if not ai_provider_ready:
        ai_warning = getattr(sys.modules[__name__], "AI_FUNCTIONAL_ERROR_MSG", None) or "AI is not ready; scan will run without AI."
    scan_should_stop.clear()
    scan_is_paused.clear()
    with lock:
        state["run_improve_after"] = run_improve_after if scan_type in {"full", "changed_only"} else False
        state["scan_type"] = scan_type
    start_background_scan()
    return jsonify({
        "status": "ok",
        "scan_type": scan_type,
        "run_improve_after": run_improve_after,
        "ai_enabled": bool(ai_provider_ready),
        "ai_warning": ai_warning,
    })

@app.route("/scan/pause", methods=["POST"])
def pause_scan():
    scan_is_paused.set()
    with lock:
        state["scanning"] = True   # still scanning, just paused
    return jsonify({"status": "ok"})


@app.route("/scan/resume", methods=["POST"])
def resume_scan():
    scan_is_paused.clear()
    # no state change needed; polling loop will continue
    return jsonify({"status": "ok"})


@app.route("/scan/stop", methods=["POST"])
def stop_scan():
    scan_should_stop.set()
    return jsonify({"status": "ok"})


@app.route("/api/scan/clear", methods=["POST"])
def clear_scan():
    """
    Clear all scan results from the database: duplicates, broken albums,
    scan editions (Tag Fixer), and last completed scan id.
    Optionally clear audio and MusicBrainz caches.
    """
    import sqlite3
    data = request.get_json() or {}
    clear_audio_cache = data.get("clear_audio_cache", False)
    clear_mb_cache = data.get("clear_mb_cache", False)
    
    try:
        # Clear all scan-derived data so Unduper, Tag Fixer, Incomplete Albums show nothing
        con = sqlite3.connect(str(STATE_DB_FILE))
        cur = con.cursor()
        cur.execute("DELETE FROM duplicates_loser")
        deleted_losers = cur.rowcount
        cur.execute("DELETE FROM duplicates_best")
        deleted_best = cur.rowcount
        cur.execute("DELETE FROM broken_albums")
        deleted_broken = cur.rowcount
        cur.execute("DELETE FROM scan_editions")
        deleted_editions = cur.rowcount
        cur.execute("DELETE FROM files_library_published_albums")
        deleted_published = cur.rowcount
        cur.execute("DELETE FROM files_pending_changes")
        deleted_pending_changes = cur.rowcount
        cur.execute("DELETE FROM settings WHERE key = 'last_completed_scan_id'")
        # Clear last scan summary so "Last scan summary" UI disappears until next scan
        cur.execute(
            "UPDATE scan_history SET summary_json = NULL WHERE scan_id = (SELECT scan_id FROM scan_history WHERE status = 'completed' AND end_time IS NOT NULL ORDER BY end_time DESC LIMIT 1)"
        )
        con.commit()
        con.close()
        
        # Clear in-memory state
        with lock:
            state["duplicates"] = {}
            state["scan_active_artists"] = {}
            fw = dict(state.get("files_watcher") or {})
            fw["dirty_count"] = 0
            fw["last_event_at"] = None
            fw["last_event_path"] = None
            state["files_watcher"] = fw
        
        result = {
            "status": "ok",
            "message": "Scan results cleared successfully",
            "cleared": {
                "duplicates_best": deleted_best,
                "duplicates_loser": deleted_losers,
                "broken_albums": deleted_broken,
                "scan_editions": deleted_editions,
                "files_library_published_albums": deleted_published,
                "files_pending_changes": deleted_pending_changes,
            }
        }
        if _get_library_mode() == "files":
            _reset_files_live_index_for_scan()

        # Optionally clear audio cache
        if clear_audio_cache:
            con = sqlite3.connect(str(CACHE_DB_FILE))
            cur = con.cursor()
            cur.execute("DELETE FROM audio_cache")
            audio_cache_deleted = cur.rowcount
            con.commit()
            con.close()
            result["cleared"]["audio_cache"] = audio_cache_deleted
            result["message"] += f", {audio_cache_deleted} audio cache entries cleared"
        
        # Optionally clear MusicBrainz cache
        if clear_mb_cache:
            con = sqlite3.connect(str(CACHE_DB_FILE))
            cur = con.cursor()
            cur.execute("DELETE FROM musicbrainz_cache")
            mb_cache_deleted = cur.rowcount
            cur.execute("DELETE FROM musicbrainz_album_lookup")
            mb_album_lookup_deleted = cur.rowcount
            cur.execute("DELETE FROM provider_album_lookup")
            provider_album_lookup_deleted = cur.rowcount
            con.commit()
            con.close()
            result["cleared"]["musicbrainz_cache"] = mb_cache_deleted
            result["cleared"]["musicbrainz_album_lookup"] = mb_album_lookup_deleted
            result["cleared"]["provider_album_lookup"] = provider_album_lookup_deleted
            result["message"] += (
                f", {mb_cache_deleted} MB cache + {mb_album_lookup_deleted} album lookup cache"
                f" + {provider_album_lookup_deleted} provider lookup cache cleared"
            )
        
        logging.info("Scan results cleared: %s", result)
        return jsonify(result)
    except Exception as e:
        logging.error("Failed to clear scan results: %s", e, exc_info=True)
        return jsonify({"status": "error", "message": str(e)}), 500


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Wizard / Web UI helpers ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

@app.get("/api/plex/check")
def api_plex_check_get():
    """Test Plex connection using current server config (PLEX_HOST, PLEX_TOKEN)."""
    return _do_plex_check(PLEX_HOST, PLEX_TOKEN)


@app.post("/api/plex/check")
def api_plex_check_post():
    """Test Plex connection; optional body { PLEX_HOST, PLEX_TOKEN } to test before saving."""
    data = request.get_json(silent=True) or {}
    host = (data.get("PLEX_HOST") or "").strip() or PLEX_HOST
    token = (data.get("PLEX_TOKEN") or "").strip() or PLEX_TOKEN
    return _do_plex_check(host, token)


def _do_plex_check(host: str, token: str):
    if not host or not token:
        return jsonify({"success": False, "message": "PLEX_HOST and PLEX_TOKEN are required"}), 400
    host = host.strip().rstrip("/")
    if host and not host.startswith(("http://", "https://")):
        host = "http://" + host
    url = f"{host}/library/sections"
    try:
        resp = requests.get(url, headers={"X-Plex-Token": token}, timeout=10)
        if resp.status_code != 200:
            return jsonify({"success": False, "message": f"Plex returned HTTP {resp.status_code}"})
        return jsonify({"success": True, "message": "Connection successful"})
    except requests.exceptions.ConnectionError as e:
        return jsonify({"success": False, "message": "Connection refused or host unreachable. Check URL and that Plex is running."})
    except requests.exceptions.Timeout:
        return jsonify({"success": False, "message": "Connection timed out. Check URL and network."})
    except Exception as e:
        return jsonify({"success": False, "message": str(e)})


def _normalize_player_target(raw: str | None) -> str:
    target = str(raw or "").strip().lower()
    if target not in {"none", "plex", "jellyfin", "navidrome"}:
        return "none"
    return target


def _normalize_http_base_url(url: str) -> str:
    out = str(url or "").strip().rstrip("/")
    if out and not out.startswith(("http://", "https://")):
        out = "http://" + out
    return out


def _jellyfin_auth_headers(api_key: str) -> dict[str, str]:
    token = str(api_key or "").strip()
    # Jellyfin API key security scheme uses Authorization header.
    return {
        "Accept": "application/json",
        "Authorization": f'MediaBrowser Token="{token}"',
    }


def _do_jellyfin_check(url: str, api_key: str) -> tuple[bool, str]:
    base = _normalize_http_base_url(url)
    key = str(api_key or "").strip()
    if not base or not key:
        return False, "Jellyfin URL and API key are required"
    try:
        # Public server info (network reachability + server alive)
        public_resp = requests.get(f"{base}/System/Info/Public", timeout=10)
        if public_resp.status_code >= 400:
            return False, f"Jellyfin public info returned HTTP {public_resp.status_code}"

        # Authenticated endpoint (token validity + permission)
        auth_resp = requests.get(
            f"{base}/Items/Counts",
            headers=_jellyfin_auth_headers(key),
            timeout=10,
        )
        if auth_resp.status_code == 401:
            return False, "Jellyfin API key invalid (401)"
        if auth_resp.status_code == 403:
            return False, "Jellyfin API key lacks permissions (403)"
        if auth_resp.status_code >= 400:
            return False, f"Jellyfin auth check returned HTTP {auth_resp.status_code}"
        return True, "Jellyfin connection successful"
    except requests.exceptions.ConnectionError:
        return False, "Jellyfin host unreachable or connection refused"
    except requests.exceptions.Timeout:
        return False, "Jellyfin request timed out"
    except Exception as e:
        return False, str(e)


def _trigger_jellyfin_refresh(url: str, api_key: str) -> tuple[bool, str]:
    base = _normalize_http_base_url(url)
    key = str(api_key or "").strip()
    if not base or not key:
        return False, "Jellyfin URL and API key are required"
    try:
        resp = requests.post(
            f"{base}/Library/Refresh",
            headers=_jellyfin_auth_headers(key),
            timeout=20,
        )
        if resp.status_code in (200, 204):
            return True, "Jellyfin library refresh triggered"
        if resp.status_code == 401:
            return False, "Jellyfin API key invalid (401)"
        if resp.status_code == 403:
            return False, "Jellyfin API key lacks required permission (403)"
        return False, f"Jellyfin refresh failed (HTTP {resp.status_code})"
    except requests.exceptions.ConnectionError:
        return False, "Jellyfin host unreachable or connection refused"
    except requests.exceptions.Timeout:
        return False, "Jellyfin refresh request timed out"
    except Exception as e:
        return False, str(e)


def _navidrome_auth_params(username: str, password: str, api_key: str = "") -> tuple[dict[str, str], str]:
    user = str(username or "").strip()
    pwd = str(password or "")
    key = str(api_key or "").strip()
    if key:
        params = {"v": "1.16.1", "c": "pmda", "f": "json", "apiKey": key}
        if user:
            params["u"] = user
        return params, ""
    if not user or not pwd:
        return {}, "Navidrome username/password are required (or API key)"
    salt = uuid.uuid4().hex[:10]
    token = hashlib.md5((pwd + salt).encode("utf-8")).hexdigest()
    return {"u": user, "t": token, "s": salt, "v": "1.16.1", "c": "pmda", "f": "json"}, ""


def _do_navidrome_check(url: str, username: str, password: str, api_key: str = "") -> tuple[bool, str]:
    base = _normalize_http_base_url(url)
    if not base:
        return False, "Navidrome URL is required"
    params, err = _navidrome_auth_params(username, password, api_key)
    if err:
        return False, err
    try:
        resp = requests.get(f"{base}/rest/ping.view", params=params, timeout=10)
        if resp.status_code >= 400:
            return False, f"Navidrome ping failed (HTTP {resp.status_code})"
        payload = resp.json() if "json" in (resp.headers.get("Content-Type") or "").lower() else {}
        sr = payload.get("subsonic-response") if isinstance(payload, dict) else {}
        status = (sr or {}).get("status")
        if status == "ok":
            return True, "Navidrome connection successful"
        err_msg = (sr or {}).get("error", {}).get("message") if isinstance(sr, dict) else None
        return False, err_msg or "Navidrome authentication failed"
    except requests.exceptions.ConnectionError:
        return False, "Navidrome host unreachable or connection refused"
    except requests.exceptions.Timeout:
        return False, "Navidrome request timed out"
    except Exception as e:
        return False, str(e)


def _trigger_navidrome_refresh(url: str, username: str, password: str, api_key: str = "") -> tuple[bool, str]:
    base = _normalize_http_base_url(url)
    if not base:
        return False, "Navidrome URL is required"
    params, err = _navidrome_auth_params(username, password, api_key)
    if err:
        return False, err
    try:
        resp = requests.get(f"{base}/rest/startScan.view", params=params, timeout=20)
        if resp.status_code >= 400:
            return False, f"Navidrome startScan failed (HTTP {resp.status_code})"
        payload = resp.json() if "json" in (resp.headers.get("Content-Type") or "").lower() else {}
        sr = payload.get("subsonic-response") if isinstance(payload, dict) else {}
        if (sr or {}).get("status") == "ok":
            return True, "Navidrome library scan triggered"
        err_msg = (sr or {}).get("error", {}).get("message") if isinstance(sr, dict) else None
        return False, err_msg or "Navidrome scan trigger failed"
    except requests.exceptions.ConnectionError:
        return False, "Navidrome host unreachable or connection refused"
    except requests.exceptions.Timeout:
        return False, "Navidrome startScan timed out"
    except Exception as e:
        return False, str(e)


def _effective_player_target(explicit_target: str | None = None) -> str:
    target = _normalize_player_target(explicit_target)
    if target != "none":
        return target
    return _normalize_player_target(getattr(sys.modules[__name__], "PIPELINE_PLAYER_TARGET", "none"))


def _trigger_player_refresh_by_target(target: str) -> tuple[bool, str]:
    tgt = _normalize_player_target(target)
    if tgt == "none":
        return False, "No player sync target configured"
    if tgt == "plex":
        _reload_section_ids_from_db()
        if not SECTION_IDS:
            return False, "No Plex library sections configured"
        refreshed = []
        errors = []
        for sid in SECTION_IDS:
            try:
                plex_api(f"/library/sections/{sid}/refresh", method="GET")
                refreshed.append(sid)
            except Exception as e:
                errors.append(f"section {sid}: {e}")
        if refreshed:
            msg = f"Plex refresh triggered for section(s) {refreshed}"
            if errors:
                msg += f" ({len(errors)} error(s))"
            return True, msg
        return False, "Plex refresh failed for all sections"
    if tgt == "jellyfin":
        return _trigger_jellyfin_refresh(
            getattr(sys.modules[__name__], "JELLYFIN_URL", ""),
            getattr(sys.modules[__name__], "JELLYFIN_API_KEY", ""),
        )
    if tgt == "navidrome":
        return _trigger_navidrome_refresh(
            getattr(sys.modules[__name__], "NAVIDROME_URL", ""),
            getattr(sys.modules[__name__], "NAVIDROME_USERNAME", ""),
            getattr(sys.modules[__name__], "NAVIDROME_PASSWORD", ""),
            getattr(sys.modules[__name__], "NAVIDROME_API_KEY", ""),
        )
    return False, f"Unsupported sync target: {tgt}"


@app.post("/api/player/check")
def api_player_check():
    """Test connectivity for Plex/Jellyfin/Navidrome based on requested or configured target."""
    data = request.get_json(silent=True) or {}
    target = _effective_player_target(data.get("target"))
    if target == "none":
        return jsonify({"success": False, "target": "none", "message": "No player target selected"}), 400
    if target == "plex":
        host = (data.get("PLEX_HOST") or "").strip() or PLEX_HOST
        token = (data.get("PLEX_TOKEN") or "").strip() or PLEX_TOKEN
        resp_obj = _do_plex_check(host, token)
        status = 200
        resp = resp_obj
        if isinstance(resp_obj, tuple) and len(resp_obj) >= 2:
            resp = resp_obj[0]
            try:
                status = int(resp_obj[1])
            except (TypeError, ValueError):
                status = 200
        payload = resp.get_json(silent=True) if hasattr(resp, "get_json") else None
        status = getattr(resp, "status_code", status)
        if isinstance(payload, dict):
            payload["target"] = "plex"
            return jsonify(payload), status
        return resp_obj
    if target == "jellyfin":
        ok, msg = _do_jellyfin_check(
            (data.get("JELLYFIN_URL") or "").strip() or getattr(sys.modules[__name__], "JELLYFIN_URL", ""),
            (data.get("JELLYFIN_API_KEY") or "").strip() or getattr(sys.modules[__name__], "JELLYFIN_API_KEY", ""),
        )
        return jsonify({"success": ok, "target": "jellyfin", "message": msg}), (200 if ok else 400)
    if target == "navidrome":
        ok, msg = _do_navidrome_check(
            (data.get("NAVIDROME_URL") or "").strip() or getattr(sys.modules[__name__], "NAVIDROME_URL", ""),
            (data.get("NAVIDROME_USERNAME") or "").strip() or getattr(sys.modules[__name__], "NAVIDROME_USERNAME", ""),
            (data.get("NAVIDROME_PASSWORD") or "").strip() or getattr(sys.modules[__name__], "NAVIDROME_PASSWORD", ""),
            (data.get("NAVIDROME_API_KEY") or "").strip() or getattr(sys.modules[__name__], "NAVIDROME_API_KEY", ""),
        )
        return jsonify({"success": ok, "target": "navidrome", "message": msg}), (200 if ok else 400)
    return jsonify({"success": False, "target": target, "message": "Unknown target"}), 400


@app.post("/api/player/refresh")
def api_player_refresh():
    """Trigger media library scan on the selected target (Plex/Jellyfin/Navidrome)."""
    data = request.get_json(silent=True) or {}
    target = _effective_player_target(data.get("target"))
    ok, msg = _trigger_player_refresh_by_target(target)
    return jsonify({"success": ok, "target": target, "message": msg}), (200 if ok else 400)


@app.post("/api/plex/refresh")
def api_plex_refresh():
    """
    Trigger a Plex library scan (refresh) for all configured SECTION_IDS.
    Use after adding or changing files in the music root (e.g. after creating pmda_tests set)
    so Plex indexes the new folders before running a PMDA scan.
    """
    _reload_section_ids_from_db()
    if not SECTION_IDS:
        return jsonify({"success": False, "message": "No library sections configured. Set SECTION_IDS in Settings."}), 400
    refreshed = []
    errors = []
    for sid in SECTION_IDS:
        try:
            plex_api(f"/library/sections/{sid}/refresh", method="GET")
            refreshed.append(sid)
        except Exception as e:
            errors.append({"section_id": sid, "error": str(e)})
    if errors and not refreshed:
        return jsonify({"success": False, "message": "Refresh failed for all sections", "errors": errors}), 502
    return jsonify({
        "success": True,
        "sections_refreshed": refreshed,
        "message": f"Plex refresh triggered for section(s) {refreshed}" + (f"; {len(errors)} error(s)" if errors else ""),
        "errors": errors if errors else None,
    })


# ‚îÄ‚îÄ‚îÄ Plex.tv PIN auth (Tautulli-style: no manual token) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
PLEX_PIN_HEADERS = {
    "X-Plex-Client-Identifier": "pmda-webui-1",
    "X-Plex-Product": "PMDA",
    "X-Plex-Version": "1.0",
    "X-Plex-Device": "Web",
    "X-Plex-Platform": "Web",
    "Accept": "application/json",
}


@app.post("/api/plex/pin")
def api_plex_pin_create():
    """
    Create a Plex.tv PIN for sign-in (like Tautulli "Fetch New Token").
    User opens https://www.plex.tv/link, enters the returned code; we poll GET /api/plex/pin?id=... for the token.
    No auth required. Returns { id, code, link_url }.
    """
    # strong=false (default) ‚Üí 4-character code for plex.tv/link; strong=true ‚Üí long code for other flows
    url = "https://plex.tv/api/v2/pins"
    try:
        resp = requests.post(url, headers=PLEX_PIN_HEADERS, timeout=15)
        resp.raise_for_status()
        data = resp.json()
        pin_id = data.get("id")
        code = data.get("code", "")
        if not pin_id:
            return jsonify({"success": False, "message": "Plex did not return a PIN id"}), 502
        return jsonify({
            "success": True,
            "id": pin_id,
            "code": code,
            "link_url": "https://www.plex.tv/link/",
        })
    except requests.exceptions.ConnectionError:
        return jsonify({"success": False, "message": "Cannot reach plex.tv. Check network and DNS."}), 502
    except requests.exceptions.Timeout:
        return jsonify({"success": False, "message": "Request to plex.tv timed out."}), 502
    except (requests.RequestException, ValueError) as e:
        return jsonify({"success": False, "message": str(e)}), 502


@app.get("/api/plex/pin")
def api_plex_pin_poll():
    """
    Poll PIN status. Query param: id (pin id from POST /api/plex/pin).
    Returns { status: 'waiting' } or { status: 'linked', token } when user has signed in on plex.tv/link.
    """
    pin_id = request.args.get("id", "").strip()
    if not pin_id:
        return jsonify({"success": False, "status": "error", "message": "Missing id"}), 400
    url = f"https://plex.tv/api/v2/pins/{pin_id}"
    try:
        resp = requests.get(url, headers=PLEX_PIN_HEADERS, timeout=10)
        if resp.status_code == 404:
            return jsonify({"success": True, "status": "expired", "message": "PIN expired"})
        resp.raise_for_status()
        data = resp.json()
        token = data.get("authToken")
        if token:
            return jsonify({"success": True, "status": "linked", "token": token})
        return jsonify({"success": True, "status": "waiting"})
    except requests.exceptions.ConnectionError:
        return jsonify({"success": False, "status": "error", "message": "Cannot reach plex.tv"}), 502
    except requests.exceptions.Timeout:
        return jsonify({"success": False, "status": "error", "message": "Request timed out"}), 502
    except (requests.RequestException, ValueError) as e:
        return jsonify({"success": False, "status": "error", "message": str(e)}), 502


def _is_lan_address(address: str) -> bool:
    """True if address looks like a classic LAN IP (192.168.x.x or 10.x.x.x), not Docker (172.16-31)."""
    if not address:
        return False
    parts = address.split(".")
    if len(parts) != 4:
        return False
    try:
        a, b, c, d = (int(x) for x in parts)
        if 0 <= a <= 255 and 0 <= b <= 255 and 0 <= c <= 255 and 0 <= d <= 255:
            if (a == 192 and b == 168) or (a == 10):
                return True
            if a == 172 and 16 <= b <= 31:  # Docker/private
                return False
    except (ValueError, TypeError):
        pass
    return False


def _parse_plex_resources_xml(text: str) -> list:
    """Parse plex.tv /api/resources XML (MediaContainer > Device > Connection). Same flow as Tautulli.
    Returns list of { name, uri, address, port, scheme, localAddresses, machineIdentifier }.
    One entry per Device; prefers LAN URL (192.168.x.x / 10.x.x.x) over Docker/plex.direct so the UI shows a reachable URL.
    """
    text = re.sub(r"&(?!(?:amp|lt|gt|quot|apos|#\d+|#x[0-9a-fA-F]+);)", "&amp;", text)
    servers = []
    try:
        root = ET.fromstring(text)
    except Exception:
        return []
    for device in root.iter("Device"):
        attr = device.attrib
        provides = (attr.get("provides") or "").strip().lower()
        if "server" not in provides:
            continue
        owned = attr.get("owned", "0")
        if owned != "1":
            continue
        name = attr.get("name", "Plex")
        client_id = attr.get("clientIdentifier", "")
        connections = []
        for conn in device.iter("Connection"):
            c = conn.attrib
            uri = (c.get("uri") or "").strip()
            if not uri or not uri.startswith("http"):
                continue
            address = (c.get("address") or "").strip()
            port = (c.get("port") or "32400").strip()
            if port == "0":
                port = "32400"
            scheme = "https" if uri.startswith("https") else "http"
            is_local = (c.get("local") or "").strip() == "1"
            is_lan = _is_lan_address(address)
            # Prefer: (1) local + LAN IP, (2) local + not Docker, (3) LAN IP, (4) local, (5) any
            if is_local and is_lan:
                rank = 0
            elif is_local and address and not (address.startswith("172.") and _is_private_172(address)):
                rank = 1
            elif is_lan:
                rank = 2
            elif is_local:
                rank = 3
            else:
                rank = 4
            connections.append((rank, address, port, scheme, uri))
        if not connections:
            continue
        connections.sort(key=lambda x: (x[0], x[1] or "zzz"))
        _, address, port, scheme, uri = connections[0]
        # Build a clean URL with dots (not plex.direct with dashes) when we have a real IP
        if address and re.match(r"^\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}$", address):
            display_uri = f"{scheme}://{address}:{port}"
        else:
            display_uri = uri
        servers.append({
            "name": name,
            "uri": display_uri,
            "address": address,
            "port": port,
            "scheme": scheme,
            "localAddresses": address,
            "machineIdentifier": client_id,
        })
    return servers


def _is_private_172(address: str) -> bool:
    """True if address is in 172.16.0.0/12 (e.g. Docker)."""
    parts = address.split(".")
    if len(parts) != 4:
        return False
    try:
        a, b, *_ = (int(x) for x in parts)
        return a == 172 and 16 <= b <= 31
    except (ValueError, TypeError):
        return False


def _parse_plex_servers_xml(text: str) -> list:
    """Parse plex.tv servers XML; fix unescaped & and extract Server elements. Returns list of dicts."""
    text = re.sub(r"&(?!(?:amp|lt|gt|quot|apos|#\d+|#x[0-9a-fA-F]+);)", "&amp;", text)

    def _extract_servers_regex() -> list:
        servers = []
        for m in re.finditer(r"<Server\s+([^>]+)/?>", text, re.DOTALL):
            attrs_str = m.group(1)
            attrs = {}
            for a in re.finditer(r'(\w+)="([^"]*)"', attrs_str):
                attrs[a.group(1)] = a.group(2).replace("&amp;", "&").replace("&lt;", "<").replace("&gt;", ">")
            name = attrs.get("name", "Plex")
            port = attrs.get("port", "32400")
            if port == "0":
                port = "32400"
            scheme = attrs.get("scheme", "http")
            address = (attrs.get("address") or "").strip()
            local_addresses = (attrs.get("localAddresses") or "").strip()
            if local_addresses:
                first_local = local_addresses.split(",")[0].strip()
                host = first_local or address or "localhost"
            else:
                host = address or "localhost"
            uri = f"{scheme}://{host}:{port}" if host else ""
            servers.append({
                "name": name,
                "uri": uri,
                "address": address,
                "port": port,
                "scheme": scheme,
                "localAddresses": local_addresses,
                "machineIdentifier": attrs.get("machineIdentifier", ""),
            })
        return servers

    try:
        root = ET.fromstring(text)
    except Exception:
        return _extract_servers_regex()
    servers = []
    for server in root.iter("Server"):
        attrs = server.attrib
        name = attrs.get("name", "Plex")
        port = attrs.get("port", "32400")
        if port == "0":
            port = "32400"
        scheme = attrs.get("scheme", "http")
        address = attrs.get("address", "").strip()
        local_addresses = (attrs.get("localAddresses") or "").strip()
        if local_addresses:
            first_local = local_addresses.split(",")[0].strip()
            host = first_local or address or "localhost"
        else:
            host = address or "localhost"
        uri = f"{scheme}://{host}:{port}" if host else ""
        servers.append({
            "name": name,
            "uri": uri,
            "address": address,
            "port": port,
            "scheme": scheme,
            "localAddresses": local_addresses,
            "machineIdentifier": attrs.get("machineIdentifier", ""),
        })
    return servers


@app.post("/api/plex/servers")
def api_plex_servers():
    """
    List Plex servers for the current account (Tautulli-style).
    Body: { "PLEX_TOKEN": "..." }. Returns list of { name, uri, localAddresses, port, ... }.
    Tries plex.tv API v2 (JSON) first, then falls back to servers.xml (XML).
    """
    data = request.get_json(silent=True) or {}
    token = (data.get("PLEX_TOKEN") or data.get("token") or "").strip()
    if not token:
        return jsonify({"success": False, "servers": [], "message": "PLEX_TOKEN is required"}), 400
    headers = {"X-Plex-Token": token, "Accept": "application/json"}

    def _build_servers_from_servers_json(data: dict) -> list:
        """Build server list from Plex GET /servers JSON (MediaContainer.Server[]).
        Docs: https://plexapi.dev/api-reference/server/get-server-list
        """
        servers = []
        if not isinstance(data, dict):
            return servers
        media = data.get("MediaContainer", data.get("mediaContainer"))
        if not isinstance(media, dict):
            return servers
        items = media.get("Server") or media.get("server") or []
        if not isinstance(items, list):
            items = [items] if items else []
        for s in items:
            if not isinstance(s, dict):
                continue
            def _g(k, default=None):
                return s.get(k) or s.get(k[0].upper() + k[1:] if k else k) or default
            name = _g("name", "Plex")
            port = str(_g("port") or "32400")
            if port == "0":
                port = "32400"
            scheme = (str(_g("scheme") or "http")).strip().lower()
            if scheme not in ("http", "https"):
                scheme = "http"
            host = (str(_g("host") or _g("address") or "")).strip()
            address = (str(_g("address") or host or "")).strip()
            local_addresses = (str(_g("localAddresses") or address or "")).strip()
            if local_addresses:
                host = local_addresses.split(",")[0].strip() or host
            if not host:
                continue
            uri = f"{scheme}://{host}:{port}"
            servers.append({
                "name": name,
                "uri": uri,
                "address": address,
                "port": port,
                "scheme": scheme,
                "localAddresses": local_addresses,
                "machineIdentifier": str(_g("machineIdentifier") or ""),
            })
        return servers

    try:
        seen_machine_ids: set[str] = set()
        servers: list[dict] = []

        # 1) servers.xml first ‚Äî often returns all servers (including multiple instances on same host, e.g. :32400 + :32401)
        resp_xml = requests.get(
            f"https://plex.tv/servers.xml?includeLite=1&X-Plex-Token={requests.utils.quote(token, safe='')}",
            headers={"X-Plex-Token": token},
            timeout=15,
        )
        if resp_xml.status_code == 401:
            return jsonify({"success": False, "servers": [], "message": "Invalid Plex token"}), 401
        if resp_xml.ok:
            try:
                from_xml = _parse_plex_servers_xml(resp_xml.text)
                for s in from_xml:
                    mid = (s.get("machineIdentifier") or "").strip()
                    if not mid or mid in seen_machine_ids:
                        continue
                    servers.append(s)
                    seen_machine_ids.add(mid)
            except Exception:
                pass

        # 2) GET https://plex.tv/api/resources?includeHttps=1 ‚Äî merge in any device not yet listed (by machineIdentifier)
        resp_resources = requests.get(
            "https://plex.tv/api/resources?includeHttps=1",
            headers={"X-Plex-Token": token},
            timeout=15,
        )
        if resp_resources.status_code == 401:
            return jsonify({"success": False, "servers": [], "message": "Invalid Plex token"}), 401
        if resp_resources.ok:
            from_resources = _parse_plex_resources_xml(resp_resources.text)
            for s in from_resources:
                mid = (s.get("machineIdentifier") or "").strip()
                if not mid or mid in seen_machine_ids:
                    continue
                # Avoid duplicate by (name, port) in case machineIdentifier differs
                if any(x.get("name") == s.get("name") and x.get("port") == s.get("port") for x in servers):
                    continue
                servers.append(s)
                seen_machine_ids.add(mid)

        # 3) GET https://plex.tv/servers (JSON or XML) ‚Äî merge any remaining
        resp = requests.get("https://plex.tv/servers", headers=headers, timeout=15)
        if resp.status_code != 401 and resp.ok:
            ct = (resp.headers.get("Content-Type") or "").lower()
            if "json" in ct:
                try:
                    data = resp.json()
                    for s in _build_servers_from_servers_json(data):
                        mid = (s.get("machineIdentifier") or "").strip()
                        if mid and mid not in seen_machine_ids:
                            servers.append(s)
                            seen_machine_ids.add(mid)
                except (ValueError, TypeError, KeyError):
                    pass
            for s in _parse_plex_servers_xml(resp.text):
                mid = (s.get("machineIdentifier") or "").strip()
                if mid and mid not in seen_machine_ids:
                    servers.append(s)
                    seen_machine_ids.add(mid)

        if servers:
            return jsonify({"success": True, "servers": servers})
        # Empty list: token accepted but no servers linked to account
        return jsonify({
            "success": True,
            "servers": [],
            "message": "No Plex servers found for this account. Link your server at plex.tv or check the token.",
        })

    except requests.exceptions.ConnectionError:
        return jsonify({"success": False, "servers": [], "message": "Cannot reach plex.tv. Check network and DNS from the machine running PMDA (e.g. Docker has outbound internet)."}), 502
    except requests.exceptions.Timeout:
        return jsonify({"success": False, "servers": [], "message": "Request to plex.tv timed out. Check network."}), 502
    except requests.RequestException as e:
        return jsonify({"success": False, "servers": [], "message": str(e)}), 502


# ‚îÄ‚îÄ‚îÄ Plex database path hints (official locations by platform / image) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Full paths to the folder containing com.plexapp.plugins.library.db (for wizard help)
# Source: https://support.plex.tv/articles/202915258-where-is-the-plex-media-server-data-directory-located/
PLEX_DATABASE_PATH_HINTS: list[dict] = [
    {"platform": "Docker (generic)", "path": "<config_mount>/Library/Application Support/Plex Media Server/Plug-in Support/Databases", "note": "Mount the host path that maps to /config in the Plex container."},
    {"platform": "plexinc/pms-docker", "path": "<config_volume>/Library/Application Support/Plex Media Server/Plug-in Support/Databases", "note": "Same as Docker generic; -v host_path:/config."},
    {"platform": "linuxserver/plex", "path": "<config_volume>/Library/Application Support/Plex Media Server/Plug-in Support/Databases", "note": "Same as Docker generic."},
    {"platform": "Debian / Ubuntu / Fedora / CentOS", "path": "/var/lib/plexmediaserver/Library/Application Support/Plex Media Server/Plug-in Support/Databases", "note": "Native Linux package."},
    {"platform": "FreeBSD", "path": "/usr/local/plexdata/Plex Media Server/Plug-in Support/Databases", "note": "Native install."},
    {"platform": "macOS", "path": "~/Library/Application Support/Plex Media Server/Plug-in Support/Databases", "note": "Expand ~ to your home."},
    {"platform": "Windows", "path": "%LOCALAPPDATA%\\Plex Media Server\\Plug-in Support\\Databases", "note": "Per-user (account running Plex)."},
    {"platform": "Synology DSM 7", "path": "/volume1/PlexMediaServer/AppData/Plex Media Server/Plug-in Support/Databases", "note": "Volume name may vary."},
    {"platform": "Synology DSM 6", "path": "/volume1/Plex/Library/Application Support/Plex Media Server/Plug-in Support/Databases", "note": "Volume name may vary."},
    {"platform": "QNAP", "path": "<Install_path>/Library/Plex Media Server/Plug-in Support/Databases", "note": "Install_path from: getcfg -f /etc/config/qpkg.conf PlexMediaServer Install_path."},
    {"platform": "ASUSTOR", "path": "/volume1/Plex/Library/Plug-in Support/Databases", "note": "Under Plex data directory."},
    {"platform": "FreeNAS 11.3+", "path": "${JAIL_ROOT}/Plex Media Server/Plug-in Support/Databases", "note": "JAIL_ROOT is the jail root."},
    {"platform": "Snap", "path": "/var/snap/plexmediaserver/common/Library/Application Support/Plex Media Server/Plug-in Support/Databases", "note": "Snap package."},
    {"platform": "ReadyNAS", "path": "/apps/plexmediaserver/MediaLibrary/Plex Media Server/Plug-in Support/Databases", "note": ""},
    {"platform": "TerraMaster", "path": "/Volume1/Plex/Library/Application Support/Plex Media Server/Plug-in Support/Databases", "note": ""},
]


def _gdm_discover_servers(timeout_sec: float = 2.0) -> list[dict]:
    """
    Discover Plex Media Servers on the local network via GDM (Good Day Mate) multicast.
    Sends M-SEARCH to 239.0.0.250:32414 and parses HTTP/1.0 200 OK responses.
    Returns list of dicts with name, uri (http://ip:port), address, port.
    """
    gdm_ip, gdm_port = "239.0.0.250", 32414
    msg = b"M-SEARCH * HTTP/1.0"
    seen: set[str] = set()
    result: list[dict] = []
    sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
    sock.setsockopt(socket.IPPROTO_IP, socket.IP_MULTICAST_TTL, struct.pack("B", 2))
    sock.settimeout(0.5)
    try:
        sock.sendto(msg, (gdm_ip, gdm_port))
        deadline = time.time() + timeout_sec
        while time.time() < deadline:
            try:
                bdata, from_addr = sock.recvfrom(1024)
            except socket.timeout:
                continue
            data = bdata.decode("utf-8", errors="replace")
            lines = data.splitlines()
            if not lines or "200 OK" not in lines[0]:
                continue
            ddata: dict[str, str] = {}
            for line in lines[1:]:
                if ":" in line:
                    k, _, v = line.partition(":")
                    ddata[k.strip()] = v.strip()
            if ddata.get("Content-Type") != "plex/media-server":
                continue
            rid = ddata.get("Resource-Identifier") or ""
            if rid in seen:
                continue
            seen.add(rid)
            name = ddata.get("Name", "Plex Media Server")
            port = ddata.get("Port", "32400")
            from_ip = from_addr[0]
            uri = f"http://{from_ip}:{port}"
            result.append({
                "name": name,
                "uri": uri,
                "address": from_ip,
                "port": port,
                "scheme": "http",
                "localAddresses": from_ip,
                "machineIdentifier": rid,
            })
    except Exception as e:
        logging.warning("GDM discover failed: %s", e)
    finally:
        sock.close()
    return result


def _probe_plex_at(host: str, port: int) -> dict | None:
    """
    Probe a single host:port to see if Plex is listening (GET /identity).
    Returns a server dict like GDM format, or None if not Plex / unreachable.
    """
    url = f"http://{host}:{port}/identity"
    try:
        resp = requests.get(url, timeout=2)
        if resp.status_code != 200:
            return None
        root = ET.fromstring(resp.text)
        if root.tag != "MediaContainer":
            return None
        rid = root.attrib.get("machineIdentifier", "")
        version = root.attrib.get("version", "")
        name = f"Plex Media Server ({host}:{port})"
        uri = f"http://{host}:{port}"
        return {
            "name": name,
            "uri": uri,
            "address": host,
            "port": str(port),
            "scheme": "http",
            "localAddresses": host,
            "machineIdentifier": rid,
        }
    except Exception:
        return None


def _discover_via_host_fallback(host_str: str, extra_ports: list[int] | None = None) -> list[dict]:
    """
    When GDM fails (e.g. in Docker, multicast is not forwarded), try the host
    the user used to reach PMDA (from Host header) and the Docker gateway (172.17.0.1).
    Tries common Plex ports 32400, 32401, 32402 and any extra_ports.
    """
    ports = [32400, 32401, 32402]
    if extra_ports:
        ports = list(dict.fromkeys(ports + list(extra_ports)))
    candidates: list[str] = []
    host = (host_str or "").strip()
    if host and not host.startswith("["):
        if ":" in host:
            host = host.rsplit(":", 1)[0]
        if host and host not in ("localhost", "127.0.0.1"):
            candidates.append(host)
    # From inside Docker, the host is often reachable via the bridge gateway
    candidates.append("172.17.0.1")
    result: list[dict] = []
    seen_uris: set[str] = set()
    for h in candidates:
        for port in ports:
            entry = _probe_plex_at(h, port)
            if entry and entry["uri"] not in seen_uris:
                result.append(entry)
                seen_uris.add(entry["uri"])
    return result


def _parse_port_from_url(url: str) -> int | None:
    """Extract port from http(s)://host:port if present."""
    if not url or not isinstance(url, str):
        return None
    url = url.strip()
    for prefix in ("https://", "http://"):
        if url.startswith(prefix):
            url = url[len(prefix) :].split("/")[0]
            break
    if ":" in url:
        try:
            return int(url.rsplit(":", 1)[1])
        except ValueError:
            return None
    return None


def _subnet_ips_24(ip_str: str) -> list[str]:
    """
    Given an IPv4 address, return a list of all /24 host IPs (x.y.z.1 .. x.y.z.254).
    Returns [] if the string is not a valid IPv4.
    """
    parts = ip_str.strip().split(".")
    if len(parts) != 4:
        return []
    try:
        a, b, c, _ = (int(p) for p in parts)
        if not all(0 <= x <= 255 for x in (a, b, c)):
            return []
        return [f"{a}.{b}.{c}.{i}" for i in range(1, 255)]
    except ValueError:
        return []


def _discover_via_subnet(client_ip: str, ports: list[int] | None = None) -> list[dict]:
    """
    Scan the client's /24 subnet for Plex (ports 32400, 32401, 32402 by default).
    Uses a thread pool and short timeouts to complete in a few seconds.
    """
    if not client_ip or client_ip.startswith("127.") or client_ip == "::1":
        return []
    ips = _subnet_ips_24(client_ip)
    if not ips:
        return []
    port_list = ports or [32400, 32401, 32402]
    seen_uris: set[str] = set()
    result: list[dict] = []
    max_workers = 48

    def probe(host: str, port: int) -> dict | None:
        return _probe_plex_at(host, port)

    with ThreadPoolExecutor(max_workers=max_workers) as ex:
        futures = {ex.submit(probe, ip, port): (ip, port) for ip in ips for port in port_list}
        for future in as_completed(futures, timeout=60):
            try:
                entry = future.result()
                if entry and entry["uri"] not in seen_uris:
                    result.append(entry)
                    seen_uris.add(entry["uri"])
            except Exception:
                pass
    return result


@app.get("/api/plex/client-ip")
def api_plex_client_ip():
    """
    Return the IP address of the client that is viewing the WebUI.
    Uses X-Forwarded-For (first hop) when behind a proxy, else request.remote_addr.
    Allows the frontend to request a discovery scan of that client's subnet (same LAN).
    """
    forwarded = request.headers.get("X-Forwarded-For")
    if forwarded:
        client_ip = forwarded.split(",")[0].strip()
    else:
        client_ip = request.remote_addr or ""
    return jsonify({"client_ip": client_ip})


@app.get("/api/plex/discover")
@app.post("/api/plex/discover")
def api_plex_discover():
    """
    Discover Plex Media Servers: GDM multicast first, then fallback to probing
    the host the user used to reach PMDA and Docker gateway 172.17.0.1 (ports 32400‚Äì32402).
    POST body may include:
      - "PLEX_HOST": "..." to add that URL's port to the probe list;
      - "client_ip": "x.y.z.w" to scan the client's /24 subnet (same LAN as the machine viewing the WebUI).
    No token required. Returns same shape as /api/plex/servers: { success, servers: [ { name, uri, ... } ] }.
    """
    servers = _gdm_discover_servers()
    extra_ports: list[int] = []
    client_ip: str | None = None
    try:
        data = request.get_json(silent=True) or {}
        if data.get("PLEX_HOST"):
            p = _parse_port_from_url(str(data.get("PLEX_HOST", "")))
            if p and p not in (32400, 32401, 32402):
                extra_ports.append(p)
        client_ip = (data.get("client_ip") or "").strip() or None
    except Exception:
        pass
    try:
        host_header = request.headers.get("Host") or request.host or ""
        fallback = _discover_via_host_fallback(host_header, extra_ports=extra_ports or None)
        seen_uris = {s["uri"] for s in servers}
        for s in fallback:
            if s["uri"] not in seen_uris:
                servers.append(s)
                seen_uris.add(s["uri"])
    except Exception as e:
        logging.debug("Discover host fallback failed: %s", e)
    if client_ip:
        try:
            subnet_servers = _discover_via_subnet(client_ip)
            seen_uris = {s["uri"] for s in servers}
            for s in subnet_servers:
                if s["uri"] not in seen_uris:
                    servers.append(s)
                    seen_uris.add(s["uri"])
        except Exception as e:
            logging.debug("Discover subnet failed: %s", e)
    return jsonify({"success": True, "servers": servers})


@app.get("/api/plex/database-paths")
def api_plex_database_paths():
    """
    Return common Plex database directory locations (by platform / image)
    so the wizard can suggest where to mount or point to.
    """
    return jsonify({"success": True, "paths": PLEX_DATABASE_PATH_HINTS})


@app.get("/api/plex/verify-db")
def api_plex_verify_db():
    """
    Verify that the current Plex DB file exists and is readable (e.g. open read-only and run a trivial query).
    Returns { success: true } or { success: false, message: "..." }.
    """
    if not Path(PLEX_DB_FILE).exists():
        return jsonify({"success": False, "message": "Plex database file not found at " + PLEX_DB_FILE}), 200
    try:
        con = sqlite3.connect(f"file:{PLEX_DB_FILE}?mode=ro", uri=True, timeout=5)
        con.execute("SELECT 1 FROM metadata_items LIMIT 1")
        con.close()
    except sqlite3.OperationalError as e:
        return jsonify({"success": False, "message": "Plex database not readable or invalid: " + str(e)}), 200
    except Exception as e:
        return jsonify({"success": False, "message": str(e)}), 200
    return jsonify({"success": True})


@app.post("/api/autodetect/libraries")
def api_autodetect_libraries():
    """Return list of Plex libraries (sections) for the wizard. Uses config or optional body { PLEX_HOST, PLEX_TOKEN }."""
    data = request.get_json(silent=True) or {}
    host = (data.get("PLEX_HOST") or "").strip() or PLEX_HOST
    token = (data.get("PLEX_TOKEN") or "").strip() or PLEX_TOKEN
    if not host or not token:
        return jsonify({"success": False, "libraries": [], "message": "PLEX_HOST and PLEX_TOKEN required"}), 400
    url = f"{host.rstrip('/')}/library/sections"
    try:
        resp = requests.get(url, headers={"X-Plex-Token": token}, timeout=10)
        resp.raise_for_status()
        root = ET.fromstring(resp.text)
        libraries = []
        for d in root.iter("Directory"):
            libraries.append({
                "id": d.attrib.get("key", ""),
                "name": d.attrib.get("title", ""),
                "type": d.attrib.get("type", ""),
            })
        return jsonify({"success": True, "libraries": libraries})
    except Exception as e:
        logging.warning("Autodetect libraries failed: %s", e)
        return jsonify({"success": False, "libraries": [], "message": str(e)})


@app.post("/api/autodetect/paths")
def api_autodetect_paths():
    """Discover PATH_MAP from Plex for given sections. Uses config or optional body { PLEX_HOST, PLEX_TOKEN, SECTION_IDS }."""
    data = request.get_json(silent=True) or {}
    host = (data.get("PLEX_HOST") or "").strip() or PLEX_HOST
    token = (data.get("PLEX_TOKEN") or "").strip() or PLEX_TOKEN
    section_ids = data.get("SECTION_IDS")
    if section_ids is None:
        section_ids = list(SECTION_IDS)
    elif isinstance(section_ids, str):
        section_ids = [int(x.strip()) for x in section_ids.split(",") if x.strip()]
    elif isinstance(section_ids, list):
        section_ids = [int(x) for x in section_ids]
    if not host or not token:
        return jsonify({"success": False, "paths": {}, "message": "PLEX_HOST and PLEX_TOKEN required"}), 400
    if not section_ids:
        return jsonify({"success": False, "paths": {}, "message": "No SECTION_IDS provided"}), 400
    paths = {}
    errors = []
    for sid in section_ids:
        try:
            part = _discover_path_map(host, token, sid)
            paths.update(part)
        except Exception as e:
            logging.warning("Autodetect paths failed for section %s: %s", sid, e)
            errors.append(f"Section {sid}: {e}")
    if not paths and errors:
        return jsonify({"success": False, "paths": {}, "message": "; ".join(errors)})
    return jsonify({"success": True, "paths": paths, "message": "; ".join(errors) if errors else None})


def _path_map_from_verify_body(raw):
    """Parse PATH_MAP from verify request: dict, or string with key=value lines or key:value pairs."""
    if isinstance(raw, dict):
        return {str(k): str(v) for k, v in raw.items()}
    if raw is None:
        return None
    s = str(raw).strip()
    if not s:
        return {}
    if s.startswith("{"):
        try:
            data = json.loads(s)
            return {str(k): str(v) for k, v in data.items()}
        except json.JSONDecodeError:
            pass
    out = {}
    for line in s.replace(",", "\n").splitlines():
        line = line.strip()
        if "=" in line:
            k, _, v = line.partition("=")
            if k.strip():
                out[k.strip()] = v.strip()
        elif ":" in line:
            k, _, v = line.partition(":")
            if k.strip():
                out[k.strip()] = v.strip()
    return out


@app.post("/api/paths/discover")
def api_paths_discover():
    """
    Discover actual container paths for each Plex library root by content matching: sample files
    from Plex DB and find which subdir of MUSIC_PARENT_PATH contains them. Body: PATH_MAP
    (required, e.g. from autodetect/paths), PLEX_DB_PATH?, MUSIC_PARENT_PATH?, CROSSCHECK_SAMPLES?.
    Returns { success, paths: discovered map, results: list of result dicts }.
    """
    data = request.get_json(silent=True) or {}
    path_map_raw = data.get("PATH_MAP")
    path_map = _path_map_from_verify_body(path_map_raw) if path_map_raw is not None else {}
    if not path_map:
        return jsonify({
            "success": False,
            "paths": {},
            "results": [],
            "message": "PATH_MAP is required and must not be empty",
        }), 400
    db_path = (data.get("PLEX_DB_PATH") or "").strip() or merged.get("PLEX_DB_PATH") or ""
    if not db_path:
        db_path = "/database"
    db_file = str(Path(db_path) / "com.plexapp.plugins.library.db")
    music_root = (data.get("MUSIC_PARENT_PATH") or "").strip() or merged.get("MUSIC_PARENT_PATH") or "/music"
    samples = max(1, int(data.get("CROSSCHECK_SAMPLES") or CROSSCHECK_SAMPLES or 15))
    if not Path(db_file).exists():
        return jsonify({
            "success": False,
            "paths": {},
            "results": [],
            "message": f"Plex DB not found: {db_file}",
        }), 400
    music_path = Path(music_root)
    if not music_path.exists() or not music_path.is_dir():
        return jsonify({
            "success": False,
            "paths": {},
            "results": [],
            "message": f"Music parent path not found or not a directory: {music_root}",
        }), 400
    out = _discover_bindings_by_content(path_map, db_file, music_root, samples)
    if out is None:
        return jsonify({
            "success": False,
            "paths": {},
            "results": [],
            "message": "Discover by content failed",
        }), 500
    discovered_map, results = out
    return jsonify({
        "success": True,
        "paths": discovered_map,
        "results": results,
    })


@app.post("/api/paths/discover-one")
def api_paths_discover_one():
    """
    Discover the actual container path for a single Plex root by content matching.
    Body: { plex_root (required), PLEX_DB_PATH?, MUSIC_PARENT_PATH?, CROSSCHECK_SAMPLES? }.
    Returns { success, host_root?, result } so the UI can show progress per mapping.
    """
    data = request.get_json(silent=True) or {}
    plex_root = (data.get("plex_root") or "").strip()
    if not plex_root:
        return jsonify({"success": False, "host_root": None, "result": None, "message": "plex_root is required"}), 400
    # Always use the effective Plex DB file currently configured in memory.
    # merged["PLEX_DB_PATH"] may be stale compared to PLEX_DB_FILE (which is
    # updated by the auto-discovery logic at startup), so we rely on PLEX_DB_FILE
    # here instead of reconstructing the path manually.
    db_file = str(getattr(sys.modules[__name__], "PLEX_DB_FILE", PLEX_DB_FILE))
    music_root = (data.get("MUSIC_PARENT_PATH") or "").strip() or merged.get("MUSIC_PARENT_PATH") or "/music"
    samples = max(1, int(data.get("CROSSCHECK_SAMPLES") or CROSSCHECK_SAMPLES or 15))
    out = _discover_one_binding(plex_root, db_file, music_root, samples)
    if out is None:
        return jsonify({
            "success": False,
            "host_root": None,
            "result": None,
            "message": "Discover failed (DB or music root invalid)",
        }), 500
    host_root, result = out
    success = host_root is not None
    return jsonify({
        "success": success,
        "host_root": host_root,
        "result": result,
        "message": (result.get("message") or "") if not success and isinstance(result, dict) else None,
    })


# Last path verification result (for GET /api/paths/verify/last)
_last_path_verify_result: list = []
_last_path_verify_at: float = 0.0


@app.get("/api/paths/verify/last")
def api_paths_verify_last():
    """Return the last path verification result and timestamp (from last POST /api/paths/verify)."""
    return jsonify({"results": _last_path_verify_result, "at": _last_path_verify_at if _last_path_verify_at else None})


@app.post("/api/paths/verify")
def api_paths_verify():
    """
    Verify PATH_MAP bindings by sampling tracks from the Plex DB and checking file existence.
    Body: { PATH_MAP?, PLEX_DB_PATH?, CROSSCHECK_SAMPLES? }. Returns list of { plex_root, host_root, status, samples_checked, message }.
    Does not modify config.
    """
    global _last_path_verify_result, _last_path_verify_at
    raw_json = request.get_json(silent=True)
    data = raw_json if isinstance(raw_json, dict) else {}
    if raw_json is None:
        logging.warning("Paths verify: request body is not valid JSON (Content-Type: %s)", request.content_type)
    logging.info(
        "Paths verify: request body keys=%s, PATH_MAP type=%s",
        list(data.keys()), type(data.get("PATH_MAP")).__name__ if "PATH_MAP" in data else "missing",
    )
    path_map_raw = data.get("PATH_MAP")
    path_map = _path_map_from_verify_body(path_map_raw)
    if path_map is None:
        path_map = dict(getattr(sys.modules[__name__], "PATH_MAP", {}))
        logging.info("Paths verify: no PATH_MAP in body, using server config ‚Üí %d entries", len(path_map))
    elif not path_map and path_map_raw is not None:
        path_map = dict(getattr(sys.modules[__name__], "PATH_MAP", {}))
        logging.info(
            "Paths verify: parsed PATH_MAP from body was empty (raw type=%s), using server config ‚Üí %d entries",
            type(path_map_raw).__name__, len(path_map),
        )
    if not path_map:
        logging.warning(
            "Paths verify: returning 400 ‚Äì PATH_MAP is empty (body keys=%s)",
            list(data.keys()),
        )
        return jsonify({"success": False, "results": [], "message": "PATH_MAP is empty"}), 400
    # Use the effective Plex DB file currently configured in memory (PLEX_DB_FILE)
    # instead of merged["PLEX_DB_PATH"], which may be out of sync with discovery.
    db_file = str(getattr(sys.modules[__name__], "PLEX_DB_FILE", PLEX_DB_FILE))
    samples = max(0, int(data.get("CROSSCHECK_SAMPLES") or CROSSCHECK_SAMPLES))
    if not Path(db_file).exists():
        logging.warning(
            "Paths verify: returning 400 ‚Äì Plex DB not found at %s (PLEX_DB_PATH=%s)",
            db_file, db_path,
        )
        return jsonify({"success": False, "results": [], "message": f"Plex DB not found: {db_file}"}), 400
    try:
        results = _run_path_verification(path_map, db_file, samples or CROSSCHECK_SAMPLES)
    except Exception as e:
        logging.warning("Paths verify exception: %s", e)
        return jsonify({"success": False, "results": [], "message": str(e) or "Path verification failed"}), 500
    if results is None:
        return jsonify({"success": False, "results": [], "message": "Path verification failed"}), 500
    _last_path_verify_result = list(results)
    _last_path_verify_at = time.time()
    has_failures = any(r.get("status") == "fail" for r in results)
    hint = None
    if has_failures:
        hint = (
            "Files were not found inside the container. Start Docker with a volume mount from your "
            "host music folder to the path shown above (e.g. -v /path/on/host/music:/music). "
            "Use the same path as 'Path to parent folder (music root)' in this step."
        )
    out = {"success": True, "results": results}
    if hint:
        out["hint"] = hint
    return jsonify(out)


@app.post("/api/openai/check")
def api_openai_check():
    """Test OpenAI API key; optional body { OPENAI_API_KEY } to test before saving."""
    data = request.get_json(silent=True) or {}
    key = (data.get("OPENAI_API_KEY") or "").strip() or OPENAI_API_KEY
    if not key:
        return jsonify({"success": False, "message": "OPENAI_API_KEY is required"}), 400
    
    # Validate key format (should start with sk-)
    if not key.startswith("sk-"):
        return jsonify({"success": False, "message": "Invalid API key format. OpenAI keys start with 'sk-'"}), 400
    
    try:
        client = OpenAI(api_key=key)
        # Try with max_completion_tokens first (newer API)
        try:
            response = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": "OK"}],
                max_completion_tokens=5,
            )
            # Verify we got a response
            if response.choices and len(response.choices) > 0:
                return jsonify({"success": True, "message": "OpenAI connection successful"})
        except Exception as e1:
            error_msg = str(e1)
            # If max_completion_tokens is not supported, try max_tokens
            if "max_completion_tokens" in error_msg or "unsupported_parameter" in error_msg.lower():
                try:
                    response = client.chat.completions.create(
                        model="gpt-4o-mini",
                        messages=[{"role": "user", "content": "OK"}],
                        max_tokens=5,
                    )
                    if response.choices and len(response.choices) > 0:
                        return jsonify({"success": True, "message": "OpenAI connection successful"})
                except Exception as e2:
                    # If both fail, check for authentication errors
                    error_msg2 = str(e2)
                    if "invalid_api_key" in error_msg2.lower() or "authentication" in error_msg2.lower() or "401" in error_msg2:
                        return jsonify({"success": False, "message": "Invalid API key. Please check your key and try again."}), 401
                    elif "insufficient_quota" in error_msg2.lower() or "quota" in error_msg2.lower():
                        return jsonify({"success": False, "message": "API key has insufficient quota. Please check your OpenAI account billing."}), 402
                    else:
                        logging.warning("OpenAI check failed with max_tokens: %s", error_msg2)
                        return jsonify({"success": False, "message": f"OpenAI API error: {error_msg2}"}), 500
            else:
                # Other error (auth, quota, etc.)
                if "invalid_api_key" in error_msg.lower() or "authentication" in error_msg.lower() or "401" in error_msg:
                    return jsonify({"success": False, "message": "Invalid API key. Please check your key and try again."}), 401
                elif "insufficient_quota" in error_msg.lower() or "quota" in error_msg.lower():
                    return jsonify({"success": False, "message": "API key has insufficient quota. Please check your OpenAI account billing."}), 402
                else:
                    logging.warning("OpenAI check failed: %s", error_msg)
                    return jsonify({"success": False, "message": f"OpenAI API error: {error_msg}"}), 500
    except Exception as e:
        error_msg = str(e)
        logging.error("OpenAI check exception: %s", error_msg)
        # Catch network errors, etc.
        if "connection" in error_msg.lower() or "timeout" in error_msg.lower():
            return jsonify({"success": False, "message": "Connection to OpenAI API failed. Please check your internet connection."}), 503
        return jsonify({"success": False, "message": f"Error: {error_msg}"}), 500


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ OpenAI OAuth (ChatGPT / Codex) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

_OPENAI_OAUTH_ISSUER = "https://auth.openai.com"
# This is the public OAuth client_id used by OpenAI Codex "Sign in with ChatGPT".
# It enables a device-code flow that works well for server-hosted PMDA (no localhost callback).
_OPENAI_OAUTH_CODEX_CLIENT_ID = "app_EMoamEEZ73f0CkXaXp7hrann"
_openai_oauth_lock = threading.Lock()
_openai_oauth_device_sessions: dict[str, dict] = {}


def _openai_oauth_device_request_user_code(
    issuer: str = _OPENAI_OAUTH_ISSUER,
    client_id: str = _OPENAI_OAUTH_CODEX_CLIENT_ID,
) -> tuple[str, str, int]:
    """
    Start the OpenAI device-code flow.
    Returns (device_auth_id, user_code, interval_sec).
    """
    url = f"{issuer.rstrip('/')}/api/accounts/deviceauth/usercode"
    resp = requests.post(url, json={"client_id": client_id}, timeout=15)
    if resp.status_code == 404:
        raise RuntimeError(
            "OpenAI device-code login is not enabled on this issuer. Use an API key instead."
        )
    if not resp.ok:
        raise RuntimeError(f"OpenAI device-code start failed (status {resp.status_code})")
    data = resp.json() if resp.content else {}
    device_auth_id = str(data.get("device_auth_id") or "").strip()
    user_code = str(data.get("user_code") or data.get("usercode") or "").strip()
    interval_raw = data.get("interval") or 5
    try:
        interval = int(str(interval_raw).strip())
    except Exception:
        interval = 5
    interval = max(1, min(interval, 30))
    if not device_auth_id or not user_code:
        raise RuntimeError("OpenAI device-code response was missing required fields")
    return device_auth_id, user_code, interval


def _openai_oauth_device_poll_for_code(
    device_auth_id: str,
    user_code: str,
    issuer: str = _OPENAI_OAUTH_ISSUER,
) -> dict | None:
    """
    Poll for device authorization completion.
    Returns code payload on success, None when still pending.
    """
    url = f"{issuer.rstrip('/')}/api/accounts/deviceauth/token"
    resp = requests.post(
        url,
        json={"device_auth_id": device_auth_id, "user_code": user_code},
        timeout=15,
    )
    if resp.status_code in (403, 404):
        return None
    if not resp.ok:
        raise RuntimeError(f"OpenAI device-code poll failed (status {resp.status_code})")
    return resp.json() if resp.content else {}


def _openai_oauth_exchange_authorization_code_for_tokens(
    authorization_code: str,
    code_verifier: str,
    issuer: str = _OPENAI_OAUTH_ISSUER,
    client_id: str = _OPENAI_OAUTH_CODEX_CLIENT_ID,
) -> dict:
    """Exchange authorization_code for (id_token, access_token, refresh_token)."""
    token_url = f"{issuer.rstrip('/')}/oauth/token"
    redirect_uri = f"{issuer.rstrip('/')}/deviceauth/callback"
    resp = requests.post(
        token_url,
        headers={"Content-Type": "application/x-www-form-urlencoded"},
        data={
            "grant_type": "authorization_code",
            "code": authorization_code,
            "redirect_uri": redirect_uri,
            "client_id": client_id,
            "code_verifier": code_verifier,
        },
        timeout=20,
    )
    if not resp.ok:
        raise RuntimeError(f"OpenAI token exchange failed (status {resp.status_code})")
    data = resp.json() if resp.content else {}
    if not str(data.get("id_token") or "").strip():
        raise RuntimeError("OpenAI token exchange returned no id_token")
    return data


def _openai_oauth_token_exchange_for_api_key(
    id_token: str,
    issuer: str = _OPENAI_OAUTH_ISSUER,
    client_id: str = _OPENAI_OAUTH_CODEX_CLIENT_ID,
) -> str:
    """Token exchange: id_token -> requested_token=openai-api-key (returns an API key-like token)."""
    token_url = f"{issuer.rstrip('/')}/oauth/token"
    resp = requests.post(
        token_url,
        headers={"Content-Type": "application/x-www-form-urlencoded"},
        data={
            "grant_type": "urn:ietf:params:oauth:grant-type:token-exchange",
            "client_id": client_id,
            "requested_token": "openai-api-key",
            "subject_token": id_token,
            "subject_token_type": "urn:ietf:params:oauth:token-type:id_token",
        },
        timeout=20,
    )
    if not resp.ok:
        detail = ""
        try:
            payload = resp.json()
            if isinstance(payload, dict):
                # Prefer known fields if present; keep the rest short to avoid log spam.
                msg = (
                    payload.get("error_description")
                    or payload.get("error")
                    or payload.get("message")
                )
                if msg:
                    detail = f": {str(msg).strip()[:300]}"
                else:
                    detail = f": {json.dumps(payload, ensure_ascii=True)[:300]}"
            else:
                detail = f": {str(payload)[:300]}"
        except Exception:
            txt = (resp.text or "").strip()
            if txt:
                detail = f": {txt[:300]}"
        raise RuntimeError(f"OpenAI API key exchange failed (status {resp.status_code}){detail}")
    data = resp.json() if resp.content else {}
    key = str(data.get("access_token") or "").strip()
    if not key:
        raise RuntimeError("OpenAI API key exchange returned empty access_token")
    return key


@app.post("/api/openai/oauth/device/start")
def api_openai_oauth_device_start():
    """
    Start OpenAI OAuth device-code flow (Codex/ChatGPT login).
    Returns { ok, session_id, verification_url, user_code, interval }.
    """
    session_id = uuid.uuid4().hex
    try:
        device_auth_id, user_code, interval = _openai_oauth_device_request_user_code()
        with _openai_oauth_lock:
            _openai_oauth_device_sessions[session_id] = {
                "status": "pending",
                "created_at": time.time(),
                "last_poll_at": 0.0,
                "issuer": _OPENAI_OAUTH_ISSUER,
                "client_id": _OPENAI_OAUTH_CODEX_CLIENT_ID,
                "device_auth_id": device_auth_id,
                "user_code": user_code,
                "interval": interval,
                "error": None,
            }
        return jsonify({
            "ok": True,
            "session_id": session_id,
            "verification_url": f"{_OPENAI_OAUTH_ISSUER.rstrip('/')}/codex/device",
            "user_code": user_code,
            "interval": interval,
            "message": "Enter the code in the OpenAI page, then return here.",
            "warning": "This may generate an OpenAI API key for your account. ChatGPT subscription may not include API billing.",
        })
    except Exception as e:
        return jsonify({"ok": False, "message": str(e) or "OAuth start failed"}), 500


@app.post("/api/openai/oauth/device/poll")
def api_openai_oauth_device_poll():
    """
    Poll OpenAI device-code session and, when authorized, persist the resulting OPENAI_API_KEY.
    Body: { session_id }
    """
    data = request.get_json(silent=True) or {}
    session_id = str(data.get("session_id") or "").strip()
    if not session_id:
        return jsonify({"status": "error", "message": "session_id is required"}), 400

    with _openai_oauth_lock:
        sess = dict(_openai_oauth_device_sessions.get(session_id) or {})
    if not sess:
        return jsonify({"status": "error", "message": "Unknown session_id"}), 404

    if sess.get("status") == "completed":
        return jsonify({"status": "completed", "message": "Already connected"})
    if sess.get("status") == "error":
        msg = str(sess.get("error") or "OAuth failed").strip()
        status = 400
        m = re.search(r"status\\s+(\\d{3})", msg)
        if m:
            try:
                status = int(m.group(1))
            except Exception:
                status = 400
        return jsonify({"status": "error", "message": msg}), status

    now = time.time()
    created_at = float(sess.get("created_at") or 0.0)
    if created_at and now - created_at > 15 * 60:
        with _openai_oauth_lock:
            _openai_oauth_device_sessions[session_id]["status"] = "error"
            _openai_oauth_device_sessions[session_id]["error"] = "Device auth timed out after 15 minutes"
        return jsonify({"status": "error", "message": "Device auth timed out after 15 minutes"}), 408

    interval = int(sess.get("interval") or 5)
    last_poll_at = float(sess.get("last_poll_at") or 0.0)
    retry_after = max(0, int((last_poll_at + interval) - now))
    if retry_after > 0:
        return jsonify({"status": "pending", "message": "Waiting for authorization‚Ä¶", "retry_after": retry_after})

    try:
        with _openai_oauth_lock:
            _openai_oauth_device_sessions[session_id]["last_poll_at"] = now
        payload = _openai_oauth_device_poll_for_code(
            str(sess.get("device_auth_id") or ""),
            str(sess.get("user_code") or ""),
            issuer=str(sess.get("issuer") or _OPENAI_OAUTH_ISSUER),
        )
        if payload is None:
            return jsonify({"status": "pending", "message": "Waiting for authorization‚Ä¶"})

        authorization_code = str(payload.get("authorization_code") or "").strip()
        code_verifier = str(payload.get("code_verifier") or "").strip()
        if not authorization_code or not code_verifier:
            raise RuntimeError("Device auth response missing authorization_code/code_verifier")

        tokens = _openai_oauth_exchange_authorization_code_for_tokens(
            authorization_code,
            code_verifier,
            issuer=str(sess.get("issuer") or _OPENAI_OAUTH_ISSUER),
            client_id=str(sess.get("client_id") or _OPENAI_OAUTH_CODEX_CLIENT_ID),
        )
        id_token = str(tokens.get("id_token") or "").strip()
        refresh_token = str(tokens.get("refresh_token") or "").strip()

        api_key = None
        api_key_error = None
        try:
            api_key = _openai_oauth_token_exchange_for_api_key(
                id_token,
                issuer=str(sess.get("issuer") or _OPENAI_OAUTH_ISSUER),
                client_id=str(sess.get("client_id") or _OPENAI_OAUTH_CODEX_CLIENT_ID),
            )
        except Exception as e:
            api_key_error = str(e) or "OpenAI API key exchange failed"
            logging.warning(
                "OpenAI OAuth connected but API key could not be generated (session=%s): %s",
                session_id,
                api_key_error,
            )

        # Persist: save OAuth token(s) and the API key (when available) into settings.db.
        init_settings_db()
        con = sqlite3.connect(str(SETTINGS_DB_FILE))
        cur = con.cursor()
        if refresh_token:
            cur.execute(
                "INSERT OR REPLACE INTO settings(key, value) VALUES(?, ?)",
                ("OPENAI_OAUTH_REFRESH_TOKEN", refresh_token),
            )
        if api_key:
            cur.execute(
                "INSERT OR REPLACE INTO settings(key, value) VALUES(?, ?)",
                ("AI_PROVIDER", "openai"),
            )
            cur.execute(
                "INSERT OR REPLACE INTO settings(key, value) VALUES(?, ?)",
                ("OPENAI_API_KEY", api_key),
            )
        con.commit()
        con.close()

        # Apply immediately in memory (re-init OpenAI client) when we got a key.
        if api_key:
            _apply_settings_in_memory({"AI_PROVIDER": "openai", "OPENAI_API_KEY": api_key})

        with _openai_oauth_lock:
            _openai_oauth_device_sessions[session_id]["status"] = "completed"
            _openai_oauth_device_sessions[session_id]["error"] = None

        if api_key:
            logging.info(
                "OpenAI OAuth completed: OPENAI_API_KEY stored in settings.db (session=%s)",
                session_id,
            )
            return jsonify({"status": "completed", "api_key_saved": True, "message": "Connected. OpenAI key saved."})

        msg = (
            "Connected, but PMDA could not generate an OpenAI API key for this account. "
            "This usually means your OpenAI API Platform access/billing is not enabled. "
            "You can paste an API key manually or enable API billing on platform.openai.com and retry."
        )
        # Keep the low-level error for power users without making the UI noisy.
        if api_key_error:
            msg = f"{msg} ({api_key_error})"
        return jsonify({"status": "completed", "api_key_saved": False, "message": msg})
    except Exception as e:
        msg = str(e) or "OAuth failed"
        with _openai_oauth_lock:
            _openai_oauth_device_sessions[session_id]["status"] = "error"
            _openai_oauth_device_sessions[session_id]["error"] = msg
        logging.warning("OpenAI OAuth failed (session=%s): %s", session_id, msg)
        status = 500
        m = re.search(r"status\\s+(\\d{3})", msg)
        if m:
            try:
                status = int(m.group(1))
            except Exception:
                status = 500
        # surface auth problems as 401 so UI can display a clear error
        if "api key exchange" in msg.lower() and status == 500:
            status = 401
        return jsonify({"status": "error", "message": msg}), status


@app.get("/api/musicbrainz/test")
@app.post("/api/musicbrainz/test")
def api_musicbrainz_test():
    """Test MusicBrainz connectivity and rate limiting.
    Returns success status and any error messages.
    Accepts USE_MUSICBRAINZ in request body (POST) to allow testing before config is saved."""
    # Check if USE_MUSICBRAINZ is provided in request body (for POST) or use global config
    data = request.get_json(silent=True) or {}
    use_mb = data.get("USE_MUSICBRAINZ")
    if use_mb is not None:
        # Use value from request body if provided
        use_mb_enabled = bool(use_mb)
    else:
        # Fall back to global config
        use_mb_enabled = USE_MUSICBRAINZ
    
    if not use_mb_enabled:
        return jsonify({"success": False, "message": "MusicBrainz is disabled. Enable it first."}), 400
    
    try:
        # Test with a well-known release-group ID
        test_mbid = "9162580e-5df4-32de-80cc-f45a8d8a9b1d"  # The Beatles - Abbey Road
        result = musicbrainzngs.get_release_group_by_id(test_mbid, includes=[])
        if result and result.get("release-group"):
            return jsonify({
                "success": True,
                "message": "MusicBrainz connection successful",
                "tested_mbid": test_mbid
            })
        else:
            return jsonify({"success": False, "message": "MusicBrainz returned empty response"}), 500
    except musicbrainzngs.WebServiceError as e:
        error_msg = str(e)
        logging.warning("MusicBrainz WebServiceError: %s", error_msg)
        # Check for specific error codes
        if hasattr(e, 'code'):
            error_code = str(e.code)
            if error_code == "503" or "rate" in error_msg.lower():
                return jsonify({
                    "success": False,
                    "message": "MusicBrainz rate limited. Please wait a moment and try again. Rate limit: 1 request per second."
                }), 503
            elif error_code == "404" or "404" in error_msg:
                return jsonify({
                    "success": False,
                    "message": f"MusicBrainz returned 404 (Not Found). This may be a temporary issue. Error: {error_msg}"
                }), 404
            elif error_code == "503":
                return jsonify({
                    "success": False,
                    "message": "MusicBrainz service temporarily unavailable (503). Please try again later."
                }), 503
        # Fallback to message-based detection
        if "503" in error_msg or "rate" in error_msg.lower() or "service unavailable" in error_msg.lower():
            return jsonify({
                "success": False,
                "message": "MusicBrainz rate limited or service unavailable. Please wait a moment and try again. Rate limit: 1 request per second."
            }), 503
        elif "404" in error_msg or "not found" in error_msg.lower():
            return jsonify({
                "success": False,
                "message": f"MusicBrainz API returned 404. This may be a temporary issue or network problem. Error details: {error_msg}"
            }), 404
        else:
            logging.warning("MusicBrainz test failed: %s", error_msg)
            return jsonify({
                "success": False,
                "message": f"MusicBrainz API error: {error_msg}"
            }), 500
    except Exception as e:
        error_msg = str(e)
        logging.error("MusicBrainz test exception: %s", error_msg)
        if "connection" in error_msg.lower() or "timeout" in error_msg.lower():
            return jsonify({
                "success": False,
                "message": "Connection to MusicBrainz failed. Please check your internet connection."
            }), 503
        return jsonify({
            "success": False,
            "message": f"Error: {error_msg}"
        }), 500




@app.get("/api/openai/models")
@app.post("/api/openai/models")
def api_openai_models():
    """Return only OpenAI model IDs that are compatible with PMDA (Chat Completions, parseable output).
    Uses a curated list so we never show models that fail PMDA's parseable-output requirements."""
    from flask import g
    data = getattr(g, 'ai_models_request_data', None) or request.get_json(silent=True) or {}
    key = (data.get("OPENAI_API_KEY") or "").strip() or OPENAI_API_KEY

    if not key:
        return jsonify({"error": "OPENAI_API_KEY is required"}), 400

    if not key.startswith("sk-"):
        return jsonify({"error": "Invalid API key format. OpenAI keys start with 'sk-'"}), 400

    try:
        client = OpenAI(api_key=key)
        # Return only curated compatible models (no API list fetch ‚Äì we never show incompatible models)
        available_models = list(OPENAI_COMPATIBLE_MODELS)
        logging.info("Returning %d compatible OpenAI models for Settings", len(available_models))
        return jsonify(available_models)
    except Exception as e:
        error_msg = str(e)
        logging.error("OpenAI client init for models list: %s", error_msg)
        if "invalid_api_key" in error_msg.lower() or "authentication" in error_msg.lower() or "401" in error_msg or "unauthorized" in error_msg.lower():
            return jsonify({"error": "Invalid API key. Please check your key and try again."}), 401
        return jsonify({"error": f"Failed to initialize OpenAI client: {error_msg}"}), 500


# Curated list of Anthropic Claude models compatible with Messages API and text output (PMDA format).
ANTHROPIC_COMPATIBLE_MODELS = [
    "claude-sonnet-4-5",
    "claude-haiku-4-5",
    "claude-opus-4-5",
    "claude-3-5-sonnet-20241022",
    "claude-3-5-sonnet-20240620",
    "claude-3-opus-20240229",
    "claude-3-sonnet-20240229",
    "claude-3-haiku-20240307",
]


@app.post("/api/anthropic/models")
def api_anthropic_models():
    """Return only Anthropic model IDs compatible with PMDA (Messages API, parseable output)."""
    from flask import g
    data = getattr(g, 'ai_models_request_data', None) or request.get_json(silent=True) or {}
    key = (data.get("ANTHROPIC_API_KEY") or "").strip() or ANTHROPIC_API_KEY

    if not key:
        return jsonify({"error": "ANTHROPIC_API_KEY is required"}), 400

    if not anthropic:
        return jsonify({"error": "Anthropic SDK not installed. Please install anthropic package."}), 500

    try:
        client = anthropic.Anthropic(api_key=key)
        try:
            client.messages.create(
                model="claude-3-5-sonnet-20241022",
                max_tokens=1,
                messages=[{"role": "user", "content": "test"}],
            )
        except anthropic.APIError as e:
            if e.status_code == 401:
                return jsonify({"error": "Invalid API key. Please check your key and try again."}), 401
            elif e.status_code == 402:
                return jsonify({"error": "API key has insufficient quota. Please check your Anthropic account billing."}), 402

        available_models = list(ANTHROPIC_COMPATIBLE_MODELS)
        logging.info("Returning %d compatible Anthropic models for Settings", len(available_models))
        return jsonify(available_models)
    except Exception as e:
        error_msg = str(e)
        logging.error("Failed to fetch Anthropic models: %s", error_msg)
        if "invalid_api_key" in error_msg.lower() or "authentication" in error_msg.lower() or "401" in error_msg or "unauthorized" in error_msg.lower():
            return jsonify({"error": "Invalid API key. Please check your key and try again."}), 401
        elif "connection" in error_msg.lower() or "timeout" in error_msg.lower() or "network" in error_msg.lower():
            return jsonify({"error": "Connection to Anthropic API failed. Please check your internet connection."}), 503
        return jsonify({"error": f"Failed to fetch models: {error_msg}"}), 500


# Curated list of Google Gemini model IDs compatible with generateContent and text output (PMDA format).
GOOGLE_COMPATIBLE_MODELS = [
    "gemini-3-pro-preview",
    "gemini-3-flash-preview",
    "gemini-2.5-flash",
    "gemini-2.5-pro",
    "gemini-2.5-flash-lite",
    "gemini-2.0-flash",
    "gemini-2.0-flash-lite",
    "gemini-1.5-pro",
    "gemini-1.5-flash",
    "gemini-1.5-flash-8b",
    "gemini-pro",
]


@app.post("/api/google/models")
def api_google_models():
    """Return only Google Gemini model IDs compatible with PMDA (generateContent, text output)."""
    from flask import g
    data = getattr(g, 'ai_models_request_data', None) or request.get_json(silent=True) or {}
    key = (data.get("GOOGLE_API_KEY") or "").strip() or GOOGLE_API_KEY

    if not key:
        return jsonify({"error": "GOOGLE_API_KEY is required"}), 400

    if not genai:
        return jsonify({"error": "Google GenAI SDK not installed. Please install google-genai package."}), 500

    try:
        try:
            client = genai.Client(api_key=key)
            models_list = client.models.list()
        except Exception as e:
            error_msg = str(e)
            if "invalid_api_key" in error_msg.lower() or "authentication" in error_msg.lower() or "401" in error_msg or "unauthorized" in error_msg.lower():
                return jsonify({"error": "Invalid API key. Please check your key and try again."}), 401
            raise

        compatible_set = set(GOOGLE_COMPATIBLE_MODELS)
        available_models = []
        for model in models_list:
            model_name = getattr(model, "name", None) or ""
            model_id = model_name.split("/")[-1] if "/" in model_name else model_name
            # google-genai exposes "supported_actions"; older SDKs used "supported_generation_methods".
            supported = getattr(model, "supported_actions", None)
            if supported is None:
                supported = getattr(model, "supported_generation_methods", [])
            if model_id in compatible_set and (not supported or "generateContent" in str(supported)):
                if model_id not in available_models:
                    available_models.append(model_id)

        if not available_models:
            available_models = list(GOOGLE_COMPATIBLE_MODELS)

        def model_sort_key(name: str) -> tuple:
            if "gemini-3" in name:
                tier = 0
            elif "gemini-2.5" in name:
                tier = 1
            elif "gemini-2.0" in name:
                tier = 2
            elif "gemini-1.5" in name:
                tier = 3
            elif "gemini-pro" in name:
                tier = 4
            else:
                tier = 5
            return (tier, name)

        available_models.sort(key=model_sort_key)
        logging.info("Returning %d compatible Google Gemini models for Settings", len(available_models))
        return jsonify(available_models)
    except Exception as e:
        error_msg = str(e)
        logging.error("Failed to fetch Google models: %s", error_msg)
        if "invalid_api_key" in error_msg.lower() or "authentication" in error_msg.lower() or "401" in error_msg or "unauthorized" in error_msg.lower():
            return jsonify({"error": "Invalid API key. Please check your key and try again."}), 401
        elif "connection" in error_msg.lower() or "timeout" in error_msg.lower() or "network" in error_msg.lower():
            return jsonify({"error": "Connection to Google API failed. Please check your internet connection."}), 503
        return jsonify({"error": f"Failed to fetch models: {error_msg}"}), 500


@app.post("/api/ollama/models")
def api_ollama_models():
    """Return list of Ollama model IDs available at the provided URL."""
    from flask import g
    data = getattr(g, 'ai_models_request_data', None) or request.get_json(silent=True) or {}
    url = (data.get("OLLAMA_URL") or "").strip() or OLLAMA_URL
    
    if not url:
        return jsonify({"error": "OLLAMA_URL is required"}), 400
    
    # Normalize URL (remove trailing slash)
    url = url.rstrip("/")
    
    try:
        # Test connection and fetch models
        models_endpoint = f"{url}/api/tags"
        response = requests.get(models_endpoint, timeout=10)
        
        if response.status_code == 404:
            return jsonify({"error": "Ollama API not found at this URL. Make sure Ollama is running and the URL is correct."}), 404
        elif response.status_code != 200:
            return jsonify({"error": f"Failed to connect to Ollama: HTTP {response.status_code}"}), response.status_code
        
        models_data = response.json()
        available_models = []
        
        if "models" in models_data:
            for model in models_data["models"]:
                model_name = model.get("name", "")
                if model_name:
                    available_models.append(model_name)
        
        if not available_models:
            logging.warning("Ollama returned no models")
            return jsonify({"error": "No models available at this Ollama instance. Please pull some models first."}), 404
        
        # Sort models alphabetically
        available_models.sort()
        logging.info("Fetched %d Ollama models from %s", len(available_models), url)
        return jsonify(available_models)
        
    except requests.exceptions.Timeout:
        return jsonify({"error": "Connection to Ollama timed out. Make sure Ollama is running and accessible."}), 503
    except requests.exceptions.ConnectionError:
        return jsonify({"error": "Failed to connect to Ollama. Make sure Ollama is running and the URL is correct."}), 503
    except Exception as e:
        error_msg = str(e)
        logging.error("Failed to fetch Ollama models: %s", error_msg)
        return jsonify({"error": f"Failed to fetch models: {error_msg}"}), 500


@app.post("/api/ai/models")
def api_ai_models():
    """Route to the appropriate AI provider's models endpoint based on AI_PROVIDER."""
    data = request.get_json(silent=True) or {}
    provider = (data.get("AI_PROVIDER") or "").strip().lower() or AI_PROVIDER.lower()
    
    # Store data in request context so sub-functions can access it
    # (Flask request body can only be read once)
    from flask import g
    g.ai_models_request_data = data
    
    if provider == "openai":
        return api_openai_models()
    elif provider == "anthropic":
        return api_anthropic_models()
    elif provider == "google":
        return api_google_models()
    elif provider == "ollama":
        return api_ollama_models()
    else:
        return jsonify({"error": f"Unknown AI provider: {provider}"}), 400


def _has_settings_in_db() -> bool:
    """Check if settings exist in the configuration database (wizard was completed)."""
    try:
        if not SETTINGS_DB_FILE.exists():
            return False
        con = sqlite3.connect(str(SETTINGS_DB_FILE))
        cur = con.cursor()
        cur.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='settings'")
        row = cur.fetchone()
        if not row:
            con.close()
            return False
        cur.execute("SELECT COUNT(*) FROM settings")
        count = cur.fetchone()[0]
        con.close()
        return count > 0
    except Exception:
        return False


def _get_config_from_db(key: str, default_value=None):
    """Get a config value from SQLite settings table, with fallback to default.

    Configuration now lives in SETTINGS_DB_FILE; this helper no longer reads from state.db.
    """
    db_path = SETTINGS_DB_FILE
    try:
        if db_path.exists():
            con = sqlite3.connect(str(db_path), timeout=5)
            cur = con.cursor()
            cur.execute("SELECT value FROM settings WHERE key = ?", (key,))
            row = cur.fetchone()
            con.close()
            if row and row[0] is not None:
                return row[0]
    except Exception:
        pass
    return default_value


def _reload_auto_move_from_db():
    """Reload AUTO_MOVE_DUPES from SQLite so the current scan uses the value saved in Settings (UI)."""
    global AUTO_MOVE_DUPES
    val = _get_config_from_db("AUTO_MOVE_DUPES")
    if val is not None:
        AUTO_MOVE_DUPES = bool(_parse_bool(val))
        logging.debug("AUTO_MOVE_DUPES reloaded from DB: %s", AUTO_MOVE_DUPES)


def _reload_musicbrainz_settings_from_db():
    """Reload MusicBrainz-related settings from SQLite so scans use the latest UI values without restart."""
    global USE_MUSICBRAINZ
    global MB_SEARCH_ALBUM_TIMEOUT_SEC, MB_CANDIDATE_FETCH_LIMIT, MB_TRACKLIST_FETCH_LIMIT, MB_FAST_FALLBACK_MODE
    global MB_QUEUE_ENABLED
    global _mb_queue

    mod = sys.modules[__name__]
    changed: dict[str, object] = {}

    def _reload_bool(key: str, current: bool) -> tuple[bool, bool]:
        raw = _get_config_from_db(key)
        if raw is None:
            return current, False
        new_val = bool(_parse_bool(raw))
        return new_val, new_val != current

    def _reload_int(key: str, current: int, *, min_v: int, max_v: int) -> tuple[int, bool]:
        raw = _get_config_from_db(key)
        if raw is None:
            return current, False
        try:
            new_val = int(str(raw).strip())
        except Exception:
            return current, False
        new_val = max(min_v, min(max_v, new_val))
        return new_val, new_val != current

    use_mb, use_mb_changed = _reload_bool("USE_MUSICBRAINZ", bool(USE_MUSICBRAINZ))
    if use_mb_changed:
        USE_MUSICBRAINZ = bool(use_mb)
        mod.merged["USE_MUSICBRAINZ"] = USE_MUSICBRAINZ
        changed["USE_MUSICBRAINZ"] = USE_MUSICBRAINZ

    mb_queue_enabled, mb_queue_changed = _reload_bool("MB_QUEUE_ENABLED", bool(MB_QUEUE_ENABLED))
    if mb_queue_changed:
        MB_QUEUE_ENABLED = bool(mb_queue_enabled)
        mod.merged["MB_QUEUE_ENABLED"] = MB_QUEUE_ENABLED
        changed["MB_QUEUE_ENABLED"] = MB_QUEUE_ENABLED
        # Update live queue instance if already created.
        try:
            if _mb_queue is not None:
                _mb_queue.enabled = bool(MB_QUEUE_ENABLED and USE_MUSICBRAINZ)
        except Exception:
            pass

    timeout_sec, timeout_changed = _reload_int(
        "MB_SEARCH_ALBUM_TIMEOUT_SEC",
        int(MB_SEARCH_ALBUM_TIMEOUT_SEC),
        min_v=10,
        max_v=300,
    )
    if timeout_changed:
        MB_SEARCH_ALBUM_TIMEOUT_SEC = int(timeout_sec)
        mod.merged["MB_SEARCH_ALBUM_TIMEOUT_SEC"] = MB_SEARCH_ALBUM_TIMEOUT_SEC
        changed["MB_SEARCH_ALBUM_TIMEOUT_SEC"] = MB_SEARCH_ALBUM_TIMEOUT_SEC

    cand_limit, cand_changed = _reload_int(
        "MB_CANDIDATE_FETCH_LIMIT",
        int(MB_CANDIDATE_FETCH_LIMIT),
        min_v=1,
        max_v=20,
    )
    if cand_changed:
        MB_CANDIDATE_FETCH_LIMIT = int(cand_limit)
        mod.merged["MB_CANDIDATE_FETCH_LIMIT"] = MB_CANDIDATE_FETCH_LIMIT
        changed["MB_CANDIDATE_FETCH_LIMIT"] = MB_CANDIDATE_FETCH_LIMIT

    track_limit, track_changed = _reload_int(
        "MB_TRACKLIST_FETCH_LIMIT",
        int(MB_TRACKLIST_FETCH_LIMIT),
        min_v=0,
        max_v=20,
    )
    if track_changed:
        MB_TRACKLIST_FETCH_LIMIT = int(track_limit)
        mod.merged["MB_TRACKLIST_FETCH_LIMIT"] = MB_TRACKLIST_FETCH_LIMIT
        changed["MB_TRACKLIST_FETCH_LIMIT"] = MB_TRACKLIST_FETCH_LIMIT

    fast_fallback, fast_changed = _reload_bool("MB_FAST_FALLBACK_MODE", bool(MB_FAST_FALLBACK_MODE))
    if fast_changed:
        MB_FAST_FALLBACK_MODE = bool(fast_fallback)
        mod.merged["MB_FAST_FALLBACK_MODE"] = MB_FAST_FALLBACK_MODE
        changed["MB_FAST_FALLBACK_MODE"] = MB_FAST_FALLBACK_MODE

    if changed:
        summary = ", ".join([f"{k}={v}" for k, v in changed.items()])
        logging.info("MusicBrainz settings reloaded from SQLite: %s", summary)


def _reload_section_ids_from_db():
    """Reload SECTION_IDS from SQLite so library APIs use latest saved selection."""
    global SECTION_IDS, SECTION_ID
    section_ids_str = _get_config_from_db("SECTION_IDS")
    if not section_ids_str:
        return
    raw = str(section_ids_str).strip()
    if not raw:
        return
    try:
        if raw.startswith("["):
            SECTION_IDS = [int(x) for x in json.loads(raw)]
        else:
            SECTION_IDS = [int(x.strip()) for x in raw.split(",") if x.strip()]
        SECTION_ID = SECTION_IDS[0] if SECTION_IDS else 0
    except Exception:
        pass


def _reload_path_map_from_db():
    """Reload PATH_MAP from SQLite so scan/dedupe use latest saved bindings (e.g. after Detect & verify)."""
    global PATH_MAP
    path_map_val = _get_config_from_db("PATH_MAP")
    if path_map_val is None:
        return
    parsed = _parse_path_map(path_map_val)
    if parsed:
        PATH_MAP = parsed
        logging.info("PATH_MAP reloaded from SQLite at scan start (%d entries)", len(PATH_MAP))


def _reload_library_mode_and_files_roots_from_db():
    """
    Reload LIBRARY_MODE and FILES_ROOTS from SQLite so each scan uses the
    latest source selection saved in Settings.
    """
    global LIBRARY_MODE, FILES_ROOTS
    mod = sys.modules[__name__]

    mode_raw = _get_config_from_db("LIBRARY_MODE")
    if mode_raw is not None:
        mode = str(mode_raw).strip().lower()
        if mode == "files":
            LIBRARY_MODE = "files"
            mod.merged["LIBRARY_MODE"] = "files"
        else:
            LIBRARY_MODE = "files"
            mod.merged["LIBRARY_MODE"] = "files"

    roots_raw = _get_config_from_db("FILES_ROOTS")
    if roots_raw is not None:
        FILES_ROOTS = _parse_files_roots(roots_raw)
        mod.merged["FILES_ROOTS"] = FILES_ROOTS

    logging.info(
        "Files mode scan sources reloaded: %d root(s): %s",
        len(FILES_ROOTS or []),
        FILES_ROOTS or [],
    )


def _parse_format_preference(val):
    """Return FORMAT_PREFERENCE as a list. Handles JSON string, comma-separated string, or list from DB/API."""
    _default = ["dsf", "aif", "aiff", "wav", "flac", "m4a", "mp4", "m4b", "m4p", "aifc", "ogg", "opus", "mp3", "wma"]
    if val is None:
        return _default
    if isinstance(val, list):
        return val
    if isinstance(val, str):
        s = val.strip()
        if s.startswith("["):
            try:
                parsed = json.loads(s)
                if isinstance(parsed, list):
                    return parsed
            except (json.JSONDecodeError, TypeError):
                pass
            return _default
        parts = [x.strip() for x in s.split(",") if x.strip()]
        return parts if parts else _default
    return _default

@app.get("/api/config")
def api_config_get():
    """Return current effective configuration for the Web UI.
    Loads from SQLite (settings table) first, then falls back to runtime variables.
    SQLite is the single source of truth for all saved configuration.
    """
    # Load from SQLite first (source of truth), fallback to runtime variables
    def get_setting(key: str, runtime_value, default=""):
        db_value = _get_config_from_db(key)
        if db_value is not None:
            return db_value
        return runtime_value if runtime_value is not None else default

    def get_setting_bool(key: str, runtime_value):
        """Return config value as a real boolean for JSON (so frontend toggles work)."""
        raw = get_setting(key, runtime_value)
        return bool(_parse_bool(raw)) if raw not in (None, "") else bool(_parse_bool(str(runtime_value)))
    
    path_map = getattr(sys.modules[__name__], "PATH_MAP", {})
    section_ids = getattr(sys.modules[__name__], "SECTION_IDS", [])
    skip_folders = getattr(sys.modules[__name__], "SKIP_FOLDERS", [])
    
    # Check if settings exist in DB (wizard was completed)
    has_settings = _has_settings_in_db()
    
    # Load SECTION_IDS from SQLite if available (stored as comma-separated "1,5" or legacy JSON "[1,5]")
    section_ids_str = _get_config_from_db("SECTION_IDS")
    if section_ids_str:
        raw = str(section_ids_str).strip()
        try:
            if raw.startswith("["):
                section_ids = [int(x) for x in json.loads(raw)]
            else:
                section_ids = [int(x.strip()) for x in raw.split(",") if x.strip()]
        except Exception:
            pass
    
    # Load PATH_MAP from SQLite if available
    path_map_str = _get_config_from_db("PATH_MAP")
    if path_map_str:
        try:
            path_map = json.loads(path_map_str) if isinstance(path_map_str, str) else path_map_str
        except Exception:
            pass
    
    files_roots_effective = _parse_files_roots(get_setting("FILES_ROOTS", ",".join(FILES_ROOTS) if isinstance(FILES_ROOTS, list) else (FILES_ROOTS or "")))
    configured = has_settings or bool(files_roots_effective)

    # Never leak secrets to the UI (browser console/network tab). Return *_SET flags instead.
    secret_keys = {
        "PLEX_TOKEN",
        "OPENAI_API_KEY",
        "ANTHROPIC_API_KEY",
        "GOOGLE_API_KEY",
        "DISCOGS_USER_TOKEN",
        "LASTFM_API_KEY",
        "LASTFM_API_SECRET",
        "FANART_API_KEY",
        "THEAUDIODB_API_KEY",
        "SERPER_API_KEY",
        "ACOUSTID_API_KEY",
        "LIDARR_API_KEY",
        "AUTOBRR_API_KEY",
        "JELLYFIN_API_KEY",
        "NAVIDROME_PASSWORD",
        "NAVIDROME_API_KEY",
    }

    def _is_set(val) -> bool:
        return bool(str(val or "").strip())

    # Fetch effective secret values (from DB/runtime) once so *_SET flags are accurate.
    plex_token_eff = get_setting("PLEX_TOKEN", PLEX_TOKEN)
    openai_key_eff = get_setting("OPENAI_API_KEY", OPENAI_API_KEY)
    anthropic_key_eff = get_setting("ANTHROPIC_API_KEY", ANTHROPIC_API_KEY)
    google_key_eff = get_setting("GOOGLE_API_KEY", GOOGLE_API_KEY)
    discogs_token_eff = get_setting("DISCOGS_USER_TOKEN", DISCOGS_USER_TOKEN)
    lastfm_key_eff = get_setting("LASTFM_API_KEY", LASTFM_API_KEY)
    lastfm_secret_eff = get_setting("LASTFM_API_SECRET", LASTFM_API_SECRET)
    fanart_key_eff = get_setting("FANART_API_KEY", FANART_API_KEY)
    theaudiodb_key_eff = get_setting("THEAUDIODB_API_KEY", THEAUDIODB_API_KEY)
    serper_key_eff = get_setting("SERPER_API_KEY", SERPER_API_KEY)
    acoustid_key_eff = get_setting("ACOUSTID_API_KEY", ACOUSTID_API_KEY)
    lidarr_key_eff = get_setting("LIDARR_API_KEY", merged.get("LIDARR_API_KEY", ""))
    autobrr_key_eff = get_setting("AUTOBRR_API_KEY", merged.get("AUTOBRR_API_KEY", ""))
    jellyfin_key_eff = get_setting("JELLYFIN_API_KEY", JELLYFIN_API_KEY)
    navidrome_pass_eff = get_setting("NAVIDROME_PASSWORD", NAVIDROME_PASSWORD)
    navidrome_key_eff = get_setting("NAVIDROME_API_KEY", NAVIDROME_API_KEY)

    payload = {
        "configured": configured,
        "PLEX_HOST": get_setting("PLEX_HOST", PLEX_HOST),
        "PLEX_TOKEN": "***" if _is_set(plex_token_eff) else "",
        "PLEX_TOKEN_SET": _is_set(plex_token_eff),
        "PLEX_BASE_PATH": get_setting("PLEX_BASE_PATH", "/database"),
        "PLEX_DB_PATH": get_setting("PLEX_DB_PATH", merged.get("PLEX_DB_PATH", "/database")),
        "PLEX_DB_FILE": "com.plexapp.plugins.library.db",
        "SECTION_IDS": ",".join(str(s) for s in section_ids),
        "PATH_MAP": path_map,
        "DUPE_ROOT": str(DUPE_ROOT),
        "PMDA_CONFIG_DIR": str(CONFIG_DIR),
        "MUSIC_PARENT_PATH": get_setting("MUSIC_PARENT_PATH", merged.get("MUSIC_PARENT_PATH", "")),
        "SCAN_THREADS": get_setting("SCAN_THREADS", SCAN_THREADS if isinstance(SCAN_THREADS, int) else "auto"),
        "FFPROBE_POOL_SIZE": get_setting("FFPROBE_POOL_SIZE", FFPROBE_POOL_SIZE),
        "IMPROVE_ALL_WORKERS": get_setting("IMPROVE_ALL_WORKERS", IMPROVE_ALL_WORKERS),
        "SKIP_FOLDERS": get_setting("SKIP_FOLDERS", ",".join(skip_folders) if isinstance(skip_folders, list) else (skip_folders or "")),
        "CROSS_LIBRARY_DEDUPE": get_setting_bool("CROSS_LIBRARY_DEDUPE", CROSS_LIBRARY_DEDUPE),
        "CROSSCHECK_SAMPLES": get_setting("CROSSCHECK_SAMPLES", CROSSCHECK_SAMPLES),
        "FORMAT_PREFERENCE": _parse_format_preference(get_setting("FORMAT_PREFERENCE", FORMAT_PREFERENCE)),
        "AI_PROVIDER": get_setting("AI_PROVIDER", AI_PROVIDER),
        "OPENAI_API_KEY": "***" if _is_set(openai_key_eff) else "",
        "OPENAI_API_KEY_SET": _is_set(openai_key_eff),
        "OPENAI_MODEL": get_setting("OPENAI_MODEL", OPENAI_MODEL),
        "OPENAI_MODEL_FALLBACKS": get_setting("OPENAI_MODEL_FALLBACKS", merged.get("OPENAI_MODEL_FALLBACKS", "")),
        "ANTHROPIC_API_KEY": "***" if _is_set(anthropic_key_eff) else "",
        "ANTHROPIC_API_KEY_SET": _is_set(anthropic_key_eff),
        "GOOGLE_API_KEY": "***" if _is_set(google_key_eff) else "",
        "GOOGLE_API_KEY_SET": _is_set(google_key_eff),
        "OLLAMA_URL": get_setting("OLLAMA_URL", OLLAMA_URL),
        "USE_MUSICBRAINZ": True,
        "MUSICBRAINZ_EMAIL": get_setting("MUSICBRAINZ_EMAIL", MUSICBRAINZ_EMAIL),
        "MB_RETRY_NOT_FOUND": get_setting_bool("MB_RETRY_NOT_FOUND", MB_RETRY_NOT_FOUND),
        "MB_SEARCH_ALBUM_TIMEOUT_SEC": max(
            10,
            min(
                300,
                int(get_setting("MB_SEARCH_ALBUM_TIMEOUT_SEC", MB_SEARCH_ALBUM_TIMEOUT_SEC) or 12),
            ),
        ),
        "MB_CANDIDATE_FETCH_LIMIT": max(
            1,
            min(
                20,
                int(get_setting("MB_CANDIDATE_FETCH_LIMIT", MB_CANDIDATE_FETCH_LIMIT) or 4),
            ),
        ),
        "MB_TRACKLIST_FETCH_LIMIT": max(
            0,
            min(
                20,
                int(get_setting("MB_TRACKLIST_FETCH_LIMIT", MB_TRACKLIST_FETCH_LIMIT) or 1),
            ),
        ),
        "MB_FAST_FALLBACK_MODE": get_setting_bool("MB_FAST_FALLBACK_MODE", MB_FAST_FALLBACK_MODE),
        "PROVIDER_IDENTITY_STRICT": get_setting_bool("PROVIDER_IDENTITY_STRICT", PROVIDER_IDENTITY_STRICT),
        "PROVIDER_IDENTITY_USE_AI": get_setting_bool("PROVIDER_IDENTITY_USE_AI", PROVIDER_IDENTITY_USE_AI),
        "PROVIDER_IDENTITY_MIN_SCORE": get_setting("PROVIDER_IDENTITY_MIN_SCORE", PROVIDER_IDENTITY_MIN_SCORE),
        "PROVIDER_IDENTITY_SCORE_MARGIN": get_setting("PROVIDER_IDENTITY_SCORE_MARGIN", PROVIDER_IDENTITY_SCORE_MARGIN),
        "PROVIDER_CACHE_FOUND_TTL_SEC": get_setting("PROVIDER_CACHE_FOUND_TTL_SEC", PROVIDER_CACHE_FOUND_TTL_SEC),
        "PROVIDER_CACHE_NOT_FOUND_TTL_SEC": get_setting("PROVIDER_CACHE_NOT_FOUND_TTL_SEC", PROVIDER_CACHE_NOT_FOUND_TTL_SEC),
        "PROVIDER_CACHE_ERROR_TTL_SEC": get_setting("PROVIDER_CACHE_ERROR_TTL_SEC", PROVIDER_CACHE_ERROR_TTL_SEC),
        "USE_AI_FOR_MB_MATCH": get_setting_bool("USE_AI_FOR_MB_MATCH", USE_AI_FOR_MB_MATCH),
        "USE_AI_FOR_MB_VERIFY": get_setting_bool("USE_AI_FOR_MB_VERIFY", USE_AI_FOR_MB_VERIFY),
        "USE_AI_VISION_FOR_COVER": get_setting_bool("USE_AI_VISION_FOR_COVER", USE_AI_VISION_FOR_COVER),
        "AI_CONFIDENCE_MIN": max(0, min(100, int(get_setting("AI_CONFIDENCE_MIN", AI_CONFIDENCE_MIN) or 50))),
        "OPENAI_VISION_MODEL": get_setting("OPENAI_VISION_MODEL", OPENAI_VISION_MODEL),
        "USE_AI_VISION_BEFORE_COVER_INJECT": get_setting_bool("USE_AI_VISION_BEFORE_COVER_INJECT", USE_AI_VISION_BEFORE_COVER_INJECT),
        "USE_WEB_SEARCH_FOR_MB": get_setting_bool("USE_WEB_SEARCH_FOR_MB", USE_WEB_SEARCH_FOR_MB),
        "SERPER_API_KEY": "***" if _is_set(serper_key_eff) else "",
        "SERPER_API_KEY_SET": _is_set(serper_key_eff),
        "USE_ACOUSTID": True,
        "ACOUSTID_API_KEY": "***" if _is_set(acoustid_key_eff) else "",
        "ACOUSTID_API_KEY_SET": _is_set(acoustid_key_eff),
        "USE_ACOUSTID_WHEN_TAGGED": False,
        "LIDARR_URL": get_setting("LIDARR_URL", merged.get("LIDARR_URL", "")),
        "LIDARR_API_KEY": "***" if _is_set(lidarr_key_eff) else "",
        "LIDARR_API_KEY_SET": _is_set(lidarr_key_eff),
        "AUTOBRR_URL": get_setting("AUTOBRR_URL", merged.get("AUTOBRR_URL", "")),
        "AUTOBRR_API_KEY": "***" if _is_set(autobrr_key_eff) else "",
        "AUTOBRR_API_KEY_SET": _is_set(autobrr_key_eff),
        "AUTO_FIX_BROKEN_ALBUMS": get_setting_bool("AUTO_FIX_BROKEN_ALBUMS", AUTO_FIX_BROKEN_ALBUMS),
        "BROKEN_ALBUM_CONSECUTIVE_THRESHOLD": get_setting("BROKEN_ALBUM_CONSECUTIVE_THRESHOLD", BROKEN_ALBUM_CONSECUTIVE_THRESHOLD),
        "BROKEN_ALBUM_PERCENTAGE_THRESHOLD": get_setting("BROKEN_ALBUM_PERCENTAGE_THRESHOLD", BROKEN_ALBUM_PERCENTAGE_THRESHOLD),
        "REQUIRED_TAGS": ["artist", "album", "genre", "year", "tracks"],
        "DISCORD_WEBHOOK": get_setting("DISCORD_WEBHOOK", DISCORD_WEBHOOK),
        "LOG_LEVEL": get_setting("LOG_LEVEL", LOG_LEVEL),
        "LOG_FILE": get_setting("LOG_FILE", LOG_FILE),
        "AUTO_MOVE_DUPES": get_setting_bool("AUTO_MOVE_DUPES", AUTO_MOVE_DUPES),
        "PIPELINE_ENABLE_MATCH_FIX": get_setting_bool("PIPELINE_ENABLE_MATCH_FIX", PIPELINE_ENABLE_MATCH_FIX),
        "PIPELINE_ENABLE_DEDUPE": get_setting_bool("PIPELINE_ENABLE_DEDUPE", PIPELINE_ENABLE_DEDUPE),
        "PIPELINE_ENABLE_INCOMPLETE_MOVE": get_setting_bool("PIPELINE_ENABLE_INCOMPLETE_MOVE", PIPELINE_ENABLE_INCOMPLETE_MOVE),
        "PIPELINE_ENABLE_EXPORT": get_setting_bool("PIPELINE_ENABLE_EXPORT", PIPELINE_ENABLE_EXPORT),
        "PIPELINE_ENABLE_PLAYER_SYNC": get_setting_bool("PIPELINE_ENABLE_PLAYER_SYNC", PIPELINE_ENABLE_PLAYER_SYNC),
        "PIPELINE_PLAYER_TARGET": get_setting("PIPELINE_PLAYER_TARGET", PIPELINE_PLAYER_TARGET),
        "NORMALIZE_PARENTHETICAL_FOR_DEDUPE": get_setting_bool("NORMALIZE_PARENTHETICAL_FOR_DEDUPE", True),
        "BACKUP_BEFORE_FIX": get_setting_bool("BACKUP_BEFORE_FIX", BACKUP_BEFORE_FIX),
        "MAGIC_MODE": get_setting_bool("MAGIC_MODE", MAGIC_MODE),
        "REPROCESS_INCOMPLETE_ALBUMS": get_setting_bool("REPROCESS_INCOMPLETE_ALBUMS", REPROCESS_INCOMPLETE_ALBUMS),
        "ARTIST_CREDIT_MODE": "picard_like_default",
        "LIVE_DEDUPE_MODE": get_setting("LIVE_DEDUPE_MODE", LIVE_DEDUPE_MODE),
        "SCAN_DISABLE_CACHE": get_setting_bool("SCAN_DISABLE_CACHE", SCAN_DISABLE_CACHE),
        "DISABLE_PATH_CROSSCHECK": get_setting_bool("DISABLE_PATH_CROSSCHECK", DISABLE_PATH_CROSSCHECK),
        "USE_DISCOGS": True,
        "DISCOGS_USER_TOKEN": "***" if _is_set(discogs_token_eff) else "",
        "DISCOGS_USER_TOKEN_SET": _is_set(discogs_token_eff),
        "USE_LASTFM": True,
        "LASTFM_API_KEY": "***" if _is_set(lastfm_key_eff) else "",
        "LASTFM_API_KEY_SET": _is_set(lastfm_key_eff),
        "LASTFM_API_SECRET": "***" if _is_set(lastfm_secret_eff) else "",
        "LASTFM_API_SECRET_SET": _is_set(lastfm_secret_eff),
        "FANART_API_KEY": "***" if _is_set(fanart_key_eff) else "",
        "FANART_API_KEY_SET": _is_set(fanart_key_eff),
        "THEAUDIODB_API_KEY": "***" if _is_set(theaudiodb_key_eff) else "",
        "THEAUDIODB_API_KEY_SET": _is_set(theaudiodb_key_eff),
        "USE_BANDCAMP": True,
        "JELLYFIN_URL": get_setting("JELLYFIN_URL", JELLYFIN_URL),
        "JELLYFIN_API_KEY": "***" if _is_set(jellyfin_key_eff) else "",
        "JELLYFIN_API_KEY_SET": _is_set(jellyfin_key_eff),
        "NAVIDROME_URL": get_setting("NAVIDROME_URL", NAVIDROME_URL),
        "NAVIDROME_USERNAME": get_setting("NAVIDROME_USERNAME", NAVIDROME_USERNAME),
        "NAVIDROME_PASSWORD": "***" if _is_set(navidrome_pass_eff) else "",
        "NAVIDROME_PASSWORD_SET": _is_set(navidrome_pass_eff),
        "NAVIDROME_API_KEY": "***" if _is_set(navidrome_key_eff) else "",
        "NAVIDROME_API_KEY_SET": _is_set(navidrome_key_eff),
        "SKIP_MB_FOR_LIVE_ALBUMS": True,
        "TRACKLIST_MATCH_MIN": "0.9",
        "LIVE_ALBUMS_MB_STRICT": False,
        "INCOMPLETE_ALBUMS_TARGET_DIR": get_setting("INCOMPLETE_ALBUMS_TARGET_DIR", "/dupes/incomplete_albums"),
        # Library backend and file-library settings
        "LIBRARY_MODE": "files",
        "FILES_ROOTS": ", ".join(files_roots_effective),
        "EXPORT_ROOT": get_setting("EXPORT_ROOT", EXPORT_ROOT),
        "EXPORT_NAMING_TEMPLATE": get_setting("EXPORT_NAMING_TEMPLATE", EXPORT_NAMING_TEMPLATE),
        "EXPORT_LINK_STRATEGY": get_setting("EXPORT_LINK_STRATEGY", EXPORT_LINK_STRATEGY),
        "MEDIA_CACHE_ROOT": get_setting("MEDIA_CACHE_ROOT", MEDIA_CACHE_ROOT),
        "AUTO_EXPORT_LIBRARY": get_setting_bool("AUTO_EXPORT_LIBRARY", AUTO_EXPORT_LIBRARY),
        "paths_status": _paths_rw_status(),
        "container_mounts": _container_mounts_status(),
        # Concert discovery (UI filtering)
        "CONCERTS_FILTER_ENABLED": get_setting_bool("CONCERTS_FILTER_ENABLED", False),
        "CONCERTS_HOME_LAT": str(get_setting("CONCERTS_HOME_LAT", "") or "").strip(),
        "CONCERTS_HOME_LON": str(get_setting("CONCERTS_HOME_LON", "") or "").strip(),
        "CONCERTS_RADIUS_KM": str(get_setting("CONCERTS_RADIUS_KM", "150") or "").strip() or "150",
    }

    return jsonify(payload)


def _restart_container():
    """Attempt to restart the container. Tries docker socket first, then falls back to signal."""
    import subprocess
    import signal
    import os
    
    def _do_restart():
        """Perform restart in a separate thread to allow HTTP response to be sent first."""
        time.sleep(2)  # Give time for HTTP response to be sent
        # Try to restart via docker socket if available
        docker_socket = Path("/var/run/docker.sock")
        container_name = os.getenv("HOSTNAME", "PMDA_WEBUI")
        
        if docker_socket.exists():
            try:
                # Try to restart via docker command
                result = subprocess.run(
                    ["docker", "restart", container_name],
                    capture_output=True,
                    timeout=5,
                )
                if result.returncode == 0:
                    logging.info("Container restart initiated via docker socket")
                    return
            except (subprocess.TimeoutExpired, FileNotFoundError, Exception) as e:
                logging.debug("Docker restart failed: %s", e)
        
        # Fallback: use signal to trigger graceful shutdown (container manager will restart)
        # This works if the container is managed by docker-compose, systemd, or has restart policy
        try:
            logging.info("Sending SIGTERM to trigger container restart")
            os.kill(os.getpid(), signal.SIGTERM)
        except Exception as e:
            logging.warning("Failed to restart container: %s", e)
    
    # Start restart in background thread
    restart_thread = threading.Thread(target=_do_restart, daemon=True)
    restart_thread.start()
    return True


def _apply_settings_in_memory(updates: dict):
    """Apply saved settings to in-memory globals so they take effect without restart."""
    global PLEX_CONFIGURED  # declared once so it can be set in any of the Plex blocks below
    mod = sys.modules[__name__]
    ai_keys = {"AI_PROVIDER", "OPENAI_API_KEY", "OPENAI_MODEL", "OPENAI_MODEL_FALLBACKS",
               "ANTHROPIC_API_KEY", "GOOGLE_API_KEY", "OLLAMA_URL"}
    need_ai_reinit = bool(ai_keys & set(updates.keys()))

    if "PLEX_HOST" in updates:
        global PLEX_HOST
        PLEX_HOST = str(updates["PLEX_HOST"] or "").strip()
        logging.info("PLEX_HOST updated in memory")
    if "PLEX_TOKEN" in updates:
        v = str(updates["PLEX_TOKEN"] or "").strip()
        if v != "***":
            global PLEX_TOKEN
            PLEX_TOKEN = v
            logging.info("PLEX_TOKEN updated in memory")
    if "PLEX_BASE_PATH" in updates:
        base = str(updates["PLEX_BASE_PATH"] or "").strip() or "/database"
        resolved = _resolve_plex_db_from_base(base)
        if resolved:
            updates = dict(updates)
            updates["PLEX_DB_PATH"] = resolved
            logging.info("PLEX_DB_PATH resolved from PLEX_BASE_PATH: %s", resolved)
    if "PLEX_DB_PATH" in updates:
        global PLEX_DB_FILE, PLEX_DB_EXISTS, PLEX_CONFIGURED
        db_path = str(updates["PLEX_DB_PATH"] or "").strip() or "/database"
        PLEX_DB_FILE = str(Path(db_path) / PLEX_DB_FILENAME)
        PLEX_DB_EXISTS = Path(PLEX_DB_FILE).exists()
        PLEX_CONFIGURED = bool(
            getattr(mod, "PLEX_HOST", "") and getattr(mod, "PLEX_TOKEN", "") and getattr(mod, "SECTION_IDS", []) and PLEX_DB_EXISTS
        )
        logging.info("PLEX_DB_PATH updated in memory: %s (exists=%s)", PLEX_DB_FILE, PLEX_DB_EXISTS)
    if "SECTION_IDS" in updates:
        global SECTION_IDS, SECTION_ID
        SECTION_IDS = list(updates["SECTION_IDS"])
        SECTION_ID = SECTION_IDS[0] if SECTION_IDS else 0
        logging.info("SECTION_IDS updated in memory: %s", SECTION_IDS)
    # Recompute PLEX_CONFIGURED whenever any Plex-related setting was updated
    plex_keys = {"PLEX_HOST", "PLEX_TOKEN", "SECTION_IDS", "PLEX_DB_PATH"}
    if plex_keys & set(updates.keys()):
        PLEX_CONFIGURED = bool(
            getattr(mod, "PLEX_HOST", "") and getattr(mod, "PLEX_TOKEN", "")
            and getattr(mod, "SECTION_IDS", []) and getattr(mod, "PLEX_DB_EXISTS", False)
        )
        logging.info("PLEX_CONFIGURED recomputed: %s (host=%s, token=%s, sections=%s, db_exists=%s)",
                     PLEX_CONFIGURED, bool(getattr(mod, "PLEX_HOST", "")), bool(getattr(mod, "PLEX_TOKEN", "")),
                     len(getattr(mod, "SECTION_IDS", [])), getattr(mod, "PLEX_DB_EXISTS", False))
    if "MUSICBRAINZ_EMAIL" in updates:
        global MUSICBRAINZ_EMAIL
        MUSICBRAINZ_EMAIL = str(updates["MUSICBRAINZ_EMAIL"] or "")
        _configure_musicbrainz_useragent()
        logging.info("MusicBrainz User-Agent updated")
    if "USE_MUSICBRAINZ" in updates:
        global USE_MUSICBRAINZ
        USE_MUSICBRAINZ = bool(_parse_bool(updates["USE_MUSICBRAINZ"]))
    if "MB_RETRY_NOT_FOUND" in updates:
        global MB_RETRY_NOT_FOUND
        MB_RETRY_NOT_FOUND = bool(_parse_bool(updates["MB_RETRY_NOT_FOUND"]))
    if "MB_SEARCH_ALBUM_TIMEOUT_SEC" in updates:
        global MB_SEARCH_ALBUM_TIMEOUT_SEC
        try:
            MB_SEARCH_ALBUM_TIMEOUT_SEC = max(10, min(300, int(updates["MB_SEARCH_ALBUM_TIMEOUT_SEC"])))
        except (TypeError, ValueError):
            pass
    if "MB_CANDIDATE_FETCH_LIMIT" in updates:
        global MB_CANDIDATE_FETCH_LIMIT
        try:
            MB_CANDIDATE_FETCH_LIMIT = max(1, min(20, int(updates["MB_CANDIDATE_FETCH_LIMIT"])))
        except (TypeError, ValueError):
            pass
    if "MB_TRACKLIST_FETCH_LIMIT" in updates:
        global MB_TRACKLIST_FETCH_LIMIT
        try:
            MB_TRACKLIST_FETCH_LIMIT = max(0, min(20, int(updates["MB_TRACKLIST_FETCH_LIMIT"])))
        except (TypeError, ValueError):
            pass
    if "MB_FAST_FALLBACK_MODE" in updates:
        global MB_FAST_FALLBACK_MODE
        MB_FAST_FALLBACK_MODE = bool(_parse_bool(updates["MB_FAST_FALLBACK_MODE"]))
    if "PROVIDER_IDENTITY_STRICT" in updates:
        global PROVIDER_IDENTITY_STRICT
        PROVIDER_IDENTITY_STRICT = bool(_parse_bool(updates["PROVIDER_IDENTITY_STRICT"]))
    if "PROVIDER_IDENTITY_USE_AI" in updates:
        global PROVIDER_IDENTITY_USE_AI
        PROVIDER_IDENTITY_USE_AI = bool(_parse_bool(updates["PROVIDER_IDENTITY_USE_AI"]))
    if "PROVIDER_IDENTITY_MIN_SCORE" in updates:
        global PROVIDER_IDENTITY_MIN_SCORE
        try:
            PROVIDER_IDENTITY_MIN_SCORE = max(0.0, min(1.0, float(updates["PROVIDER_IDENTITY_MIN_SCORE"])))
        except (TypeError, ValueError):
            pass
    if "PROVIDER_IDENTITY_SCORE_MARGIN" in updates:
        global PROVIDER_IDENTITY_SCORE_MARGIN
        try:
            PROVIDER_IDENTITY_SCORE_MARGIN = max(0.0, min(1.0, float(updates["PROVIDER_IDENTITY_SCORE_MARGIN"])))
        except (TypeError, ValueError):
            pass
    if "PROVIDER_CACHE_FOUND_TTL_SEC" in updates:
        global PROVIDER_CACHE_FOUND_TTL_SEC
        try:
            PROVIDER_CACHE_FOUND_TTL_SEC = max(60, min(60 * 60 * 24 * 365, int(updates["PROVIDER_CACHE_FOUND_TTL_SEC"])))
        except (TypeError, ValueError):
            pass
    if "PROVIDER_CACHE_NOT_FOUND_TTL_SEC" in updates:
        global PROVIDER_CACHE_NOT_FOUND_TTL_SEC
        try:
            PROVIDER_CACHE_NOT_FOUND_TTL_SEC = max(60, min(60 * 60 * 24 * 30, int(updates["PROVIDER_CACHE_NOT_FOUND_TTL_SEC"])))
        except (TypeError, ValueError):
            pass
    if "PROVIDER_CACHE_ERROR_TTL_SEC" in updates:
        global PROVIDER_CACHE_ERROR_TTL_SEC
        try:
            PROVIDER_CACHE_ERROR_TTL_SEC = max(30, min(60 * 60 * 24, int(updates["PROVIDER_CACHE_ERROR_TTL_SEC"])))
        except (TypeError, ValueError):
            pass
    if "SCAN_DISABLE_CACHE" in updates:
        global SCAN_DISABLE_CACHE
        SCAN_DISABLE_CACHE = bool(_parse_bool(updates["SCAN_DISABLE_CACHE"]))
        logging.info("SCAN_DISABLE_CACHE updated in memory: %s", SCAN_DISABLE_CACHE)
    # Recompute MB_DISABLE_CACHE so that SCAN_DISABLE_CACHE also forces full MB lookups.
    if "MB_DISABLE_CACHE" in updates or "SCAN_DISABLE_CACHE" in updates:
        global MB_DISABLE_CACHE
        current_mb_disable = MB_DISABLE_CACHE
        if "MB_DISABLE_CACHE" in updates:
            current_mb_disable = bool(_parse_bool(updates["MB_DISABLE_CACHE"]))
        MB_DISABLE_CACHE = bool(current_mb_disable or SCAN_DISABLE_CACHE)
        logging.info(
            "MB_DISABLE_CACHE updated in memory (effective): MB_DISABLE_CACHE=%s, SCAN_DISABLE_CACHE=%s",
            MB_DISABLE_CACHE,
            SCAN_DISABLE_CACHE,
        )
    if "USE_AI_FOR_MB_MATCH" in updates:
        global USE_AI_FOR_MB_MATCH
        USE_AI_FOR_MB_MATCH = bool(_parse_bool(updates["USE_AI_FOR_MB_MATCH"]))
    if "USE_AI_FOR_MB_VERIFY" in updates:
        global USE_AI_FOR_MB_VERIFY
        USE_AI_FOR_MB_VERIFY = bool(_parse_bool(updates["USE_AI_FOR_MB_VERIFY"]))
    if "USE_AI_VISION_FOR_COVER" in updates:
        global USE_AI_VISION_FOR_COVER
        USE_AI_VISION_FOR_COVER = bool(_parse_bool(updates["USE_AI_VISION_FOR_COVER"]))
    if "AI_CONFIDENCE_MIN" in updates:
        global AI_CONFIDENCE_MIN
        try:
            v = updates["AI_CONFIDENCE_MIN"]
            AI_CONFIDENCE_MIN = max(0, min(100, int(v))) if v is not None and str(v).strip().isdigit() else 50
        except (TypeError, ValueError):
            AI_CONFIDENCE_MIN = 50
    if "OPENAI_VISION_MODEL" in updates:
        global OPENAI_VISION_MODEL
        OPENAI_VISION_MODEL = str(updates.get("OPENAI_VISION_MODEL") or "").strip()
    if "USE_AI_VISION_BEFORE_COVER_INJECT" in updates:
        global USE_AI_VISION_BEFORE_COVER_INJECT
        USE_AI_VISION_BEFORE_COVER_INJECT = bool(_parse_bool(updates["USE_AI_VISION_BEFORE_COVER_INJECT"]))
    if "USE_WEB_SEARCH_FOR_MB" in updates:
        global USE_WEB_SEARCH_FOR_MB
        USE_WEB_SEARCH_FOR_MB = bool(_parse_bool(updates["USE_WEB_SEARCH_FOR_MB"]))
    if "SERPER_API_KEY" in updates:
        global SERPER_API_KEY
        v = str(updates.get("SERPER_API_KEY") or "").strip()
        if v != "***":
            SERPER_API_KEY = v
    if "USE_ACOUSTID" in updates:
        global USE_ACOUSTID
        USE_ACOUSTID = bool(_parse_bool(updates["USE_ACOUSTID"]))
    if "ACOUSTID_API_KEY" in updates:
        global ACOUSTID_API_KEY
        v = str(updates.get("ACOUSTID_API_KEY") or "").strip()
        if v != "***":
            ACOUSTID_API_KEY = v
    if "USE_ACOUSTID_WHEN_TAGGED" in updates:
        global USE_ACOUSTID_WHEN_TAGGED
        USE_ACOUSTID_WHEN_TAGGED = bool(_parse_bool(updates["USE_ACOUSTID_WHEN_TAGGED"]))
    if "ARTIST_CREDIT_MODE" in updates:
        global ARTIST_CREDIT_MODE
        ARTIST_CREDIT_MODE = str(updates.get("ARTIST_CREDIT_MODE") or "album_artist_strict").strip().lower()
        logging.info("ARTIST_CREDIT_MODE updated in memory: %s", ARTIST_CREDIT_MODE)
    if "LIVE_DEDUPE_MODE" in updates:
        global LIVE_DEDUPE_MODE
        LIVE_DEDUPE_MODE = str(updates.get("LIVE_DEDUPE_MODE") or "safe").strip().lower()
        logging.info("LIVE_DEDUPE_MODE updated in memory: %s", LIVE_DEDUPE_MODE)
    if "SCAN_THREADS" in updates:
        global SCAN_THREADS
        v = updates["SCAN_THREADS"]
        if isinstance(v, str) and str(v).strip().lower() == "auto":
            SCAN_THREADS = max(1, os.cpu_count() or 4)
        else:
            try:
                SCAN_THREADS = max(1, int(v))
            except (ValueError, TypeError):
                pass
        logging.info("SCAN_THREADS updated in memory: %s", SCAN_THREADS)
    if "FFPROBE_POOL_SIZE" in updates:
        global FFPROBE_POOL_SIZE, _ffprobe_pool
        try:
            FFPROBE_POOL_SIZE = max(1, min(64, int(updates["FFPROBE_POOL_SIZE"])))
            _ffprobe_pool = None  # Next scan will create pool with new size
        except (ValueError, TypeError):
            pass
        logging.info("FFPROBE_POOL_SIZE updated in memory: %s", FFPROBE_POOL_SIZE)
    # Library backend & file-library settings
    if "LIBRARY_MODE" in updates:
        mode = str(updates["LIBRARY_MODE"] or "plex").strip().lower()
        if mode not in {"plex", "files"}:
            logging.warning("Ignoring invalid LIBRARY_MODE '%s' (expected 'plex' or 'files')", mode)
        else:
            global LIBRARY_MODE
            LIBRARY_MODE = mode
            mod.merged["LIBRARY_MODE"] = mode
            logging.info("LIBRARY_MODE updated in memory: %s", mode)
            if mode == "files":
                _trigger_files_index_rebuild_async(reason="settings_library_mode_files")
            _restart_files_watcher_if_needed()
    if "FILES_ROOTS" in updates:
        roots = _parse_files_roots(updates["FILES_ROOTS"])
        global FILES_ROOTS
        FILES_ROOTS = roots
        mod.merged["FILES_ROOTS"] = roots
        logging.info("FILES_ROOTS updated in memory: %s", FILES_ROOTS)
        if _get_library_mode() == "files":
            _trigger_files_index_rebuild_async(reason="settings_files_roots")
        _restart_files_watcher_if_needed()
    if "EXPORT_ROOT" in updates:
        root = str(updates["EXPORT_ROOT"] or "").strip()
        global EXPORT_ROOT
        EXPORT_ROOT = root
        mod.merged["EXPORT_ROOT"] = root
        logging.info("EXPORT_ROOT updated in memory: %s", EXPORT_ROOT)
    if "EXPORT_NAMING_TEMPLATE" in updates:
        tpl = str(updates["EXPORT_NAMING_TEMPLATE"] or "").strip()
        global EXPORT_NAMING_TEMPLATE
        EXPORT_NAMING_TEMPLATE = tpl
        mod.merged["EXPORT_NAMING_TEMPLATE"] = tpl
        logging.info("EXPORT_NAMING_TEMPLATE updated in memory")
    if "EXPORT_LINK_STRATEGY" in updates:
        strat = str(updates["EXPORT_LINK_STRATEGY"] or "hardlink").strip().lower()
        if strat not in {"hardlink", "symlink", "copy", "move"}:
            logging.warning("Ignoring invalid EXPORT_LINK_STRATEGY '%s' (expected hardlink/symlink/copy/move)", strat)
        else:
            global EXPORT_LINK_STRATEGY
            EXPORT_LINK_STRATEGY = strat
            mod.merged["EXPORT_LINK_STRATEGY"] = strat
            logging.info("EXPORT_LINK_STRATEGY updated in memory: %s", EXPORT_LINK_STRATEGY)
    if "MEDIA_CACHE_ROOT" in updates:
        root = str(updates["MEDIA_CACHE_ROOT"] or "").strip() or str(CONFIG_DIR / "media_cache")
        global MEDIA_CACHE_ROOT
        MEDIA_CACHE_ROOT = root
        mod.merged["MEDIA_CACHE_ROOT"] = root
        try:
            (Path(MEDIA_CACHE_ROOT) / "album").mkdir(parents=True, exist_ok=True)
            (Path(MEDIA_CACHE_ROOT) / "artist").mkdir(parents=True, exist_ok=True)
            logging.info("MEDIA_CACHE_ROOT updated in memory: %s", MEDIA_CACHE_ROOT)
        except Exception as e:
            logging.warning("Could not initialize MEDIA_CACHE_ROOT=%s: %s", MEDIA_CACHE_ROOT, e)
    if "IMPROVE_ALL_WORKERS" in updates:
        global IMPROVE_ALL_WORKERS
        try:
            IMPROVE_ALL_WORKERS = max(1, min(8, int(updates["IMPROVE_ALL_WORKERS"])))
        except (ValueError, TypeError):
            pass
        logging.info("IMPROVE_ALL_WORKERS updated in memory: %s", IMPROVE_ALL_WORKERS)
    if "LOG_LEVEL" in updates:
        global LOG_LEVEL
        LOG_LEVEL = str(updates["LOG_LEVEL"] or "INFO").upper()
        root_logger = logging.getLogger()
        root_logger.setLevel(getattr(logging, LOG_LEVEL, logging.INFO))
    if "DISCORD_WEBHOOK" in updates:
        v = str(updates["DISCORD_WEBHOOK"] or "").strip()
        if v != "***":
            global DISCORD_WEBHOOK
            DISCORD_WEBHOOK = v
    if "SKIP_FOLDERS" in updates:
        global SKIP_FOLDERS
        v = updates["SKIP_FOLDERS"]
        if isinstance(v, list):
            SKIP_FOLDERS = [str(p).strip() for p in v if str(p).strip()]
        else:
            SKIP_FOLDERS = [p.strip() for p in str(v or "").split(",") if p.strip()]
    if "REQUIRED_TAGS" in updates:
        global REQUIRED_TAGS
        REQUIRED_TAGS = _parse_required_tags(updates["REQUIRED_TAGS"]) if updates["REQUIRED_TAGS"] else []
    if "BROKEN_ALBUM_CONSECUTIVE_THRESHOLD" in updates:
        global BROKEN_ALBUM_CONSECUTIVE_THRESHOLD
        try:
            BROKEN_ALBUM_CONSECUTIVE_THRESHOLD = int(updates["BROKEN_ALBUM_CONSECUTIVE_THRESHOLD"])
        except (ValueError, TypeError):
            pass
    if "BROKEN_ALBUM_PERCENTAGE_THRESHOLD" in updates:
        global BROKEN_ALBUM_PERCENTAGE_THRESHOLD
        try:
            BROKEN_ALBUM_PERCENTAGE_THRESHOLD = float(updates["BROKEN_ALBUM_PERCENTAGE_THRESHOLD"])
        except (ValueError, TypeError):
            pass
    if "AUTO_MOVE_DUPES" in updates:
        global AUTO_MOVE_DUPES
        AUTO_MOVE_DUPES = bool(_parse_bool(updates["AUTO_MOVE_DUPES"]))
    if "PIPELINE_ENABLE_MATCH_FIX" in updates:
        global PIPELINE_ENABLE_MATCH_FIX
        PIPELINE_ENABLE_MATCH_FIX = bool(_parse_bool(updates["PIPELINE_ENABLE_MATCH_FIX"]))
        mod.merged["PIPELINE_ENABLE_MATCH_FIX"] = PIPELINE_ENABLE_MATCH_FIX
    if "PIPELINE_ENABLE_DEDUPE" in updates:
        global PIPELINE_ENABLE_DEDUPE
        PIPELINE_ENABLE_DEDUPE = bool(_parse_bool(updates["PIPELINE_ENABLE_DEDUPE"]))
        mod.merged["PIPELINE_ENABLE_DEDUPE"] = PIPELINE_ENABLE_DEDUPE
    if "PIPELINE_ENABLE_INCOMPLETE_MOVE" in updates:
        global PIPELINE_ENABLE_INCOMPLETE_MOVE
        PIPELINE_ENABLE_INCOMPLETE_MOVE = bool(_parse_bool(updates["PIPELINE_ENABLE_INCOMPLETE_MOVE"]))
        mod.merged["PIPELINE_ENABLE_INCOMPLETE_MOVE"] = PIPELINE_ENABLE_INCOMPLETE_MOVE
    if "PIPELINE_ENABLE_EXPORT" in updates:
        global PIPELINE_ENABLE_EXPORT
        PIPELINE_ENABLE_EXPORT = bool(_parse_bool(updates["PIPELINE_ENABLE_EXPORT"]))
        mod.merged["PIPELINE_ENABLE_EXPORT"] = PIPELINE_ENABLE_EXPORT
    if "PIPELINE_ENABLE_PLAYER_SYNC" in updates:
        global PIPELINE_ENABLE_PLAYER_SYNC
        PIPELINE_ENABLE_PLAYER_SYNC = bool(_parse_bool(updates["PIPELINE_ENABLE_PLAYER_SYNC"]))
        mod.merged["PIPELINE_ENABLE_PLAYER_SYNC"] = PIPELINE_ENABLE_PLAYER_SYNC
    if "PIPELINE_PLAYER_TARGET" in updates:
        global PIPELINE_PLAYER_TARGET
        PIPELINE_PLAYER_TARGET = _normalize_player_target(updates.get("PIPELINE_PLAYER_TARGET"))
        mod.merged["PIPELINE_PLAYER_TARGET"] = PIPELINE_PLAYER_TARGET
        logging.info("PIPELINE_PLAYER_TARGET updated in memory: %s", PIPELINE_PLAYER_TARGET)
    if "AUTO_EXPORT_LIBRARY" in updates:
        global AUTO_EXPORT_LIBRARY
        AUTO_EXPORT_LIBRARY = bool(_parse_bool(updates["AUTO_EXPORT_LIBRARY"]))
    if "BACKUP_BEFORE_FIX" in updates:
        global BACKUP_BEFORE_FIX
        BACKUP_BEFORE_FIX = bool(_parse_bool(updates["BACKUP_BEFORE_FIX"]))
    if "MAGIC_MODE" in updates:
        global MAGIC_MODE
        MAGIC_MODE = bool(_parse_bool(updates["MAGIC_MODE"]))
    if "REPROCESS_INCOMPLETE_ALBUMS" in updates:
        global REPROCESS_INCOMPLETE_ALBUMS
        REPROCESS_INCOMPLETE_ALBUMS = bool(_parse_bool(updates["REPROCESS_INCOMPLETE_ALBUMS"]))
    if "AUTO_FIX_BROKEN_ALBUMS" in updates:
        global AUTO_FIX_BROKEN_ALBUMS
        AUTO_FIX_BROKEN_ALBUMS = bool(_parse_bool(updates["AUTO_FIX_BROKEN_ALBUMS"]))
    if "LIDARR_URL" in updates:
        mod.LIDARR_URL = str(updates["LIDARR_URL"] or "")
    if "LIDARR_API_KEY" in updates:
        mod.LIDARR_API_KEY = str(updates["LIDARR_API_KEY"] or "")
    if "AUTOBRR_URL" in updates:
        mod.AUTOBRR_URL = str(updates["AUTOBRR_URL"] or "")
    if "AUTOBRR_API_KEY" in updates:
        mod.AUTOBRR_API_KEY = str(updates["AUTOBRR_API_KEY"] or "")
    if "DISABLE_PATH_CROSSCHECK" in updates:
        global DISABLE_PATH_CROSSCHECK
        DISABLE_PATH_CROSSCHECK = bool(_parse_bool(updates["DISABLE_PATH_CROSSCHECK"]))
    if "PATH_MAP" in updates:
        global PATH_MAP
        PATH_MAP = _parse_path_map(updates["PATH_MAP"])
        logging.info("PATH_MAP updated in memory (%d entries)", len(PATH_MAP))
    if "DUPE_ROOT" in updates:
        global DUPE_ROOT
        DUPE_ROOT = Path(str(updates["DUPE_ROOT"] or "").strip() or "/dupes")
        logging.info("DUPE_ROOT updated in memory: %s", DUPE_ROOT)
    if "MUSIC_PARENT_PATH" in updates:
        merged["MUSIC_PARENT_PATH"] = str(updates["MUSIC_PARENT_PATH"] or "").strip() or ""
        logging.info("MUSIC_PARENT_PATH updated in memory")
    if "LOG_FILE" in updates:
        global LOG_FILE
        LOG_FILE = str(updates["LOG_FILE"] or "").strip() or str(CONFIG_DIR / "pmda.log")
        logging.info("LOG_FILE updated in memory: %s", LOG_FILE)
    if "CROSS_LIBRARY_DEDUPE" in updates:
        global CROSS_LIBRARY_DEDUPE
        CROSS_LIBRARY_DEDUPE = bool(_parse_bool(updates["CROSS_LIBRARY_DEDUPE"]))
        logging.info("CROSS_LIBRARY_DEDUPE updated in memory: %s", CROSS_LIBRARY_DEDUPE)
    if "CROSSCHECK_SAMPLES" in updates:
        global CROSSCHECK_SAMPLES
        try:
            CROSSCHECK_SAMPLES = max(0, int(updates["CROSSCHECK_SAMPLES"]))
        except (ValueError, TypeError):
            CROSSCHECK_SAMPLES = 20
        logging.info("CROSSCHECK_SAMPLES updated in memory: %s", CROSSCHECK_SAMPLES)
    if "FORMAT_PREFERENCE" in updates:
        global FORMAT_PREFERENCE, FMT_SCORE
        FORMAT_PREFERENCE = _parse_format_preference_early(updates["FORMAT_PREFERENCE"])
        FMT_SCORE = {ext: len(FORMAT_PREFERENCE) - i for i, ext in enumerate(FORMAT_PREFERENCE)}
        logging.info("FORMAT_PREFERENCE updated in memory (%d formats)", len(FORMAT_PREFERENCE))
    if "OPENAI_MODEL_FALLBACKS" in updates:
        merged["OPENAI_MODEL_FALLBACKS"] = str(updates["OPENAI_MODEL_FALLBACKS"] or "").strip()
        logging.info("OPENAI_MODEL_FALLBACKS updated in memory")
    if "INCOMPLETE_ALBUMS_TARGET_DIR" in updates:
        target_dir = (updates.get("INCOMPLETE_ALBUMS_TARGET_DIR") or "").strip() or "/dupes/incomplete_albums"
        try:
            Path(target_dir).mkdir(parents=True, exist_ok=True)
            logging.debug("Ensured quarantine folder for incomplete albums exists: %s", target_dir)
        except Exception as e:
            logging.warning("Could not create incomplete albums target dir %s: %s", target_dir, e)

    # Metadata fallback providers (Discogs, Last.fm, Bandcamp)
    if "USE_DISCOGS" in updates:
        mod.USE_DISCOGS = bool(_parse_bool(updates["USE_DISCOGS"]))
    if "DISCOGS_USER_TOKEN" in updates:
        mod.DISCOGS_USER_TOKEN = str(updates["DISCOGS_USER_TOKEN"] or "").strip()
    if "USE_LASTFM" in updates:
        mod.USE_LASTFM = bool(_parse_bool(updates["USE_LASTFM"]))
    if "LASTFM_API_KEY" in updates:
        mod.LASTFM_API_KEY = str(updates["LASTFM_API_KEY"] or "").strip()
    if "LASTFM_API_SECRET" in updates:
        mod.LASTFM_API_SECRET = str(updates["LASTFM_API_SECRET"] or "").strip()
    if "FANART_API_KEY" in updates:
        mod.FANART_API_KEY = str(updates["FANART_API_KEY"] or "").strip()
    if "THEAUDIODB_API_KEY" in updates:
        mod.THEAUDIODB_API_KEY = str(updates["THEAUDIODB_API_KEY"] or "").strip()
    if "USE_BANDCAMP" in updates:
        mod.USE_BANDCAMP = bool(_parse_bool(updates["USE_BANDCAMP"]))
    if "JELLYFIN_URL" in updates:
        global JELLYFIN_URL
        JELLYFIN_URL = _normalize_http_base_url(updates.get("JELLYFIN_URL"))
        mod.merged["JELLYFIN_URL"] = JELLYFIN_URL
    if "JELLYFIN_API_KEY" in updates:
        global JELLYFIN_API_KEY
        v = str(updates.get("JELLYFIN_API_KEY") or "").strip()
        if v != "***":
            JELLYFIN_API_KEY = v
            mod.merged["JELLYFIN_API_KEY"] = JELLYFIN_API_KEY
    if "NAVIDROME_URL" in updates:
        global NAVIDROME_URL
        NAVIDROME_URL = _normalize_http_base_url(updates.get("NAVIDROME_URL"))
        mod.merged["NAVIDROME_URL"] = NAVIDROME_URL
    if "NAVIDROME_USERNAME" in updates:
        global NAVIDROME_USERNAME
        NAVIDROME_USERNAME = str(updates.get("NAVIDROME_USERNAME") or "").strip()
        mod.merged["NAVIDROME_USERNAME"] = NAVIDROME_USERNAME
    if "NAVIDROME_PASSWORD" in updates:
        global NAVIDROME_PASSWORD
        v = str(updates.get("NAVIDROME_PASSWORD") or "")
        if v != "***":
            NAVIDROME_PASSWORD = v
            mod.merged["NAVIDROME_PASSWORD"] = NAVIDROME_PASSWORD
    if "NAVIDROME_API_KEY" in updates:
        global NAVIDROME_API_KEY
        v = str(updates.get("NAVIDROME_API_KEY") or "").strip()
        if v != "***":
            NAVIDROME_API_KEY = v
            mod.merged["NAVIDROME_API_KEY"] = NAVIDROME_API_KEY
    if "SKIP_MB_FOR_LIVE_ALBUMS" in updates:
        mod.SKIP_MB_FOR_LIVE_ALBUMS = bool(_parse_bool(updates["SKIP_MB_FOR_LIVE_ALBUMS"]))
    if "TRACKLIST_MATCH_MIN" in updates:
        try:
            mod.TRACKLIST_MATCH_MIN = float(updates["TRACKLIST_MATCH_MIN"])
        except (TypeError, ValueError):
            pass
    if "LIVE_ALBUMS_MB_STRICT" in updates:
        mod.LIVE_ALBUMS_MB_STRICT = bool(_parse_bool(updates["LIVE_ALBUMS_MB_STRICT"]))

    # AI-related: update globals then reinit clients
    if "AI_PROVIDER" in updates:
        mod.AI_PROVIDER = str(updates["AI_PROVIDER"] or "openai").strip().lower()
    if "OPENAI_API_KEY" in updates:
        v = str(updates["OPENAI_API_KEY"] or "").strip()
        if v != "***":
            mod.OPENAI_API_KEY = v
    if "OPENAI_MODEL" in updates:
        mod.OPENAI_MODEL = str(updates["OPENAI_MODEL"] or "gpt-4")
    if "ANTHROPIC_API_KEY" in updates:
        mod.ANTHROPIC_API_KEY = str(updates["ANTHROPIC_API_KEY"] or "")
    if "GOOGLE_API_KEY" in updates:
        mod.GOOGLE_API_KEY = str(updates["GOOGLE_API_KEY"] or "")
    if "OLLAMA_URL" in updates:
        mod.OLLAMA_URL = str(updates["OLLAMA_URL"] or "").strip().rstrip("/")

    if need_ai_reinit:
        _reinit_ai_from_globals()

    # Enforce product defaults for simplified UX (files-only + always-on metadata pipeline).
    _apply_forced_runtime_defaults()


@app.put("/api/config")
def api_config_put():
    """Persist configuration updates to SQLite (single source of truth).
    Only updates keys present in the request; existing values in SQLite are preserved.
    Settings are applied in memory immediately so no restart is needed.
    """
    data = request.get_json() or {}
    allowed = {
        "PLEX_HOST", "PLEX_TOKEN", "PLEX_BASE_PATH", "PLEX_DB_PATH", "SECTION_IDS", "PATH_MAP",
        "DUPE_ROOT", "PMDA_CONFIG_DIR", "MUSIC_PARENT_PATH",
        "SCAN_THREADS", "LOG_LEVEL", "LOG_FILE", "AI_PROVIDER", "OPENAI_API_KEY", "OPENAI_MODEL",
        "OPENAI_MODEL_FALLBACKS", "ANTHROPIC_API_KEY", "GOOGLE_API_KEY", "OLLAMA_URL",
        "DISCORD_WEBHOOK", "USE_MUSICBRAINZ", "MUSICBRAINZ_EMAIL", "MB_RETRY_NOT_FOUND",
        "MB_SEARCH_ALBUM_TIMEOUT_SEC", "MB_CANDIDATE_FETCH_LIMIT", "MB_TRACKLIST_FETCH_LIMIT", "MB_FAST_FALLBACK_MODE",
        "PROVIDER_IDENTITY_STRICT", "PROVIDER_IDENTITY_USE_AI", "PROVIDER_IDENTITY_MIN_SCORE", "PROVIDER_IDENTITY_SCORE_MARGIN",
        "PROVIDER_CACHE_FOUND_TTL_SEC", "PROVIDER_CACHE_NOT_FOUND_TTL_SEC", "PROVIDER_CACHE_ERROR_TTL_SEC",
        "USE_AI_FOR_MB_MATCH", "USE_AI_FOR_MB_VERIFY",
        "USE_AI_VISION_FOR_COVER", "AI_CONFIDENCE_MIN", "OPENAI_VISION_MODEL", "USE_AI_VISION_BEFORE_COVER_INJECT", "USE_WEB_SEARCH_FOR_MB", "SERPER_API_KEY",
        "USE_ACOUSTID", "ACOUSTID_API_KEY", "USE_ACOUSTID_WHEN_TAGGED",
        "SKIP_FOLDERS", "CROSS_LIBRARY_DEDUPE", "CROSSCHECK_SAMPLES", "DISABLE_PATH_CROSSCHECK",
        "FORMAT_PREFERENCE", "AUTO_MOVE_DUPES", "NORMALIZE_PARENTHETICAL_FOR_DEDUPE", "BACKUP_BEFORE_FIX", "MAGIC_MODE", "REPROCESS_INCOMPLETE_ALBUMS", "IMPROVE_ALL_WORKERS", "FFPROBE_POOL_SIZE",
        "PIPELINE_ENABLE_MATCH_FIX", "PIPELINE_ENABLE_DEDUPE", "PIPELINE_ENABLE_INCOMPLETE_MOVE", "PIPELINE_ENABLE_EXPORT", "PIPELINE_ENABLE_PLAYER_SYNC", "PIPELINE_PLAYER_TARGET",
        "ARTIST_CREDIT_MODE", "LIVE_DEDUPE_MODE",
        "LIDARR_URL", "LIDARR_API_KEY", "AUTOBRR_URL", "AUTOBRR_API_KEY", "AUTO_FIX_BROKEN_ALBUMS",
        "BROKEN_ALBUM_CONSECUTIVE_THRESHOLD", "BROKEN_ALBUM_PERCENTAGE_THRESHOLD", "REQUIRED_TAGS",
        "USE_DISCOGS", "DISCOGS_USER_TOKEN", "USE_LASTFM", "LASTFM_API_KEY", "LASTFM_API_SECRET", "USE_BANDCAMP",
        "FANART_API_KEY", "THEAUDIODB_API_KEY",
        "JELLYFIN_URL", "JELLYFIN_API_KEY",
        "NAVIDROME_URL", "NAVIDROME_USERNAME", "NAVIDROME_PASSWORD", "NAVIDROME_API_KEY",
        "INCOMPLETE_ALBUMS_TARGET_DIR",
        "SCAN_DISABLE_CACHE",
        # Concert discovery (UI filtering)
        "CONCERTS_FILTER_ENABLED", "CONCERTS_HOME_LAT", "CONCERTS_HOME_LON", "CONCERTS_RADIUS_KM",
        # Library backend & file-library settings
        "LIBRARY_MODE", "FILES_ROOTS", "EXPORT_ROOT", "EXPORT_NAMING_TEMPLATE", "EXPORT_LINK_STRATEGY", "MEDIA_CACHE_ROOT", "AUTO_EXPORT_LIBRARY",
    }
    # Only process keys that are in the request AND in the allowed list
    # This preserves existing values in SQLite for keys not in the request
    updates = {k: v for k, v in data.items() if k in allowed}
    if not updates:
        return jsonify({"status": "ok", "message": "Nothing to save"})
    # When PLEX_BASE_PATH is updated, clear PLEX_DB_PATH so next startup re-discovers
    if "PLEX_BASE_PATH" in updates:
        updates["PLEX_DB_PATH"] = ""
    if "SECTION_IDS" in updates:
        raw = updates["SECTION_IDS"]
        if isinstance(raw, str):
            updates["SECTION_IDS"] = [int(x.strip()) for x in raw.split(",") if x.strip()]
        elif isinstance(raw, list):
            updates["SECTION_IDS"] = [int(x) for x in raw]
    if "SKIP_FOLDERS" in updates and isinstance(updates["SKIP_FOLDERS"], str):
        updates["SKIP_FOLDERS"] = [p.strip() for p in updates["SKIP_FOLDERS"].split(",") if p.strip()]
    if "FILES_ROOTS" in updates:
        updates["FILES_ROOTS"] = _parse_files_roots(updates["FILES_ROOTS"])
    if "EXPORT_LINK_STRATEGY" in updates:
        strategy = str(updates["EXPORT_LINK_STRATEGY"] or "hardlink").strip().lower()
        updates["EXPORT_LINK_STRATEGY"] = strategy if strategy in {"hardlink", "symlink", "copy", "move"} else "hardlink"
    if "PIPELINE_PLAYER_TARGET" in updates:
        updates["PIPELINE_PLAYER_TARGET"] = _normalize_player_target(updates["PIPELINE_PLAYER_TARGET"])
    if "JELLYFIN_URL" in updates:
        updates["JELLYFIN_URL"] = _normalize_http_base_url(updates["JELLYFIN_URL"])
    if "NAVIDROME_URL" in updates:
        updates["NAVIDROME_URL"] = _normalize_http_base_url(updates["NAVIDROME_URL"])
    if "NAVIDROME_USERNAME" in updates:
        updates["NAVIDROME_USERNAME"] = str(updates["NAVIDROME_USERNAME"] or "").strip()
    if "REQUIRED_TAGS" in updates:
        updates["REQUIRED_TAGS"] = _parse_required_tags(updates["REQUIRED_TAGS"])
    if "BROKEN_ALBUM_CONSECUTIVE_THRESHOLD" in updates:
        try:
            updates["BROKEN_ALBUM_CONSECUTIVE_THRESHOLD"] = int(updates["BROKEN_ALBUM_CONSECUTIVE_THRESHOLD"])
        except (ValueError, TypeError):
            updates["BROKEN_ALBUM_CONSECUTIVE_THRESHOLD"] = 2
    if "BROKEN_ALBUM_PERCENTAGE_THRESHOLD" in updates:
        try:
            updates["BROKEN_ALBUM_PERCENTAGE_THRESHOLD"] = float(updates["BROKEN_ALBUM_PERCENTAGE_THRESHOLD"])
        except (ValueError, TypeError):
            updates["BROKEN_ALBUM_PERCENTAGE_THRESHOLD"] = 0.20
    if "MB_SEARCH_ALBUM_TIMEOUT_SEC" in updates:
        try:
            updates["MB_SEARCH_ALBUM_TIMEOUT_SEC"] = max(10, min(300, int(updates["MB_SEARCH_ALBUM_TIMEOUT_SEC"])))
        except (ValueError, TypeError):
            updates["MB_SEARCH_ALBUM_TIMEOUT_SEC"] = 20
    if "MB_CANDIDATE_FETCH_LIMIT" in updates:
        try:
            updates["MB_CANDIDATE_FETCH_LIMIT"] = max(1, min(20, int(updates["MB_CANDIDATE_FETCH_LIMIT"])))
        except (ValueError, TypeError):
            updates["MB_CANDIDATE_FETCH_LIMIT"] = 4
    if "MB_TRACKLIST_FETCH_LIMIT" in updates:
        try:
            updates["MB_TRACKLIST_FETCH_LIMIT"] = max(0, min(20, int(updates["MB_TRACKLIST_FETCH_LIMIT"])))
        except (ValueError, TypeError):
            updates["MB_TRACKLIST_FETCH_LIMIT"] = 2
    if "PROVIDER_IDENTITY_MIN_SCORE" in updates:
        try:
            updates["PROVIDER_IDENTITY_MIN_SCORE"] = max(0.0, min(1.0, float(updates["PROVIDER_IDENTITY_MIN_SCORE"])))
        except (ValueError, TypeError):
            updates["PROVIDER_IDENTITY_MIN_SCORE"] = 0.72
    if "PROVIDER_IDENTITY_SCORE_MARGIN" in updates:
        try:
            updates["PROVIDER_IDENTITY_SCORE_MARGIN"] = max(0.0, min(1.0, float(updates["PROVIDER_IDENTITY_SCORE_MARGIN"])))
        except (ValueError, TypeError):
            updates["PROVIDER_IDENTITY_SCORE_MARGIN"] = 0.08
    if "PROVIDER_CACHE_FOUND_TTL_SEC" in updates:
        try:
            updates["PROVIDER_CACHE_FOUND_TTL_SEC"] = max(60, min(60 * 60 * 24 * 365, int(updates["PROVIDER_CACHE_FOUND_TTL_SEC"])))
        except (ValueError, TypeError):
            updates["PROVIDER_CACHE_FOUND_TTL_SEC"] = 60 * 60 * 24 * 30
    if "PROVIDER_CACHE_NOT_FOUND_TTL_SEC" in updates:
        try:
            updates["PROVIDER_CACHE_NOT_FOUND_TTL_SEC"] = max(60, min(60 * 60 * 24 * 30, int(updates["PROVIDER_CACHE_NOT_FOUND_TTL_SEC"])))
        except (ValueError, TypeError):
            updates["PROVIDER_CACHE_NOT_FOUND_TTL_SEC"] = 60 * 60 * 8
    if "PROVIDER_CACHE_ERROR_TTL_SEC" in updates:
        try:
            updates["PROVIDER_CACHE_ERROR_TTL_SEC"] = max(30, min(60 * 60 * 24, int(updates["PROVIDER_CACHE_ERROR_TTL_SEC"])))
        except (ValueError, TypeError):
            updates["PROVIDER_CACHE_ERROR_TTL_SEC"] = 60 * 20

    # Concert discovery (UI filtering)
    if "CONCERTS_FILTER_ENABLED" in updates:
        updates["CONCERTS_FILTER_ENABLED"] = bool(_parse_bool(updates["CONCERTS_FILTER_ENABLED"]))
    if "CONCERTS_RADIUS_KM" in updates:
        try:
            updates["CONCERTS_RADIUS_KM"] = str(max(1, min(2000, int(float(updates["CONCERTS_RADIUS_KM"])))))
        except (ValueError, TypeError):
            updates["CONCERTS_RADIUS_KM"] = "150"
    for k in ("CONCERTS_HOME_LAT", "CONCERTS_HOME_LON"):
        if k in updates:
            raw = str(updates.get(k) or "").strip()
            if not raw:
                updates[k] = ""
                continue
            try:
                # Keep as string for config storage; clamp lat/lon bounds.
                v = float(raw)
                if k.endswith("_LAT"):
                    v = max(-90.0, min(90.0, v))
                else:
                    v = max(-180.0, min(180.0, v))
                updates[k] = f"{v:.6f}".rstrip("0").rstrip(".")
            except Exception:
                updates[k] = ""

    # Force files-only mode regardless of incoming value.
    if "LIBRARY_MODE" in updates:
        updates["LIBRARY_MODE"] = "files"
    
    # Serialize complex types for SQLite storage. Do not overwrite secret keys with mask (e.g. "***").
    MASKED_SECRETS = (
        "OPENAI_API_KEY", "PLEX_TOKEN", "DISCORD_WEBHOOK",
        "ANTHROPIC_API_KEY", "GOOGLE_API_KEY",
        "DISCOGS_USER_TOKEN", "LASTFM_API_KEY", "LASTFM_API_SECRET",
        "LIDARR_API_KEY", "AUTOBRR_API_KEY",
        "JELLYFIN_API_KEY", "NAVIDROME_PASSWORD", "NAVIDROME_API_KEY",
    )
    # Drop masked secrets from the in-memory apply path as well.
    for k in list(updates.keys()):
        if k in MASKED_SECRETS and str(updates.get(k) or "").strip() == "***":
            updates.pop(k, None)
    updates_for_db = {}
    forced_db_updates = {
        "LIBRARY_MODE": "files",
    }
    for k, v in updates.items():
        if k in MASKED_SECRETS and str(v).strip() == "***":
            continue  # Do not overwrite real key with mask
        if k == "SECTION_IDS" and isinstance(v, list):
            # Store SECTION_IDS as comma-separated string so GET /api/config can parse it consistently
            updates_for_db[k] = ",".join(str(x) for x in v) if v else ""
        elif k == "SKIP_FOLDERS" and isinstance(v, list):
            # Store as comma-separated string; load uses _parse_skip_folders (accepts JSON or CSV)
            updates_for_db[k] = ",".join(str(p).strip() for p in v if str(p).strip()) if v else ""
        elif k == "FILES_ROOTS" and isinstance(v, list):
            # Store as comma-separated string to avoid double-encoded JSON values.
            updates_for_db[k] = ",".join(str(p).strip() for p in v if str(p).strip()) if v else ""
        elif isinstance(v, (dict, list)):
            updates_for_db[k] = json.dumps(v)
        else:
            # Preserve empty strings (they are valid values, e.g. empty webhook = disabled)
            updates_for_db[k] = str(v) if v is not None else ""
    for k, v in forced_db_updates.items():
        updates_for_db[k] = v
    
    try:
        # Save to dedicated settings.db (single source of truth for configuration)
        init_settings_db()
        con = sqlite3.connect(str(SETTINGS_DB_FILE))
        cur = con.cursor()
        for key, value in updates_for_db.items():
            cur.execute("INSERT OR REPLACE INTO settings(key, value) VALUES(?, ?)", (key, value))
        con.commit()
        con.close()
        logging.info("Settings saved to settings.db: %s", list(updates_for_db.keys()))
    except Exception as e:
        logging.warning("Failed to save settings to settings.db: %s", e)
        return jsonify({"status": "error", "message": f"Failed to save to database: {str(e)}"}), 500
    
    # Apply all saved settings in memory (no restart needed)
    _apply_settings_in_memory(updates)

    # Only restart container if this is the initial wizard setup (first time saving)
    # After wizard is complete, settings are saved but container doesn't restart
    has_settings = _has_settings_in_db()
    restart_success = False
    if not has_settings:
        # First time saving (wizard), restart container
        logging.info("First time configuration save detected, restarting container")
        restart_success = _restart_container()
        if not restart_success:
            logging.warning("Container restart may have failed, but settings were saved")
    else:
        # Settings already exist, just save without restart
        logging.info("Settings updated (wizard already completed), saving without restart")
    
    return jsonify({"status": "ok", "restart_initiated": restart_success, "message": "Settings saved successfully"})


@app.post("/api/files/export/rebuild")
def api_files_export_rebuild():
    """Start rebuilding the export library (hardlinks/symlinks/copies/moves) in the background. Files mode only."""
    if _get_library_mode() != "files":
        return jsonify({"status": "error", "message": "Export is only available in Files library mode"}), 400
    with lock:
        prog = state.get("export_progress") or {}
        if prog.get("running"):
            return jsonify({"status": "already_running", "message": "Export already in progress"}), 409
    thread = threading.Thread(target=_run_export_library, daemon=True)
    thread.start()
    return jsonify({"status": "started"})


@app.get("/api/files/export/status")
def api_files_export_status():
    """Return current export progress (tracks_done, total_tracks, albums_done, total_albums, running, error)."""
    with lock:
        prog = state.get("export_progress")
    if not prog:
        return jsonify({"running": False, "tracks_done": 0, "total_tracks": 0, "albums_done": 0, "total_albums": 0, "error": None})
    return jsonify({
        "running": prog.get("running", False),
        "tracks_done": prog.get("tracks_done", 0),
        "total_tracks": prog.get("total_tracks", 0),
        "albums_done": prog.get("albums_done", 0),
        "total_albums": prog.get("total_albums", 0),
        "error": prog.get("error"),
    })


@app.get("/api/files/structure/overview")
def api_files_structure_overview():
    """Return structure analysis: sampled paths, inferred templates, metrics (Files mode)."""
    if _get_library_mode() != "files":
        return jsonify({"error": "Structure overview is only available in Files library mode"}), 400
    roots = FILES_ROOTS or []
    if not roots:
        return jsonify({"templates": [], "metrics": {}, "samples": [], "sample_count": 0})
    data = analyse_directory_structure(roots)
    return jsonify(data)


@app.get("/api/fs/list")
def api_fs_list():
    """
    List subdirectories for folder navigation in the Settings UI.
    Returns container-visible paths.
    """
    path_raw = str(request.args.get("path") or "").strip()
    if not path_raw:
        roots = _parse_files_roots(FILES_ROOTS)
        path_raw = roots[0] if roots else "/"
    include_hidden = bool(_parse_bool(request.args.get("hidden") or False))
    try:
        limit = max(20, min(1000, int(request.args.get("limit") or 300)))
    except (TypeError, ValueError):
        limit = 300

    path_obj = Path(path_raw)
    if not path_obj.is_absolute():
        return jsonify({"error": "Path must be absolute"}), 400
    path_obj = path_for_fs_access(path_obj)
    if not path_obj.exists():
        return jsonify({"error": "Path does not exist", "path": str(path_obj)}), 404
    if not path_obj.is_dir():
        return jsonify({"error": "Path is not a directory", "path": str(path_obj)}), 400

    directories = []
    truncated = False
    try:
        children = sorted(path_obj.iterdir(), key=lambda p: p.name.lower())
    except PermissionError:
        return jsonify({"error": "Permission denied", "path": str(path_obj)}), 403
    except OSError as e:
        return jsonify({"error": str(e), "path": str(path_obj)}), 500

    for child in children:
        try:
            if not child.is_dir():
                continue
            if not include_hidden and child.name.startswith("."):
                continue
            directories.append({
                "name": child.name,
                "path": str(child),
                "writable": bool(os.access(child, os.W_OK)),
            })
            if len(directories) >= limit:
                truncated = True
                break
        except OSError:
            continue

    common_roots: list[str] = []
    for candidate in [
        *(_parse_files_roots(FILES_ROOTS) or []),
        (EXPORT_ROOT or "").strip(),
        (MEDIA_CACHE_ROOT or "").strip(),
        "/music",
        "/config",
        "/",
    ]:
        c = str(candidate or "").strip()
        if not c or c in common_roots:
            continue
        common_roots.append(c)

    parent = None
    if path_obj.parent != path_obj:
        parent = str(path_obj.parent)

    return jsonify({
        "path": str(path_obj),
        "parent": parent,
        "writable": bool(os.access(path_obj, os.W_OK)),
        "directories": directories,
        "truncated": truncated,
        "roots": common_roots,
    })


@app.post("/api/library/files-index/rebuild")
def api_library_files_index_rebuild():
    """Trigger a full files-library index rebuild in PostgreSQL."""
    if _get_library_mode() != "files":
        return jsonify({"status": "error", "message": "Files index rebuild is only available in Files library mode"}), 400
    if _trigger_files_index_rebuild_async(reason="api_rebuild"):
        return jsonify({"status": "started"})
    st = _files_index_get_state()
    return jsonify({"status": "already_running", "progress": st}), 409


@app.get("/api/library/files-index/status")
def api_library_files_index_status():
    """Return current files-library index build status."""
    st = _files_index_get_state()
    if not st:
        st = {"running": False, "phase": None, "error": None}
    artists, albums, tracks = _files_index_read_counts()
    st["indexed_artists"] = artists
    st["indexed_albums"] = albums
    st["indexed_tracks"] = tracks
    return jsonify(st)


@app.get("/api/library/files-profile-backfill/status")
def api_library_files_profile_backfill_status():
    """Return current artist profile backfill status (bios/tags/similar + external images)."""
    with _files_profile_backfill_lock:
        st = dict(_files_profile_backfill_state or {})
    if not st:
        st = {"running": False}
    return jsonify(st)


@app.post("/api/library/files-profile-backfill/start")
def api_library_files_profile_backfill_start():
    """Trigger a background backfill to enrich artist pages and cache external artist images."""
    if _get_library_mode() != "files":
        return jsonify({"status": "error", "message": "Backfill is only available in Files library mode"}), 400
    payload = request.get_json(silent=True) or {}
    reason = str(payload.get("reason") or "api_manual")
    if _trigger_files_profile_backfill_async(reason=reason):
        return jsonify({"status": "started"})
    with _files_profile_backfill_lock:
        st = dict(_files_profile_backfill_state or {})
    return jsonify({"status": "already_running", "progress": st}), 409


@app.post("/api/library/files-profile-backfill/stop")
def api_library_files_profile_backfill_stop():
    """Request the running backfill job to stop."""
    with _files_profile_backfill_lock:
        _files_profile_backfill_state["running"] = False
    return jsonify({"status": "stopping"})


def get_duplicate_groups_from_library():
    """
    Return duplicate groups from Plex (same logic as Library: same artist + same album name).
    Returns list of { artist, norm_title, album_ids } for groups with >1 album.
    Used to show in Unduper even when scan produced 0 groups (e.g. all same-folder).
    """
    _reload_section_ids_from_db()
    if not PLEX_CONFIGURED or not SECTION_IDS:
        return []
    db_conn = plex_connect()
    try:
        ph = ",".join("?" for _ in SECTION_IDS)
        rows = db_conn.execute(f"""
            SELECT alb.id, alb.title, alb.parent_id
            FROM metadata_items alb
            WHERE alb.metadata_type = 9 AND alb.library_section_id IN ({ph})
        """, list(SECTION_IDS)).fetchall()
        artist_cache = {}
        norm_to_albums: dict[tuple[str, str], list[int]] = {}
        for album_id, title, parent_id in rows:
            artist_name = ""
            if parent_id:
                if parent_id not in artist_cache:
                    r = db_conn.execute(
                        "SELECT title FROM metadata_items WHERE id = ?", (parent_id,)
                    ).fetchone()
                    artist_cache[parent_id] = (r[0] or "").strip() if r else ""
                artist_name = artist_cache[parent_id]
            normalize_parenthetical = bool(_parse_bool(_get_config_from_db("NORMALIZE_PARENTHETICAL_FOR_DEDUPE") or "true"))
            norm = norm_album_for_dedup(title or "", normalize_parenthetical)
            key = (artist_name, norm)
            norm_to_albums.setdefault(key, []).append(album_id)
        out = []
        for (artist_name, norm_title), album_ids in norm_to_albums.items():
            if len(album_ids) > 1:
                out.append({"artist": artist_name, "norm_title": norm_title, "album_ids": album_ids})
        return out
    finally:
        db_conn.close()


@app.get("/api/duplicates")
def api_duplicates():
    """
    Return duplicate-group cards for the Web UI.

    Query param: source=scan (default) | source=all
    - source=scan: only groups found by the last scan (fast, small list).
    - source=all: scan groups + library-only groups (same artist+album in Plex, no_move).
      Can be very slow on large libraries (thousands of cards).
    """
    if not PLEX_CONFIGURED:
        resp = jsonify([])
        resp.headers["X-PMDA-Requires-Config"] = "true"
        return resp
    include_library_groups = request.args.get("source", "scan").strip().lower() == "all"
    with lock:
        # During scan, always reload from DB so incremental writer updates are visible
        if state.get("scanning") or not state["duplicates"]:
            if not state.get("_api_duplicates_load_logged") and not state.get("scanning"):
                logging.debug("api_duplicates(): loading scan results from DB into memory")
                state["_api_duplicates_load_logged"] = True
            state["duplicates"] = load_scan_from_db()
        cards = _build_card_list(state["duplicates"])
        if not include_library_groups:
            return jsonify(cards)
        scan_keys = set()
        for artist, groups in state["duplicates"].items():
            for g in groups:
                if "best" not in g:
                    continue
                norm = (g["best"].get("album_norm") or "").strip().lower()
                if norm:
                    scan_keys.add((artist, norm))
        # Add library-only groups only when explicitly requested (source=all)
        if cards or scan_keys:
            library_groups = get_duplicate_groups_from_library()
            db_plex = None
            try:
                db_plex = plex_connect()
            except Exception:
                pass
            for lg in library_groups:
                artist, norm_title = lg["artist"], (lg["norm_title"] or "").strip().lower()
                if (artist, norm_title) in scan_keys:
                    continue
                album_ids = lg["album_ids"]
                # Skip group if any edition is already under /dupes (already deduped by auto-move or manual)
                if db_plex:
                    if any(_album_path_under_dupes(db_plex, aid) for aid in album_ids):
                        continue
                first_id = album_ids[0]
                n = len(album_ids)
                display_title = (norm_title or "").title() or "Unknown"
                cards.append({
                    "artist_key": artist.replace(" ", "_"),
                    "artist": artist,
                    "album_id": first_id,
                    "n": n,
                    "best_thumb": thumb_url(first_id),
                    "best_title": display_title,
                    "best_fmt": "‚Äî",
                    "formats": ["‚Äî"] * n,
                    "used_ai": False,
                    "ai_provider": "",
                    "ai_model": "",
                    "size": 0,
                    "size_mb": 0,
                    "track_count": 0,
                    "path": "",
                    "no_move": True,
                })
            if db_plex:
                try:
                    db_plex.close()
                except Exception:
                    pass
    return jsonify(cards)


@app.get("/api/progress")
def api_progress():
    with lock:
        # Do NOT set scanning=False here when progress >= total. The scan thread still runs
        # the AI batch and finally block (save_scan_to_db, scan_history) after progress hits
        # 100%; only that thread must set scanning=False so the UI does not show "finished"
        # while the scan is still running.
        scanning = state["scanning"]
        status = "paused" if (scanning and scan_is_paused.is_set()) else ("running" if scanning else "stopped")
        # Step-based progress for bar: progress/total = steps done / step total (3*albums+2 or +3)
        progress = state.get("scan_step_progress", 0)
        total = state.get("scan_step_total", 0) or state["scan_total"]
        format_done_count = state.get("scan_format_done_count", 0)
        mb_done_count = state.get("scan_mb_done_count", 0)
        scan_steps_log = state.get("scan_steps_log") or []
        current_scan_type = (state.get("scan_type") or "full")
        scan_resume_run_id = state.get("scan_resume_run_id")
        
        # Keep scan_progress for effective_progress / legacy; bar uses step progress
        active_artists_dict = state.get("scan_active_artists", {})
        effective_progress = state.get("scan_progress", 0)
        
        # ETA from step progress
        eta_seconds = None
        threads_in_use = SCAN_THREADS
        if scanning and state.get("scan_start_time") and total > 0:
            current_time = time.time()
            start_time = state["scan_start_time"]
            elapsed_time = current_time - start_time
            if elapsed_time > 0 and progress > 0:
                speed = progress / elapsed_time
                remaining_steps = total - progress
                if speed > 0 and remaining_steps > 0:
                    eta_seconds = int(remaining_steps / speed)
        
        active_artists_list = [
            {
                "artist_name": name,
                "total_albums": info.get("total_albums", 0) if isinstance(info, dict) else 0,
                "albums_processed": info.get("albums_processed", 0) if isinstance(info, dict) else 0,
                "current_album": info.get("current_album") if isinstance(info, dict) else None  # Include current album tracking
            }
            for name, info in active_artists_dict.items()
            if not name.startswith("_") and isinstance(info, dict)  # Filter out internal keys like "_ai_batch"
        ]
        
        # Copy all state values we need while still in the lock
        artists_processed = state.get("scan_artists_processed", 0)
        artists_total = state.get("scan_artists_total", 0)
        detected_artists_total = state.get("scan_detected_artists_total", 0)
        detected_albums_total = state.get("scan_detected_albums_total", 0)
        resume_skipped_artists = state.get("scan_resume_skipped_artists", 0)
        resume_skipped_albums = state.get("scan_resume_skipped_albums", 0)
        ai_used_count = state.get("scan_ai_used_count", 0)
        mb_used_count = state.get("scan_mb_used_count", 0)
        ai_enabled = state.get("scan_ai_enabled", False)
        mb_enabled = state.get("scan_mb_enabled", False)
        audio_cache_hits = state.get("scan_audio_cache_hits", 0)
        audio_cache_misses = state.get("scan_audio_cache_misses", 0)
        mb_cache_hits = state.get("scan_mb_cache_hits", 0)
        mb_cache_misses = state.get("scan_mb_cache_misses", 0)
        duplicate_groups_count = state.get("scan_duplicate_groups_count", 0)
        total_duplicates_count = state.get("scan_total_duplicates_count", 0)
        broken_albums_count = state.get("scan_broken_albums_count", 0)
        missing_albums_count = state.get("scan_missing_albums_count", 0)
        albums_without_artist_image = state.get("scan_albums_without_artist_image", 0)
        albums_without_album_image = state.get("scan_albums_without_album_image", 0)
        albums_without_complete_tags = state.get("scan_albums_without_complete_tags", 0)
        albums_without_mb_id = state.get("scan_albums_without_mb_id", 0)
        albums_without_artist_mb_id = state.get("scan_albums_without_artist_mb_id", 0)
        format_done_count = state.get("scan_format_done_count", 0)
        mb_done_count = state.get("scan_mb_done_count", 0)
        scan_ai_batch_total = state.get("scan_ai_batch_total", 0)
        scan_ai_batch_processed = state.get("scan_ai_batch_processed", 0)
        scan_ai_current_label = state.get("scan_ai_current_label")
        last_fix_all_by_provider = state.get("last_fix_all_by_provider")
        last_fix_all_total_albums = state.get("last_fix_all_total_albums", 0)
        total_albums = state.get("scan_total_albums", 0)
        improve_all_state = state.get("improve_all") or {}
        improve_all_running = bool(improve_all_state.get("running"))
        scan_post_processing = bool(state.get("scan_post_processing") or improve_all_running)
        scan_post_total = int(state.get("scan_post_total") or 0)
        scan_post_done = int(state.get("scan_post_done") or 0)
        scan_post_current_artist = state.get("scan_post_current_artist")
        scan_post_current_album = state.get("scan_post_current_album")
        scan_discovery_running = bool(state.get("scan_discovery_running"))
        scan_discovery_current_root = state.get("scan_discovery_current_root")
        scan_discovery_roots_done = int(state.get("scan_discovery_roots_done") or 0)
        scan_discovery_roots_total = int(state.get("scan_discovery_roots_total") or 0)
        scan_discovery_files_found = int(state.get("scan_discovery_files_found") or 0)
        scan_discovery_folders_found = int(state.get("scan_discovery_folders_found") or 0)
        scan_discovery_albums_found = int(state.get("scan_discovery_albums_found") or 0)
        scan_discovery_artists_found = int(state.get("scan_discovery_artists_found") or 0)
        files_watcher_state = dict(state.get("files_watcher") or {})
        scan_discogs_matched = int(state.get("scan_discogs_matched") or 0)
        scan_lastfm_matched = int(state.get("scan_lastfm_matched") or 0)
        scan_bandcamp_matched = int(state.get("scan_bandcamp_matched") or 0)
        scan_start_time = state.get("scan_start_time")
        scan_pipeline_flags = dict(state.get("scan_pipeline_flags") or {})
        scan_pipeline_sync_target = state.get("scan_pipeline_sync_target")
        scan_incomplete_moved_count = int(state.get("scan_incomplete_moved_count") or 0)
        scan_incomplete_moved_mb = int(state.get("scan_incomplete_moved_mb") or 0)
        scan_player_sync_target = state.get("scan_player_sync_target")
        scan_player_sync_ok = state.get("scan_player_sync_ok")
        scan_player_sync_message = state.get("scan_player_sync_message")
        if improve_all_running:
            scan_post_total = max(scan_post_total, int(improve_all_state.get("total") or 0))
            scan_post_done = max(scan_post_done, int(improve_all_state.get("current") or 0))
            if not scan_post_current_artist:
                scan_post_current_artist = improve_all_state.get("current_artist")
            if not scan_post_current_album:
                scan_post_current_album = improve_all_state.get("current_album")
        
        # Current micro-step (from first active artist with non-done status) for live indicators
        current_step = None
        for _name, info in active_artists_dict.items():
            if not _name.startswith("_") and isinstance(info, dict):
                cur_album = info.get("current_album")
                if isinstance(cur_album, dict):
                    s = cur_album.get("status")
                    if s and s != "done":
                        current_step = s
                        break
        finalizing = state.get("scan_finalizing", False)
        deduping = state.get("deduping", False)
        dedupe_progress = state.get("dedupe_progress", 0)
        dedupe_total = state.get("dedupe_total", 0)
        dedupe_current_group = state.get("dedupe_current_group")
        auto_move_enabled = bool(
            (scan_pipeline_flags or {}).get(
                "dedupe",
                getattr(sys.modules[__name__], "AUTO_MOVE_DUPES", False),
            )
        )
        # Phase: derive from current_step and flags for UI (format_analysis | identification_tags | ia_analysis | finalizing | moving_dupes)
        if not scanning:
            phase = None
        elif deduping:
            phase = "moving_dupes"
        elif finalizing:
            phase = "finalizing"
        elif scan_post_processing and current_step in (None, "", "done") and "_ai_batch" not in active_artists_dict:
            phase = "post_processing"
        elif current_step in ("comparing_versions", "detecting_best") or "_ai_batch" in active_artists_dict:
            phase = "ia_analysis"
        elif current_step == "searching_mb":
            phase = "identification_tags"
        elif current_step == "analyzing_format":
            phase = "format_analysis"
        else:
            phase = "format_analysis"
    
    # AI provider/model for display (read outside lock)
    ai_provider_display = AI_PROVIDER or ""
    ai_model_display = getattr(sys.modules[__name__], "RESOLVED_MODEL", None) or OPENAI_MODEL or ""
    
    # When not scanning, attach last completed scan summary for "Scan complete ‚Äì Summary" UI
    last_scan_summary = None
    if not scanning:
        try:
            con = sqlite3.connect(str(STATE_DB_FILE), timeout=2)
            cur = con.cursor()
            cur.execute(
                "SELECT summary_json FROM scan_history WHERE status = 'completed' AND end_time IS NOT NULL ORDER BY end_time DESC LIMIT 1"
            )
            row = cur.fetchone()
            con.close()
            if row and row[0]:
                last_scan_summary = json.loads(row[0])
                # Merge mb_match (from scan) and discogs/lastfm/bandcamp match (from scan fallback or last fix-all run) for chart-ready summary
                aw = last_scan_summary.get("albums_with_mb_id") or 0
                awo = last_scan_summary.get("albums_without_mb_id") or 0
                total_mb = aw + awo
                last_scan_summary["mb_match"] = {"matched": aw, "total": total_mb} if total_mb else {"matched": 0, "total": 0}
                albums_scanned = last_scan_summary.get("albums_scanned") or 0
                # Prefer scan fallback stats when present (Discogs/Last.fm/Bandcamp during scan when MB found nothing)
                scan_discogs = last_scan_summary.get("scan_discogs_matched")
                scan_lastfm = last_scan_summary.get("scan_lastfm_matched")
                scan_bandcamp = last_scan_summary.get("scan_bandcamp_matched")
                if scan_discogs is not None and albums_scanned:
                    last_scan_summary["discogs_match"] = {"matched": scan_discogs, "total": albums_scanned}
                elif last_fix_all_total_albums:
                    bp = (last_fix_all_by_provider or {}).get("discogs", {})
                    identified = (bp.get("identified") or 0) if isinstance(bp, dict) else 0
                    last_scan_summary["discogs_match"] = {"matched": identified, "total": last_fix_all_total_albums}
                else:
                    last_scan_summary["discogs_match"] = {"matched": 0, "total": 0}
                if scan_lastfm is not None and albums_scanned:
                    last_scan_summary["lastfm_match"] = {"matched": scan_lastfm, "total": albums_scanned}
                elif last_fix_all_total_albums:
                    bp = (last_fix_all_by_provider or {}).get("lastfm", {})
                    identified = (bp.get("identified") or 0) if isinstance(bp, dict) else 0
                    last_scan_summary["lastfm_match"] = {"matched": identified, "total": last_fix_all_total_albums}
                else:
                    last_scan_summary["lastfm_match"] = {"matched": 0, "total": 0}
                if scan_bandcamp is not None and albums_scanned:
                    last_scan_summary["bandcamp_match"] = {"matched": scan_bandcamp, "total": albums_scanned}
                elif last_fix_all_total_albums:
                    bp = (last_fix_all_by_provider or {}).get("bandcamp", {})
                    identified = (bp.get("identified") or 0) if isinstance(bp, dict) else 0
                    last_scan_summary["bandcamp_match"] = {"matched": identified, "total": last_fix_all_total_albums}
                else:
                    last_scan_summary["bandcamp_match"] = {"matched": 0, "total": 0}
        except Exception as e:
            logging.debug("api_progress: could not load last_scan_summary: %s", e)
    
    return jsonify(
        scanning=scanning,
        progress=progress,
        total=total,
        effective_progress=effective_progress,
        status=status,
        phase=phase,
        current_step=current_step,
        ai_provider=ai_provider_display,
        ai_model=ai_model_display,
        # Scan details
        artists_processed=artists_processed,
        artists_total=artists_total,
        detected_artists_total=detected_artists_total,
        detected_albums_total=detected_albums_total,
        resume_skipped_artists=resume_skipped_artists,
        resume_skipped_albums=resume_skipped_albums,
        ai_used_count=ai_used_count,
        mb_used_count=mb_used_count,
        ai_enabled=ai_enabled,
        mb_enabled=mb_enabled,
        # Cache statistics
        audio_cache_hits=audio_cache_hits,
        audio_cache_misses=audio_cache_misses,
        mb_cache_hits=mb_cache_hits,
        mb_cache_misses=mb_cache_misses,
        # Detailed statistics
        duplicate_groups_count=duplicate_groups_count,
        total_duplicates_count=total_duplicates_count,
        broken_albums_count=broken_albums_count,
        missing_albums_count=missing_albums_count,
        albums_without_artist_image=albums_without_artist_image,
        albums_without_album_image=albums_without_album_image,
        albums_without_complete_tags=albums_without_complete_tags,
        albums_without_mb_id=albums_without_mb_id,
        albums_without_artist_mb_id=albums_without_artist_mb_id,
        format_done_count=format_done_count,
        mb_done_count=mb_done_count,
        # Settings visible in scanner (e.g. link to configure)
        mb_retry_not_found=getattr(sys.modules[__name__], "MB_RETRY_NOT_FOUND", False),
        # ETA
        eta_seconds=eta_seconds,
        threads_in_use=threads_in_use,
        active_artists=active_artists_list,
        last_scan_summary=last_scan_summary,
        scan_steps_log=scan_steps_log,
        scan_type=current_scan_type,
        scan_resume_run_id=scan_resume_run_id,
        finalizing=finalizing,
        deduping=deduping,
        dedupe_progress=dedupe_progress,
        dedupe_total=dedupe_total,
        dedupe_current_group=dedupe_current_group,
        auto_move_enabled=auto_move_enabled,
        paths_status=_paths_rw_status(),
        # IA analysis step: current group label and N/M progress
        scan_ai_batch_total=scan_ai_batch_total,
        scan_ai_batch_processed=scan_ai_batch_processed,
        scan_ai_current_label=scan_ai_current_label,
        total_albums=total_albums,
        post_processing=scan_post_processing,
        post_processing_done=scan_post_done,
        post_processing_total=scan_post_total,
        post_processing_current_artist=scan_post_current_artist,
        post_processing_current_album=scan_post_current_album,
        scan_discovery_running=scan_discovery_running,
        scan_discovery_current_root=scan_discovery_current_root,
        scan_discovery_roots_done=scan_discovery_roots_done,
        scan_discovery_roots_total=scan_discovery_roots_total,
        scan_discovery_files_found=scan_discovery_files_found,
        scan_discovery_folders_found=scan_discovery_folders_found,
        scan_discovery_albums_found=scan_discovery_albums_found,
        scan_discovery_artists_found=scan_discovery_artists_found,
        files_watcher_running=bool(files_watcher_state.get("running")),
        files_watcher_roots=list(files_watcher_state.get("roots") or []),
        files_watcher_dirty_count=int(files_watcher_state.get("dirty_count") or 0),
        files_watcher_last_event_at=files_watcher_state.get("last_event_at"),
        files_watcher_last_event_path=files_watcher_state.get("last_event_path"),
        scan_discogs_matched=scan_discogs_matched,
        scan_lastfm_matched=scan_lastfm_matched,
        scan_bandcamp_matched=scan_bandcamp_matched,
        scan_start_time=scan_start_time,
        scan_pipeline_flags=scan_pipeline_flags,
        scan_pipeline_sync_target=scan_pipeline_sync_target,
        scan_incomplete_moved_count=scan_incomplete_moved_count,
        scan_incomplete_moved_mb=scan_incomplete_moved_mb,
        scan_player_sync_target=scan_player_sync_target,
        scan_player_sync_ok=scan_player_sync_ok,
        scan_player_sync_message=scan_player_sync_message,
    )


def _tail_log_lines(path: Path, lines: int = 200, max_bytes: int = 512 * 1024) -> list[str]:
    """Return last `lines` lines from `path`, stripping ANSI escape codes for Web UI."""
    if lines <= 0:
        return []
    ansi_re = re.compile(r"\x1B\[[0-?]*[ -/]*[@-~]")
    try:
        with path.open("rb") as fh:
            fh.seek(0, os.SEEK_END)
            size = fh.tell()
            read_size = min(max_bytes, size)
            if read_size > 0:
                fh.seek(-read_size, os.SEEK_END)
            raw = fh.read()
    except FileNotFoundError:
        return []
    except Exception:
        logging.debug("Could not tail log file %s", path, exc_info=True)
        return []
    text = raw.decode("utf-8", "replace")
    out = text.splitlines()
    if len(out) > lines:
        out = out[-lines:]
    return [ansi_re.sub("", ln) for ln in out]


@app.get("/api/logs/tail")
def api_logs_tail():
    """Return recent backend logs for Scan page power-user panel."""
    try:
        lines = int(request.args.get("lines", 180))
    except Exception:
        lines = 180
    lines = max(20, min(lines, 1200))
    log_path = Path(str(LOG_FILE or "")).expanduser()
    return jsonify(
        path=str(log_path),
        lines=_tail_log_lines(log_path, lines=lines),
    )


@app.get("/api/logs/download")
def api_logs_download():
    """Download backend logs as a text file (tail by default, safe size)."""
    try:
        lines = int(request.args.get("lines", 20000))
    except Exception:
        lines = 20000
    lines = max(200, min(lines, 50000))
    log_path = Path(str(LOG_FILE or "")).expanduser()
    out_lines = _tail_log_lines(log_path, lines=lines, max_bytes=10 * 1024 * 1024)
    payload = ("\n".join(out_lines) + "\n") if out_lines else ""
    ts = time.strftime("%Y%m%d-%H%M%S")
    fname = f"pmda-log-{ts}.log"
    return Response(
        payload,
        mimetype="text/plain",
        headers={"Content-Disposition": f'attachment; filename="{fname}"'},
    )


@app.get("/api/statistics/cache-control")
def api_statistics_cache_control():
    """Return cache/runtime telemetry for the Statistics Cache Control Center."""
    force = _parse_bool(request.args.get("force", "false"))
    return jsonify(_collect_cache_control_metrics(force=force))


@app.get("/api/scan-history")
def api_scan_history():
    """Return list of all scan history entries."""
    import sqlite3
    con = sqlite3.connect(str(STATE_DB_FILE))
    cur = con.cursor()
    cur.execute("PRAGMA table_info(scan_history)")
    cols_info = [r[1] for r in cur.fetchall()]
    has_entry_type = "entry_type" in cols_info
    has_summary_json = "summary_json" in cols_info
    if has_entry_type and has_summary_json:
        cur.execute("""
            SELECT scan_id, start_time, end_time, duration_seconds, albums_scanned,
                   duplicates_found, artists_processed, artists_total, ai_used_count,
                   mb_used_count, ai_enabled, mb_enabled, auto_move_enabled,
                   space_saved_mb, albums_moved, status,
                   duplicate_groups_count, total_duplicates_count, broken_albums_count,
                   missing_albums_count, albums_without_artist_image, albums_without_album_image,
                   albums_without_complete_tags, albums_without_mb_id, albums_without_artist_mb_id,
                   entry_type, summary_json
            FROM scan_history
            ORDER BY start_time DESC
        """)
    elif has_entry_type:
        cur.execute("""
            SELECT scan_id, start_time, end_time, duration_seconds, albums_scanned,
                   duplicates_found, artists_processed, artists_total, ai_used_count,
                   mb_used_count, ai_enabled, mb_enabled, auto_move_enabled,
                   space_saved_mb, albums_moved, status,
                   duplicate_groups_count, total_duplicates_count, broken_albums_count,
                   missing_albums_count, albums_without_artist_image, albums_without_album_image,
                   albums_without_complete_tags, albums_without_mb_id, albums_without_artist_mb_id,
                   entry_type
            FROM scan_history
            ORDER BY start_time DESC
        """)
    elif has_summary_json:
        cur.execute("""
            SELECT scan_id, start_time, end_time, duration_seconds, albums_scanned,
                   duplicates_found, artists_processed, artists_total, ai_used_count,
                   mb_used_count, ai_enabled, mb_enabled, auto_move_enabled,
                   space_saved_mb, albums_moved, status,
                   duplicate_groups_count, total_duplicates_count, broken_albums_count,
                   missing_albums_count, albums_without_artist_image, albums_without_album_image,
                   albums_without_complete_tags, albums_without_mb_id, albums_without_artist_mb_id,
                   summary_json
            FROM scan_history
            ORDER BY start_time DESC
        """)
    else:
        cur.execute("""
            SELECT scan_id, start_time, end_time, duration_seconds, albums_scanned,
                   duplicates_found, artists_processed, artists_total, ai_used_count,
                   mb_used_count, ai_enabled, mb_enabled, auto_move_enabled,
                   space_saved_mb, albums_moved, status,
                   duplicate_groups_count, total_duplicates_count, broken_albums_count,
                   missing_albums_count, albums_without_artist_image, albums_without_album_image,
                   albums_without_complete_tags, albums_without_mb_id, albums_without_artist_mb_id
            FROM scan_history
            ORDER BY start_time DESC
        """)
    rows = cur.fetchall()
    con.close()

    history = []
    for row in rows:
        base = 25 if has_entry_type else 24
        entry = {
            "scan_id": row[0],
            "start_time": row[1],
            "end_time": row[2],
            "duration_seconds": row[3],
            "albums_scanned": row[4] or 0,
            "duplicates_found": row[5] or 0,
            "artists_processed": row[6] or 0,
            "artists_total": row[7] or 0,
            "ai_used_count": row[8] or 0,
            "mb_used_count": row[9] or 0,
            "ai_enabled": bool(row[10]),
            "mb_enabled": bool(row[11]),
            "auto_move_enabled": bool(row[12]),
            "space_saved_mb": row[13] or 0,
            "albums_moved": row[14] or 0,
            "status": row[15] or "completed",
            "duplicate_groups_count": row[16] or 0 if len(row) > 16 else 0,
            "total_duplicates_count": row[17] or 0 if len(row) > 17 else 0,
            "broken_albums_count": row[18] or 0 if len(row) > 18 else 0,
            "missing_albums_count": row[19] or 0 if len(row) > 19 else 0,
            "albums_without_artist_image": row[20] or 0 if len(row) > 20 else 0,
            "albums_without_album_image": row[21] or 0 if len(row) > 21 else 0,
            "albums_without_complete_tags": row[22] or 0 if len(row) > 22 else 0,
            "albums_without_mb_id": row[23] or 0 if len(row) > 23 else 0,
            "albums_without_artist_mb_id": row[24] or 0 if len(row) > 24 else 0,
        }
        if has_entry_type and len(row) > 25:
            entry["entry_type"] = row[25] or "scan"
        else:
            entry["entry_type"] = "scan"
        if has_summary_json and len(row) > base + 1:
            try:
                raw = row[base + 1]
                entry["summary_json"] = json.loads(raw) if raw else None
            except (TypeError, ValueError):
                entry["summary_json"] = None
        else:
            entry["summary_json"] = None
        history.append(entry)

    return jsonify(history)

@app.delete("/api/scan-history")
def api_scan_history_clear():
    """Delete all scan history entries (and related scan_editions). Requires confirmation from client."""
    import sqlite3
    con = sqlite3.connect(str(STATE_DB_FILE))
    cur = con.cursor()
    try:
        cur.execute("DELETE FROM scan_editions")
        cur.execute("DELETE FROM scan_history")
        con.commit()
        deleted = cur.rowcount if hasattr(cur, "rowcount") else 0
    finally:
        con.close()
    return jsonify({"status": "ok", "message": "Scan history cleared."})

def add_broken_album_to_lidarr(artist_name: str, album_id: int, musicbrainz_release_group_id: str, album_title: str) -> bool:
    """
    Add a broken album to Lidarr for re-download.
    Returns True if successful, False otherwise.
    """
    if not LIDARR_URL or not LIDARR_API_KEY:
        logging.warning("Lidarr not configured (LIDARR_URL or LIDARR_API_KEY missing)")
        return False
    
    try:
        # First, search for the artist in Lidarr
        search_url = f"{LIDARR_URL.rstrip('/')}/api/v1/artist/lookup"
        headers = {"X-Api-Key": LIDARR_API_KEY}
        
        # Try to find artist by MusicBrainz ID or name
        # We need the artist MBID - for now, search by name
        search_params = {"term": artist_name}
        response = requests.get(search_url, headers=headers, params=search_params, timeout=10)
        
        if response.status_code != 200:
            logging.error("Lidarr artist search failed: %s", response.text)
            return False
        
        artists = response.json()
        if not artists:
            logging.warning("Artist '%s' not found in Lidarr", artist_name)
            return False
        
        # Use first matching artist
        lidarr_artist = artists[0]
        lidarr_artist_id = lidarr_artist.get('id')
        
        if not lidarr_artist_id:
            logging.warning("Lidarr artist '%s' has no ID", artist_name)
            return False
        
        # Now add the album to Lidarr
        # First, check if album already exists
        album_lookup_url = f"{LIDARR_URL.rstrip('/')}/api/v1/album/lookup"
        album_params = {"term": f"mbid:{musicbrainz_release_group_id}"}
        album_response = requests.get(album_lookup_url, headers=headers, params=album_params, timeout=10)
        
        if album_response.status_code == 200:
            albums = album_response.json()
            if albums:
                # Album found, add it to the artist
                album_data = albums[0]
                add_album_url = f"{LIDARR_URL.rstrip('/')}/api/v1/album"
                add_payload = {
                    "artistId": lidarr_artist_id,
                    "album": album_data,
                    "addOptions": {
                        "searchForMissingAlbums": True,
                        "monitor": "missing"
                    }
                }
                add_response = requests.post(add_album_url, headers=headers, json=add_payload, timeout=10)
                
                if add_response.status_code in (200, 201):
                    logging.info("Successfully added broken album '%s' by '%s' to Lidarr", album_title, artist_name)
                    return True
                else:
                    logging.error("Failed to add album to Lidarr: %s", add_response.text)
                    return False
        
        logging.warning("Album with MBID %s not found in Lidarr", musicbrainz_release_group_id)
        return False
        
    except requests.exceptions.RequestException as e:
        logging.error("Lidarr API request failed: %s", e)
        return False
    except Exception as e:
        logging.error("Unexpected error adding album to Lidarr: %s", e, exc_info=True)
        return False

@app.get("/api/broken-albums")
def api_broken_albums():
    """Return list of broken albums in selected library sections only (SECTION_IDS)."""
    _reload_section_ids_from_db()
    import sqlite3
    import json
    con = sqlite3.connect(str(STATE_DB_FILE), timeout=30)
    cur = con.cursor()
    cur.execute("""
        SELECT artist, album_id, expected_track_count, actual_track_count,
               missing_indices, musicbrainz_release_group_id, detected_at, sent_to_lidarr
        FROM broken_albums
        ORDER BY detected_at DESC
    """)
    rows = cur.fetchall()
    con.close()

    # Filter by SECTION_IDS: only include albums that belong to selected library sections
    broken_albums = []
    db_conn = None
    try:
        db_conn = plex_connect()
        placeholders = ",".join("?" for _ in SECTION_IDS) if SECTION_IDS else ""
        for row in rows:
            album_id = row[1]
            if SECTION_IDS and placeholders:
                section_row = db_conn.execute(
                    f"SELECT library_section_id FROM metadata_items WHERE id = ? AND metadata_type = 9",
                    (album_id,),
                ).fetchone()
                if not section_row or section_row[0] not in SECTION_IDS:
                    continue
            title = album_title(db_conn, album_id) if db_conn else f"Album {album_id}"
            missing_indices = json.loads(row[4]) if row[4] else []
            broken_albums.append({
                "artist": row[0],
                "album_id": album_id,
                "album_title": title,
                "expected_track_count": row[2],
                "actual_track_count": row[3],
                "missing_indices": missing_indices,
                "musicbrainz_release_group_id": row[5],
                "detected_at": row[6],
                "sent_to_lidarr": bool(row[7]) if row[7] is not None else False
            })
    except Exception as e:
        logging.warning("Failed to fetch album titles for broken albums: %s", e)
        # Fallback without titles
        for row in rows:
            missing_indices = json.loads(row[4]) if row[4] else []
            broken_albums.append({
                "artist": row[0],
                "album_id": row[1],
                "album_title": f"Album {row[1]}",
                "expected_track_count": row[2],
                "actual_track_count": row[3],
                "missing_indices": missing_indices,
                "musicbrainz_release_group_id": row[5],
                "detected_at": row[6],
                "sent_to_lidarr": bool(row[7]) if row[7] is not None else False
            })
    finally:
        if db_conn:
            try:
                db_conn.close()
            except Exception:
                pass

    return jsonify(broken_albums)


def _run_incomplete_albums_scan():
    """Background job: scan library for incomplete (broken) albums only; double-check Plex vs disk; write to incomplete_album_diagnostics."""
    global SECTION_IDS, PATH_MAP
    _reload_section_ids_from_db()
    _reload_path_map_from_db()
    if not SECTION_IDS:
        with lock:
            if state.get("incomplete_scan") is not None:
                state["incomplete_scan"]["running"] = False
                state["incomplete_scan"]["error"] = "SECTION_IDS is empty. Configure libraries in Settings."
        return
    # Set running state immediately so the UI shows progress instead of "Starting scan" while we load the album list
    with lock:
        state["incomplete_scan"] = {
            "running": True,
            "run_id": None,
            "progress": 0,
            "total": 0,
            "current_artist": "",
            "current_album": "Preparing‚Ä¶",
            "count": 0,
            "error": None,
        }
    start_time = time.time()
    artists_merged = []
    total_albums = 0
    run_id = None
    try:
        db_conn = plex_connect()
        try:
            placeholders = ",".join("?" for _ in SECTION_IDS)
            total_albums = db_conn.execute(
                f"SELECT COUNT(*) FROM metadata_items "
                f"WHERE metadata_type=9 AND library_section_id IN ({placeholders})",
                SECTION_IDS,
            ).fetchone()[0]
            artists_raw = db_conn.execute(
                f"SELECT id, title FROM metadata_items "
                f"WHERE metadata_type=8 AND library_section_id IN ({placeholders})",
                SECTION_IDS,
            ).fetchall()
            artists_by_name = defaultdict(list)
            for artist_id, artist_name in artists_raw:
                name_norm = (artist_name or "").strip().lower()
                artists_by_name[name_norm].append((artist_id, artist_name))
            for name_norm, id_name_list in artists_by_name.items():
                artist_ids = [aid for aid, _ in id_name_list]
                primary_id, primary_name = id_name_list[0]
                ph = ",".join("?" for _ in artist_ids)
                album_ids_for_name = [
                    row[0] for row in db_conn.execute(
                        f"SELECT id FROM metadata_items "
                        f"WHERE metadata_type=9 AND parent_id IN ({ph})",
                        artist_ids,
                    ).fetchall()
                ]
                artists_merged.append((primary_id, primary_name, album_ids_for_name))
        finally:
            db_conn.close()

        con = sqlite3.connect(str(STATE_DB_FILE))
        cur = con.cursor()
        cur.execute("PRAGMA table_info(scan_history)")
        cols = [r[1] for r in cur.fetchall()]
        has_entry_type = "entry_type" in cols
        if has_entry_type:
            cur.execute("""
                INSERT INTO scan_history
                (start_time, albums_scanned, artists_total, ai_enabled, mb_enabled, auto_move_enabled, status,
                 duplicate_groups_count, total_duplicates_count, broken_albums_count, missing_albums_count, entry_type)
                VALUES (?, ?, ?, 0, 0, 0, 'running', 0, 0, 0, 0, 'incomplete')
            """, (start_time, total_albums, len(artists_merged)))
        else:
            cur.execute("""
                INSERT INTO scan_history
                (start_time, albums_scanned, artists_total, ai_enabled, mb_enabled, auto_move_enabled, status,
                 duplicate_groups_count, total_duplicates_count, broken_albums_count, missing_albums_count)
                VALUES (?, ?, ?, 0, 0, 0, 'running', 0, 0, 0, 0)
            """, (start_time, total_albums, len(artists_merged)))
        run_id = cur.lastrowid
        con.commit()
        con.close()

        with lock:
            if state.get("incomplete_scan"):
                state["incomplete_scan"]["run_id"] = run_id
                state["incomplete_scan"]["total"] = total_albums
                state["incomplete_scan"]["current_album"] = ""
    except Exception as e:
        logging.exception("Incomplete albums scan failed during preparation: %s", e)
        with lock:
            if state.get("incomplete_scan"):
                state["incomplete_scan"]["running"] = False
                state["incomplete_scan"]["error"] = str(e)
        return

    db_conn = plex_connect()
    processed = 0
    incomplete_count = 0
    try:
        for _primary_id, artist_name, album_ids_list in artists_merged:
            for aid in album_ids_list:
                with lock:
                    if state.get("incomplete_scan") and not state["incomplete_scan"].get("running"):
                        db_conn.close()
                        return
                tracks = get_tracks(db_conn, aid)
                if not tracks:
                    processed += 1
                    with lock:
                        if state.get("incomplete_scan"):
                            state["incomplete_scan"]["progress"] = processed
                            state["incomplete_scan"]["total"] = total_albums
                    continue
                folder = first_part_path(db_conn, aid)
                if not folder:
                    processed += 1
                    with lock:
                        if state.get("incomplete_scan"):
                            state["incomplete_scan"]["progress"] = processed
                    continue
                is_broken, _exp, actual_count, _gaps = detect_broken_album(db_conn, aid, tracks, None)
                if not is_broken:
                    processed += 1
                    with lock:
                        if state.get("incomplete_scan"):
                            state["incomplete_scan"]["progress"] = processed
                    continue
                title_str = album_title(db_conn, aid) or f"Album {aid}"
                with lock:
                    if state.get("incomplete_scan"):
                        state["incomplete_scan"]["current_artist"] = artist_name
                        state["incomplete_scan"]["current_album"] = title_str
                diag = _incomplete_album_disk_crosscheck(db_conn, artist_name, aid, tracks, folder, title_str)
                con = sqlite3.connect(str(STATE_DB_FILE))
                c = con.cursor()
                c.execute("""
                    INSERT OR REPLACE INTO incomplete_album_diagnostics
                    (run_id, artist, album_id, title_raw, folder, classification, missing_in_plex, missing_on_disk, track_titles, expected_track_count, actual_track_count, detected_at)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    run_id,
                    artist_name,
                    aid,
                    title_str,
                    str(folder),
                    diag["classification"],
                    diag["missing_in_plex"],
                    diag["missing_on_disk"],
                    diag["track_titles"],
                    diag["expected_track_count"],
                    diag["actual_track_count"],
                    time.time(),
                ))
                con.commit()
                con.close()
                incomplete_count += 1
                with lock:
                    if state.get("incomplete_scan"):
                        state["incomplete_scan"]["count"] = incomplete_count
                processed += 1
                with lock:
                    if state.get("incomplete_scan"):
                        state["incomplete_scan"]["progress"] = processed
    except Exception as e:
        logging.exception("Incomplete albums scan failed: %s", e)
        with lock:
            if state.get("incomplete_scan"):
                state["incomplete_scan"]["running"] = False
                state["incomplete_scan"]["error"] = str(e)
        db_conn.close()
        return
    finally:
        db_conn.close()

    end_time = time.time()
    duration_seconds = int(end_time - start_time)
    artists_total_val = len(artists_merged)
    con = sqlite3.connect(str(STATE_DB_FILE))
    cur = con.cursor()
    cur.execute(
        "UPDATE scan_history SET status = 'completed', end_time = ?, duration_seconds = ?, broken_albums_count = ?, artists_processed = ? WHERE scan_id = ?",
        (end_time, duration_seconds, incomplete_count, artists_total_val, run_id),
    )
    con.commit()
    con.close()
    with lock:
        if state.get("incomplete_scan"):
            state["incomplete_scan"]["running"] = False
            state["incomplete_scan"]["count"] = incomplete_count


@app.post("/api/incomplete-albums/scan")
def api_incomplete_albums_scan_start():
    """Start the incomplete-albums-only scan (Plex + disk cross-check)."""
    if not PLEX_CONFIGURED:
        return jsonify({"error": "Plex not configured"}), 503
    with lock:
        if state.get("incomplete_scan") and state["incomplete_scan"].get("running"):
            return jsonify({"error": "Incomplete scan already running", "started": False}), 409
    thread = threading.Thread(target=_run_incomplete_albums_scan, daemon=True)
    thread.start()
    return jsonify({"started": True})


@app.get("/api/incomplete-albums/scan/progress")
def api_incomplete_albums_scan_progress():
    """Return current incomplete-albums scan progress (running, run_id, progress, total, current_artist, current_album, count, error)."""
    with lock:
        inc = state.get("incomplete_scan")
    if not inc:
        return jsonify({"running": False, "run_id": None})
    return jsonify({
        "running": inc.get("running", False),
        "run_id": inc.get("run_id"),
        "progress": inc.get("progress", 0),
        "total": inc.get("total", 0),
        "current_artist": inc.get("current_artist", ""),
        "current_album": inc.get("current_album", ""),
        "count": inc.get("count", 0),
        "error": inc.get("error"),
    })


@app.get("/api/incomplete-albums/results")
def api_incomplete_albums_results():
    """Return list of incomplete album diagnostics for a run_id (default: latest run)."""
    run_id = request.args.get("run_id", type=int)
    con = sqlite3.connect(str(STATE_DB_FILE))
    cur = con.cursor()
    if run_id is None:
        cur.execute("SELECT MAX(run_id) FROM incomplete_album_diagnostics")
        row = cur.fetchone()
        run_id = row[0] if row and row[0] is not None else None
    if run_id is None:
        con.close()
        return jsonify({"run_id": None, "items": []})
    cur.execute("""
        SELECT artist, album_id, title_raw, folder, classification, missing_in_plex, missing_on_disk, expected_track_count, actual_track_count, detected_at
        FROM incomplete_album_diagnostics WHERE run_id = ? ORDER BY artist, album_id
    """, (run_id,))
    rows = cur.fetchall()
    con.close()
    items = []
    for r in rows:
        try:
            missing_plex = json.loads(r[5]) if r[5] else []
            missing_disk = json.loads(r[6]) if r[6] else []
        except (TypeError, ValueError):
            missing_plex, missing_disk = [], []
        items.append({
            "artist": r[0],
            "album_id": r[1],
            "title_raw": r[2],
            "folder": r[3],
            "classification": r[4] or "",
            "missing_in_plex": missing_plex,
            "missing_on_disk": missing_disk,
            "expected_track_count": r[7],
            "actual_track_count": r[8],
            "detected_at": r[9],
        })
    return jsonify({"run_id": run_id, "items": items})


@app.post("/api/incomplete-albums/move")
def api_incomplete_albums_move():
    """Move selected incomplete albums to the configured target dir. Body: { run_id: int, items: [ { artist, album_id }, ... ] }."""
    r = _requires_config()
    if r is not None:
        return r
    data = request.get_json() or {}
    run_id = data.get("run_id")
    items = data.get("items") or []
    if run_id is None or not items:
        return jsonify({"error": "Missing run_id or items"}), 400
    try:
        run_id = int(run_id)
    except (TypeError, ValueError):
        return jsonify({"error": "Invalid run_id"}), 400
    target_dir = get_setting("INCOMPLETE_ALBUMS_TARGET_DIR", "/dupes/incomplete_albums")
    target_path = Path(target_dir)
    target_path.mkdir(parents=True, exist_ok=True)
    con = sqlite3.connect(str(STATE_DB_FILE))
    cur = con.cursor()
    cur.execute("SELECT artist, album_id, folder FROM incomplete_album_diagnostics WHERE run_id = ?", (run_id,))
    diag_rows = { (r[0], r[1]): r[2] for r in cur.fetchall() }
    con.close()
    moved = []
    for it in items:
        artist = it.get("artist")
        album_id = it.get("album_id")
        if not artist or album_id is None:
            continue
        key = (artist, int(album_id))
        src = diag_rows.get(key)
        if not src:
            continue
        src_folder = path_for_fs_access(Path(src))
        if not src_folder.exists():
            continue
        dst = target_path / src_folder.name
        counter = 1
        while dst.exists():
            dst = target_path / f"{src_folder.name} ({counter})"
            counter += 1
        try:
            safe_move(str(src_folder), str(dst))
            moved.append({"artist": artist, "album_id": album_id, "moved_to": str(dst)})
            scan_id = run_id
            if scan_id:
                con = sqlite3.connect(str(STATE_DB_FILE))
                c = con.cursor()
                c.execute("PRAGMA table_info(scan_moves)")
                cols = [c[1] for c in c.execute("PRAGMA table_info(scan_moves)").fetchall()]
                if "move_reason" in cols:
                    c.execute(
                        """
                        INSERT INTO scan_moves
                        (scan_id, artist, album_id, original_path, moved_to_path, size_mb, moved_at, album_title, fmt_text, move_reason)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                        """,
                        (
                            scan_id,
                            artist,
                            album_id,
                            str(src_folder),
                            str(dst),
                            0,
                            time.time(),
                            it.get("title_raw", ""),
                            "incomplete",
                            "incomplete",
                        ),
                    )
                else:
                    c.execute(
                        """
                        INSERT INTO scan_moves
                        (scan_id, artist, album_id, original_path, moved_to_path, size_mb, moved_at, album_title, fmt_text)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
                        """,
                        (
                            scan_id,
                            artist,
                            album_id,
                            str(src_folder),
                            str(dst),
                            0,
                            time.time(),
                            it.get("title_raw", ""),
                            "incomplete",
                        ),
                    )
                con.commit()
                con.close()
        except Exception as e:
            logging.warning("Move incomplete album failed %s -> %s: %s", src_folder, dst, e)
    return jsonify({"moved": moved})


@app.get("/api/incomplete-albums/export/<int:run_id>")
def api_incomplete_albums_export(run_id):
    """Export incomplete album diagnostics for run_id as JSON or CSV. Query: format=json|csv (default json)."""
    fmt = (request.args.get("format") or "json").strip().lower()
    con = sqlite3.connect(str(STATE_DB_FILE))
    cur = con.cursor()
    cur.execute("""
        SELECT artist, album_id, title_raw, folder, classification, missing_in_plex, missing_on_disk, expected_track_count, actual_track_count, detected_at
        FROM incomplete_album_diagnostics WHERE run_id = ? ORDER BY artist, album_id
    """, (run_id,))
    rows = cur.fetchall()
    con.close()
    if fmt == "csv":
        import io
        import csv
        buf = io.StringIO()
        w = csv.writer(buf)
        w.writerow(["artist", "album_id", "title_raw", "folder", "classification", "missing_in_plex", "missing_on_disk", "expected_track_count", "actual_track_count", "detected_at"])
        for r in rows:
            w.writerow(list(r))
        return Response(buf.getvalue(), mimetype="text/csv", headers={"Content-Disposition": f"attachment; filename=incomplete_albums_run_{run_id}.csv"})
    # default json
    items = []
    for r in rows:
        try:
            missing_plex = json.loads(r[5]) if r[5] else []
            missing_disk = json.loads(r[6]) if r[6] else []
        except (TypeError, ValueError):
            missing_plex, missing_disk = [], []
        items.append({
            "artist": r[0], "album_id": r[1], "title_raw": r[2], "folder": r[3], "classification": r[4] or "",
            "missing_in_plex": missing_plex, "missing_on_disk": missing_disk,
            "expected_track_count": r[7], "actual_track_count": r[8], "detected_at": r[9],
        })
    return Response(
        json.dumps({"run_id": run_id, "items": items}, indent=2),
        mimetype="application/json",
        headers={"Content-Disposition": f"attachment; filename=incomplete_albums_run_{run_id}.json"},
    )


@app.get("/api/library/stats")
def api_library_stats():
    """Return library stats (artists count, albums count) for selected sections. Used by Unduper and others."""
    if _get_library_mode() == "files":
        cache_key = "library:stats"
        cached = _files_cache_get_json(cache_key)
        if cached is not None:
            return jsonify(cached)
        ok, err = _ensure_files_index_ready()
        if not ok:
            return jsonify({"error": err or "Files index unavailable"}), 503
        conn = _files_pg_connect()
        if conn is None:
            return jsonify({"error": "PostgreSQL unavailable"}), 503
        try:
            with conn.cursor() as cur:
                cur.execute("SELECT COUNT(*) FROM files_artists")
                artists = int(cur.fetchone()[0] or 0)
                cur.execute("SELECT COUNT(*) FROM files_albums")
                albums = int(cur.fetchone()[0] or 0)
                cur.execute("SELECT COUNT(*) FROM files_tracks")
                tracks = int(cur.fetchone()[0] or 0)
            payload = {"artists": artists, "albums": albums, "tracks": tracks}
            _files_cache_set_json(cache_key, payload, ttl=30)
            return jsonify(payload)
        finally:
            conn.close()

    _reload_section_ids_from_db()
    if not PLEX_CONFIGURED:
        return jsonify({"error": "Plex not configured"}), 503
    if not SECTION_IDS:
        return jsonify({"artists": 0, "albums": 0})
    placeholders = ",".join("?" for _ in SECTION_IDS)
    section_args = list(SECTION_IDS)
    artist_section_filter = f"AND art.library_section_id IN ({placeholders})"
    album_section_filter = f"AND alb.library_section_id IN ({placeholders})"
    db_conn = plex_connect()
    # Artists with at least one album in selected sections
    artist_count_row = db_conn.execute(f"""
        SELECT COUNT(DISTINCT art.id)
        FROM metadata_items art
        INNER JOIN metadata_items alb ON alb.parent_id = art.id AND alb.metadata_type = 9
            {album_section_filter}
        WHERE art.metadata_type = 8
            {artist_section_filter}
    """, section_args + section_args).fetchone()
    artists = (artist_count_row[0] if artist_count_row else 0) or 0
    # Albums in selected sections
    album_count_row = db_conn.execute(f"""
        SELECT COUNT(DISTINCT alb.id)
        FROM metadata_items alb
        WHERE alb.metadata_type = 9
            AND alb.library_section_id IN ({placeholders})
    """, section_args).fetchone()
    albums = (album_count_row[0] if album_count_row else 0) or 0
    db_conn.close()
    return jsonify({"artists": artists, "albums": albums})


@app.get("/api/library/stats/library")
def api_library_stats_library():
    """Return library-wide distributions for charts (Files mode only)."""
    if _get_library_mode() != "files":
        return jsonify({"error": "Files mode required"}), 400
    cache_key = "library:stats:library"
    cached = _files_cache_get_json(cache_key)
    if cached is not None:
        return jsonify(cached)
    ok, err = _ensure_files_index_ready()
    if not ok:
        return jsonify({"error": err or "Files index unavailable"}), 503
    conn = _files_pg_connect()
    if conn is None:
        return jsonify({"error": "PostgreSQL unavailable"}), 503
    try:
        with conn.cursor() as cur:
            cur.execute("SELECT COUNT(*) FROM files_artists")
            artists = int((cur.fetchone() or [0])[0] or 0)
            cur.execute("SELECT COUNT(*) FROM files_albums")
            albums = int((cur.fetchone() or [0])[0] or 0)
            cur.execute("SELECT COUNT(*) FROM files_tracks")
            tracks = int((cur.fetchone() or [0])[0] or 0)

            cur.execute(
                """
                SELECT COALESCE(year, 0) AS year, COUNT(*) AS c
                FROM files_albums
                WHERE COALESCE(year, 0) > 0
                GROUP BY COALESCE(year, 0)
                ORDER BY year ASC
                """
            )
            years = [{"year": int(r[0] or 0), "count": int(r[1] or 0)} for r in cur.fetchall() if int(r[0] or 0) > 0]

            # Growth: albums by month (created_at).
            cur.execute(
                """
                SELECT to_char(date_trunc('month', created_at), 'YYYY-MM') AS ym, COUNT(*) AS c
                FROM files_albums
                GROUP BY ym
                ORDER BY ym ASC
                """
            )
            growth = [{"month": str(r[0] or ""), "count": int(r[1] or 0)} for r in cur.fetchall() if str(r[0] or "").strip()]

            # Formats distribution (album-level).
            cur.execute(
                """
                SELECT UPPER(TRIM(COALESCE(format, ''))) AS fmt, COUNT(*) AS c
                FROM files_albums
                GROUP BY UPPER(TRIM(COALESCE(format, '')))
                ORDER BY c DESC, fmt ASC
                LIMIT 40
                """
            )
            formats = [{"format": str(r[0] or "").strip() or "‚Äî", "count": int(r[1] or 0)} for r in cur.fetchall()]

            # Cover + lossless quality.
            cur.execute("SELECT SUM(CASE WHEN has_cover THEN 1 ELSE 0 END), SUM(CASE WHEN is_lossless THEN 1 ELSE 0 END) FROM files_albums")
            row = cur.fetchone() or [0, 0]
            with_cover = int(row[0] or 0)
            lossless = int(row[1] or 0)

            # Genre distribution: prefer tags_json (multi-genre), fall back to album.genre.
            cur.execute(
                """
                SELECT lower(trim(g.value)) AS genre, COUNT(*) AS c
                FROM files_albums alb
                JOIN LATERAL jsonb_array_elements_text(COALESCE(NULLIF(alb.tags_json, ''), '[]')::jsonb) AS g(value) ON TRUE
                WHERE COALESCE(trim(g.value), '') <> ''
                GROUP BY lower(trim(g.value))
                ORDER BY c DESC, genre ASC
                LIMIT 80
                """
            )
            genre_rows = cur.fetchall()
            genres = [{"genre": str(r[0] or "").strip(), "count": int(r[1] or 0)} for r in genre_rows if str(r[0] or "").strip()]
            if not genres:
                cur.execute(
                    """
                    SELECT lower(trim(COALESCE(genre, ''))) AS genre, COUNT(*) AS c
                    FROM files_albums
                    WHERE COALESCE(trim(genre), '') <> ''
                    GROUP BY lower(trim(COALESCE(genre, '')))
                    ORDER BY c DESC, genre ASC
                    LIMIT 80
                    """
                )
                genres = [{"genre": str(r[0] or "").strip(), "count": int(r[1] or 0)} for r in cur.fetchall() if str(r[0] or "").strip()]

            # Labels distribution.
            cur.execute(
                """
                SELECT TRIM(COALESCE(label, '')) AS label, COUNT(*) AS c
                FROM files_albums
                WHERE COALESCE(trim(label), '') <> ''
                GROUP BY TRIM(COALESCE(label, ''))
                ORDER BY c DESC, label ASC
                LIMIT 80
                """
            )
            labels = [{"label": str(r[0] or "").strip(), "count": int(r[1] or 0)} for r in cur.fetchall() if str(r[0] or "").strip()]

        payload = {
            "artists": artists,
            "albums": albums,
            "tracks": tracks,
            "years": years,
            "growth": growth,
            "genres": genres,
            "labels": labels,
            "formats": formats,
            "quality": {
                "with_cover": with_cover,
                "without_cover": max(0, albums - with_cover),
                "lossless": lossless,
                "lossy": max(0, albums - lossless),
            },
        }
        _files_cache_set_json(cache_key, payload, ttl=30)
        return jsonify(payload)
    finally:
        conn.close()


@app.get("/api/library/discover")
def api_library_discover():
    """Personalized discovery feed (Files mode only).

    Returns a set of album carousels driven by listening telemetry:
    - genres you listen to
    - your top artists
    - similar artists (from cached artist profiles)
    - labels you tend to play
    """
    if _get_library_mode() != "files":
        return jsonify({"error": "Files mode required"}), 400

    days = max(7, min(365, _parse_int_loose(request.args.get("days"), 90)))
    limit = max(6, min(36, _parse_int_loose(request.args.get("limit"), 18)))
    refresh = bool(_parse_bool(request.args.get("refresh")))

    cache_key = f"library:discover:{days}:{limit}"
    if not refresh:
        cached = _files_cache_get_json(cache_key)
        if cached is not None:
            return jsonify(cached)

    ok, err = _ensure_files_index_ready()
    if not ok:
        return jsonify({"error": err or "Files index unavailable"}), 503
    conn = _files_pg_connect()
    if conn is None:
        return jsonify({"error": "PostgreSQL unavailable"}), 503

    def _split_genre_string(raw: str) -> list[str]:
        parts = []
        try:
            parts = _split_genre_values(raw or "")
        except Exception:
            parts = []
        out: list[str] = []
        seen: set[str] = set()
        for p in parts:
            v = re.sub(r"\s+", " ", str(p or "").strip())
            if not v:
                continue
            k = v.lower()
            if k in seen:
                continue
            seen.add(k)
            out.append(v)
        return out

    def _album_rows_to_payload(rows: list[tuple]) -> list[dict]:
        base_url = request.url_root.rstrip("/")
        albums_out: list[dict] = []
        for album_id, title, year, genre, label, tags_json, track_count, fmt, is_lossless, has_cover, artist_id, artist_name, short_desc, profile_source in rows:
            aid = int(album_id or 0)
            arid = int(artist_id or 0)
            thumb = f"{base_url}/api/library/files/album/{aid}/cover?size=512" if bool(has_cover) else None
            short_desc_clean = (short_desc or "").strip()
            genres_list: list[str] = []
            try:
                tags_list = json.loads(tags_json) if tags_json else []
                if isinstance(tags_list, list):
                    for t in tags_list:
                        v = str(t or "").strip()
                        if v:
                            genres_list.append(v)
            except Exception:
                genres_list = []
            if not genres_list:
                genres_list = _split_genre_string(genre or "")
            if genres_list:
                seen = set()
                deduped = []
                for g in genres_list:
                    gg = re.sub(r"\s+", " ", (g or "").strip())
                    if not gg:
                        continue
                    key = gg.lower()
                    if key in seen:
                        continue
                    seen.add(key)
                    deduped.append(gg)
                genres_list = deduped[:20]
            albums_out.append(
                {
                    "album_id": aid,
                    "title": title or "",
                    "year": int(year or 0) or None,
                    "genre": (genre or "").strip() or None,
                    "genres": genres_list,
                    "label": (label or "").strip() or None,
                    "track_count": int(track_count or 0),
                    "format": (fmt or "").strip() or None,
                    "is_lossless": bool(is_lossless),
                    "thumb": thumb,
                    "artist_id": arid,
                    "artist_name": artist_name or "",
                    "short_description": short_desc_clean or None,
                    "profile_source": (profile_source or "").strip() or None,
                }
            )
        return albums_out

    def _fetch_random_albums(where_sql: str, params: list, n: int, exclude_album_ids: set[int]) -> list[dict]:
        if n <= 0:
            return []
        with conn.cursor() as cur:
            cur.execute(
                f"""
                SELECT
                    alb.id,
                    alb.title,
                    COALESCE(alb.year, 0) AS year,
                    COALESCE(alb.genre, '') AS genre,
                    COALESCE(alb.label, '') AS label,
                    COALESCE(alb.tags_json, '[]') AS tags_json,
                    alb.track_count,
                    COALESCE(alb.format, '') AS format,
                    alb.is_lossless,
                    alb.has_cover,
                    ar.id AS artist_id,
                    ar.name AS artist_name,
                    COALESCE(pr.short_description, '') AS short_description,
                    COALESCE(pr.source, '') AS profile_source
                FROM files_albums alb
                JOIN files_artists ar ON ar.id = alb.artist_id
                LEFT JOIN files_album_profiles pr
                       ON pr.artist_norm = ar.name_norm
                      AND pr.title_norm = alb.title_norm
                WHERE {where_sql}
                ORDER BY random()
                LIMIT %s
                """,
                [*params, int(n)],
            )
            rows = cur.fetchall()
        albums = _album_rows_to_payload(rows)
        # Defensive: ensure no duplicates in caller exclude set.
        out = []
        for a in albums:
            aid = int(a.get("album_id") or 0)
            if aid <= 0:
                continue
            if aid in exclude_album_ids:
                continue
            out.append(a)
            exclude_album_ids.add(aid)
        return out

    try:
        generated_at = int(time.time())
        with conn.cursor() as cur:
            # Prefer explicit playback telemetry; fall back to recommendation telemetry when empty.
            cur.execute(
                """
                SELECT COUNT(*)
                FROM files_playback_events
                WHERE user_id = 1
                  AND created_at >= NOW() - (%s || ' days')::interval
                  AND played_seconds >= 12
                """,
                (int(days),),
            )
            playback_count = int((cur.fetchone() or [0])[0] or 0)
            use_reco = playback_count <= 0
            # NOTE: files_reco_events has no user_id (single-user), but does store played_seconds + created_at.
            ev_table = "files_reco_events" if use_reco else "files_playback_events"
            ev_user_filter = "1=1" if use_reco else "e.user_id = 1"

            # Recently played albums (to reduce repetition).
            cur.execute(
                f"""
                SELECT t.album_id
                FROM {ev_table} e
                JOIN files_tracks t ON t.id = e.track_id
                WHERE {ev_user_filter}
                  AND e.created_at >= NOW() - (%s || ' days')::interval
                  AND COALESCE(e.played_seconds, 0) >= 12
                ORDER BY e.created_at DESC
                LIMIT 600
                """,
                (int(days),),
            )
            recent_album_rows = cur.fetchall()

            cur.execute(
                f"""
                SELECT
                    a.id AS artist_id,
                    a.name AS artist_name,
                    a.name_norm AS artist_norm,
                    SUM(e.played_seconds) AS sec
                FROM {ev_table} e
                JOIN files_tracks t ON t.id = e.track_id
                JOIN files_albums alb ON alb.id = t.album_id
                JOIN files_artists a ON a.id = alb.artist_id
                WHERE {ev_user_filter}
                  AND e.created_at >= NOW() - (%s || ' days')::interval
                  AND COALESCE(e.played_seconds, 0) >= 12
                GROUP BY a.id, a.name, a.name_norm
                ORDER BY sec DESC, a.name ASC
                LIMIT 10
                """,
                (int(days),),
            )
            top_artist_rows = cur.fetchall()

            cur.execute(
                f"""
                SELECT
                    lower(trim(COALESCE(alb.label, ''))) AS label_norm,
                    MIN(trim(COALESCE(alb.label, ''))) AS label,
                    SUM(e.played_seconds) AS sec
                FROM {ev_table} e
                JOIN files_tracks t ON t.id = e.track_id
                JOIN files_albums alb ON alb.id = t.album_id
                WHERE {ev_user_filter}
                  AND e.created_at >= NOW() - (%s || ' days')::interval
                  AND COALESCE(e.played_seconds, 0) >= 12
                  AND COALESCE(trim(alb.label), '') <> ''
                GROUP BY lower(trim(COALESCE(alb.label, '')))
                ORDER BY sec DESC, label ASC
                LIMIT 10
                """,
                (int(days),),
            )
            top_label_rows = cur.fetchall()

            # Album-level rows to compute genre preference (multi-genre albums: split seconds across tags).
            cur.execute(
                f"""
                SELECT
                    alb.id AS album_id,
                    COALESCE(alb.tags_json, '[]') AS tags_json,
                    COALESCE(alb.genre, '') AS genre,
                    SUM(e.played_seconds) AS sec
                FROM {ev_table} e
                JOIN files_tracks t ON t.id = e.track_id
                JOIN files_albums alb ON alb.id = t.album_id
                WHERE {ev_user_filter}
                  AND e.created_at >= NOW() - (%s || ' days')::interval
                  AND COALESCE(e.played_seconds, 0) >= 12
                GROUP BY alb.id, alb.tags_json, alb.genre
                ORDER BY sec DESC
                LIMIT 1200
                """,
                (int(days),),
            )
            album_genre_rows = cur.fetchall()

            cur.execute(
                f"""
                SELECT
                    COALESCE(alb.year, 0) AS year,
                    SUM(e.played_seconds) AS sec
                FROM {ev_table} e
                JOIN files_tracks t ON t.id = e.track_id
                JOIN files_albums alb ON alb.id = t.album_id
                WHERE {ev_user_filter}
                  AND e.created_at >= NOW() - (%s || ' days')::interval
                  AND COALESCE(e.played_seconds, 0) >= 12
                  AND alb.year IS NOT NULL AND alb.year > 0
                GROUP BY COALESCE(alb.year, 0)
                ORDER BY sec DESC, year DESC
                LIMIT 10
                """,
                (int(days),),
            )
            year_rows = cur.fetchall()

            # Fallback seeds: liked artists (when listening telemetry is empty).
            cur.execute(
                """
                SELECT l.entity_id, a.name, a.name_norm, EXTRACT(EPOCH FROM l.updated_at)::BIGINT AS ts
                FROM files_entity_likes l
                JOIN files_artists a ON a.id = l.entity_id
                WHERE l.entity_type = 'artist' AND l.liked = TRUE
                ORDER BY l.updated_at DESC, l.entity_id DESC
                LIMIT 10
                """
            )
            liked_artist_rows = cur.fetchall()

        recent_album_ids: set[int] = set()
        for r in recent_album_rows:
            try:
                aid = int(r[0] or 0)
            except Exception:
                aid = 0
            if aid > 0:
                recent_album_ids.add(aid)

        top_artists = [
            {"artist_id": int(r[0] or 0), "artist_name": (r[1] or ""), "artist_norm": (r[2] or ""), "seconds": int(r[3] or 0)}
            for r in top_artist_rows
            if int(r[0] or 0) > 0
        ]
        liked_artists = [
            {"artist_id": int(r[0] or 0), "artist_name": (r[1] or ""), "artist_norm": (r[2] or ""), "updated_at": int(r[3] or 0)}
            for r in liked_artist_rows
            if int(r[0] or 0) > 0
        ]
        top_labels = [
            {"label": (r[1] or "").strip(), "seconds": int(r[2] or 0)}
            for r in top_label_rows
            if str(r[1] or "").strip()
        ]

        # Genre preference map.
        genre_seconds: dict[str, float] = {}
        for album_id, tags_json, genre_raw, sec in album_genre_rows:
            seconds = float(sec or 0)
            if seconds <= 0:
                continue
            tags_list: list[str] = []
            try:
                t = json.loads(tags_json or "[]") if tags_json else []
                if isinstance(t, list):
                    tags_list = [re.sub(r"\s+", " ", str(x or "").strip()) for x in t]
                    tags_list = [x for x in tags_list if x]
            except Exception:
                tags_list = []
            if not tags_list:
                tags_list = _split_genre_string(genre_raw or "")
            tags_list = [x for x in tags_list if x]
            if not tags_list:
                continue
            share = seconds / max(1.0, float(len(tags_list)))
            for g in tags_list[:12]:
                k = g.lower()
                genre_seconds[k] = genre_seconds.get(k, 0.0) + share

        top_genres = sorted(genre_seconds.items(), key=lambda kv: (-kv[1], kv[0]))[:10]
        top_genre_names = [g for g, _ in top_genres if g]
        top_years = sorted([(int(y or 0), float(sec or 0.0)) for y, sec in (year_rows or []) if int(y or 0) > 0], key=lambda kv: (-kv[1], -kv[0]))
        top_year = int(top_years[0][0]) if top_years else 0

        # Build carousels (avoid duplicates across sections).
        used_album_ids: set[int] = set(recent_album_ids)
        sections: list[dict] = []

        # 1) Genres you listen to.
        if top_genre_names:
            g = top_genre_names[0]
            albums = _fetch_random_albums(
                """
                NOT (alb.id = ANY(%s))
                AND EXISTS (
                    SELECT 1
                    FROM jsonb_array_elements_text(COALESCE(alb.tags_json, '[]')::jsonb) AS gg(value)
                    WHERE lower(trim(gg.value)) = %s
                )
                """,
                [list(used_album_ids), str(g)],
                limit,
                used_album_ids,
            )
            if albums:
                sections.append(
                    {
                        "key": "genre",
                        "title": f"More in {g}",
                        "reason": f"Because you listen to {g}.",
                        "seed": {"genre": g},
                        "albums": albums,
                    }
                )

        # 2) Your top artists (or liked artists).
        seed_artists = [a for a in top_artists[:4]]
        if not seed_artists and liked_artists:
            seed_artists = [{"artist_id": a["artist_id"], "artist_name": a["artist_name"], "artist_norm": a["artist_norm"], "seconds": 0} for a in liked_artists[:3]]
        if seed_artists:
            artist_ids = [int(a["artist_id"]) for a in seed_artists if int(a.get("artist_id") or 0) > 0]
            if artist_ids:
                label_artist = seed_artists[0]["artist_name"]
                albums = _fetch_random_albums(
                    "NOT (alb.id = ANY(%s)) AND alb.artist_id = ANY(%s)",
                    [list(used_album_ids), artist_ids],
                    limit,
                    used_album_ids,
                )
                if albums:
                    sections.append(
                        {
                            "key": "artists",
                            "title": "Because you play these artists",
                            "reason": f"Because you often listen to {label_artist}.",
                            "seed": {"artist_ids": artist_ids},
                            "albums": albums,
                        }
                    )

        # 3) Similar artists (from cached profile of your top artist).
        if top_artists:
            seed = top_artists[0]
            artist_norm = (seed.get("artist_norm") or "").strip()
            artist_name = (seed.get("artist_name") or "").strip()
            base_url = request.url_root.rstrip("/")
            sim_ids: list[int] = []
            try:
                prof = _files_get_artist_profile_cached(artist_name, artist_norm)
                sim = prof.get("similar_artists") if isinstance(prof, dict) else []
                if isinstance(sim, list) and sim:
                    sim = _files_attach_similar_artist_refs(conn, sim, base_url)
                    for it in sim:
                        if not isinstance(it, dict):
                            continue
                        sid = _parse_int_loose(it.get("artist_id"), 0)
                        if sid > 0:
                            sim_ids.append(int(sid))
                # De-dupe while preserving order.
                seen = set()
                sim_ids = [x for x in sim_ids if not (x in seen or seen.add(x))]
                sim_ids = sim_ids[:20]
            except Exception:
                sim_ids = []
            if sim_ids:
                albums = _fetch_random_albums(
                    "NOT (alb.id = ANY(%s)) AND alb.artist_id = ANY(%s)",
                    [list(used_album_ids), sim_ids],
                    limit,
                    used_album_ids,
                )
                if albums:
                    sections.append(
                        {
                            "key": "similar",
                            "title": "From similar artists",
                            "reason": f"Artists similar to {artist_name}.",
                            "seed": {"artist_id": int(seed.get("artist_id") or 0), "similar_artist_ids": sim_ids[:10]},
                            "albums": albums,
                        }
                    )

        # 4) Labels you tend to play.
        if top_labels:
            label = (top_labels[0].get("label") or "").strip()
            if label:
                albums = _fetch_random_albums(
                    "NOT (alb.id = ANY(%s)) AND lower(trim(COALESCE(alb.label, ''))) = lower(%s)",
                    [list(used_album_ids), label],
                    limit,
                    used_album_ids,
                )
                if albums:
                    sections.append(
                        {
                            "key": "labels",
                            "title": f"More from {label}",
                            "reason": f"Because you often play releases on {label}.",
                            "seed": {"label": label},
                            "albums": albums,
                        }
                    )

        # 5) Most played year.
        if top_year and int(top_year) > 0:
            albums = _fetch_random_albums(
                "NOT (alb.id = ANY(%s)) AND COALESCE(alb.year, 0) = %s",
                [list(used_album_ids), int(top_year)],
                limit,
                used_album_ids,
            )
            if albums:
                sections.append(
                    {
                        "key": "year",
                        "title": f"Most played in {int(top_year)}",
                        "reason": f"Because you often listen to music from {int(top_year)}.",
                        "seed": {"year": int(top_year)},
                        "albums": albums,
                    }
                )

        # Fallback: no listening telemetry yet (or empty library signals). Show random picks so Discover isn't empty.
        if not sections:
            albums = _fetch_random_albums(
                "NOT (alb.id = ANY(%s))",
                [list(used_album_ids)],
                limit,
                used_album_ids,
            )
            if albums:
                sections.append(
                    {
                        "key": "random",
                        "title": "Random picks",
                        "reason": "Start listening to personalize Discover. For now, here are random albums from your library.",
                        "albums": albums,
                    }
                )

        payload = {"days": days, "limit": limit, "generated_at": generated_at, "sections": sections}
        _files_cache_set_json(cache_key, payload, ttl=30)
        return jsonify(payload)
    finally:
        conn.close()


@app.get("/api/library/albums-with-parenthetical-names")
def api_library_albums_with_parenthetical_names():
    """Return albums whose folder name (or Plex title) has removable parenthetical suffixes like (flac), (mp3), (EP)."""
    if _get_library_mode() == "files":
        ok, err = _ensure_files_index_ready()
        if not ok:
            return jsonify({"albums": [], "error": err or "Files index unavailable"}), 503
        conn = _files_pg_connect()
        if conn is None:
            return jsonify({"albums": [], "error": "PostgreSQL unavailable"}), 503
        try:
            with conn.cursor() as cur:
                cur.execute(
                    """
                    SELECT alb.id, alb.title, alb.folder_path, art.name
                    FROM files_albums alb
                    JOIN files_artists art ON art.id = alb.artist_id
                    ORDER BY art.name, alb.title
                    """
                )
                rows = cur.fetchall()
            out = []
            for album_id, title, folder_path_raw, artist_name in rows:
                if not folder_path_raw:
                    continue
                folder = path_for_fs_access(Path(folder_path_raw))
                current_name = folder.name
                proposed_name = strip_parenthetical_suffixes(current_name)
                if not proposed_name or proposed_name == current_name:
                    continue
                proposed_path = folder.parent / proposed_name
                out.append({
                    "album_id": int(album_id),
                    "artist": artist_name or "",
                    "title": title or current_name,
                    "current_path": str(folder),
                    "proposed_path": str(proposed_path),
                    "current_name": current_name,
                    "proposed_name": proposed_name,
                })
            return jsonify({"albums": out})
        finally:
            conn.close()

    _reload_section_ids_from_db()
    _reload_path_map_from_db()
    if not PLEX_CONFIGURED or not SECTION_IDS:
        return jsonify({"albums": []})
    ph = ",".join("?" for _ in SECTION_IDS)
    db_conn = plex_connect()
    try:
        rows = db_conn.execute(f"""
            SELECT alb.id, alb.title, alb.parent_id
            FROM metadata_items alb
            WHERE alb.metadata_type = 9 AND alb.library_section_id IN ({ph})
        """, list(SECTION_IDS)).fetchall()
        artist_cache = {}
        out = []
        for album_id, title, parent_id in rows:
            artist_name = ""
            if parent_id:
                if parent_id not in artist_cache:
                    r = db_conn.execute("SELECT title FROM metadata_items WHERE id = ?", (parent_id,)).fetchone()
                    artist_cache[parent_id] = (r[0] or "").strip() if r else ""
                artist_name = artist_cache[parent_id]
            folder = first_part_path(db_conn, album_id)
            if not folder or not folder.exists():
                continue
            current_name = folder.name
            proposed_name = strip_parenthetical_suffixes(current_name)
            if not proposed_name or proposed_name == current_name:
                continue
            proposed_path = folder.parent / proposed_name
            if proposed_path == folder:
                continue
            out.append({
                "album_id": album_id,
                "artist": artist_name,
                "title": title or current_name,
                "current_path": str(folder),
                "proposed_path": str(proposed_path),
                "current_name": current_name,
                "proposed_name": proposed_name,
            })
        return jsonify({"albums": out})
    finally:
        db_conn.close()


@app.post("/api/library/normalize-album-names")
def api_library_normalize_album_names():
    """Rename album folders by removing parenthetical format/version suffixes (e.g. (flac), (EP)). Body: { album_ids: number[] } or empty for all from GET list."""
    if _get_library_mode() == "files":
        data = request.get_json() or {}
        album_ids = data.get("album_ids")
        if album_ids is not None and not isinstance(album_ids, list):
            return jsonify({"error": "album_ids must be an array"}), 400
        ok, err = _ensure_files_index_ready()
        if not ok:
            return jsonify({"error": err or "Files index unavailable"}), 503
        conn = _files_pg_connect()
        if conn is None:
            return jsonify({"error": "PostgreSQL unavailable"}), 503
        try:
            with conn.cursor() as cur:
                if album_ids:
                    placeholders = ",".join(["%s"] * len(album_ids))
                    cur.execute(
                        f"""
                        SELECT id, folder_path
                        FROM files_albums
                        WHERE id IN ({placeholders})
                        """,
                        tuple(int(x) for x in album_ids),
                    )
                else:
                    cur.execute("SELECT id, folder_path FROM files_albums")
                rows = cur.fetchall()
        finally:
            conn.close()

        renamed = []
        errors = []
        for album_id, folder_path_raw in rows:
            if not folder_path_raw:
                continue
            folder = path_for_fs_access(Path(folder_path_raw))
            if not folder.exists():
                errors.append({"album_id": int(album_id), "message": "Folder not found"})
                continue
            current_name = folder.name
            proposed_name = strip_parenthetical_suffixes(current_name)
            if not proposed_name or proposed_name == current_name:
                continue
            proposed_path = folder.parent / proposed_name
            if proposed_path.exists():
                errors.append({"album_id": int(album_id), "path": str(proposed_path), "message": "Target path already exists"})
                continue
            try:
                folder.rename(proposed_path)
                renamed.append({"album_id": int(album_id), "from": str(folder), "to": str(proposed_path)})
            except OSError as e:
                errors.append({"album_id": int(album_id), "message": str(e)})
        if renamed:
            _trigger_files_index_rebuild_async(reason="normalize_album_names")
        return jsonify({"renamed": renamed, "errors": errors})

    _reload_path_map_from_db()
    data = request.get_json() or {}
    album_ids = data.get("album_ids")
    if album_ids is not None and not isinstance(album_ids, list):
        return jsonify({"error": "album_ids must be an array"}), 400
    _reload_section_ids_from_db()
    if not PLEX_CONFIGURED or not SECTION_IDS:
        return jsonify({"error": "Plex not configured"}), 503
    db_conn = plex_connect()
    try:
        if not album_ids:
            ph = ",".join("?" for _ in SECTION_IDS)
            rows = db_conn.execute(f"""
                SELECT id FROM metadata_items
                WHERE metadata_type = 9 AND library_section_id IN ({ph})
            """, list(SECTION_IDS)).fetchall()
            album_ids = [r[0] for r in rows]
        renamed = []
        errors = []
        for album_id in album_ids:
            folder = first_part_path(db_conn, album_id)
            if not folder or not folder.exists():
                errors.append({"album_id": album_id, "message": "Folder not found"})
                continue
            current_name = folder.name
            proposed_name = strip_parenthetical_suffixes(current_name)
            if not proposed_name or proposed_name == current_name:
                continue
            proposed_path = folder.parent / proposed_name
            if proposed_path == folder:
                continue
            if proposed_path.exists():
                errors.append({"album_id": album_id, "path": str(proposed_path), "message": "Target path already exists"})
                continue
            try:
                folder.rename(proposed_path)
                renamed.append({"album_id": album_id, "from": str(folder), "to": str(proposed_path)})
            except OSError as e:
                errors.append({"album_id": album_id, "message": str(e)})
        return jsonify({"renamed": renamed, "errors": errors})
    finally:
        db_conn.close()


@app.get("/api/library/artists")
def api_library_artists():
    """Return list of artists with statistics. Supports search and pagination.
    Always restricted to SECTION_IDS (selected libraries) ‚Äî CROSS_LIBRARY_DEDUPE only affects duplicate detection, not which artists are listed.
    """
    if _get_library_mode() == "files":
        search_query = request.args.get("search", "").strip()
        genre = (request.args.get("genre") or "").strip()
        label = (request.args.get("label") or "").strip()
        year = _parse_int_loose(request.args.get("year"), 0)
        limit = max(1, min(500, _parse_int_loose(request.args.get("limit"), 100)))
        offset = max(0, _parse_int_loose(request.args.get("offset"), 0))
        cache_key = f"library:artists:{search_query.lower()}:{genre.lower()}:{label.lower()}:{int(year or 0)}:{limit}:{offset}"
        cached = _files_cache_get_json(cache_key)
        if cached is not None:
            return jsonify(cached)
        ok, err = _ensure_files_index_ready()
        if not ok:
            return jsonify({"error": err or "Files index unavailable"}), 503
        conn = _files_pg_connect()
        if conn is None:
            return jsonify({"error": "PostgreSQL unavailable"}), 503
        try:
            with conn.cursor() as cur:
                where_parts = ["1=1"]
                params: list = []

                if search_query:
                    like = f"%{search_query}%"
                    where_parts.append("a.name ILIKE %s")
                    params.append(like)

                if year and int(year) > 0:
                    where_parts.append(
                        """
                        EXISTS (
                            SELECT 1
                            FROM files_albums alb
                            WHERE alb.artist_id = a.id
                              AND COALESCE(alb.year, 0) = %s
                        )
                        """
                    )
                    params.append(int(year))

                if label:
                    parts = [p.strip() for p in str(label).split(",") if p.strip()]
                    if parts:
                        where_parts.append(
                            """
                            EXISTS (
                                SELECT 1
                                FROM files_albums alb
                                WHERE alb.artist_id = a.id
                                  AND lower(trim(COALESCE(alb.label, ''))) = ANY(%s)
                            )
                            """
                        )
                        params.append([p.lower() for p in parts])

                if genre:
                    parts = [p.strip() for p in str(genre).split(",") if p.strip()]
                    if parts:
                        norms = [p.lower() for p in parts]
                        where_parts.append(
                            """
                            EXISTS (
                                SELECT 1
                                FROM files_albums alb
                                WHERE alb.artist_id = a.id
                                  AND (
                                    EXISTS (
                                        SELECT 1
                                        FROM jsonb_array_elements_text(COALESCE(alb.tags_json, '[]')::jsonb) AS g(value)
                                        WHERE lower(trim(g.value)) = ANY(%s)
                                    )
                                    OR (
                                        COALESCE(alb.tags_json, '[]') = '[]'
                                        AND lower(trim(COALESCE(alb.genre, ''))) = ANY(%s)
                                    )
                                  )
                            )
                            """
                        )
                        params.append(norms)
                        params.append(norms)

                cur.execute(
                    f"""
                    SELECT COUNT(*)
                    FROM files_artists a
                    WHERE {" AND ".join(where_parts)}
                    """,
                    params,
                )
                total = int((cur.fetchone() or [0])[0] or 0)

                if search_query:
                    try:
                        cur.execute(
                            f"""
                            SELECT
                                a.id,
                                a.name,
                                a.album_count,
                                a.broken_albums_count,
                                (a.has_image OR COALESCE(ext.image_path, '') <> '') AS has_image,
                                similarity(a.name, %s) AS score,
                                CASE WHEN lower(a.name) LIKE lower(%s) || '%%' THEN 0 ELSE 1 END AS prefix_rank
                            FROM files_artists a
                            LEFT JOIN files_external_artist_images ext ON ext.name_norm = a.name_norm
                            WHERE {" AND ".join(where_parts)}
                            ORDER BY prefix_rank ASC, score DESC, a.album_count DESC, a.name ASC
                            LIMIT %s OFFSET %s
                            """,
                            [search_query, search_query, *params, int(limit), int(offset)],
                        )
                    except Exception:
                        cur.execute(
                            f"""
                            SELECT
                                a.id,
                                a.name,
                                a.album_count,
                                a.broken_albums_count,
                                (a.has_image OR COALESCE(ext.image_path, '') <> '') AS has_image
                            FROM files_artists a
                            LEFT JOIN files_external_artist_images ext ON ext.name_norm = a.name_norm
                            WHERE {" AND ".join(where_parts)}
                            ORDER BY a.album_count DESC, a.name ASC
                            LIMIT %s OFFSET %s
                            """,
                            [*params, int(limit), int(offset)],
                        )
                else:
                    cur.execute(
                        f"""
                        SELECT
                            a.id,
                            a.name,
                            a.album_count,
                            a.broken_albums_count,
                            (a.has_image OR COALESCE(ext.image_path, '') <> '') AS has_image
                        FROM files_artists a
                        LEFT JOIN files_external_artist_images ext ON ext.name_norm = a.name_norm
                        WHERE {" AND ".join(where_parts)}
                        ORDER BY a.album_count DESC, a.name ASC
                        LIMIT %s OFFSET %s
                        """,
                        [*params, int(limit), int(offset)],
                    )
                rows = cur.fetchall()
            base_url = request.url_root.rstrip("/")
            payload = {
                "artists": [
                    {
                        "artist_id": int(r[0]),
                        "artist_name": r[1] or "",
                        "album_count": int(r[2] or 0),
                        "broken_albums_count": int(r[3] or 0),
                        "artist_thumb": f"{base_url}/api/library/files/artist/{int(r[0])}/image?size=96" if bool(r[4]) else None,
                    }
                    for r in rows
                ],
                "total": total,
                "limit": limit,
                "offset": offset,
            }
            _files_cache_set_json(cache_key, payload, ttl=30)
            return jsonify(payload)
        finally:
            conn.close()

    _reload_section_ids_from_db()
    if not PLEX_CONFIGURED:
        return jsonify({"error": "Plex not configured"}), 503
    if not SECTION_IDS:
        return jsonify({"artists": [], "total": 0, "limit": 100, "offset": 0})
    
    import sqlite3
    search_query = request.args.get("search", "").strip()
    limit = int(request.args.get("limit", 100))
    offset = int(request.args.get("offset", 0))
    
    db_conn = plex_connect()
    
    # Build search filter
    if search_query:
        search_filter = "AND art.title LIKE ? ESCAPE '\\'"
        escaped_query = search_query.replace('%', '\\%').replace('_', '\\_')
        search_args = [f"%{escaped_query}%"]
    else:
        search_filter = ""
        search_args = []
    
    # Always filter by selected library sections (SECTION_IDS) for listing ‚Äî not affected by CROSS_LIBRARY_DEDUPE
    placeholders = ",".join("?" for _ in SECTION_IDS)
    section_args = list(SECTION_IDS)
    artist_section_filter = f"AND art.library_section_id IN ({placeholders})"
    album_section_filter = f"AND alb.library_section_id IN ({placeholders})"
    
    # Get total count for pagination (artists in selected sections only)
    count_cursor = db_conn.execute(f"""
        SELECT COUNT(DISTINCT art.id)
        FROM metadata_items art
        LEFT JOIN metadata_items alb ON alb.parent_id = art.id 
            AND alb.metadata_type = 9
            {album_section_filter}
        WHERE art.metadata_type = 8
            {artist_section_filter}
            {search_filter}
        HAVING COUNT(DISTINCT alb.id) > 0
    """, section_args + section_args + search_args)
    count_row = count_cursor.fetchone() if count_cursor else None
    total_count = count_row[0] if count_row else 0

    # Get paginated artists (selected sections only)
    cursor = db_conn.execute(f"""
        SELECT 
            art.id,
            art.title as artist_name,
            COUNT(DISTINCT alb.id) as album_count
        FROM metadata_items art
        LEFT JOIN metadata_items alb ON alb.parent_id = art.id 
            AND alb.metadata_type = 9
            {album_section_filter}
        WHERE art.metadata_type = 8
            {artist_section_filter}
            {search_filter}
        GROUP BY art.id, art.title
        HAVING COUNT(DISTINCT alb.id) > 0
        ORDER BY art.title
        LIMIT ? OFFSET ?
    """, section_args + section_args + search_args + [limit, offset])
    
    artists = []
    con = sqlite3.connect(str(STATE_DB_FILE), timeout=30)
    cur = con.cursor()
    
    aggregated: dict[str, dict] = {}
    for row in cursor.fetchall():
        artist_id, artist_name, album_count = row
        # Get broken albums count
        cur.execute("SELECT COUNT(*) FROM broken_albums WHERE artist = ?", (artist_name,))
        broken_count = cur.fetchone()[0] or 0
        name_norm = (artist_name or "").strip().lower()
        if name_norm not in aggregated:
            aggregated[name_norm] = {
                "artist_id": artist_id,
                "artist_name": artist_name,
                "album_count": album_count or 0,
                "broken_albums_count": broken_count,
                "all_ids": [artist_id],
            }
        else:
            aggregated[name_norm]["album_count"] += album_count or 0
            aggregated[name_norm]["broken_albums_count"] += broken_count
            aggregated[name_norm]["all_ids"].append(artist_id)
    for data in aggregated.values():
        artists.append({
            "artist_id": data["artist_id"],
            "artist_name": data["artist_name"],
            "album_count": data["album_count"],
            "broken_albums_count": data["broken_albums_count"],
        })
    
    con.close()
    db_conn.close()
    return jsonify({
        "artists": artists,
        "total": total_count,
        "limit": limit,
        "offset": offset
    })


@app.get("/api/library/artists/suggest")
def api_library_artists_suggest():
    """Ultra-fast artist suggestions for typeahead search."""
    query = (request.args.get("q") or "").strip()
    limit = max(1, min(50, _parse_int_loose(request.args.get("limit"), 12)))
    if not query:
        return jsonify({"query": "", "artists": []})
    if _get_library_mode() != "files":
        return jsonify({"query": query, "artists": []})
    cache_key = f"library:artists:suggest:{query.lower()}:{limit}"
    cached = _files_cache_get_json(cache_key)
    if cached is not None:
        return jsonify(cached)
    ok, err = _ensure_files_index_ready()
    if not ok:
        return jsonify({"query": query, "artists": [], "error": err or "Files index unavailable"}), 503
    conn = _files_pg_connect()
    if conn is None:
        return jsonify({"query": query, "artists": [], "error": "PostgreSQL unavailable"}), 503
    try:
        with conn.cursor() as cur:
            like = f"%{query}%"
            try:
                cur.execute(
                    """
                    SELECT
                        a.id,
                        a.name,
                        a.album_count,
                        a.broken_albums_count,
                        (a.has_image OR COALESCE(ext.image_path, '') <> '') AS has_image,
                        similarity(a.name, %s) AS score,
                        CASE WHEN lower(a.name) LIKE lower(%s) || '%%' THEN 0 ELSE 1 END AS prefix_rank
                    FROM files_artists a
                    LEFT JOIN files_external_artist_images ext ON ext.name_norm = a.name_norm
                    WHERE a.name ILIKE %s
                    ORDER BY prefix_rank ASC, score DESC, a.album_count DESC, a.name ASC
                    LIMIT %s
                    """,
                    (query, query, like, limit),
                )
            except Exception:
                cur.execute(
                    """
                    SELECT
                        a.id,
                        a.name,
                        a.album_count,
                        a.broken_albums_count,
                        (a.has_image OR COALESCE(ext.image_path, '') <> '') AS has_image
                    FROM files_artists a
                    LEFT JOIN files_external_artist_images ext ON ext.name_norm = a.name_norm
                    WHERE a.name ILIKE %s
                    ORDER BY a.album_count DESC, a.name ASC
                    LIMIT %s
                    """,
                    (like, limit),
                )
            rows = cur.fetchall()
        base_url = request.url_root.rstrip("/")
        payload = {
            "query": query,
            "artists": [
                {
                    "artist_id": int(r[0]),
                    "artist_name": r[1] or "",
                    "album_count": int(r[2] or 0),
                    "broken_albums_count": int(r[3] or 0),
                    "artist_thumb": f"{base_url}/api/library/files/artist/{int(r[0])}/image?size=96" if bool(r[4]) else None,
                }
                for r in rows
            ],
        }
        _files_cache_set_json(cache_key, payload, ttl=20)
        return jsonify(payload)
    finally:
        conn.close()


@app.get("/api/library/search/suggest")
def api_library_search_suggest():
    """Unified typeahead search across artists, albums and tracks (Files mode)."""
    query = (request.args.get("q") or "").strip()
    limit = max(1, min(40, _parse_int_loose(request.args.get("limit"), 12)))
    if not query:
        return jsonify({"query": "", "items": []})
    if _get_library_mode() != "files":
        return jsonify({"query": query, "items": []})

    cache_key = f"library:search:suggest:{query.lower()}:{limit}"
    cached = _files_cache_get_json(cache_key)
    if cached is not None:
        return jsonify(cached)

    ok, err = _ensure_files_index_ready()
    if not ok:
        return jsonify({"query": query, "items": [], "error": err or "Files index unavailable"}), 503
    conn = _files_pg_connect()
    if conn is None:
        return jsonify({"query": query, "items": [], "error": "PostgreSQL unavailable"}), 503

    like = f"%{query}%"
    per_kind = max(8, min(120, limit * 4))
    base_url = request.url_root.rstrip("/")
    merged: list[dict] = []
    try:
        with conn.cursor() as cur:
            # Artists
            try:
                cur.execute(
                    """
                    SELECT
                        a.id,
                        a.name,
                        a.album_count,
                        (a.has_image OR COALESCE(ext.image_path, '') <> '') AS has_image,
                        similarity(a.name, %s) AS score,
                        CASE WHEN lower(a.name) LIKE lower(%s) || '%%' THEN 0 ELSE 1 END AS prefix_rank
                    FROM files_artists a
                    LEFT JOIN files_external_artist_images ext ON ext.name_norm = a.name_norm
                    WHERE a.name ILIKE %s
                    ORDER BY prefix_rank ASC, score DESC, a.album_count DESC, a.name ASC
                    LIMIT %s
                    """,
                    (query, query, like, per_kind),
                )
            except Exception:
                cur.execute(
                    """
                    SELECT
                        a.id,
                        a.name,
                        a.album_count,
                        (a.has_image OR COALESCE(ext.image_path, '') <> '') AS has_image,
                        0.0 AS score,
                        1 AS prefix_rank
                    FROM files_artists a
                    LEFT JOIN files_external_artist_images ext ON ext.name_norm = a.name_norm
                    WHERE a.name ILIKE %s
                    ORDER BY a.album_count DESC, a.name ASC
                    LIMIT %s
                    """,
                    (like, per_kind),
                )
            for row in cur.fetchall():
                artist_id = int(row[0] or 0)
                merged.append(
                    {
                        "type": "artist",
                        "artist_id": artist_id,
                        "title": row[1] or "",
                        "subtitle": f"{int(row[2] or 0)} album(s)",
                        "thumb": f"{base_url}/api/library/files/artist/{artist_id}/image?size=96" if bool(row[3]) else None,
                        "_score": float(row[4] or 0.0),
                        "_prefix": int(row[5] or 1),
                        "_rank": 0,
                    }
                )

            # Albums
            try:
                cur.execute(
                    """
                    SELECT
                        alb.id,
                        alb.title,
                        ar.id AS artist_id,
                        ar.name AS artist_name,
                        COALESCE(alb.year, 0) AS year,
                        alb.has_cover,
                        (similarity(alb.title, %s) * 0.78 + similarity(ar.name, %s) * 0.22) AS score,
                        CASE
                            WHEN lower(alb.title) LIKE lower(%s) || '%%' THEN 0
                            WHEN lower(ar.name) LIKE lower(%s) || '%%' THEN 1
                            ELSE 2
                        END AS prefix_rank
                    FROM files_albums alb
                    JOIN files_artists ar ON ar.id = alb.artist_id
                    WHERE alb.title ILIKE %s OR ar.name ILIKE %s
                    ORDER BY prefix_rank ASC, score DESC, alb.track_count DESC, alb.title ASC
                    LIMIT %s
                    """,
                    (query, query, query, query, like, like, per_kind),
                )
            except Exception:
                cur.execute(
                    """
                    SELECT
                        alb.id,
                        alb.title,
                        ar.id AS artist_id,
                        ar.name AS artist_name,
                        COALESCE(alb.year, 0) AS year,
                        alb.has_cover,
                        0.0 AS score,
                        2 AS prefix_rank
                    FROM files_albums alb
                    JOIN files_artists ar ON ar.id = alb.artist_id
                    WHERE alb.title ILIKE %s OR ar.name ILIKE %s
                    ORDER BY alb.track_count DESC, alb.title ASC
                    LIMIT %s
                    """,
                    (like, like, per_kind),
                )
            for row in cur.fetchall():
                album_id = int(row[0] or 0)
                artist_id = int(row[2] or 0)
                year = int(row[4] or 0)
                merged.append(
                    {
                        "type": "album",
                        "album_id": album_id,
                        "artist_id": artist_id,
                        "title": row[1] or "",
                        "subtitle": f"{row[3] or ''}{' ¬∑ ' + str(year) if year > 0 else ''}",
                        "thumb": f"{base_url}/api/library/files/album/{album_id}/cover?size=96" if bool(row[5]) else None,
                        "_score": float(row[6] or 0.0),
                        "_prefix": int(row[7] or 2),
                        "_rank": 1,
                    }
                )

            # Tracks
            try:
                cur.execute(
                    """
                    SELECT
                        tr.id,
                        tr.title,
                        tr.duration_sec,
                        tr.track_num,
                        alb.id AS album_id,
                        alb.title AS album_title,
                        ar.id AS artist_id,
                        ar.name AS artist_name,
                        alb.has_cover,
                        COALESCE(st.play_count, 0) AS play_count,
                        (
                            similarity(tr.title, %s) * 0.70 +
                            similarity(alb.title, %s) * 0.15 +
                            similarity(ar.name, %s) * 0.15
                        ) AS score,
                        CASE
                            WHEN lower(tr.title) LIKE lower(%s) || '%%' THEN 0
                            WHEN lower(alb.title) LIKE lower(%s) || '%%' THEN 1
                            WHEN lower(ar.name) LIKE lower(%s) || '%%' THEN 2
                            ELSE 3
                        END AS prefix_rank
                    FROM files_tracks tr
                    JOIN files_albums alb ON alb.id = tr.album_id
                    JOIN files_artists ar ON ar.id = alb.artist_id
                    LEFT JOIN files_reco_track_stats st ON st.track_id = tr.id
                    WHERE tr.title ILIKE %s OR alb.title ILIKE %s OR ar.name ILIKE %s
                    ORDER BY prefix_rank ASC, score DESC, play_count DESC, tr.id DESC
                    LIMIT %s
                    """,
                    (query, query, query, query, query, query, like, like, like, per_kind),
                )
            except Exception:
                cur.execute(
                    """
                    SELECT
                        tr.id,
                        tr.title,
                        tr.duration_sec,
                        tr.track_num,
                        alb.id AS album_id,
                        alb.title AS album_title,
                        ar.id AS artist_id,
                        ar.name AS artist_name,
                        alb.has_cover,
                        0 AS play_count,
                        0.0 AS score,
                        3 AS prefix_rank
                    FROM files_tracks tr
                    JOIN files_albums alb ON alb.id = tr.album_id
                    JOIN files_artists ar ON ar.id = alb.artist_id
                    WHERE tr.title ILIKE %s OR alb.title ILIKE %s OR ar.name ILIKE %s
                    ORDER BY tr.id DESC
                    LIMIT %s
                    """,
                    (like, like, like, per_kind),
                )
            for row in cur.fetchall():
                track_id = int(row[0] or 0)
                album_id = int(row[4] or 0)
                artist_id = int(row[6] or 0)
                merged.append(
                    {
                        "type": "track",
                        "track_id": track_id,
                        "album_id": album_id,
                        "artist_id": artist_id,
                        "title": row[1] or "",
                        "subtitle": f"{row[7] or ''} ¬∑ {row[5] or ''}",
                        "duration_sec": int(row[2] or 0),
                        "track_num": int(row[3] or 0),
                        "thumb": f"{base_url}/api/library/files/album/{album_id}/cover?size=96" if bool(row[8]) else None,
                        "_score": float(row[10] or 0.0),
                        "_prefix": int(row[11] or 3),
                        "_rank": 2,
                    }
                )

        merged.sort(
            key=lambda x: (
                int(x.get("_prefix", 9)),
                -float(x.get("_score", 0.0)),
                int(x.get("_rank", 9)),
                str(x.get("title", "")).lower(),
            )
        )
        items = []
        for item in merged[:limit]:
            clean = dict(item)
            clean.pop("_score", None)
            clean.pop("_prefix", None)
            clean.pop("_rank", None)
            items.append(clean)
        payload = {"query": query, "items": items}
        _files_cache_set_json(cache_key, payload, ttl=20)
        return jsonify(payload)
    finally:
        conn.close()


@app.get("/api/library/albums")
def api_library_albums():
    """List albums (Files mode). Used for Roon-like album grid and carousels."""
    if _get_library_mode() != "files":
        return jsonify({"albums": [], "total": 0, "limit": 0, "offset": 0, "error": "Files mode required"}), 400

    search_query = (request.args.get("search") or "").strip()
    genre = (request.args.get("genre") or "").strip()
    label = (request.args.get("label") or "").strip()
    year = _parse_int_loose(request.args.get("year"), 0)
    sort = (request.args.get("sort") or "recent").strip().lower()
    limit = max(1, min(240, _parse_int_loose(request.args.get("limit"), 80)))
    offset = max(0, _parse_int_loose(request.args.get("offset"), 0))

    cache_key = f"library:albums:{search_query.lower()}:{genre.lower()}:{label.lower()}:{int(year or 0)}:{sort}:{limit}:{offset}"
    cached = _files_cache_get_json(cache_key)
    if cached is not None:
        return jsonify(cached)

    ok, err = _ensure_files_index_ready()
    if not ok:
        return jsonify({"albums": [], "total": 0, "limit": limit, "offset": offset, "error": err or "Files index unavailable"}), 503
    conn = _files_pg_connect()
    if conn is None:
        return jsonify({"albums": [], "total": 0, "limit": limit, "offset": offset, "error": "PostgreSQL unavailable"}), 503

    try:
        where_parts = ["1=1"]
        params: list = []
        if search_query:
            where_parts.append("(alb.title ILIKE %s OR ar.name ILIKE %s)")
            like = f"%{search_query}%"
            params.extend([like, like])
        if year and int(year) > 0:
            where_parts.append("alb.year = %s")
            params.append(int(year))
        if genre:
            parts = [p.strip() for p in str(genre).split(",") if p.strip()]
            if parts:
                # Multi-genre support: album tags_json stores the full list (e.g. ["ambient","electronic"]).
                # We match case-insensitively against any token in that array.
                where_parts.append(
                    """
                    EXISTS (
                        SELECT 1
                        FROM jsonb_array_elements_text(COALESCE(alb.tags_json, '[]')::jsonb) AS g(value)
                        WHERE lower(trim(g.value)) = ANY(%s)
                    )
                    """
                )
                params.append([p.lower() for p in parts])
        if label:
            parts = [p.strip() for p in str(label).split(",") if p.strip()]
            if parts:
                where_parts.append("lower(COALESCE(alb.label, '')) = ANY(%s)")
                params.append([p.lower() for p in parts])

        if sort == "year_desc":
            order_sql = "ORDER BY COALESCE(alb.year, 0) DESC, alb.title ASC, alb.id DESC"
        elif sort == "alpha":
            order_sql = "ORDER BY alb.title ASC, alb.id DESC"
        elif sort == "artist":
            order_sql = "ORDER BY ar.name ASC, COALESCE(alb.year, 0) DESC, alb.title ASC, alb.id DESC"
        else:
            # recent (default): created/updated activity during index build
            order_sql = "ORDER BY alb.created_at DESC, alb.id DESC"

        with conn.cursor() as cur:
            cur.execute(
                f"""
                SELECT COUNT(*)
                FROM files_albums alb
                JOIN files_artists ar ON ar.id = alb.artist_id
                WHERE {" AND ".join(where_parts)}
                """,
                params,
            )
            total = int((cur.fetchone() or [0])[0] or 0)

            cur.execute(
                f"""
                SELECT
                    alb.id,
                    alb.title,
                    COALESCE(alb.year, 0) AS year,
                    COALESCE(alb.genre, '') AS genre,
                    COALESCE(alb.label, '') AS label,
                    COALESCE(alb.tags_json, '[]') AS tags_json,
                    alb.track_count,
                    COALESCE(alb.format, '') AS format,
                    alb.is_lossless,
                    alb.has_cover,
                    ar.id AS artist_id,
                    ar.name AS artist_name,
                    COALESCE(pr.short_description, '') AS short_description,
                    COALESCE(pr.source, '') AS profile_source
                FROM files_albums alb
                JOIN files_artists ar ON ar.id = alb.artist_id
                LEFT JOIN files_album_profiles pr
                       ON pr.artist_norm = ar.name_norm
                      AND pr.title_norm = alb.title_norm
                WHERE {" AND ".join(where_parts)}
                {order_sql}
                LIMIT %s OFFSET %s
                """,
                [*params, int(limit), int(offset)],
            )
            rows = cur.fetchall()

        base_url = request.url_root.rstrip("/")
        albums = []
        for album_id, title, year, genre, label, tags_json, track_count, fmt, is_lossless, has_cover, artist_id, artist_name, short_desc, profile_source in rows:
            aid = int(album_id or 0)
            arid = int(artist_id or 0)
            thumb = f"{base_url}/api/library/files/album/{aid}/cover?size=512" if bool(has_cover) else None
            short_desc_clean = (short_desc or "").strip()
            # Parsed list of genres for UI badges (multi-genre albums).
            genres_list: list[str] = []
            try:
                tags_list = json.loads(tags_json) if tags_json else []
                if isinstance(tags_list, list):
                    for t in tags_list:
                        v = str(t or "").strip()
                        if v:
                            genres_list.append(v)
            except Exception:
                genres_list = []
            if not genres_list:
                try:
                    genres_list = _split_genre_values(genre or "")
                except Exception:
                    genres_list = []
            # Defensive: dedupe while keeping order.
            if genres_list:
                seen = set()
                deduped = []
                for g in genres_list:
                    gg = re.sub(r"\s+", " ", (g or "").strip())
                    if not gg:
                        continue
                    key = gg.lower()
                    if key in seen:
                        continue
                    seen.add(key)
                    deduped.append(gg)
                genres_list = deduped[:20]
            albums.append(
                {
                    "album_id": aid,
                    "title": title or "",
                    "year": int(year or 0) or None,
                    "genre": (genre or "").strip() or None,
                    "genres": genres_list,
                    "label": (label or "").strip() or None,
                    "track_count": int(track_count or 0),
                    "format": (fmt or "").strip() or None,
                    "is_lossless": bool(is_lossless),
                    "thumb": thumb,
                    "artist_id": arid,
                    "artist_name": artist_name or "",
                    "short_description": short_desc_clean or None,
                    "profile_source": (profile_source or "").strip() or None,
                }
            )

        payload = {"albums": albums, "total": total, "limit": limit, "offset": offset}
        _files_cache_set_json(cache_key, payload, ttl=20)
        return jsonify(payload)
    finally:
        conn.close()


@app.get("/api/library/digest")
def api_library_digest():
    """
    Library Digest feed (Files mode): recent albums that *have* a review snippet (short_description).

    Important behavior:
    - Albums without a review snippet are omitted (no placeholder).
    - Optionally triggers background enrichment for missing snippets on the returned recent albums.
    """
    if _get_library_mode() != "files":
        return jsonify({"albums": [], "limit": 0, "generated_at": int(time.time()), "enrichment": {"triggered": False, "missing_total": 0, "available_total": 0, "error": "Files mode required"}}), 400

    ok, err = _ensure_files_index_ready()
    if not ok:
        return jsonify({"albums": [], "limit": 0, "generated_at": int(time.time()), "enrichment": {"triggered": False, "missing_total": 0, "available_total": 0, "error": err or "Files index unavailable"}}), 503

    limit = max(1, min(36, _parse_int_loose(request.args.get("limit"), 12)))
    trigger = str(request.args.get("trigger", "1")).strip().lower() in {"1", "true", "yes"}

    conn = _files_pg_connect()
    if conn is None:
        return jsonify({"albums": [], "limit": limit, "generated_at": int(time.time()), "enrichment": {"triggered": False, "missing_total": 0, "available_total": 0, "error": "PostgreSQL unavailable"}}), 503

    try:
        with conn.cursor() as cur:
            cur.execute(
                """
                SELECT
                    alb.id,
                    alb.title,
                    alb.title_norm,
                    COALESCE(alb.year, 0) AS year,
                    COALESCE(alb.genre, '') AS genre,
                    COALESCE(alb.label, '') AS label,
                    COALESCE(alb.tags_json, '[]') AS tags_json,
                    alb.track_count,
                    COALESCE(alb.format, '') AS format,
                    alb.is_lossless,
                    alb.has_cover,
                    ar.id AS artist_id,
                    ar.name AS artist_name,
                    ar.name_norm AS artist_norm,
                    COALESCE(pr.short_description, '') AS short_description,
                    COALESCE(pr.source, '') AS profile_source
                FROM files_albums alb
                JOIN files_artists ar ON ar.id = alb.artist_id
                LEFT JOIN files_album_profiles pr
                       ON pr.artist_norm = ar.name_norm
                      AND pr.title_norm = alb.title_norm
                ORDER BY alb.created_at DESC, alb.id DESC
                LIMIT %s
                """,
                (int(limit),),
            )
            rows = cur.fetchall()

        base_url = request.url_root.rstrip("/")
        albums_with_reviews: list[dict] = []
        missing_by_artist: dict[str, dict] = {}
        missing_total = 0

        for (
            album_id,
            title,
            title_norm,
            year,
            genre,
            label,
            tags_json,
            track_count,
            fmt,
            is_lossless,
            has_cover,
            artist_id,
            artist_name,
            artist_norm,
            short_desc,
            profile_source,
        ) in rows:
            aid = int(album_id or 0)
            arid = int(artist_id or 0)
            if aid <= 0 or arid <= 0:
                continue

            # Parsed list of genres for UI badges (multi-genre albums).
            genres_list: list[str] = []
            try:
                tags_list = json.loads(tags_json) if tags_json else []
                if isinstance(tags_list, list):
                    for t in tags_list:
                        v = str(t or "").strip()
                        if v:
                            genres_list.append(v)
            except Exception:
                genres_list = []
            if not genres_list:
                try:
                    genres_list = _split_genre_values(genre or "")
                except Exception:
                    genres_list = []
            if genres_list:
                seen = set()
                deduped = []
                for g in genres_list:
                    gg = re.sub(r"\s+", " ", (g or "").strip())
                    if not gg:
                        continue
                    key = gg.lower()
                    if key in seen:
                        continue
                    seen.add(key)
                    deduped.append(gg)
                genres_list = deduped[:20]

            short_desc_clean = (short_desc or "").strip()
            prof_source_clean = (profile_source or "").strip() or None

            if not short_desc_clean:
                missing_total += 1
                an = str(artist_norm or "").strip()
                tn = str(title_norm or "").strip()
                if an and tn:
                    slot = missing_by_artist.setdefault(
                        an,
                        {
                            "artist_name": str(artist_name or "").strip(),
                            "artist_norm": an,
                            "albums": [],
                        },
                    )
                    slot["albums"].append((str(title or "").strip(), tn))
                continue

            thumb = f"{base_url}/api/library/files/album/{aid}/cover?size=512" if bool(has_cover) else None
            albums_with_reviews.append(
                {
                    "album_id": aid,
                    "title": title or "",
                    "year": int(year or 0) or None,
                    "genre": (genre or "").strip() or None,
                    "genres": genres_list,
                    "label": (label or "").strip() or None,
                    "track_count": int(track_count or 0),
                    "format": (fmt or "").strip() or None,
                    "is_lossless": bool(is_lossless),
                    "thumb": thumb,
                    "artist_id": arid,
                    "artist_name": artist_name or "",
                    "short_description": short_desc_clean,
                    "profile_source": prof_source_clean,
                }
            )

        triggered = False
        if trigger and missing_by_artist:
            # Keep this bounded: we only want to backfill "recent digest" coverage, not the full library.
            max_artists = 10
            max_albums_per_artist = 24
            for idx, (_artist_norm, info) in enumerate(list(missing_by_artist.items())[:max_artists]):
                try:
                    artist_name = str(info.get("artist_name") or "").strip()
                    artist_norm = str(info.get("artist_norm") or "").strip()
                    albums = list(info.get("albums") or [])[:max_albums_per_artist]
                    if artist_name and artist_norm and albums:
                        ok_enq = _enqueue_files_profile_enrichment(artist_name, artist_norm, albums)
                        triggered = triggered or bool(ok_enq)
                except Exception:
                    continue

        with _files_profile_jobs_lock:
            active_jobs = int(len(_files_profile_jobs_active))
        with _files_profile_backfill_lock:
            backfill_state = dict(_files_profile_backfill_state or {})

        payload = {
            "limit": int(limit),
            "generated_at": int(time.time()),
            "albums": albums_with_reviews,
            "enrichment": {
                "triggered": bool(triggered),
                "missing_total": int(missing_total),
                "available_total": int(len(albums_with_reviews)),
                "active_jobs": active_jobs,
                "profile_backfill": backfill_state,
            },
        }
        return jsonify(payload)
    finally:
        conn.close()


@app.get("/api/library/likes")
def api_library_likes_get():
    """Return like state for one or more entities (Files mode only)."""
    if _get_library_mode() != "files":
        return jsonify({"items": [], "error": "Files mode required"}), 400
    ok, err = _ensure_files_index_ready()
    if not ok:
        return jsonify({"items": [], "error": err or "Files index unavailable"}), 503
    entity_type = (request.args.get("entity_type") or "").strip().lower()
    if entity_type not in {"artist", "album", "track"}:
        return jsonify({"items": [], "error": "entity_type must be one of: artist, album, track"}), 400
    ids_raw = str(request.args.get("ids") or "").strip()
    limit = max(1, min(1000, _parse_int_loose(request.args.get("limit"), 250)))

    ids: list[int] = []
    if ids_raw:
        for part in ids_raw.split(","):
            part = part.strip()
            if not part:
                continue
            try:
                n = int(part)
            except ValueError:
                continue
            if n > 0:
                ids.append(n)
            if len(ids) >= 1000:
                break

    conn = _files_pg_connect()
    if conn is None:
        return jsonify({"items": [], "error": "PostgreSQL unavailable"}), 503
    try:
        with conn.cursor() as cur:
            if ids:
                cur.execute(
                    """
                    SELECT entity_id, liked, EXTRACT(EPOCH FROM updated_at)::BIGINT
                    FROM files_entity_likes
                    WHERE entity_type = %s
                      AND entity_id = ANY(%s)
                    """,
                    (entity_type, ids),
                )
            else:
                cur.execute(
                    """
                    SELECT entity_id, liked, EXTRACT(EPOCH FROM updated_at)::BIGINT
                    FROM files_entity_likes
                    WHERE entity_type = %s
                      AND liked = TRUE
                    ORDER BY updated_at DESC, entity_id DESC
                    LIMIT %s
                    """,
                    (entity_type, int(limit)),
                )
            rows = cur.fetchall()
        items = [
            {"entity_id": int(r[0] or 0), "liked": bool(r[1]), "updated_at": int(r[2] or 0)}
            for r in rows
            if int(r[0] or 0) > 0
        ]
        return jsonify({"entity_type": entity_type, "items": items})
    finally:
        conn.close()


@app.put("/api/library/likes")
def api_library_likes_put():
    """Set like state for a single entity (Files mode only)."""
    if _get_library_mode() != "files":
        return jsonify({"error": "Files mode required"}), 400
    ok, err = _ensure_files_index_ready()
    if not ok:
        return jsonify({"error": err or "Files index unavailable"}), 503
    data = request.get_json(silent=True) or {}
    if not isinstance(data, dict):
        data = {}
    entity_type = str(data.get("entity_type") or "").strip().lower()
    if entity_type not in {"artist", "album", "track"}:
        return jsonify({"error": "entity_type must be one of: artist, album, track"}), 400
    entity_id = _parse_int_loose(data.get("entity_id"), 0)
    if entity_id <= 0:
        return jsonify({"error": "entity_id must be a positive integer"}), 400
    liked = bool(_parse_bool(data.get("liked") if data.get("liked") is not None else True))
    source = str(data.get("source") or "ui").strip()[:64] or "ui"

    conn = _files_pg_connect()
    if conn is None:
        return jsonify({"error": "PostgreSQL unavailable"}), 503
    try:
        with conn.transaction():
            with conn.cursor() as cur:
                cur.execute(
                    """
                    INSERT INTO files_entity_likes(entity_type, entity_id, liked, source, created_at, updated_at)
                    VALUES (%s, %s, %s, %s, NOW(), NOW())
                    ON CONFLICT (entity_type, entity_id) DO UPDATE SET
                        liked = EXCLUDED.liked,
                        source = EXCLUDED.source,
                        updated_at = NOW()
                    """,
                    (entity_type, int(entity_id), bool(liked), source),
                )
        return jsonify({"entity_type": entity_type, "entity_id": int(entity_id), "liked": bool(liked), "updated_at": int(time.time())})
    finally:
        conn.close()


@app.get("/api/library/artists/top")
def api_library_top_artists():
    """Return top listened artists (Files mode only)."""
    if _get_library_mode() != "files":
        return jsonify({"artists": [], "error": "Files mode required"}), 400
    ok, err = _ensure_files_index_ready()
    if not ok:
        return jsonify({"artists": [], "error": err or "Files index unavailable"}), 503
    limit = max(1, min(60, _parse_int_loose(request.args.get("limit"), 18)))
    days = max(0, min(3650, _parse_int_loose(request.args.get("days"), 0)))

    cache_key = f"library:top_artists:{limit}:{days}"
    cached = _files_cache_get_json(cache_key)
    if cached is not None:
        return jsonify(cached)

    conn = _files_pg_connect()
    if conn is None:
        return jsonify({"artists": [], "error": "PostgreSQL unavailable"}), 503
    try:
        base_url = request.url_root.rstrip("/")
        with conn.cursor() as cur:
            if days > 0:
                cur.execute(
                    """
                    SELECT
                        ar.id,
                        ar.name,
                        ar.album_count,
                        ar.has_image,
                        COALESCE(SUM(CASE WHEN e.event_type IN ('play_complete', 'like') THEN 1 ELSE 0 END), 0) AS completion_count,
                        COALESCE(SUM(CASE WHEN e.event_type IN ('play_start', 'play_partial', 'play_complete', 'like') THEN 1 ELSE 0 END), 0) AS play_events
                    FROM files_reco_events e
                    JOIN files_artists ar ON ar.id = e.artist_id
                    WHERE e.created_at >= (NOW() - (%s || ' days')::INTERVAL)
                    GROUP BY ar.id
                    ORDER BY completion_count DESC, play_events DESC, ar.album_count DESC, ar.name ASC
                    LIMIT %s
                    """,
                    (int(days), int(limit)),
                )
            else:
                cur.execute(
                    """
                    SELECT
                        ar.id,
                        ar.name,
                        ar.album_count,
                        ar.has_image,
                        COALESCE(SUM(st.completion_count), 0) AS completion_count,
                        COALESCE(SUM(st.play_count), 0) AS play_count
                    FROM files_reco_track_stats st
                    JOIN files_tracks tr ON tr.id = st.track_id
                    JOIN files_albums alb ON alb.id = tr.album_id
                    JOIN files_artists ar ON ar.id = alb.artist_id
                    GROUP BY ar.id
                    ORDER BY completion_count DESC, play_count DESC, ar.album_count DESC, ar.name ASC
                    LIMIT %s
                    """,
                    (int(limit),),
                )
            rows = cur.fetchall()

        artists = []
        for artist_id, name, album_count, has_image, completion_count, play_count in rows:
            aid = int(artist_id or 0)
            if aid <= 0:
                continue
            thumb = f"{base_url}/api/library/files/artist/{aid}/image?size=192" if bool(has_image) else None
            artists.append(
                {
                    "artist_id": aid,
                    "artist_name": name or "",
                    "album_count": int(album_count or 0),
                    "completion_count": int(completion_count or 0),
                    "play_count": int(play_count or 0),
                    "thumb": thumb,
                }
            )

        payload = {"artists": artists, "limit": int(limit), "days": int(days)}
        _files_cache_set_json(cache_key, payload, ttl=30)
        return jsonify(payload)
    finally:
        conn.close()


@app.get("/api/library/facets")
def api_library_facets():
    """Return library facets (genres/labels/years) for discovery chips (Files mode only)."""
    if _get_library_mode() != "files":
        return jsonify({"genres": [], "labels": [], "years": [], "error": "Files mode required"}), 400
    ok, err = _ensure_files_index_ready()
    if not ok:
        return jsonify({"genres": [], "labels": [], "years": [], "error": err or "Files index unavailable"}), 503
    limit_genres = max(1, min(80, _parse_int_loose(request.args.get("limit_genres"), 24)))
    limit_labels = max(1, min(80, _parse_int_loose(request.args.get("limit_labels"), 24)))
    limit_years = max(1, min(200, _parse_int_loose(request.args.get("limit_years"), 50)))

    cache_key = f"library:facets:{limit_genres}:{limit_labels}:{limit_years}"
    cached = _files_cache_get_json(cache_key)
    if cached is not None:
        return jsonify(cached)

    conn = _files_pg_connect()
    if conn is None:
        return jsonify({"genres": [], "labels": [], "years": [], "error": "PostgreSQL unavailable"}), 503
    try:
        with conn.cursor() as cur:
            cur.execute(
                """
                WITH genre_tokens AS (
                    -- Primary source: tags_json array (multi-genre support)
                    SELECT
                        alb.id AS album_id,
                        TRIM(g.value) AS genre_disp
                    FROM files_albums alb
                    CROSS JOIN LATERAL jsonb_array_elements_text(COALESCE(alb.tags_json, '[]')::jsonb) AS g(value)
                    WHERE COALESCE(TRIM(g.value), '') <> ''
                    UNION ALL
                    -- Fallback: legacy single-genre column when tags_json is empty
                    SELECT
                        alb.id AS album_id,
                        TRIM(alb.genre) AS genre_disp
                    FROM files_albums alb
                    WHERE COALESCE(TRIM(alb.genre), '') <> ''
                      AND COALESCE(alb.tags_json, '[]') = '[]'
                ),
                norm_tokens AS (
                    SELECT
                        album_id,
                        LOWER(genre_disp) AS genre_norm,
                        genre_disp
                    FROM genre_tokens
                    WHERE COALESCE(genre_disp, '') <> ''
                ),
                counts AS (
                    SELECT genre_norm, COUNT(DISTINCT album_id) AS c
                    FROM norm_tokens
                    GROUP BY genre_norm
                ),
                best_disp AS (
                    SELECT
                        genre_norm,
                        genre_disp,
                        ROW_NUMBER() OVER (
                            PARTITION BY genre_norm
                            ORDER BY COUNT(DISTINCT album_id) DESC, LENGTH(genre_disp) DESC, genre_disp ASC
                        ) AS rn
                    FROM norm_tokens
                    GROUP BY genre_norm, genre_disp
                )
                SELECT b.genre_disp AS genre, c.c
                FROM counts c
                JOIN best_disp b ON b.genre_norm = c.genre_norm AND b.rn = 1
                ORDER BY c.c DESC, genre ASC
                LIMIT %s
                """,
                (int(limit_genres),),
            )
            genres = [{"value": (r[0] or "").strip(), "count": int(r[1] or 0)} for r in cur.fetchall() if str(r[0] or "").strip()]

            cur.execute(
                """
                WITH label_tokens AS (
                    SELECT TRIM(COALESCE(label, '')) AS label_disp
                    FROM files_albums
                    WHERE COALESCE(TRIM(label), '') <> ''
                ),
                norm_tokens AS (
                    SELECT LOWER(label_disp) AS label_norm, label_disp
                    FROM label_tokens
                    WHERE COALESCE(label_disp, '') <> ''
                ),
                counts AS (
                    SELECT label_norm, COUNT(*) AS c
                    FROM norm_tokens
                    GROUP BY label_norm
                ),
                best_disp AS (
                    SELECT
                        label_norm,
                        label_disp,
                        ROW_NUMBER() OVER (
                            PARTITION BY label_norm
                            ORDER BY COUNT(*) DESC, LENGTH(label_disp) DESC, label_disp ASC
                        ) AS rn
                    FROM norm_tokens
                    GROUP BY label_norm, label_disp
                )
                SELECT b.label_disp AS label, c.c
                FROM counts c
                JOIN best_disp b ON b.label_norm = c.label_norm AND b.rn = 1
                ORDER BY c.c DESC, label ASC
                LIMIT %s
                """,
                (int(limit_labels),),
            )
            labels = [{"value": (r[0] or "").strip(), "count": int(r[1] or 0)} for r in cur.fetchall() if str(r[0] or "").strip()]

            cur.execute(
                """
                SELECT year, COUNT(*) AS c
                FROM files_albums
                WHERE year IS NOT NULL AND year > 0
                GROUP BY year
                ORDER BY year DESC
                LIMIT %s
                """,
                (int(limit_years),),
            )
            years = [{"value": int(r[0] or 0), "count": int(r[1] or 0)} for r in cur.fetchall() if int(r[0] or 0) > 0]

        payload = {"genres": genres, "labels": labels, "years": years}
        _files_cache_set_json(cache_key, payload, ttl=60)
        return jsonify(payload)
    finally:
        conn.close()


@app.get("/api/library/genres/suggest")
def api_library_genres_suggest():
    """Suggest genres with album counts (Files mode only). Used for advanced filtering UI."""
    if _get_library_mode() != "files":
        return jsonify({"query": "", "genres": [], "error": "Files mode required"}), 400
    ok, err = _ensure_files_index_ready()
    if not ok:
        return jsonify({"query": "", "genres": [], "error": err or "Files index unavailable"}), 503

    query = (request.args.get("q") or "").strip()
    label = (request.args.get("label") or "").strip()
    year = _parse_int_loose(request.args.get("year"), 0)
    limit = max(1, min(80, _parse_int_loose(request.args.get("limit"), 16)))
    refresh = bool(_parse_bool(request.args.get("refresh")))

    cache_key = f"library:genres:suggest:{query.lower()}:{label.lower()}:{int(year or 0)}:{limit}"
    if not refresh:
        cached = _files_cache_get_json(cache_key)
        if cached is not None:
            return jsonify(cached)

    conn = _files_pg_connect()
    if conn is None:
        return jsonify({"query": query, "genres": [], "error": "PostgreSQL unavailable"}), 503
    try:
        album_filters = ["1=1"]
        album_params: list = []
        if year and int(year) > 0:
            album_filters.append("COALESCE(alb.year, 0) = %s")
            album_params.append(int(year))
        if label:
            parts = [p.strip() for p in str(label).split(",") if p.strip()]
            if parts:
                album_filters.append("lower(trim(COALESCE(alb.label, ''))) = ANY(%s)")
                album_params.append([p.lower() for p in parts])

        like = f"%{query}%"
        with conn.cursor() as cur:
            if query:
                cur.execute(
                    f"""
                    WITH genre_tokens AS (
                        SELECT
                            alb.id AS album_id,
                            TRIM(g.value) AS genre_disp
                        FROM files_albums alb
                        CROSS JOIN LATERAL jsonb_array_elements_text(COALESCE(alb.tags_json, '[]')::jsonb) AS g(value)
                        WHERE COALESCE(TRIM(g.value), '') <> ''
                          AND {" AND ".join(album_filters)}
                        UNION ALL
                        SELECT
                            alb.id AS album_id,
                            TRIM(alb.genre) AS genre_disp
                        FROM files_albums alb
                        WHERE COALESCE(TRIM(alb.genre), '') <> ''
                          AND {" AND ".join(album_filters)}
                          AND COALESCE(alb.tags_json, '[]') = '[]'
                    ),
                    norm_tokens AS (
                        SELECT
                            album_id,
                            LOWER(genre_disp) AS genre_norm,
                            genre_disp
                        FROM genre_tokens
                        WHERE COALESCE(genre_disp, '') <> ''
                          AND genre_disp ILIKE %s
                    ),
                    counts AS (
                        SELECT genre_norm, COUNT(DISTINCT album_id) AS c
                        FROM norm_tokens
                        GROUP BY genre_norm
                    ),
                    best_disp AS (
                        SELECT
                            genre_norm,
                            genre_disp,
                            ROW_NUMBER() OVER (
                                PARTITION BY genre_norm
                                ORDER BY COUNT(DISTINCT album_id) DESC, LENGTH(genre_disp) DESC, genre_disp ASC
                            ) AS rn
                        FROM norm_tokens
                        GROUP BY genre_norm, genre_disp
                    )
                    SELECT b.genre_disp AS genre, c.c
                    FROM counts c
                    JOIN best_disp b ON b.genre_norm = c.genre_norm AND b.rn = 1
                    ORDER BY c.c DESC, genre ASC
                    LIMIT %s
                    """,
                    (*album_params, like, int(limit)),
                )
            else:
                cur.execute(
                    f"""
                    WITH genre_tokens AS (
                        SELECT
                            alb.id AS album_id,
                            TRIM(g.value) AS genre_disp
                        FROM files_albums alb
                        CROSS JOIN LATERAL jsonb_array_elements_text(COALESCE(alb.tags_json, '[]')::jsonb) AS g(value)
                        WHERE COALESCE(TRIM(g.value), '') <> ''
                          AND {" AND ".join(album_filters)}
                        UNION ALL
                        SELECT
                            alb.id AS album_id,
                            TRIM(alb.genre) AS genre_disp
                        FROM files_albums alb
                        WHERE COALESCE(TRIM(alb.genre), '') <> ''
                          AND {" AND ".join(album_filters)}
                          AND COALESCE(alb.tags_json, '[]') = '[]'
                    ),
                    norm_tokens AS (
                        SELECT
                            album_id,
                            LOWER(genre_disp) AS genre_norm,
                            genre_disp
                        FROM genre_tokens
                        WHERE COALESCE(genre_disp, '') <> ''
                    ),
                    counts AS (
                        SELECT genre_norm, COUNT(DISTINCT album_id) AS c
                        FROM norm_tokens
                        GROUP BY genre_norm
                    ),
                    best_disp AS (
                        SELECT
                            genre_norm,
                            genre_disp,
                            ROW_NUMBER() OVER (
                                PARTITION BY genre_norm
                                ORDER BY COUNT(DISTINCT album_id) DESC, LENGTH(genre_disp) DESC, genre_disp ASC
                            ) AS rn
                        FROM norm_tokens
                        GROUP BY genre_norm, genre_disp
                    )
                    SELECT b.genre_disp AS genre, c.c
                    FROM counts c
                    JOIN best_disp b ON b.genre_norm = c.genre_norm AND b.rn = 1
                    ORDER BY c.c DESC, genre ASC
                    LIMIT %s
                    """,
                    (*album_params, int(limit)),
                )
            rows = cur.fetchall()

        genres = [{"value": (r[0] or "").strip(), "count": int(r[1] or 0)} for r in rows if str(r[0] or "").strip()]
        payload = {"query": query, "genres": genres}
        _files_cache_set_json(cache_key, payload, ttl=30)
        return jsonify(payload)
    finally:
        conn.close()


@app.get("/api/library/labels/suggest")
def api_library_labels_suggest():
    """Suggest labels with album counts (Files mode only). Used for advanced filtering UI."""
    if _get_library_mode() != "files":
        return jsonify({"query": "", "labels": [], "error": "Files mode required"}), 400
    ok, err = _ensure_files_index_ready()
    if not ok:
        return jsonify({"query": "", "labels": [], "error": err or "Files index unavailable"}), 503

    query = (request.args.get("q") or "").strip()
    genre = (request.args.get("genre") or "").strip()
    year = _parse_int_loose(request.args.get("year"), 0)
    limit = max(1, min(80, _parse_int_loose(request.args.get("limit"), 16)))
    refresh = bool(_parse_bool(request.args.get("refresh")))

    cache_key = f"library:labels:suggest:{query.lower()}:{genre.lower()}:{int(year or 0)}:{limit}"
    if not refresh:
        cached = _files_cache_get_json(cache_key)
        if cached is not None:
            return jsonify(cached)

    conn = _files_pg_connect()
    if conn is None:
        return jsonify({"query": query, "labels": [], "error": "PostgreSQL unavailable"}), 503
    try:
        album_filters = ["1=1"]
        album_params: list = []
        if year and int(year) > 0:
            album_filters.append("COALESCE(alb.year, 0) = %s")
            album_params.append(int(year))
        if genre:
            parts = [p.strip() for p in str(genre).split(",") if p.strip()]
            if parts:
                norms = [p.lower() for p in parts]
                album_filters.append(
                    """
                    (
                        EXISTS (
                            SELECT 1
                            FROM jsonb_array_elements_text(COALESCE(alb.tags_json, '[]')::jsonb) AS g(value)
                            WHERE lower(trim(g.value)) = ANY(%s)
                        )
                        OR (
                            COALESCE(alb.tags_json, '[]') = '[]'
                            AND lower(trim(COALESCE(alb.genre, ''))) = ANY(%s)
                        )
                    )
                    """
                )
                album_params.append(norms)
                album_params.append(norms)

        like = f"%{query}%"
        with conn.cursor() as cur:
            if query:
                cur.execute(
                    f"""
                    WITH label_tokens AS (
                        SELECT TRIM(COALESCE(alb.label, '')) AS label_disp
                        FROM files_albums alb
                        WHERE COALESCE(TRIM(alb.label), '') <> ''
                          AND {" AND ".join(album_filters)}
                          AND TRIM(COALESCE(alb.label, '')) ILIKE %s
                    ),
                    norm_tokens AS (
                        SELECT LOWER(label_disp) AS label_norm, label_disp
                        FROM label_tokens
                        WHERE COALESCE(label_disp, '') <> ''
                    ),
                    counts AS (
                        SELECT label_norm, COUNT(*) AS c
                        FROM norm_tokens
                        GROUP BY label_norm
                    ),
                    best_disp AS (
                        SELECT
                            label_norm,
                            label_disp,
                            ROW_NUMBER() OVER (
                                PARTITION BY label_norm
                                ORDER BY COUNT(*) DESC, LENGTH(label_disp) DESC, label_disp ASC
                            ) AS rn
                        FROM norm_tokens
                        GROUP BY label_norm, label_disp
                    )
                    SELECT b.label_disp AS label, c.c
                    FROM counts c
                    JOIN best_disp b ON b.label_norm = c.label_norm AND b.rn = 1
                    ORDER BY c.c DESC, label ASC
                    LIMIT %s
                    """,
                    (*album_params, like, int(limit)),
                )
            else:
                cur.execute(
                    f"""
                    WITH label_tokens AS (
                        SELECT TRIM(COALESCE(alb.label, '')) AS label_disp
                        FROM files_albums alb
                        WHERE COALESCE(TRIM(alb.label), '') <> ''
                          AND {" AND ".join(album_filters)}
                    ),
                    norm_tokens AS (
                        SELECT LOWER(label_disp) AS label_norm, label_disp
                        FROM label_tokens
                        WHERE COALESCE(label_disp, '') <> ''
                    ),
                    counts AS (
                        SELECT label_norm, COUNT(*) AS c
                        FROM norm_tokens
                        GROUP BY label_norm
                    ),
                    best_disp AS (
                        SELECT
                            label_norm,
                            label_disp,
                            ROW_NUMBER() OVER (
                                PARTITION BY label_norm
                                ORDER BY COUNT(*) DESC, LENGTH(label_disp) DESC, label_disp ASC
                            ) AS rn
                        FROM norm_tokens
                        GROUP BY label_norm, label_disp
                    )
                    SELECT b.label_disp AS label, c.c
                    FROM counts c
                    JOIN best_disp b ON b.label_norm = c.label_norm AND b.rn = 1
                    ORDER BY c.c DESC, label ASC
                    LIMIT %s
                    """,
                    (*album_params, int(limit)),
                )
            rows = cur.fetchall()

        labels = [{"value": (r[0] or "").strip(), "count": int(r[1] or 0)} for r in rows if str(r[0] or "").strip()]
        payload = {"query": query, "labels": labels}
        _files_cache_set_json(cache_key, payload, ttl=30)
        return jsonify(payload)
    finally:
        conn.close()


@app.get("/api/library/genres")
def api_library_genres():
    """List genres with album counts (Files mode only). Supports search + pagination and optional label/year filtering."""
    if _get_library_mode() != "files":
        return jsonify({"genres": [], "total": 0, "limit": 0, "offset": 0, "error": "Files mode required"}), 400
    ok, err = _ensure_files_index_ready()
    if not ok:
        return jsonify({"genres": [], "total": 0, "limit": 0, "offset": 0, "error": err or "Files index unavailable"}), 503

    search = (request.args.get("search") or request.args.get("q") or "").strip()
    label = (request.args.get("label") or "").strip()
    year = _parse_int_loose(request.args.get("year"), 0)
    limit = max(1, min(200, _parse_int_loose(request.args.get("limit"), 80)))
    offset = max(0, _parse_int_loose(request.args.get("offset"), 0))
    refresh = bool(_parse_bool(request.args.get("refresh")))

    cache_key = f"library:genres:list:{search.lower()}:{label.lower()}:{int(year or 0)}:{limit}:{offset}"
    if not refresh:
        cached = _files_cache_get_json(cache_key)
        if cached is not None:
            return jsonify(cached)

    album_filters = ["1=1"]
    album_params: list = []
    if year and int(year) > 0:
        album_filters.append("COALESCE(alb.year, 0) = %s")
        album_params.append(int(year))
    if label:
        parts = [p.strip() for p in str(label).split(",") if p.strip()]
        if parts:
            album_filters.append("lower(trim(COALESCE(alb.label, ''))) = ANY(%s)")
            album_params.append([p.lower() for p in parts])

    conn = _files_pg_connect()
    if conn is None:
        return jsonify({"genres": [], "total": 0, "limit": limit, "offset": offset, "error": "PostgreSQL unavailable"}), 503
    try:
        like = f"%{search}%"
        with conn.cursor() as cur:
            cur.execute(
                f"""
                WITH genre_tokens AS (
                    SELECT
                        alb.id AS album_id,
                        TRIM(g.value) AS genre_disp
                    FROM files_albums alb
                    CROSS JOIN LATERAL jsonb_array_elements_text(COALESCE(alb.tags_json, '[]')::jsonb) AS g(value)
                    WHERE COALESCE(TRIM(g.value), '') <> ''
                      AND {" AND ".join(album_filters)}
                    UNION ALL
                    SELECT
                        alb.id AS album_id,
                        TRIM(alb.genre) AS genre_disp
                    FROM files_albums alb
                    WHERE COALESCE(TRIM(alb.genre), '') <> ''
                      AND {" AND ".join(album_filters)}
                      AND COALESCE(alb.tags_json, '[]') = '[]'
                ),
                norm_tokens AS (
                    SELECT
                        album_id,
                        LOWER(genre_disp) AS genre_norm,
                        genre_disp
                    FROM genre_tokens
                    WHERE COALESCE(genre_disp, '') <> ''
                      AND (%s = '' OR genre_disp ILIKE %s)
                ),
                counts AS (
                    SELECT genre_norm, COUNT(DISTINCT album_id) AS c
                    FROM norm_tokens
                    GROUP BY genre_norm
                ),
                best_disp AS (
                    SELECT
                        genre_norm,
                        genre_disp,
                        ROW_NUMBER() OVER (
                            PARTITION BY genre_norm
                            ORDER BY COUNT(DISTINCT album_id) DESC, LENGTH(genre_disp) DESC, genre_disp ASC
                        ) AS rn
                    FROM norm_tokens
                    GROUP BY genre_norm, genre_disp
                )
                SELECT COUNT(*) FROM counts
                """,
                (*album_params, search, like),
            )
            total = int((cur.fetchone() or [0])[0] or 0)

            cur.execute(
                f"""
                WITH genre_tokens AS (
                    SELECT
                        alb.id AS album_id,
                        TRIM(g.value) AS genre_disp
                    FROM files_albums alb
                    CROSS JOIN LATERAL jsonb_array_elements_text(COALESCE(alb.tags_json, '[]')::jsonb) AS g(value)
                    WHERE COALESCE(TRIM(g.value), '') <> ''
                      AND {" AND ".join(album_filters)}
                    UNION ALL
                    SELECT
                        alb.id AS album_id,
                        TRIM(alb.genre) AS genre_disp
                    FROM files_albums alb
                    WHERE COALESCE(TRIM(alb.genre), '') <> ''
                      AND {" AND ".join(album_filters)}
                      AND COALESCE(alb.tags_json, '[]') = '[]'
                ),
                norm_tokens AS (
                    SELECT
                        album_id,
                        LOWER(genre_disp) AS genre_norm,
                        genre_disp
                    FROM genre_tokens
                    WHERE COALESCE(genre_disp, '') <> ''
                      AND (%s = '' OR genre_disp ILIKE %s)
                ),
                counts AS (
                    SELECT genre_norm, COUNT(DISTINCT album_id) AS c
                    FROM norm_tokens
                    GROUP BY genre_norm
                ),
                best_disp AS (
                    SELECT
                        genre_norm,
                        genre_disp,
                        ROW_NUMBER() OVER (
                            PARTITION BY genre_norm
                            ORDER BY COUNT(DISTINCT album_id) DESC, LENGTH(genre_disp) DESC, genre_disp ASC
                        ) AS rn
                    FROM norm_tokens
                    GROUP BY genre_norm, genre_disp
                )
                SELECT b.genre_disp AS genre, c.c
                FROM counts c
                JOIN best_disp b ON b.genre_norm = c.genre_norm AND b.rn = 1
                ORDER BY c.c DESC, genre ASC
                LIMIT %s OFFSET %s
                """,
                (*album_params, search, like, int(limit), int(offset)),
            )
            rows = cur.fetchall()

        genres = [{"value": (r[0] or "").strip(), "count": int(r[1] or 0)} for r in rows if str(r[0] or "").strip()]
        payload = {"genres": genres, "total": total, "limit": int(limit), "offset": int(offset)}
        _files_cache_set_json(cache_key, payload, ttl=45)
        return jsonify(payload)
    finally:
        conn.close()


@app.get("/api/library/labels")
def api_library_labels():
    """List labels with album counts (Files mode only). Supports search + pagination and optional genre/year filtering."""
    if _get_library_mode() != "files":
        return jsonify({"labels": [], "total": 0, "limit": 0, "offset": 0, "error": "Files mode required"}), 400
    ok, err = _ensure_files_index_ready()
    if not ok:
        return jsonify({"labels": [], "total": 0, "limit": 0, "offset": 0, "error": err or "Files index unavailable"}), 503

    search = (request.args.get("search") or request.args.get("q") or "").strip()
    genre = (request.args.get("genre") or "").strip()
    year = _parse_int_loose(request.args.get("year"), 0)
    limit = max(1, min(200, _parse_int_loose(request.args.get("limit"), 80)))
    offset = max(0, _parse_int_loose(request.args.get("offset"), 0))
    refresh = bool(_parse_bool(request.args.get("refresh")))

    cache_key = f"library:labels:list:{search.lower()}:{genre.lower()}:{int(year or 0)}:{limit}:{offset}"
    if not refresh:
        cached = _files_cache_get_json(cache_key)
        if cached is not None:
            return jsonify(cached)

    album_filters = ["1=1"]
    album_params: list = []
    if year and int(year) > 0:
        album_filters.append("COALESCE(alb.year, 0) = %s")
        album_params.append(int(year))
    if genre:
        parts = [p.strip() for p in str(genre).split(",") if p.strip()]
        if parts:
            norms = [p.lower() for p in parts]
            album_filters.append(
                """
                (
                    EXISTS (
                        SELECT 1
                        FROM jsonb_array_elements_text(COALESCE(alb.tags_json, '[]')::jsonb) AS g(value)
                        WHERE lower(trim(g.value)) = ANY(%s)
                    )
                    OR (
                        COALESCE(alb.tags_json, '[]') = '[]'
                        AND lower(trim(COALESCE(alb.genre, ''))) = ANY(%s)
                    )
                )
                """
            )
            album_params.append(norms)
            album_params.append(norms)

    conn = _files_pg_connect()
    if conn is None:
        return jsonify({"labels": [], "total": 0, "limit": limit, "offset": offset, "error": "PostgreSQL unavailable"}), 503
    try:
        like = f"%{search}%"
        with conn.cursor() as cur:
            cur.execute(
                f"""
                WITH label_tokens AS (
                    SELECT TRIM(COALESCE(alb.label, '')) AS label_disp
                    FROM files_albums alb
                    WHERE COALESCE(TRIM(alb.label), '') <> ''
                      AND {" AND ".join(album_filters)}
                      AND (%s = '' OR TRIM(COALESCE(alb.label, '')) ILIKE %s)
                ),
                norm_tokens AS (
                    SELECT LOWER(label_disp) AS label_norm, label_disp
                    FROM label_tokens
                    WHERE COALESCE(label_disp, '') <> ''
                ),
                counts AS (
                    SELECT label_norm, COUNT(*) AS c
                    FROM norm_tokens
                    GROUP BY label_norm
                )
                SELECT COUNT(*) FROM counts
                """,
                (*album_params, search, like),
            )
            total = int((cur.fetchone() or [0])[0] or 0)

            cur.execute(
                f"""
                WITH label_tokens AS (
                    SELECT TRIM(COALESCE(alb.label, '')) AS label_disp
                    FROM files_albums alb
                    WHERE COALESCE(TRIM(alb.label), '') <> ''
                      AND {" AND ".join(album_filters)}
                      AND (%s = '' OR TRIM(COALESCE(alb.label, '')) ILIKE %s)
                ),
                norm_tokens AS (
                    SELECT LOWER(label_disp) AS label_norm, label_disp
                    FROM label_tokens
                    WHERE COALESCE(label_disp, '') <> ''
                ),
                counts AS (
                    SELECT label_norm, COUNT(*) AS c
                    FROM norm_tokens
                    GROUP BY label_norm
                ),
                best_disp AS (
                    SELECT
                        label_norm,
                        label_disp,
                        ROW_NUMBER() OVER (
                            PARTITION BY label_norm
                            ORDER BY COUNT(*) DESC, LENGTH(label_disp) DESC, label_disp ASC
                        ) AS rn
                    FROM norm_tokens
                    GROUP BY label_norm, label_disp
                )
                SELECT b.label_disp AS label, c.c
                FROM counts c
                JOIN best_disp b ON b.label_norm = c.label_norm AND b.rn = 1
                ORDER BY c.c DESC, label ASC
                LIMIT %s OFFSET %s
                """,
                (*album_params, search, like, int(limit), int(offset)),
            )
            rows = cur.fetchall()

        labels = [{"value": (r[0] or "").strip(), "count": int(r[1] or 0)} for r in rows if str(r[0] or "").strip()]
        payload = {"labels": labels, "total": total, "limit": int(limit), "offset": int(offset)}
        _files_cache_set_json(cache_key, payload, ttl=45)
        return jsonify(payload)
    finally:
        conn.close()


@app.get("/api/library/genre/<path:genre>/labels")
def api_library_genre_labels(genre: str):
    """Return labels publishing a given genre (Files mode only)."""
    if _get_library_mode() != "files":
        return jsonify({"genre": genre or "", "album_count": 0, "labels": [], "error": "Files mode required"}), 400
    ok, err = _ensure_files_index_ready()
    if not ok:
        return jsonify({"genre": genre or "", "album_count": 0, "labels": [], "error": err or "Files index unavailable"}), 503
    g = (genre or "").strip()
    if not g:
        return jsonify({"genre": "", "album_count": 0, "labels": [], "error": "Invalid genre"}), 400
    limit = max(1, min(200, _parse_int_loose(request.args.get("limit"), 80)))
    refresh = bool(_parse_bool(request.args.get("refresh")))
    cache_key = f"library:genre:labels:{g.lower()}:{limit}"
    if not refresh:
        cached = _files_cache_get_json(cache_key)
        if cached is not None:
            return jsonify(cached)

    conn = _files_pg_connect()
    if conn is None:
        return jsonify({"genre": g, "album_count": 0, "labels": [], "error": "PostgreSQL unavailable"}), 503
    try:
        with conn.cursor() as cur:
            cur.execute(
                """
                WITH matched_albums AS (
                    SELECT DISTINCT alb.id AS album_id
                    FROM files_albums alb
                    WHERE EXISTS (
                        SELECT 1
                        FROM jsonb_array_elements_text(COALESCE(alb.tags_json, '[]')::jsonb) AS gg(value)
                        WHERE lower(trim(gg.value)) = lower(%s)
                    )
                    UNION
                    SELECT DISTINCT alb.id AS album_id
                    FROM files_albums alb
                    WHERE COALESCE(alb.tags_json, '[]') = '[]'
                      AND lower(trim(COALESCE(alb.genre, ''))) = lower(%s)
                      AND COALESCE(trim(alb.genre), '') <> ''
                )
                SELECT COUNT(*) FROM matched_albums
                """,
                (g, g),
            )
            album_count = int((cur.fetchone() or [0])[0] or 0)

            cur.execute(
                """
                WITH matched_albums AS (
                    SELECT DISTINCT alb.id AS album_id
                    FROM files_albums alb
                    WHERE EXISTS (
                        SELECT 1
                        FROM jsonb_array_elements_text(COALESCE(alb.tags_json, '[]')::jsonb) AS gg(value)
                        WHERE lower(trim(gg.value)) = lower(%s)
                    )
                    UNION
                    SELECT DISTINCT alb.id AS album_id
                    FROM files_albums alb
                    WHERE COALESCE(alb.tags_json, '[]') = '[]'
                      AND lower(trim(COALESCE(alb.genre, ''))) = lower(%s)
                      AND COALESCE(trim(alb.genre), '') <> ''
                ),
                label_tokens AS (
                    SELECT TRIM(COALESCE(alb.label, '')) AS label_disp
                    FROM files_albums alb
                    JOIN matched_albums m ON m.album_id = alb.id
                    WHERE COALESCE(TRIM(alb.label), '') <> ''
                ),
                norm_tokens AS (
                    SELECT LOWER(label_disp) AS label_norm, label_disp
                    FROM label_tokens
                    WHERE COALESCE(label_disp, '') <> ''
                ),
                counts AS (
                    SELECT label_norm, COUNT(*) AS c
                    FROM norm_tokens
                    GROUP BY label_norm
                ),
                best_disp AS (
                    SELECT
                        label_norm,
                        label_disp,
                        ROW_NUMBER() OVER (
                            PARTITION BY label_norm
                            ORDER BY COUNT(*) DESC, LENGTH(label_disp) DESC, label_disp ASC
                        ) AS rn
                    FROM norm_tokens
                    GROUP BY label_norm, label_disp
                )
                SELECT b.label_disp AS label, c.c
                FROM counts c
                JOIN best_disp b ON b.label_norm = c.label_norm AND b.rn = 1
                ORDER BY c.c DESC, label ASC
                LIMIT %s
                """,
                (g, g, int(limit)),
            )
            rows = cur.fetchall()

        labels = [{"label": (r[0] or "").strip(), "count": int(r[1] or 0)} for r in rows if str(r[0] or "").strip()]
        payload = {"genre": g, "album_count": int(album_count), "labels": labels}
        _files_cache_set_json(cache_key, payload, ttl=60)
        return jsonify(payload)
    finally:
        conn.close()


@app.get("/api/library/recently-played/albums")
def api_library_recently_played_albums():
    """Return recently played albums based on listening telemetry (Files mode only)."""
    if _get_library_mode() != "files":
        return jsonify({"days": 0, "limit": 0, "generated_at": int(time.time()), "albums": [], "error": "Files mode required"}), 400
    ok, err = _ensure_files_index_ready()
    if not ok:
        return jsonify({"days": 0, "limit": 0, "generated_at": int(time.time()), "albums": [], "error": err or "Files index unavailable"}), 503

    days = max(7, min(365, _parse_int_loose(request.args.get("days"), 90)))
    limit = max(1, min(60, _parse_int_loose(request.args.get("limit"), 18)))
    refresh = bool(_parse_bool(request.args.get("refresh")))

    cache_key = f"library:recently_played_albums:{days}:{limit}"
    if not refresh:
        cached = _files_cache_get_json(cache_key)
        if cached is not None:
            return jsonify(cached)

    conn = _files_pg_connect()
    if conn is None:
        return jsonify({"days": days, "limit": limit, "generated_at": int(time.time()), "albums": [], "error": "PostgreSQL unavailable"}), 503
    try:
        base_url = request.url_root.rstrip("/")
        with conn.cursor() as cur:
            # Prefer explicit playback telemetry; fall back to reco telemetry when empty.
            cur.execute(
                """
                SELECT COUNT(*)
                FROM files_playback_events
                WHERE user_id = 1
                  AND created_at >= NOW() - (%s || ' days')::interval
                  AND played_seconds >= 12
                """,
                (int(days),),
            )
            playback_count = int((cur.fetchone() or [0])[0] or 0)
            use_reco = playback_count <= 0
            ev_table = "files_reco_events" if use_reco else "files_playback_events"
            ev_user_filter = "1=1" if use_reco else "e.user_id = 1"

            cur.execute(
                f"""
                SELECT
                    t.album_id,
                    MAX(e.created_at) AS last_played_at
                FROM {ev_table} e
                JOIN files_tracks t ON t.id = e.track_id
                WHERE {ev_user_filter}
                  AND e.created_at >= NOW() - (%s || ' days')::interval
                  AND COALESCE(e.played_seconds, 0) >= 12
                GROUP BY t.album_id
                ORDER BY last_played_at DESC
                LIMIT %s
                """,
                (int(days), int(limit)),
            )
            rows = cur.fetchall()

            album_ids: list[int] = []
            last_played: dict[int, int] = {}
            for album_id, ts in rows:
                try:
                    aid = int(album_id or 0)
                except Exception:
                    aid = 0
                if aid <= 0:
                    continue
                album_ids.append(aid)
                last_played[aid] = int(_dt_to_epoch(ts)) if ts else 0

            albums_out: list[dict] = []
            if album_ids:
                cur.execute(
                    """
                    WITH ids AS (
                        SELECT album_id, ord
                        FROM unnest(%s::bigint[]) WITH ORDINALITY AS u(album_id, ord)
                    )
                    SELECT
                        alb.id,
                        alb.title,
                        COALESCE(alb.year, 0) AS year,
                        COALESCE(alb.genre, '') AS genre,
                        COALESCE(alb.label, '') AS label,
                        COALESCE(alb.tags_json, '[]') AS tags_json,
                        alb.track_count,
                        COALESCE(alb.format, '') AS format,
                        alb.is_lossless,
                        alb.has_cover,
                        ar.id AS artist_id,
                        ar.name AS artist_name,
                        COALESCE(pr.short_description, '') AS short_description,
                        COALESCE(pr.source, '') AS profile_source,
                        ids.ord
                    FROM ids
                    JOIN files_albums alb ON alb.id = ids.album_id
                    JOIN files_artists ar ON ar.id = alb.artist_id
                    LEFT JOIN files_album_profiles pr
                           ON pr.artist_norm = ar.name_norm
                          AND pr.title_norm = alb.title_norm
                    ORDER BY ids.ord ASC
                    """,
                    (album_ids,),
                )
                alb_rows = cur.fetchall()
                for album_id, title, year, genre, label, tags_json, track_count, fmt, is_lossless, has_cover, artist_id, artist_name, short_desc, profile_source, _ord in alb_rows:
                    aid = int(album_id or 0)
                    arid = int(artist_id or 0)
                    thumb = f"{base_url}/api/library/files/album/{aid}/cover?size=512" if bool(has_cover) else None
                    short_desc_clean = (short_desc or "").strip()
                    genres_list: list[str] = []
                    try:
                        tags_list = json.loads(tags_json) if tags_json else []
                        if isinstance(tags_list, list):
                            for t in tags_list:
                                v = str(t or "").strip()
                                if v:
                                    genres_list.append(v)
                    except Exception:
                        genres_list = []
                    if not genres_list:
                        try:
                            genres_list = _split_genre_values(genre or "")
                        except Exception:
                            genres_list = []
                    if genres_list:
                        seen = set()
                        deduped = []
                        for g in genres_list:
                            gg = re.sub(r"\\s+", " ", (g or "").strip())
                            if not gg:
                                continue
                            key = gg.lower()
                            if key in seen:
                                continue
                            seen.add(key)
                            deduped.append(gg)
                        genres_list = deduped[:20]
                    albums_out.append(
                        {
                            "album_id": aid,
                            "title": title or "",
                            "year": int(year or 0) or None,
                            "genre": (genre or "").strip() or None,
                            "genres": genres_list,
                            "label": (label or "").strip() or None,
                            "track_count": int(track_count or 0),
                            "format": (fmt or "").strip() or None,
                            "is_lossless": bool(is_lossless),
                            "thumb": thumb,
                            "artist_id": arid,
                            "artist_name": artist_name or "",
                            "short_description": short_desc_clean or None,
                            "profile_source": (profile_source or "").strip() or None,
                            "last_played_at": int(last_played.get(aid) or 0),
                        }
                    )

        payload = {
            "days": int(days),
            "limit": int(limit),
            "generated_at": int(time.time()),
            "source": "reco" if bool(use_reco) else "playback",
            "albums": albums_out,
        }
        _files_cache_set_json(cache_key, payload, ttl=30)
        return jsonify(payload)
    finally:
        conn.close()


@app.get("/api/library/playlists")
def api_library_playlists():
    """List local playlists (Files mode only)."""
    if _get_library_mode() != "files":
        return jsonify({"playlists": [], "error": "Files mode required"}), 400
    ok, err = _ensure_files_index_ready()
    if not ok:
        return jsonify({"playlists": [], "error": err or "Files index unavailable"}), 503
    conn = _files_pg_connect()
    if conn is None:
        return jsonify({"playlists": [], "error": "PostgreSQL unavailable"}), 503
    try:
        with conn.cursor() as cur:
            cur.execute(
                """
                SELECT
                    pl.id,
                    pl.name,
                    COALESCE(pl.description, '') AS description,
                    pl.updated_at,
                    COUNT(it.id) AS item_count
                FROM files_playlists pl
                LEFT JOIN files_playlist_items it ON it.playlist_id = pl.id
                GROUP BY pl.id
                ORDER BY pl.updated_at DESC, pl.id DESC
                """
            )
            rows = cur.fetchall()
        playlists = [
            {
                "playlist_id": int(r[0]),
                "name": r[1] or "",
                "description": r[2] or "",
                "item_count": int(r[4] or 0),
                "updated_at": int(_dt_to_epoch(r[3])) if r[3] else 0,
            }
            for r in rows
        ]
        return jsonify({"playlists": playlists})
    finally:
        conn.close()


@app.post("/api/library/playlists")
def api_library_playlists_create():
    """Create a local playlist (Files mode only)."""
    if _get_library_mode() != "files":
        return jsonify({"error": "Files mode required"}), 400
    ok, err = _ensure_files_index_ready()
    if not ok:
        return jsonify({"error": err or "Files index unavailable"}), 503
    data = request.get_json(silent=True) or {}
    if not isinstance(data, dict):
        data = {}
    name = str(data.get("name") or "").strip()
    description = str(data.get("description") or "").strip() or None
    if not name:
        return jsonify({"error": "name is required"}), 400
    if len(name) > 160:
        return jsonify({"error": "name too long"}), 400
    conn = _files_pg_connect()
    if conn is None:
        return jsonify({"error": "PostgreSQL unavailable"}), 503
    try:
        with conn.transaction():
            with conn.cursor() as cur:
                cur.execute(
                    "INSERT INTO files_playlists(name, description, created_at, updated_at) VALUES (%s, %s, NOW(), NOW()) RETURNING id",
                    (name, description),
                )
                pid = int((cur.fetchone() or [0])[0] or 0)
        return jsonify({"playlist_id": pid, "name": name, "description": description or "", "item_count": 0, "updated_at": int(time.time())})
    finally:
        conn.close()


@app.get("/api/library/playlists/<int:playlist_id>")
def api_library_playlist_detail(playlist_id: int):
    """Return playlist details and items (Files mode only)."""
    if _get_library_mode() != "files":
        return jsonify({"error": "Files mode required"}), 400
    ok, err = _ensure_files_index_ready()
    if not ok:
        return jsonify({"error": err or "Files index unavailable"}), 503
    conn = _files_pg_connect()
    if conn is None:
        return jsonify({"error": "PostgreSQL unavailable"}), 503
    try:
        with conn.cursor() as cur:
            cur.execute("SELECT id, name, COALESCE(description, ''), updated_at FROM files_playlists WHERE id = %s", (int(playlist_id),))
            pl = cur.fetchone()
            if not pl:
                return jsonify({"error": "Playlist not found"}), 404
            cur.execute(
                """
                SELECT
                    it.id AS item_id,
                    it.position,
                    it.added_at,
                    tr.id AS track_id,
                    tr.title AS track_title,
                    tr.duration_sec,
                    tr.track_num,
                    tr.disc_num,
                    alb.id AS album_id,
                    alb.title AS album_title,
                    alb.has_cover,
                    art.id AS artist_id,
                    art.name AS artist_name
                FROM files_playlist_items it
                JOIN files_tracks tr ON tr.id = it.track_id
                JOIN files_albums alb ON alb.id = tr.album_id
                JOIN files_artists art ON art.id = alb.artist_id
                WHERE it.playlist_id = %s
                ORDER BY it.position ASC, it.id ASC
                """,
                (int(playlist_id),),
            )
            rows = cur.fetchall()
        base_url = request.url_root.rstrip("/")
        items: list[dict] = []
        for r in rows:
            item_id = int(r[0] or 0)
            track_id = int(r[3] or 0)
            album_id = int(r[8] or 0)
            has_cover = bool(r[10])
            items.append(
                {
                    "item_id": item_id,
                    "position": int(r[1] or 0),
                    "added_at": int(_dt_to_epoch(r[2])) if r[2] else 0,
                    "track": {
                        "track_id": track_id,
                        "title": r[4] or "",
                        "artist_id": int(r[11] or 0),
                        "artist_name": r[12] or "",
                        "album_id": album_id,
                        "album_title": r[9] or "",
                        "duration_sec": int(r[5] or 0),
                        "track_num": int(r[6] or 0),
                        "disc_num": int(r[7] or 0),
                        "thumb": f"{base_url}/api/library/files/album/{album_id}/cover?size=96" if has_cover else None,
                        "file_url": f"{base_url}/api/library/track/{track_id}/stream",
                    },
                }
            )
        return jsonify(
            {
                "playlist_id": int(pl[0]),
                "name": pl[1] or "",
                "description": pl[2] or "",
                "updated_at": int(_dt_to_epoch(pl[3])) if pl[3] else 0,
                "items": items,
            }
        )
    finally:
        conn.close()


@app.delete("/api/library/playlists/<int:playlist_id>")
def api_library_playlist_delete(playlist_id: int):
    """Delete a playlist (Files mode only)."""
    if _get_library_mode() != "files":
        return jsonify({"error": "Files mode required"}), 400
    ok, err = _ensure_files_index_ready()
    if not ok:
        return jsonify({"error": err or "Files index unavailable"}), 503
    conn = _files_pg_connect()
    if conn is None:
        return jsonify({"error": "PostgreSQL unavailable"}), 503
    try:
        with conn.transaction():
            with conn.cursor() as cur:
                cur.execute("DELETE FROM files_playlists WHERE id = %s", (int(playlist_id),))
                if cur.rowcount <= 0:
                    return jsonify({"error": "Playlist not found"}), 404
        return jsonify({"ok": True, "playlist_id": int(playlist_id)})
    finally:
        conn.close()


@app.post("/api/library/playlists/<int:playlist_id>/items")
def api_library_playlist_items_add(playlist_id: int):
    """Append tracks (or an album) to a playlist (Files mode only)."""
    if _get_library_mode() != "files":
        return jsonify({"error": "Files mode required"}), 400
    ok, err = _ensure_files_index_ready()
    if not ok:
        return jsonify({"error": err or "Files index unavailable"}), 503
    data = request.get_json(silent=True) or {}
    if not isinstance(data, dict):
        data = {}
    track_ids_raw = data.get("track_ids")
    track_id_single = _parse_int_loose(data.get("track_id"), 0)
    album_id = _parse_int_loose(data.get("album_id"), 0)

    track_ids: list[int] = []
    if isinstance(track_ids_raw, list):
        for x in track_ids_raw:
            tid = _parse_int_loose(x, 0)
            if tid > 0:
                track_ids.append(int(tid))
    if track_id_single > 0:
        track_ids.append(int(track_id_single))

    conn = _files_pg_connect()
    if conn is None:
        return jsonify({"error": "PostgreSQL unavailable"}), 503
    try:
        with conn.transaction():
            with conn.cursor() as cur:
                cur.execute("SELECT id FROM files_playlists WHERE id = %s", (int(playlist_id),))
                if not cur.fetchone():
                    return jsonify({"error": "Playlist not found"}), 404

                # Expand album -> tracks (ordered).
                if album_id > 0:
                    cur.execute(
                        """
                        SELECT id
                        FROM files_tracks
                        WHERE album_id = %s
                        ORDER BY disc_num ASC, track_num ASC, id ASC
                        """,
                        (int(album_id),),
                    )
                    for (tid,) in cur.fetchall():
                        tid_int = int(tid or 0)
                        if tid_int > 0:
                            track_ids.append(tid_int)

                # Deduplicate while preserving order.
                seen = set()
                track_ids = [t for t in track_ids if not (t in seen or seen.add(t))]
                if not track_ids:
                    return jsonify({"error": "track_ids (or album_id) required"}), 400

                cur.execute("SELECT COALESCE(MAX(position), -1) FROM files_playlist_items WHERE playlist_id = %s", (int(playlist_id),))
                max_pos = int((cur.fetchone() or [-1])[0] or -1)
                pos = max_pos + 1
                inserted = 0
                for tid in track_ids:
                    cur.execute(
                        """
                        INSERT INTO files_playlist_items(playlist_id, track_id, position, added_at)
                        VALUES (%s, %s, %s, NOW())
                        """,
                        (int(playlist_id), int(tid), int(pos)),
                    )
                    inserted += 1
                    pos += 1
                cur.execute("UPDATE files_playlists SET updated_at = NOW() WHERE id = %s", (int(playlist_id),))
        return jsonify({"ok": True, "playlist_id": int(playlist_id), "inserted": int(inserted)})
    finally:
        conn.close()


@app.delete("/api/library/playlists/<int:playlist_id>/items/<int:item_id>")
def api_library_playlist_item_delete(playlist_id: int, item_id: int):
    """Remove a single playlist item (Files mode only)."""
    if _get_library_mode() != "files":
        return jsonify({"error": "Files mode required"}), 400
    ok, err = _ensure_files_index_ready()
    if not ok:
        return jsonify({"error": err or "Files index unavailable"}), 503
    conn = _files_pg_connect()
    if conn is None:
        return jsonify({"error": "PostgreSQL unavailable"}), 503
    try:
        with conn.transaction():
            with conn.cursor() as cur:
                cur.execute(
                    "DELETE FROM files_playlist_items WHERE id = %s AND playlist_id = %s",
                    (int(item_id), int(playlist_id)),
                )
                if cur.rowcount <= 0:
                    return jsonify({"error": "Item not found"}), 404
                cur.execute("UPDATE files_playlists SET updated_at = NOW() WHERE id = %s", (int(playlist_id),))
        return jsonify({"ok": True, "playlist_id": int(playlist_id), "item_id": int(item_id)})
    finally:
        conn.close()


@app.post("/api/library/playlists/<int:playlist_id>/reorder")
def api_library_playlist_reorder(playlist_id: int):
    """Reorder playlist items by item_id list (Files mode only)."""
    if _get_library_mode() != "files":
        return jsonify({"error": "Files mode required"}), 400
    ok, err = _ensure_files_index_ready()
    if not ok:
        return jsonify({"error": err or "Files index unavailable"}), 503
    data = request.get_json(silent=True) or {}
    if not isinstance(data, dict):
        data = {}
    item_ids_raw = data.get("item_ids")
    if not isinstance(item_ids_raw, list) or not item_ids_raw:
        return jsonify({"error": "item_ids is required"}), 400
    item_ids: list[int] = []
    for x in item_ids_raw:
        iid = _parse_int_loose(x, 0)
        if iid > 0:
            item_ids.append(int(iid))
    if not item_ids:
        return jsonify({"error": "item_ids is required"}), 400

    conn = _files_pg_connect()
    if conn is None:
        return jsonify({"error": "PostgreSQL unavailable"}), 503
    try:
        with conn.transaction():
            with conn.cursor() as cur:
                cur.execute("SELECT id FROM files_playlists WHERE id = %s", (int(playlist_id),))
                if not cur.fetchone():
                    return jsonify({"error": "Playlist not found"}), 404
                for idx, iid in enumerate(item_ids):
                    cur.execute(
                        "UPDATE files_playlist_items SET position = %s WHERE id = %s AND playlist_id = %s",
                        (int(idx), int(iid), int(playlist_id)),
                    )
                cur.execute("UPDATE files_playlists SET updated_at = NOW() WHERE id = %s", (int(playlist_id),))
        return jsonify({"ok": True, "playlist_id": int(playlist_id), "count": len(item_ids)})
    finally:
        conn.close()


@app.post("/api/library/reco/event")
def api_library_reco_event():
    """Record a playback/session event for recommendation ranking (Files mode)."""
    if _get_library_mode() != "files":
        return jsonify({"error": "Files mode required"}), 400
    ok, err = _ensure_files_index_ready()
    if not ok:
        return jsonify({"error": err or "Files index unavailable"}), 503
    data = request.get_json() or {}
    session_id = str(data.get("session_id") or "").strip()
    track_id = _parse_int_loose(data.get("track_id"), 0)
    event_type = str(data.get("event_type") or "").strip().lower()
    played_seconds = _parse_int_loose(data.get("played_seconds"), 0)
    if not session_id:
        return jsonify({"error": "session_id is required"}), 400
    if not re.match(r"^[a-zA-Z0-9._:-]{6,128}$", session_id):
        return jsonify({"error": "Invalid session_id format"}), 400
    if track_id <= 0:
        return jsonify({"error": "track_id is required"}), 400
    conn = _files_pg_connect()
    if conn is None:
        return jsonify({"error": "PostgreSQL unavailable"}), 503
    try:
        with conn.transaction():
            success, message = _reco_record_event(conn, session_id, track_id, event_type, played_seconds)
        if not success:
            status = 404 if "track not found" in message else 400
            return jsonify({"error": message}), status
        return jsonify({"ok": True, "session_id": session_id, "track_id": track_id, "event_type": event_type})
    finally:
        conn.close()


@app.post("/api/library/playback/event")
def api_library_playback_event():
    """Record a listening event (Files mode). Used for user listening statistics charts."""
    if _get_library_mode() != "files":
        return jsonify({"error": "Files mode required"}), 400
    ok, err = _ensure_files_index_ready()
    if not ok:
        return jsonify({"error": err or "Files index unavailable"}), 503
    data = request.get_json(silent=True) or {}
    if not isinstance(data, dict):
        data = {}
    track_id = _parse_int_loose(data.get("track_id"), 0)
    event_type = str(data.get("event_type") or "").strip().lower() or "play_partial"
    played_seconds = _parse_int_loose(data.get("played_seconds"), 0)
    user_id = 1
    if track_id <= 0:
        return jsonify({"error": "track_id is required"}), 400
    if played_seconds < 0:
        played_seconds = 0
    if event_type not in {"play_complete", "play_partial", "skip", "stop", "play_start"}:
        event_type = "play_partial"

    conn = _files_pg_connect()
    if conn is None:
        return jsonify({"error": "PostgreSQL unavailable"}), 503
    try:
        with conn.cursor() as cur:
            cur.execute("SELECT duration_sec FROM files_tracks WHERE id = %s", (int(track_id),))
            row = cur.fetchone()
            if not row:
                return jsonify({"error": "track not found"}), 404
            dur = int(row[0] or 0)
        if dur > 0:
            played_seconds = max(0, min(int(played_seconds), int(dur)))
        else:
            played_seconds = max(0, min(int(played_seconds), 60 * 60 * 8))

        with conn.transaction():
            with conn.cursor() as cur:
                cur.execute(
                    """
                    INSERT INTO files_playback_events(user_id, track_id, event_type, played_seconds, created_at)
                    VALUES (%s, %s, %s, %s, NOW())
                    """,
                    (int(user_id), int(track_id), str(event_type), int(played_seconds)),
                )
        return jsonify({"ok": True, "track_id": int(track_id), "event_type": event_type, "played_seconds": int(played_seconds)})
    finally:
        conn.close()


@app.get("/api/library/playback/stats")
def api_library_playback_stats():
    """Return listening statistics for charts (Files mode)."""
    if _get_library_mode() != "files":
        return jsonify({"error": "Files mode required"}), 400
    ok, err = _ensure_files_index_ready()
    if not ok:
        return jsonify({"error": err or "Files index unavailable"}), 503
    days = max(1, min(365, _parse_int_loose(request.args.get("days"), 30)))
    user_id = 1

    conn = _files_pg_connect()
    if conn is None:
        return jsonify({"error": "PostgreSQL unavailable"}), 503
    try:
        with conn.cursor() as cur:
            cur.execute(
                """
                WITH filtered AS (
                    SELECT track_id, event_type, played_seconds, created_at
                    FROM files_playback_events
                    WHERE user_id = %s
                      AND created_at >= NOW() - (%s || ' days')::interval
                )
                SELECT
                    COALESCE(SUM(played_seconds), 0) AS total_seconds,
                    COUNT(*) AS events,
                    COUNT(DISTINCT track_id) AS distinct_tracks
                FROM filtered
                """,
                (int(user_id), int(days)),
            )
            row = cur.fetchone() or [0, 0, 0]
            total_seconds = int(row[0] or 0)
            events = int(row[1] or 0)
            distinct_tracks = int(row[2] or 0)

            cur.execute(
                """
                WITH filtered AS (
                    SELECT track_id, event_type, played_seconds, created_at
                    FROM files_playback_events
                    WHERE user_id = %s
                      AND created_at >= NOW() - (%s || ' days')::interval
                )
                SELECT
                    ar.id AS artist_id,
                    ar.name AS artist_name,
                    COALESCE(SUM(f.played_seconds), 0) AS seconds,
                    COUNT(*) AS plays
                FROM filtered f
                JOIN files_tracks tr ON tr.id = f.track_id
                JOIN files_albums alb ON alb.id = tr.album_id
                JOIN files_artists ar ON ar.id = alb.artist_id
                GROUP BY ar.id, ar.name
                ORDER BY seconds DESC, plays DESC, artist_name ASC
                LIMIT 10
                """
            , (int(user_id), int(days)))
            top_artists = [
                {"artist_id": int(r[0] or 0), "artist_name": r[1] or "", "seconds": int(r[2] or 0), "plays": int(r[3] or 0)}
                for r in cur.fetchall()
                if int(r[0] or 0) > 0
            ]

            cur.execute(
                """
                WITH filtered AS (
                    SELECT track_id, event_type, played_seconds, created_at
                    FROM files_playback_events
                    WHERE user_id = %s
                      AND created_at >= NOW() - (%s || ' days')::interval
                )
                SELECT
                    tr.id AS track_id,
                    tr.title AS track_title,
                    ar.id AS artist_id,
                    ar.name AS artist_name,
                    alb.id AS album_id,
                    alb.title AS album_title,
                    COALESCE(SUM(f.played_seconds), 0) AS seconds,
                    COUNT(*) AS plays
                FROM filtered f
                JOIN files_tracks tr ON tr.id = f.track_id
                JOIN files_albums alb ON alb.id = tr.album_id
                JOIN files_artists ar ON ar.id = alb.artist_id
                GROUP BY tr.id, tr.title, ar.id, ar.name, alb.id, alb.title
                ORDER BY seconds DESC, plays DESC, track_title ASC
                LIMIT 10
                """
            , (int(user_id), int(days)))
            top_tracks = [
                {
                    "track_id": int(r[0] or 0),
                    "track_title": r[1] or "",
                    "artist_id": int(r[2] or 0),
                    "artist_name": r[3] or "",
                    "album_id": int(r[4] or 0),
                    "album_title": r[5] or "",
                    "seconds": int(r[6] or 0),
                    "plays": int(r[7] or 0),
                }
                for r in cur.fetchall()
                if int(r[0] or 0) > 0
            ]

            # Genres: split seconds across tags_json elements to avoid inflating totals.
            cur.execute(
                """
                WITH filtered AS (
                    SELECT track_id, played_seconds, created_at
                    FROM files_playback_events
                    WHERE user_id = %s
                      AND created_at >= NOW() - (%s || ' days')::interval
                ),
                joined AS (
                    SELECT
                        f.played_seconds,
                        COALESCE(NULLIF(alb.tags_json, ''), '[]')::jsonb AS tags
                    FROM filtered f
                    JOIN files_tracks tr ON tr.id = f.track_id
                    JOIN files_albums alb ON alb.id = tr.album_id
                ),
                expanded AS (
                    SELECT
                        lower(trim(g.value)) AS genre,
                        (joined.played_seconds::double precision / GREATEST(1, jsonb_array_length(joined.tags))) AS sec_share
                    FROM joined
                    JOIN LATERAL jsonb_array_elements_text(joined.tags) AS g(value) ON TRUE
                    WHERE COALESCE(trim(g.value), '') <> ''
                )
                SELECT genre, ROUND(SUM(sec_share))::BIGINT AS seconds
                FROM expanded
                GROUP BY genre
                ORDER BY seconds DESC, genre ASC
                LIMIT 10
                """
            , (int(user_id), int(days)))
            top_genres = [{"genre": str(r[0] or ""), "seconds": int(r[1] or 0)} for r in cur.fetchall() if str(r[0] or "").strip()]

            cur.execute(
                """
                WITH filtered AS (
                    SELECT track_id, event_type, played_seconds, created_at
                    FROM files_playback_events
                    WHERE user_id = %s
                      AND created_at >= NOW() - (%s || ' days')::interval
                )
                SELECT to_char(date_trunc('day', created_at), 'YYYY-MM-DD') AS day,
                       COALESCE(SUM(played_seconds), 0) AS seconds,
                       COUNT(*) AS plays
                FROM filtered
                GROUP BY day
                ORDER BY day ASC
                """
            , (int(user_id), int(days)))
            daily = [{"day": str(r[0] or ""), "seconds": int(r[1] or 0), "plays": int(r[2] or 0)} for r in cur.fetchall() if str(r[0] or "").strip()]

            cur.execute(
                """
                WITH filtered AS (
                    SELECT event_type, played_seconds, created_at
                    FROM files_playback_events
                    WHERE user_id = %s
                      AND created_at >= NOW() - (%s || ' days')::interval
                )
                SELECT event_type, COUNT(*) AS c
                FROM filtered
                GROUP BY event_type
                ORDER BY c DESC, event_type ASC
                """
            , (int(user_id), int(days)))
            event_types = [{"event_type": str(r[0] or ""), "count": int(r[1] or 0)} for r in cur.fetchall() if str(r[0] or "").strip()]

            cur.execute(
                """
                WITH filtered AS (
                    SELECT played_seconds, created_at
                    FROM files_playback_events
                    WHERE user_id = %s
                      AND created_at >= NOW() - (%s || ' days')::interval
                )
                SELECT EXTRACT(HOUR FROM created_at)::INT AS hour, COALESCE(SUM(played_seconds), 0) AS seconds
                FROM filtered
                GROUP BY hour
                ORDER BY hour ASC
                """
            , (int(user_id), int(days)))
            hours = [{"hour": int(r[0] or 0), "seconds": int(r[1] or 0)} for r in cur.fetchall()]

        return jsonify(
            {
                "days": int(days),
                "total_seconds": total_seconds,
                "events": events,
                "distinct_tracks": distinct_tracks,
                "top_artists": top_artists,
                "top_tracks": top_tracks,
                "top_genres": top_genres,
                "daily": daily,
                "event_types": event_types,
                "hours": hours,
            }
        )
    finally:
        conn.close()


@app.get("/api/library/reco/for-you")
def api_library_reco_for_you():
    """Return personalized track recommendations for the active session (Files mode)."""
    if _get_library_mode() != "files":
        return jsonify({"session_id": "", "tracks": []})
    ok, err = _ensure_files_index_ready()
    if not ok:
        return jsonify({"session_id": "", "tracks": [], "error": err or "Files index unavailable"}), 503

    session_id = str(request.args.get("session_id") or "").strip()
    limit = max(1, min(40, _parse_int_loose(request.args.get("limit"), 12)))
    exclude_track_id = _parse_int_loose(request.args.get("exclude_track_id"), 0)
    if session_id and not re.match(r"^[a-zA-Z0-9._:-]{6,128}$", session_id):
        return jsonify({"error": "Invalid session_id format"}), 400

    conn = _files_pg_connect()
    if conn is None:
        return jsonify({"session_id": session_id, "tracks": [], "error": "PostgreSQL unavailable"}), 503
    try:
        with conn.cursor() as cur:
            if session_id:
                cur.execute("SELECT COALESCE(MAX(id), 0) FROM files_reco_events WHERE session_id = %s", (session_id,))
            else:
                cur.execute("SELECT COALESCE(MAX(id), 0) FROM files_reco_events")
            token = int((cur.fetchone() or [0])[0] or 0)
        cache_key = f"library:reco:for_you:{session_id or 'global'}:{limit}:{exclude_track_id}:{token}"
        cached = _files_cache_get_json(cache_key)
        if cached is not None:
            return jsonify(cached)

        profile = _reco_build_session_profile(conn, session_id) if session_id else {"has_data": False}
        candidate_limit = max(220, min(4000, limit * 90))
        candidates = _reco_fetch_candidates(conn, profile, candidate_limit)
        if exclude_track_id > 0:
            candidates = [c for c in candidates if int(c.get("track_id") or 0) != int(exclude_track_id)]
        ranked = _reco_rank_candidates(profile, candidates, limit)
        base_url = request.url_root.rstrip("/")
        tracks = []
        for c in ranked:
            track_id = int(c.get("track_id") or 0)
            album_id = int(c.get("album_id") or 0)
            artist_id = int(c.get("artist_id") or 0)
            if track_id <= 0:
                continue
            tracks.append(
                {
                    "track_id": track_id,
                    "title": c.get("title") or "",
                    "artist_id": artist_id,
                    "artist_name": c.get("artist_name") or "",
                    "album_id": album_id,
                    "album_title": c.get("album_title") or "",
                    "duration_sec": int(c.get("duration_sec") or 0),
                    "track_num": int(c.get("track_num") or 0),
                    "score": round(float(c.get("score") or 0.0), 4),
                    "reasons": c.get("reasons") or [],
                    "thumb": f"{base_url}/api/library/files/album/{album_id}/cover?size=96" if bool(c.get("has_cover")) else None,
                    "file_url": f"{base_url}/api/library/track/{track_id}/stream",
                }
            )
        payload = {
            "session_id": session_id,
            "tracks": tracks,
            "session_event_count": int(profile.get("session_event_count") or 0),
            "algorithm": RECO_EMBED_SOURCE,
        }
        _files_cache_set_json(cache_key, payload, ttl=20)
        return jsonify(payload)
    finally:
        conn.close()


@app.get("/api/library/artist/<int:artist_id>")
def api_library_artist_detail(artist_id):
    """Return detailed information about an artist including all albums with images and types."""
    if _get_library_mode() == "files":
        cache_key = f"library:artist:{artist_id}"
        cached = _files_cache_get_json(cache_key)
        if cached is not None:
            return jsonify(cached)
        ok, err = _ensure_files_index_ready()
        if not ok:
            return jsonify({"error": err or "Files index unavailable"}), 503
        conn = _files_pg_connect()
        if conn is None:
            return jsonify({"error": "PostgreSQL unavailable"}), 503
        try:
            with conn.cursor() as cur:
                cur.execute(
                    "SELECT id, name, has_image, image_path FROM files_artists WHERE id = %s",
                    (artist_id,),
                )
                artist_row = cur.fetchone()
                if not artist_row:
                    return jsonify({"error": "Artist not found"}), 404
                artist_name = artist_row[1] or ""
                artist_norm = " ".join((artist_name or "").split()).lower()
                has_artist_image = bool(artist_row[2])
                artist_image_path = (artist_row[3] or "").strip()
                cur.execute(
                    """
                    SELECT
                        id, title, title_norm, year, date_text, track_count, is_broken, format, is_lossless,
                        has_cover, mb_identified, musicbrainz_release_group_id,
                        discogs_release_id, lastfm_album_mbid, bandcamp_album_url, metadata_source,
                        expected_track_count, actual_track_count, missing_indices_json,
                        COUNT(*) OVER (PARTITION BY title_norm) AS dup_count
                    FROM files_albums
                    WHERE artist_id = %s
                    ORDER BY COALESCE(year, 0) DESC, title ASC
                    """,
                    (artist_id,),
                )
                rows = cur.fetchall()

            artist_profile = _files_get_artist_profile_cached(artist_name, artist_norm)
            title_norms = [str(r[2] or "") for r in rows if str(r[2] or "").strip()]
            album_profile_map = _files_get_album_profiles_cached(artist_norm, title_norms)
            profile_enriching = _enqueue_files_profile_enrichment(
                artist_name,
                artist_norm,
                [(str(r[1] or ""), str(r[2] or "")) for r in rows],
            )

            albums = []
            stats_duplicates = 0
            stats_no_cover = 0
            stats_mb = 0
            stats_broken = 0
            for row in rows:
                album_id = int(row[0])
                title_norm = str(row[2] or "")
                track_count = int(row[5] or 0)
                is_broken = bool(row[6])
                fmt = (row[7] or "").strip() or None
                is_lossless = bool(row[8])
                has_cover = bool(row[9])
                mb_identified = bool(row[10])
                mbid = (row[11] or "").strip() or None
                discogs_release_id = (row[12] or "").strip() or None
                lastfm_album_mbid = (row[13] or "").strip() or None
                bandcamp_album_url = (row[14] or "").strip() or None
                metadata_source = (row[15] or "").strip() or None
                expected_track_count = row[16]
                actual_track_count = row[17]
                missing_indices_raw = row[18] or "[]"
                dup_count = int(row[19] or 0)
                album_profile = album_profile_map.get(title_norm, {}) if title_norm else {}

                if dup_count > 1:
                    stats_duplicates += 1
                if not has_cover:
                    stats_no_cover += 1
                if mb_identified:
                    stats_mb += 1
                if is_broken:
                    stats_broken += 1

                album_type = "Album"
                if track_count <= 3:
                    album_type = "Single"
                elif track_count <= 6:
                    album_type = "EP"

                try:
                    missing_indices = json.loads(missing_indices_raw) if missing_indices_raw else []
                except (TypeError, ValueError):
                    missing_indices = []

                broken_detail = None
                if is_broken:
                    broken_detail = {
                        "expected_track_count": int(expected_track_count or track_count),
                        "actual_track_count": int(actual_track_count or track_count),
                        "missing_indices": missing_indices if isinstance(missing_indices, list) else [],
                    }

                thumb_url_files = f"{request.url_root.rstrip('/')}/api/library/files/album/{album_id}/cover" if has_cover else None
                can_improve = (not is_lossless) or (not has_cover) or (not mb_identified) or is_broken
                albums.append({
                    "album_id": album_id,
                    "title": row[1] or "",
                    "year": row[3],
                    "date": row[4] or "",
                    "track_count": track_count,
                    "is_broken": is_broken,
                    "thumb": f"{thumb_url_files}?size=320" if thumb_url_files else None,
                    "type": album_type,
                    "format": fmt,
                    "is_lossless": is_lossless,
                    "thumb_empty": not has_cover,
                    "mb_identified": mb_identified,
                    "musicbrainz_release_group_id": mbid,
                    "discogs_release_id": discogs_release_id,
                    "lastfm_album_mbid": lastfm_album_mbid,
                    "bandcamp_album_url": bandcamp_album_url,
                    "metadata_source": metadata_source,
                    "in_duplicate_group": dup_count > 1,
                    "can_improve": can_improve,
                    "broken_detail": broken_detail,
                    "description": album_profile.get("description"),
                    "short_description": album_profile.get("short_description"),
                    "description_source": album_profile.get("source"),
                })

            artist_thumb = None
            base_url = request.url_root.rstrip("/")
            if has_artist_image and artist_image_path:
                artist_thumb = f"{base_url}/api/library/files/artist/{artist_id}/image?size=320"
            else:
                # External cached image (e.g. Last.fm) for artists without on-disk artwork.
                try:
                    with conn.cursor() as cur:
                        cur.execute(
                            "SELECT COALESCE(image_path,'') FROM files_external_artist_images WHERE name_norm = %s",
                            (artist_norm,),
                        )
                        erow = cur.fetchone()
                    ext_path = (erow[0] or "").strip() if erow else ""
                    if ext_path and Path(ext_path).exists():
                        artist_thumb = f"{base_url}/api/library/external/artist-image/{quote(artist_norm, safe='')}?size=320"
                except Exception:
                    pass
            # Patch similar artists with local IDs + images (local first, then cached external).
            try:
                if isinstance(artist_profile, dict):
                    sim = artist_profile.get("similar_artists")
                    if isinstance(sim, list) and sim:
                        artist_profile = dict(artist_profile)
                        artist_profile["similar_artists"] = _files_attach_similar_artist_refs(conn, sim, base_url)
                        # If images are still missing (e.g. provider only returned placeholders),
                        # warm external image cache in the background so the grid becomes pretty.
                        missing_names: list[str] = []
                        for it in artist_profile.get("similar_artists") or []:
                            if not isinstance(it, dict):
                                continue
                            nm = str(it.get("name") or "").strip()
                            if not nm:
                                continue
                            if not str(it.get("image_url") or "").strip():
                                missing_names.append(nm)
                        if missing_names:
                            _enqueue_files_similar_images_warm(artist_norm, missing_names[:12])
            except Exception:
                pass
            payload = {
                "artist_id": artist_id,
                "artist_name": artist_name,
                "artist_thumb": artist_thumb,
                "artist_profile": artist_profile,
                "profile_enriching": profile_enriching,
                "albums": albums,
                "total_albums": len(albums),
                "stats": {
                    "duplicates": stats_duplicates,
                    "no_cover": stats_no_cover,
                    "mb_identified": stats_mb,
                    "broken": stats_broken,
                },
            }
            _files_cache_set_json(cache_key, payload, ttl=30)
            return jsonify(payload)
        finally:
            conn.close()

    _reload_section_ids_from_db()
    if not PLEX_CONFIGURED:
        return jsonify({"error": "Plex not configured"}), 503
    
    import sqlite3
    db_conn = plex_connect()
    
    # Get artist info
    artist_row = db_conn.execute(
        "SELECT id, title FROM metadata_items WHERE id = ? AND metadata_type = 8",
        (artist_id,)
    ).fetchone()
    
    if not artist_row:
        db_conn.close()
        return jsonify({"error": "Artist not found"}), 404
    
    artist_name = artist_row[1]
    name_norm = (artist_name or "").strip().lower()
    # Collect all artist IDs with same normalized name in selected sections
    artist_ids_same_name = [artist_id]
    try:
        placeholders_sections = ",".join("?" for _ in SECTION_IDS) if SECTION_IDS else ""
        if placeholders_sections:
            rows_same = db_conn.execute(
                f"""
                SELECT id FROM metadata_items 
                WHERE metadata_type = 8 
                  AND title IS NOT NULL 
                  AND LOWER(TRIM(title)) = ? 
                  AND library_section_id IN ({placeholders_sections})
                """,
                [name_norm] + list(SECTION_IDS)
            ).fetchall()
            artist_ids_same_name = list({r[0] for r in rows_same} | {artist_id})
    except Exception:
        artist_ids_same_name = [artist_id]
    
    # Get artist thumb from Plex
    artist_thumb = None
    try:
        thumb_row = db_conn.execute(
            "SELECT thumb FROM metadata_items WHERE id = ? AND metadata_type = 8",
            (artist_id,)
        ).fetchone()
        if thumb_row and thumb_row[0]:
            artist_thumb = thumb_url(artist_id)
    except Exception:
        pass
    
    # Get all albums for this artist (only from selected sections ‚Äî SECTION_IDS)
    placeholders = ",".join("?" for _ in SECTION_IDS) if SECTION_IDS else ""
    if not placeholders:
        album_rows = []
    else:
        section_filter = f"AND alb.library_section_id IN ({placeholders})"
        section_args = artist_ids_same_name + list(SECTION_IDS)
        # Do not select alb.thumb ‚Äî column may not exist in all Plex DB versions
        album_rows = db_conn.execute(f"""
            SELECT 
                alb.id,
                alb.title,
                alb.year,
                alb.originally_available_at,
                COUNT(DISTINCT tr.id) as track_count
            FROM metadata_items alb
            LEFT JOIN metadata_items tr ON tr.parent_id = alb.id AND tr.metadata_type = 10
            WHERE alb.parent_id IN ({",".join("?" for _ in artist_ids_same_name)}) AND alb.metadata_type = 9
                {section_filter}
            GROUP BY alb.id, alb.title, alb.year, alb.originally_available_at
            ORDER BY alb.originally_available_at DESC, alb.title
        """, section_args).fetchall()
    
    # Batch-fetch track indices from Plex for gap detection (incomplete albums) without relying on scan
    album_ids = [r[0] for r in album_rows]
    indices_by_album: dict[int, list[int]] = {}
    if album_ids:
        try:
            ph = ",".join("?" for _ in album_ids)
            track_index_rows = db_conn.execute(
                f'SELECT parent_id, "index" FROM metadata_items WHERE metadata_type = 10 AND parent_id IN ({ph})',
                album_ids,
            ).fetchall()
            for pid, idx in track_index_rows:
                if pid is not None and idx is not None:
                    indices_by_album.setdefault(pid, []).append(int(idx))
        except Exception as e:
            logging.debug("Batch track indices query failed: %s", e)
    
    albums = []
    
    # Prefer scan_editions when a completed scan exists (source of truth for format, tags, broken, duplicate group)
    con = sqlite3.connect(str(STATE_DB_FILE), timeout=30)
    cur = con.cursor()
    scan_id = get_last_completed_scan_id()
    scan_editions_by_album: dict[int, dict] = {}
    dup_album_ids_from_scan: set[int] = set()
    if scan_id and artist_name:
        try:
            cur.execute("""
                SELECT album_id, title_raw, folder, fmt_text, br, sr, bd, meta_json, musicbrainz_id,
                       is_broken, expected_track_count, actual_track_count, missing_indices, has_cover, missing_required_tags
                       , discogs_release_id, lastfm_album_mbid, bandcamp_album_url, metadata_source
                FROM scan_editions WHERE scan_id = ? AND artist = ?
            """, (scan_id, artist_name))
            for row in cur.fetchall():
                aid = row[0]
                scan_editions_by_album[aid] = {
                    "title_raw": row[1], "folder": row[2], "fmt_text": row[3], "br": row[4], "sr": row[5], "bd": row[6],
                    "meta_json": row[7], "musicbrainz_id": row[8], "is_broken": row[9], "expected_track_count": row[10],
                    "actual_track_count": row[11], "missing_indices": row[12], "has_cover": row[13], "missing_required_tags": row[14],
                    "discogs_release_id": row[15], "lastfm_album_mbid": row[16], "bandcamp_album_url": row[17],
                    "metadata_source": _normalize_identity_provider(str(row[18] or "")),
                }
            cur.execute("SELECT album_id FROM duplicates_best WHERE artist = ?", (artist_name,))
            dup_album_ids_from_scan.update(r[0] for r in cur.fetchall())
            cur.execute("SELECT album_id FROM duplicates_loser WHERE artist = ?", (artist_name,))
            dup_album_ids_from_scan.update(r[0] for r in cur.fetchall())
        except Exception as e:
            logging.debug("scan_editions query for Library artist failed: %s", e)
            scan_editions_by_album = {}
            dup_album_ids_from_scan = set()
    
    lossless_formats = {"FLAC", "ALAC", "APE", "WV", "WAV", "AIFF", "OGG"}
    
    for album_row in album_rows:
        album_id, title, year, date, track_count = album_row
        se = scan_editions_by_album.get(album_id)
        in_duplicate_group = (album_id in dup_album_ids_from_scan) if dup_album_ids_from_scan else False
        musicbrainz_release_group_id = None

        if se:
            # Use scan_editions as source of truth
            format_str = se.get("fmt_text") or None
            is_lossless = bool(format_str and format_str.upper() in lossless_formats)
            mb_identified = bool(se.get("musicbrainz_id"))
            if se.get("musicbrainz_id"):
                musicbrainz_release_group_id = se.get("musicbrainz_id")
            discogs_release_id = str(se.get("discogs_release_id") or "").strip() or None
            lastfm_album_mbid = str(se.get("lastfm_album_mbid") or "").strip() or None
            bandcamp_album_url = str(se.get("bandcamp_album_url") or "").strip() or None
            metadata_source = _normalize_identity_provider(str(se.get("metadata_source") or "")) or None
            thumb_empty = not (se.get("has_cover"))
            is_broken = bool(se.get("is_broken"))
            missing_raw = se.get("missing_indices")
            broken_detail = None
            if is_broken and (se.get("expected_track_count") is not None or missing_raw):
                broken_detail = {
                    "expected_track_count": se.get("expected_track_count") or 0,
                    "actual_track_count": se.get("actual_track_count") or 0,
                    "missing_indices": json.loads(missing_raw) if isinstance(missing_raw, str) and missing_raw else (missing_raw or []),
                }
            album_type = "Album"
            if (track_count or 0) <= 3:
                album_type = "Single"
            elif (track_count or 0) <= 6:
                album_type = "EP"
        else:
            # Fallback: broken_albums + on-disk format/tags
            cur.execute("SELECT expected_track_count, actual_track_count, missing_indices FROM broken_albums WHERE artist = ? AND album_id = ?", (artist_name, album_id))
            broken_row = cur.fetchone()
            is_broken = broken_row is not None
            broken_detail = None
            if broken_row:
                missing_raw = broken_row[2]
                broken_detail = {
                    "expected_track_count": broken_row[0] or 0,
                    "actual_track_count": broken_row[1] or 0,
                    "missing_indices": json.loads(missing_raw) if isinstance(missing_raw, str) and missing_raw else []
                }
            else:
                indices = indices_by_album.get(album_id, [])
                if indices:
                    broken_from_gaps, actual_count, gaps = _detect_gaps_in_indices(indices)
                    if broken_from_gaps:
                        is_broken = True
                        broken_detail = {
                            "expected_track_count": None,
                            "actual_track_count": actual_count,
                            "missing_indices": list(gaps),
                        }
            thumb_empty = False
            format_str = None
            is_lossless = False
            mb_identified = False
            discogs_release_id = None
            lastfm_album_mbid = None
            bandcamp_album_url = None
            metadata_source = None
            album_type = "Album"
            folder = first_part_path(db_conn, album_id)
            if folder:
                if not album_folder_has_cover(folder):
                    thumb_empty = True
                format_str = get_primary_format(folder)
                if format_str and format_str.upper() in lossless_formats:
                    is_lossless = True
                first_audio = next((p for p in folder.rglob("*") if AUDIO_RE.search(p.name)), None)
                if first_audio:
                    meta = extract_tags(first_audio)
                    mb_identified = bool(meta.get("musicbrainz_releasegroupid") or meta.get("musicbrainz_releaseid"))
                    if meta.get("compilation") == "1" or meta.get("compilation") == "true":
                        album_type = "Compilation"
                    elif USE_MUSICBRAINZ:
                        mbid = meta.get("musicbrainz_releasegroupid") or meta.get("musicbrainz_releaseid")
                        if mbid:
                            if meta.get("musicbrainz_releasegroupid"):
                                musicbrainz_release_group_id = meta.get("musicbrainz_releasegroupid")
                            tag_src = "musicbrainz_releasegroupid" if meta.get("musicbrainz_releasegroupid") else "musicbrainz_releaseid"
                            rgid = resolve_mbid_to_release_group(mbid, tag_src)
                            if rgid:
                                if not musicbrainz_release_group_id:
                                    musicbrainz_release_group_id = rgid
                                try:
                                    result = musicbrainzngs.get_release_group_by_id(rgid, includes=["tags"])
                                    release_group = result.get("release-group", {})
                                    primary_type = release_group.get("primary-type", "")
                                    secondary_types = release_group.get("secondary-type-list", [])
                                    if primary_type:
                                        album_type = primary_type
                                    if "Compilation" in secondary_types:
                                        album_type = "Compilation"
                                    elif "Anthology" in secondary_types:
                                        album_type = "Anthology"
                                except Exception:
                                    pass
                    if (track_count or 0) <= 3 and album_type == "Album":
                        album_type = "Single"
                    elif (track_count or 0) <= 6 and album_type == "Album":
                        album_type = "EP"

        try:
            album_thumb = thumb_url(album_id)
        except Exception:
            album_thumb = None

        can_improve = not is_lossless or thumb_empty or not mb_identified or is_broken
        
        albums.append({
            "album_id": album_id,
            "title": title,
            "year": year,
            "date": date,
            "track_count": track_count or 0,
            "is_broken": is_broken,
            "thumb": album_thumb,
            "type": album_type,
            "format": format_str,
            "is_lossless": is_lossless,
            "thumb_empty": thumb_empty,
            "mb_identified": mb_identified,
            "musicbrainz_release_group_id": musicbrainz_release_group_id,
            "discogs_release_id": discogs_release_id,
            "lastfm_album_mbid": lastfm_album_mbid,
            "bandcamp_album_url": bandcamp_album_url,
            "metadata_source": metadata_source,
            "in_duplicate_group": in_duplicate_group,
            "can_improve": can_improve,
            "broken_detail": broken_detail,
        })
    
    # Duplicates: when scan data exists use dup_album_ids_from_scan; otherwise same artist + same album name (normalized)
    if dup_album_ids_from_scan:
        stats_duplicates = len(dup_album_ids_from_scan)
    else:
        norm_to_album_ids: dict[str, list[int]] = {}
        for a in albums:
            norm = norm_album(a.get("title", "") or "")
            norm_to_album_ids.setdefault(norm, []).append(a["album_id"])
        dup_album_ids: set[int] = set()
        for aid_list in norm_to_album_ids.values():
            if len(aid_list) > 1:
                dup_album_ids.update(aid_list)
        stats_duplicates = len(dup_album_ids)
        for a in albums:
            if a["album_id"] in dup_album_ids:
                a["in_duplicate_group"] = True
    stats_no_cover = sum(1 for a in albums if a.get("thumb_empty"))
    stats_mb = sum(1 for a in albums if a.get("mb_identified"))
    stats_broken = sum(1 for a in albums if a.get("is_broken"))
    
    con.close()
    db_conn.close()
    
    return jsonify({
        "artist_id": artist_id,
        "artist_name": artist_name,
        "artist_thumb": artist_thumb,
        "albums": albums,
        "total_albums": len(albums),
        "stats": {
            "duplicates": stats_duplicates,
            "no_cover": stats_no_cover,
            "mb_identified": stats_mb,
            "broken": stats_broken,
        },
    })


@app.get("/api/library/artist/<int:artist_id>/profile")
def api_library_artist_profile(artist_id: int):
    """Return cached artist profile (bio/tags/similar) and trigger async enrichment when missing/stale."""
    if _get_library_mode() != "files":
        return jsonify({"error": "Artist profile endpoint is available in Files mode only"}), 400
    ok, err = _ensure_files_index_ready()
    if not ok:
        return jsonify({"error": err or "Files index unavailable"}), 503
    conn = _files_pg_connect()
    if conn is None:
        return jsonify({"error": "PostgreSQL unavailable"}), 503
    try:
        with conn.cursor() as cur:
            cur.execute(
                """
                SELECT id, name, name_norm
                FROM files_artists
                WHERE id = %s
                """,
                (artist_id,),
            )
            row = cur.fetchone()
            if not row:
                return jsonify({"error": "Artist not found"}), 404
            artist_name = row[1] or ""
            artist_norm = row[2] or " ".join((artist_name or "").split()).lower()
            cur.execute(
                """
                SELECT title, title_norm
                FROM files_albums
                WHERE artist_id = %s
                ORDER BY COALESCE(year, 0) DESC, title ASC
                LIMIT 180
                """,
                (artist_id,),
            )
            albums = [(str(r[0] or ""), str(r[1] or "")) for r in cur.fetchall()]
        profile = _files_get_artist_profile_cached(artist_name, artist_norm)
        force_refresh = str(request.args.get("refresh", "")).strip().lower() in {"1", "true", "yes"}
        should_enrich = force_refresh or (not bool(profile.get("short_bio"))) or bool(profile.get("stale"))
        enriching = False
        if should_enrich:
            enriching = _enqueue_files_profile_enrichment(artist_name, artist_norm, albums)

        # Attach local IDs + images to similar artists.
        base_url = request.url_root.rstrip("/")
        try:
            if isinstance(profile, dict):
                sim = profile.get("similar_artists")
                if isinstance(sim, list) and sim:
                    profile = dict(profile)
                    profile["similar_artists"] = _files_attach_similar_artist_refs(conn, sim, base_url)
        except Exception:
            pass

        album_profiles = _files_get_album_profiles_cached(artist_norm, [norm for _, norm in albums if norm])
        return jsonify(
            {
                "artist_id": artist_id,
                "artist_name": artist_name,
                "artist_norm": artist_norm,
                "profile": profile,
                "album_profiles": album_profiles,
                "enriching": enriching,
            }
        )
    finally:
        conn.close()


def _assistant_preferred_lang() -> str:
    try:
        raw = (request.headers.get("Accept-Language") or "").lower()
    except Exception:
        raw = ""
    # Very small heuristic (good enough for UI defaults).
    if "fr" in raw:
        return "fr"
    if "en" in raw:
        return "en"
    return "en"


@app.get("/api/library/artist/<int:artist_id>/summary")
def api_library_artist_summary(artist_id: int):
    """Return original vs AI summary for an artist (Files mode only)."""
    if _get_library_mode() != "files":
        return jsonify({"error": "Artist summary endpoint is available in Files mode only"}), 400
    ok, err = _ensure_files_index_ready()
    if not ok:
        return jsonify({"error": err or "Files index unavailable"}), 503
    conn = _files_pg_connect()
    if conn is None:
        return jsonify({"error": "PostgreSQL unavailable"}), 503
    try:
        with conn.cursor() as cur:
            cur.execute("SELECT id, name, name_norm FROM files_artists WHERE id = %s", (int(artist_id),))
            row = cur.fetchone()
            if not row:
                return jsonify({"error": "Artist not found"}), 404
            artist_name = (row[1] or "").strip()
            artist_norm = (row[2] or "").strip() or " ".join((artist_name or "").split()).lower()

            cur.execute(
                """
                SELECT bio, short_bio, source, updated_at
                FROM files_artist_profiles
                WHERE name_norm = %s
                """,
                (artist_norm,),
            )
            prof_row = cur.fetchone()
            original_text = ""
            original_source = ""
            original_updated_at = 0
            if prof_row:
                # Prefer long-form bio, fall back to short.
                original_text = (prof_row[0] or "").strip() or (prof_row[1] or "").strip()
                original_source = (prof_row[2] or "").strip()
                original_updated_at = int(_dt_to_epoch(prof_row[3])) if prof_row[3] else 0
            # Hide garbage bios (e.g. "Read more on Last.fm") so UI can fall back to AI.
            if _is_garbage_bio(original_text):
                original_text = ""

            cur.execute(
                """
                SELECT content, source, provider, model, lang, updated_at
                FROM assistant_docs
                WHERE entity_type = 'artist'
                  AND entity_id = %s
                  AND doc_type = %s
                ORDER BY updated_at DESC
                LIMIT 1
                """,
                (int(artist_id), "artist_summary_ai"),
            )
            ai_row = cur.fetchone()
            ai_text = ""
            ai_source = ""
            ai_provider = ""
            ai_model = ""
            ai_lang = ""
            ai_updated_at = 0
            if ai_row:
                ai_text = (ai_row[0] or "").strip()
                ai_source = (ai_row[1] or "").strip()
                ai_provider = (ai_row[2] or "").strip()
                ai_model = (ai_row[3] or "").strip()
                ai_lang = (ai_row[4] or "").strip()
                ai_updated_at = int(_dt_to_epoch(ai_row[5])) if ai_row[5] else 0
                # Back-compat: older rows used internal markers like "ai_generated".
                if ai_source.lower() in {"ai_generated", "ai"}:
                    ai_source = "web"

        return jsonify(
            {
                "artist_id": int(artist_id),
                "artist_name": artist_name,
                "original": {
                    "text": original_text,
                    "source": original_source,
                    "updated_at": original_updated_at,
                },
                "ai": {
                    "text": ai_text,
                    "source": ai_source,
                    "provider": ai_provider,
                    "model": ai_model,
                    "lang": ai_lang,
                    "updated_at": ai_updated_at,
                },
            }
        )
    finally:
        conn.close()


@app.post("/api/library/artist/<int:artist_id>/summary/ai")
def api_library_artist_summary_ai(artist_id: int):
    """Generate/refresh an AI summary (100-200 words) for an artist and persist it in PostgreSQL."""
    if _get_library_mode() != "files":
        return jsonify({"error": "Artist summary endpoint is available in Files mode only"}), 400
    if not bool(getattr(sys.modules[__name__], "ai_provider_ready", False)):
        msg = getattr(sys.modules[__name__], "AI_FUNCTIONAL_ERROR_MSG", None) or "AI is not configured"
        return jsonify({"error": msg}), 503

    body = request.get_json(silent=True) or {}
    if not isinstance(body, dict):
        body = {}
    lang = str(body.get("lang") or "").strip().lower() or _assistant_preferred_lang()
    if lang not in {"en", "fr"}:
        lang = "en"

    ok, err = _ensure_files_index_ready()
    if not ok:
        return jsonify({"error": err or "Files index unavailable"}), 503
    conn = _files_pg_connect()
    if conn is None:
        return jsonify({"error": "PostgreSQL unavailable"}), 503

    try:
        # Ensure we have the latest local context docs.
        context_info = _assistant_ingest_artist_rag(conn, int(artist_id))
        if not context_info:
            return jsonify({"error": "Artist not found"}), 404
        artist_name = context_info.get("artist_name") or ""

        # Collect context: original bio (if any) + local snapshot.
        with conn.cursor() as cur:
            cur.execute(
                """
                SELECT doc_type, source, content
                FROM assistant_docs
                WHERE entity_type = 'artist'
                  AND entity_id = %s
                  AND doc_type IN ('artist_profile_bio', 'artist_profile_short', 'artist_library_snapshot')
                ORDER BY updated_at DESC
                """,
                (int(artist_id),),
            )
            rows = cur.fetchall()
        doc_map: dict[str, dict] = {}
        for dt, src, content in rows:
            key = str(dt or "").strip().lower()
            if not key or key in doc_map:
                continue
            doc_map[key] = {"source": (src or "").strip(), "content": (content or "").strip()}

        original_text = (doc_map.get("artist_profile_bio") or {}).get("content") or (doc_map.get("artist_profile_short") or {}).get("content") or ""
        original_source = (doc_map.get("artist_profile_bio") or {}).get("source") or (doc_map.get("artist_profile_short") or {}).get("source") or ""
        snapshot_text = (doc_map.get("artist_library_snapshot") or {}).get("content") or ""

        # External context for "real" summaries when local/provider bios are missing.
        wiki_text = ""
        wiki_source = ""
        wiki_url = ""
        wiki_lang = ""
        serper_snippets_text = ""
        concerts_text = ""
        concerts_provider_used = ""

        try:
            if _is_garbage_bio(original_text):
                original_text = ""
        except Exception:
            pass

        if not original_text.strip():
            try:
                wiki_pref = "fr" if lang == "fr" else "en"
                wiki_info = (
                    _fetch_wikipedia_artist_bio(artist_name, lang=wiki_pref)
                    or _fetch_wikipedia_artist_bio(artist_name, lang="en")
                    or _fetch_wikipedia_artist_bio(artist_name, lang="fr")
                    or {}
                )
                if isinstance(wiki_info, dict) and str(wiki_info.get("bio") or "").strip():
                    wiki_text = str(wiki_info.get("bio") or "").strip()
                    wiki_source = str(wiki_info.get("source") or "").strip() or "wikipedia"
                    wiki_url = str(wiki_info.get("url") or "").strip()
                    wiki_lang = str(wiki_info.get("lang") or "").strip()
                    _assistant_upsert_doc(
                        conn,
                        entity_type="artist",
                        entity_id=int(artist_id),
                        doc_type="artist_external_wikipedia_intro",
                        source=wiki_source,
                        title=str(artist_name),
                        url=wiki_url,
                        lang=wiki_lang,
                        content=wiki_text,
                    )
            except Exception:
                pass

            # Serper web snippets (optional, best-effort).
            try:
                if (getattr(sys.modules[__name__], "SERPER_API_KEY", "") or "").strip():
                    hits = _serper_web_search(f"{artist_name} musician", num=5)
                    lines: list[str] = []
                    for h in hits[:5]:
                        if not isinstance(h, dict):
                            continue
                        title = str(h.get("title") or "").strip()
                        link = str(h.get("link") or "").strip()
                        snippet = str(h.get("snippet") or "").strip()
                        if not (title or snippet):
                            continue
                        chunk = f"- {title} | {snippet}"
                        if link:
                            chunk += f" | {link}"
                        lines.append(chunk)
                    serper_snippets_text = "\n".join(lines).strip()
                    if serper_snippets_text:
                        _assistant_upsert_doc(
                            conn,
                            entity_type="artist",
                            entity_id=int(artist_id),
                            doc_type="artist_external_web_snippets",
                            source="serper",
                            provider="serper",
                            title=str(artist_name),
                            content=serper_snippets_text,
                        )
            except Exception:
                pass

            # Upcoming concerts (cached; refresh if missing).
            try:
                with conn.cursor() as cur:
                    cur.execute(
                        """
                        SELECT provider, events_json, source_url, updated_at
                        FROM files_artist_concerts
                        WHERE artist_id = %s
                        """,
                        (int(artist_id),),
                    )
                    crow = cur.fetchone()
                events: list[dict] = []
                provider = "bandsintown"
                source_url = ""
                if crow:
                    provider = str(crow[0] or "").strip() or provider
                    source_url = str(crow[2] or "").strip()
                    try:
                        events = json.loads(crow[1] or "[]") if crow[1] else []
                    except Exception:
                        events = []
                # If missing, fetch once (Songkick first; Bandsintown may be blocked with 403).
                if not events:
                    provider = "songkick"
                    events, sk_url = _songkick_fetch_upcoming_events(artist_name)
                    if sk_url:
                        source_url = sk_url
                    if not events:
                        provider = "bandsintown"
                        app_id = str(_get_config_from_db("BANDSINTOWN_APP_ID", "") or "").strip() or "pmda"
                        events = _bandsintown_fetch_upcoming_events(artist_name, app_id)
                        if not source_url:
                            source_url = f"https://www.bandsintown.com/a/{quote(artist_name, safe='')}" if artist_name else ""
                    with conn.transaction():
                        with conn.cursor() as cur:
                            cur.execute(
                                """
                                INSERT INTO files_artist_concerts(artist_id, provider, events_json, source_url, updated_at)
                                VALUES (%s, %s, %s, %s, NOW())
                                ON CONFLICT (artist_id) DO UPDATE SET
                                    provider = EXCLUDED.provider,
                                    events_json = EXCLUDED.events_json,
                                    source_url = EXCLUDED.source_url,
                                    updated_at = NOW()
                                """,
                                (int(artist_id), provider, json.dumps(events, ensure_ascii=False), source_url or None),
                            )
                if isinstance(events, list) and events:
                    lines = []
                    for ev in events[:8]:
                        if not isinstance(ev, dict):
                            continue
                        dt = str(ev.get("datetime") or "").strip()
                        venue = ev.get("venue") if isinstance(ev.get("venue"), dict) else {}
                        city = str((venue or {}).get("city") or "").strip()
                        country = str((venue or {}).get("country") or "").strip()
                        vname = str((venue or {}).get("name") or "").strip()
                        where = ", ".join([x for x in [city, country] if x])
                        parts = [p for p in [dt[:10] if dt else "", where, vname] if p]
                        if parts:
                            lines.append("- " + " ¬∑ ".join(parts))
                    concerts_text = "\n".join(lines).strip()
                    concerts_provider_used = str(provider or "").strip().lower()
                    if concerts_text:
                        _assistant_upsert_doc(
                            conn,
                            entity_type="artist",
                            entity_id=int(artist_id),
                            doc_type="artist_concerts_upcoming",
                            source=str(provider or "bandsintown"),
                            title=str(artist_name),
                            url=str(source_url or ""),
                            content=concerts_text,
                        )
            except Exception:
                pass

        # AI prompt: produce a clean artist description (no "AI" mentions, no local file/format talk).
        lang_hint = "French" if lang == "fr" else "English"
        system_msg = (
            "You are PMDA Intelligence, a meticulous music librarian.\n"
            "Rules:\n"
            "- Use ONLY the provided context.\n"
            "- Do not invent facts.\n"
            "- Output must be plain text (no markdown).\n"
            f"- Write in {lang_hint}.\n"
            "- Length: 100 to 200 words.\n"
            "- Focus on the artist (bio, style, era, notable works, labels, collaborations, scene).\n"
            "- Do NOT mention audio formats, file quality, local file paths, IDs, or anything about the user's library.\n"
            "- If the context is insufficient, keep it generic and short rather than guessing.\n"
        )
        external_blocks: list[str] = []
        if wiki_text:
            external_blocks.append(f"Wikipedia intro (source={wiki_source}, lang={wiki_lang}, url={wiki_url or 'n/a'}):\n{wiki_text}")
        if serper_snippets_text:
            external_blocks.append(f"Web snippets (Serper):\n{serper_snippets_text}")
        if concerts_text:
            external_blocks.append(f"Upcoming concerts:\n{concerts_text}")
        external_context = "\n\n".join([b for b in external_blocks if b]).strip()
        external_section = ""
        if external_context:
            external_section = "External sources:\n" + external_context + "\n\n"
        user_msg = (
            f"Artist: {artist_name}\n\n"
            f"Original bio (source={original_source or 'unknown'}):\n{original_text or '(none)'}\n\n"
            f"{external_section}"
            "Task: Write the best possible artist description for a music library UI."
        )

        provider = getattr(sys.modules[__name__], "AI_PROVIDER", "openai")
        model = getattr(sys.modules[__name__], "RESOLVED_MODEL", None) or getattr(sys.modules[__name__], "OPENAI_MODEL", "gpt-4o-mini")
        ai_text = call_ai_provider_longform(provider, model, system_msg, user_msg, max_tokens=520)
        ai_text = (ai_text or "").strip()
        if not ai_text:
            return jsonify({"error": "AI returned empty summary"}), 502

        # Persist as assistant doc so it can be chunked and re-used by the chat RAG.
        sources_used: list[str] = []
        try:
            if original_text.strip() and (original_source or "").strip():
                sources_used.append(str(original_source).strip())
            if wiki_text:
                sources_used.append("wikipedia")
            if serper_snippets_text:
                sources_used.append("web")
            if concerts_text:
                sources_used.append(concerts_provider_used or "concerts")
        except Exception:
            sources_used = []
        # Deduplicate while preserving order.
        sources_used = list(dict.fromkeys([s for s in sources_used if (s or "").strip()]))[:8]
        source_label = ", ".join(sources_used) if sources_used else "web"
        _assistant_upsert_doc(
            conn,
            entity_type="artist",
            entity_id=int(artist_id),
            doc_type="artist_summary_ai",
            source=source_label,
            provider=str(provider),
            model=str(model),
            title=str(artist_name),
            lang=lang,
            content=ai_text,
        )
        _files_cache_invalidate_all()

        return jsonify(
            {
                "artist_id": int(artist_id),
                "artist_name": artist_name,
                "ai": {
                    "text": ai_text,
                    "source": source_label,
                    "provider": str(provider),
                    "model": str(model),
                    "lang": lang,
                    "updated_at": int(time.time()),
                },
            }
        )
    finally:
        conn.close()


def _bandsintown_fetch_upcoming_events(artist_name: str, app_id: str) -> list[dict]:
    """
    Fetch upcoming events for an artist from Bandsintown.
    Notes:
    - Bandsintown uses `app_id` (string) for identification; it is not a secret.
    - We normalize the payload for UI stability.
    """
    name = (artist_name or "").strip()
    if not name:
        return []
    encoded = quote(name, safe="")
    url = f"https://rest.bandsintown.com/artists/{encoded}/events"
    try:
        r = requests.get(
            url,
            params={"app_id": (app_id or "pmda"), "date": "upcoming"},
            headers={"User-Agent": "PMDA/0.7.5"},
            timeout=12,
        )
        if r.status_code == 404:
            return []
        # Bandsintown frequently returns 403 from some hosting environments.
        # Keep it non-fatal so we can fall back to Songkick scraping.
        if r.status_code == 403:
            logging.info("[Concerts] Bandsintown forbidden (403) for artist=%s", name)
            return []
        r.raise_for_status()
        data = r.json()
        if not isinstance(data, list):
            return []
        out: list[dict] = []
        for ev in data:
            if not isinstance(ev, dict):
                continue
            dt = str(ev.get("datetime") or "").strip()
            venue = ev.get("venue") if isinstance(ev.get("venue"), dict) else {}
            offers = ev.get("offers") if isinstance(ev.get("offers"), list) else []
            ticket_url = str(ev.get("url") or "").strip()
            if not ticket_url and offers:
                for off in offers:
                    if isinstance(off, dict) and str(off.get("url") or "").strip():
                        ticket_url = str(off.get("url") or "").strip()
                        break
            out.append(
                {
                    "provider": "bandsintown",
                    "id": str(ev.get("id") or ""),
                    "datetime": dt,
                    "title": str(ev.get("title") or "").strip(),
                    "url": ticket_url,
                    "lineup": ev.get("lineup") if isinstance(ev.get("lineup"), list) else [],
                    "venue": {
                        "name": str(venue.get("name") or "").strip(),
                        "city": str(venue.get("city") or "").strip(),
                        "region": str(venue.get("region") or "").strip(),
                        "country": str(venue.get("country") or "").strip(),
                        "latitude": str(venue.get("latitude") or "").strip(),
                        "longitude": str(venue.get("longitude") or "").strip(),
                    },
                }
            )
        return out
    except Exception:
        return []


# --- Geo helpers (concert map) ------------------------------------------------

_GEO_OSM_CACHE: dict[str, tuple[float, str, str]] = {}
_GEO_OSM_CACHE_TTL_SEC = 60 * 60 * 24 * 30


def _osm_geocode_place(query: str) -> tuple[str, str]:
    """Best-effort geocode using Nominatim (OpenStreetMap). Returns (lat, lon) strings or ('','')."""
    q = (query or "").strip()
    if not q:
        return ("", "")
    key = q.lower()
    now = float(time.time())
    try:
        cached = _GEO_OSM_CACHE.get(key)
        if cached and (now - float(cached[0] or 0.0)) < _GEO_OSM_CACHE_TTL_SEC:
            return (str(cached[1] or ""), str(cached[2] or ""))
    except Exception:
        pass
    try:
        r = requests.get(
            "https://nominatim.openstreetmap.org/search",
            params={"q": q, "format": "jsonv2", "limit": 1, "addressdetails": 0},
            headers={"User-Agent": "PMDA/0.7.5 (concert-map)"},
            timeout=8,
        )
        r.raise_for_status()
        data = r.json()
        if not isinstance(data, list) or not data:
            return ("", "")
        item = data[0] if isinstance(data[0], dict) else {}
        lat = str(item.get("lat") or "").strip()
        lon = str(item.get("lon") or "").strip()
        if lat and lon:
            _GEO_OSM_CACHE[key] = (now, lat, lon)
            return (lat, lon)
    except Exception:
        return ("", "")
    return ("", "")


def _songkick_search_artist(artist_name: str) -> dict | None:
    """Best-effort Songkick artist search (HTML). Returns {name, href, id, upcoming_events}."""
    name = (artist_name or "").strip()
    if not name:
        return None
    try:
        r = requests.get(
            "https://www.songkick.com/search",
            params={"query": name, "type": "artists"},
            headers={"User-Agent": "Mozilla/5.0 (PMDA)"},
            timeout=12,
        )
        r.raise_for_status()
        html_text = r.text or ""
    except Exception:
        return None

    def _norm(s: str) -> str:
        s = (s or "").strip().lower()
        s = re.sub(r"[^a-z0-9]+", " ", s)
        return re.sub(r"\s+", " ", s).strip()

    qn = _norm(name)
    blocks = re.findall(r'<li class="artist">.*?</li>', html_text, flags=re.S | re.I)
    best: dict | None = None
    best_score = -1.0
    for b in blocks[:50]:
        href_m = re.search(r'href="(/artists/[^"]+)"', b, flags=re.I)
        name_m = re.search(r"<strong>(.*?)</strong>", b, flags=re.I | re.S)
        if not href_m or not name_m:
            continue
        href = str(href_m.group(1) or "").strip()
        cand_name = html.unescape(str(name_m.group(1) or "")).strip()
        if not href or not cand_name:
            continue
        upcoming = 0
        um = re.search(r"(\d+)\s+upcoming\s+events", b, flags=re.I)
        if um:
            try:
                upcoming = int(um.group(1) or 0)
            except Exception:
                upcoming = 0
        cn = _norm(cand_name)
        # Prefer exact normalized matches, otherwise best similarity.
        score = 0.0
        if cn == qn:
            score = 1000.0 + float(upcoming)
        else:
            try:
                import difflib

                ratio = difflib.SequenceMatcher(None, cn, qn).ratio()
                score = ratio * 100.0 + min(50.0, float(upcoming) / 2.0)
            except Exception:
                score = float(upcoming)
        if score > best_score:
            best_score = score
            best = {
                "name": cand_name,
                "href": href,
                "id": _parse_int_loose(re.search(r"/artists/(\d+)", href).group(1) if re.search(r"/artists/(\d+)", href) else 0),
                "upcoming_events": upcoming,
            }
    return best


def _songkick_fetch_upcoming_events(artist_name: str) -> tuple[list[dict], str | None]:
    """Fetch upcoming events for an artist from Songkick (scrape HTML). Returns (events, source_url)."""
    name = (artist_name or "").strip()
    if not name:
        return ([], None)
    hit = _songkick_search_artist(name)
    if not hit or not str(hit.get("href") or "").strip():
        return ([], None)
    href = str(hit.get("href") or "").strip()
    source_url = f"https://www.songkick.com{href}"
    try:
        r = requests.get(source_url, headers={"User-Agent": "Mozilla/5.0 (PMDA)"}, timeout=12)
        r.raise_for_status()
        page = r.text or ""
    except Exception:
        return ([], source_url)

    low = page.lower()
    start = low.find('id="coming-up"')
    end = low.find('id="past-events"', start + 1) if start != -1 else -1
    scope = page[start:end] if start != -1 and end != -1 else page

    items = re.findall(r'<li[^>]*class="event-listing-item[^"]*"[^>]*>.*?</li>', scope, flags=re.S | re.I)
    out: list[dict] = []
    for it in items[:80]:
        try:
            dt_m = re.search(r'<time[^>]*datetime="([^"]+)"', it, flags=re.I)
            href_m = re.search(r'href="(/concerts/[^"]+)"', it, flags=re.I)
            loc_m = re.search(r'<div[^>]*class="primary-detail"[^>]*>(.*?)</div>', it, flags=re.I | re.S)
            venue_m = re.search(r'<div[^>]*class="secondary-detail"[^>]*>(.*?)</div>', it, flags=re.I | re.S)
            if not href_m:
                continue
            event_href = str(href_m.group(1) or "").strip()
            event_url = f"https://www.songkick.com{event_href}"
            ev_id_m = re.search(r"/concerts/(\d+)", event_href)
            ev_id = str(ev_id_m.group(1) or "") if ev_id_m else ""
            dt = str(dt_m.group(1) or "").strip() if dt_m else ""
            location = html.unescape(re.sub(r"<[^>]+>", " ", (loc_m.group(1) if loc_m else "")).strip())
            location = re.sub(r"\s+", " ", location).strip()
            venue = html.unescape(re.sub(r"<[^>]+>", " ", (venue_m.group(1) if venue_m else "")).strip())
            venue = re.sub(r"\s+", " ", venue).strip()

            city = location
            region = ""
            country = ""
            parts = [p.strip() for p in location.split(",") if p.strip()]
            if len(parts) >= 3:
                city, region, country = parts[0], parts[1], parts[2]
            elif len(parts) == 2:
                city, country = parts[0], parts[1]

            out.append(
                {
                    "provider": "songkick",
                    "id": ev_id,
                    "datetime": dt,
                    "title": name,
                    "url": event_url,
                    "lineup": [name],
                    "venue": {
                        "name": venue,
                        "city": city,
                        "region": region,
                        "country": country,
                        "latitude": "",
                        "longitude": "",
                    },
                }
            )
        except Exception:
            continue

    # Best-effort geocoding so the UI can render a small map with pins.
    # We intentionally geocode only city/region/country (not the venue name) to reduce noise.
    try:
        # Deduplicate queries and cap requests so this endpoint remains responsive.
        q_to_latlon: dict[str, tuple[str, str]] = {}
        queries: list[str] = []
        for ev in out[:32]:
            v = ev.get("venue") if isinstance(ev.get("venue"), dict) else {}
            if not isinstance(v, dict):
                continue
            if str(v.get("latitude") or "").strip() and str(v.get("longitude") or "").strip():
                continue
            city = str(v.get("city") or "").strip()
            region = str(v.get("region") or "").strip()
            country = str(v.get("country") or "").strip()
            if not (city and country):
                continue
            q = ", ".join([p for p in [city, region, country] if p]).strip()
            if not q or q in q_to_latlon:
                continue
            queries.append(q)
            if len(queries) >= 8:
                break
        for q in queries:
            lat, lon = _osm_geocode_place(q)
            if lat and lon:
                q_to_latlon[q] = (lat, lon)
        if q_to_latlon:
            for ev in out:
                v = ev.get("venue") if isinstance(ev.get("venue"), dict) else {}
                if not isinstance(v, dict):
                    continue
                if str(v.get("latitude") or "").strip() and str(v.get("longitude") or "").strip():
                    continue
                city = str(v.get("city") or "").strip()
                region = str(v.get("region") or "").strip()
                country = str(v.get("country") or "").strip()
                q = ", ".join([p for p in [city, region, country] if p]).strip()
                if q in q_to_latlon:
                    v["latitude"], v["longitude"] = q_to_latlon[q]
    except Exception:
        pass
    return (out, source_url)


@app.get("/api/library/artist/<int:artist_id>/concerts")
def api_library_artist_concerts(artist_id: int):
    """Return cached upcoming concerts for an artist (Files mode only)."""
    if _get_library_mode() != "files":
        return jsonify({"error": "Artist concerts endpoint is available in Files mode only"}), 400
    ok, err = _ensure_files_index_ready()
    if not ok:
        return jsonify({"error": err or "Files index unavailable"}), 503

    refresh = str(request.args.get("refresh") or "").strip().lower() in {"1", "true", "yes"}
    provider = str(request.args.get("provider") or "auto").strip().lower() or "auto"
    ttl_sec = max(600, min(24 * 3600, _parse_int_loose(request.args.get("ttl_sec"), 6 * 3600)))

    if provider not in {"auto", "bandsintown", "songkick"}:
        return jsonify({"error": f"Unsupported provider: {provider}"}), 400

    conn = _files_pg_connect()
    if conn is None:
        return jsonify({"error": "PostgreSQL unavailable"}), 503
    try:
        with conn.cursor() as cur:
            cur.execute("SELECT id, name FROM files_artists WHERE id = %s", (int(artist_id),))
            row = cur.fetchone()
            if not row:
                return jsonify({"error": "Artist not found"}), 404
            artist_name = (row[1] or "").strip()

            cur.execute(
                """
                SELECT provider, events_json, source_url, updated_at
                FROM files_artist_concerts
                WHERE artist_id = %s
                """,
                (int(artist_id),),
            )
            cached = cur.fetchone()

        now_epoch = int(time.time())
        if cached and not refresh:
            cached_provider = str(cached[0] or "").strip() or provider
            cached_events_json = str(cached[1] or "[]")
            cached_source_url = str(cached[2] or "").strip() or None
            cached_updated_epoch = int(_dt_to_epoch(cached[3])) if cached[3] else 0
            if cached_updated_epoch and (now_epoch - cached_updated_epoch) < ttl_sec:
                try:
                    events = json.loads(cached_events_json) if cached_events_json else []
                except (TypeError, ValueError):
                    events = []
                if not isinstance(events, list):
                    events = []
                return jsonify(
                    {
                        "artist_id": int(artist_id),
                        "artist_name": artist_name,
                        "provider": cached_provider,
                        "events": events,
                        "source_url": cached_source_url,
                        "updated_at": cached_updated_epoch,
                        "cached": True,
                    }
                )

        # Refresh from provider.
        events: list[dict] = []
        source_url: str | None = None
        provider_used = provider
        if provider == "songkick":
            events, source_url = _songkick_fetch_upcoming_events(artist_name)
            provider_used = "songkick"
        elif provider == "bandsintown":
            app_id = str(_get_config_from_db("BANDSINTOWN_APP_ID", "") or "").strip() or "pmda"
            events = _bandsintown_fetch_upcoming_events(artist_name, app_id)
            source_url = f"https://www.bandsintown.com/a/{quote(artist_name, safe='')}" if artist_name else None
            provider_used = "bandsintown"
        else:
            # auto: prefer Songkick HTML (works reliably when Bandsintown blocks API access).
            events, source_url = _songkick_fetch_upcoming_events(artist_name)
            provider_used = "songkick"
            if not events:
                app_id = str(_get_config_from_db("BANDSINTOWN_APP_ID", "") or "").strip() or "pmda"
                events = _bandsintown_fetch_upcoming_events(artist_name, app_id)
                if events:
                    provider_used = "bandsintown"
                    source_url = f"https://www.bandsintown.com/a/{quote(artist_name, safe='')}" if artist_name else source_url

        events_json = json.dumps(events, ensure_ascii=False)
        with conn.transaction():
            with conn.cursor() as cur:
                cur.execute(
                    """
                    INSERT INTO files_artist_concerts(artist_id, provider, events_json, source_url, updated_at)
                    VALUES (%s, %s, %s, %s, NOW())
                    ON CONFLICT (artist_id) DO UPDATE SET
                        provider = EXCLUDED.provider,
                        events_json = EXCLUDED.events_json,
                        source_url = EXCLUDED.source_url,
                        updated_at = NOW()
                    """,
                    (int(artist_id), provider_used, events_json, source_url),
                )

        return jsonify(
            {
                "artist_id": int(artist_id),
                "artist_name": artist_name,
                "provider": provider_used,
                "events": events,
                "source_url": source_url,
                "updated_at": int(time.time()),
                "cached": False,
            }
        )
    finally:
        conn.close()


@app.get("/api/library/artist/<int:artist_id>/facts")
def api_library_artist_facts(artist_id: int):
    """Return extracted structured facts for an artist (Files mode only)."""
    if _get_library_mode() != "files":
        return jsonify({"error": "Artist facts endpoint is available in Files mode only"}), 400
    ok, err = _ensure_files_index_ready()
    if not ok:
        return jsonify({"error": err or "Files index unavailable"}), 503
    conn = _files_pg_connect()
    if conn is None:
        return jsonify({"error": "PostgreSQL unavailable"}), 503
    try:
        with conn.cursor() as cur:
            cur.execute("SELECT name FROM files_artists WHERE id = %s", (int(artist_id),))
            row = cur.fetchone()
            if not row:
                return jsonify({"error": "Artist not found"}), 404
            artist_name = (row[0] or "").strip()

            cur.execute(
                """
                SELECT facts_json, evidence_json, source, provider, model, updated_at
                FROM assistant_entity_facts
                WHERE entity_type = 'artist' AND entity_id = %s
                """,
                (int(artist_id),),
            )
            frow = cur.fetchone()
        if not frow:
            return jsonify(
                {
                    "artist_id": int(artist_id),
                    "artist_name": artist_name,
                    "facts": {},
                    "evidence": [],
                    "source": "",
                    "provider": "",
                    "model": "",
                    "updated_at": 0,
                }
            )
        try:
            facts = json.loads(frow[0] or "{}") if frow[0] else {}
        except (TypeError, ValueError):
            facts = {}
        try:
            evidence = json.loads(frow[1] or "[]") if frow[1] else []
        except (TypeError, ValueError):
            evidence = []
        if not isinstance(facts, dict):
            facts = {}
        if not isinstance(evidence, list):
            evidence = []
        return jsonify(
            {
                "artist_id": int(artist_id),
                "artist_name": artist_name,
                "facts": facts,
                "evidence": evidence,
                "source": str(frow[2] or "").strip(),
                "provider": str(frow[3] or "").strip(),
                "model": str(frow[4] or "").strip(),
                "updated_at": int(_dt_to_epoch(frow[5])) if frow[5] else 0,
            }
        )
    finally:
        conn.close()


@app.post("/api/library/artist/<int:artist_id>/facts/extract")
def api_library_artist_facts_extract(artist_id: int):
    """Extract artist facts via AI (stored in PostgreSQL)."""
    if _get_library_mode() != "files":
        return jsonify({"error": "Artist facts endpoint is available in Files mode only"}), 400
    if not bool(getattr(sys.modules[__name__], "ai_provider_ready", False)):
        msg = getattr(sys.modules[__name__], "AI_FUNCTIONAL_ERROR_MSG", None) or "AI is not configured"
        return jsonify({"error": msg}), 503
    ok, err = _ensure_files_index_ready()
    if not ok:
        return jsonify({"error": err or "Files index unavailable"}), 503
    conn = _files_pg_connect()
    if conn is None:
        return jsonify({"error": "PostgreSQL unavailable"}), 503
    try:
        context_info = _assistant_ingest_artist_rag(conn, int(artist_id))
        if not context_info:
            return jsonify({"error": "Artist not found"}), 404
        artist_name = (context_info.get("artist_name") or "").strip()

        # Pull best-effort context for grounded extraction.
        with conn.cursor() as cur:
            cur.execute(
                """
                SELECT doc_type, source, title, url, content
                FROM assistant_docs
                WHERE entity_type = 'artist'
                  AND entity_id = %s
                  AND doc_type IN (
                    'artist_profile_bio',
                    'artist_profile_short',
                    'artist_summary_ai',
                    'artist_external_wikipedia_intro',
                    'artist_external_web_snippets'
                  )
                ORDER BY updated_at DESC
                """,
                (int(artist_id),),
            )
            rows = cur.fetchall()

        ctx_parts: list[str] = []
        evidence: list[dict] = []
        for dt, src, title, url, content in rows:
            dt_s = str(dt or "").strip()
            src_s = str(src or "").strip()
            title_s = str(title or "").strip()
            url_s = str(url or "").strip()
            text = (content or "").strip()
            if not text:
                continue
            # Keep context bounded to avoid runaway tokens.
            excerpt = text[:2400]
            ctx_parts.append(f"[{dt_s} source={src_s} title={title_s} url={url_s}]\n{excerpt}")
        ctx = "\n\n".join(ctx_parts) if ctx_parts else "(no context)"

        system_msg = (
            "You are PMDA Intelligence.\n"
            "Task: extract structured artist facts from the given context.\n"
            "Rules:\n"
            "- Use ONLY the provided context.\n"
            "- Do NOT invent facts.\n"
            "- Output must be STRICT JSON only.\n"
            "- If unknown, use empty arrays/empty strings.\n"
        )
        user_msg = (
            f"Artist: {artist_name}\n\n"
            "Return JSON with exactly these keys:\n"
            "{\n"
            '  "facts": {\n'
            '    "aka": [string],\n'
            '    "aliases": [string],\n'
            '    "member_of": [string],\n'
            '    "collaborated_with": [string],\n'
            '    "labels": [string],\n'
            '    "notable_cities": [string]\n'
            "  },\n"
            '  "evidence": [ { "fact_path": string, "excerpt": string, "source": string } ]\n'
            "}\n\n"
            "Context:\n"
            f"{ctx}\n"
        )

        provider = getattr(sys.modules[__name__], "AI_PROVIDER", "openai")
        model = getattr(sys.modules[__name__], "RESOLVED_MODEL", None) or getattr(sys.modules[__name__], "OPENAI_MODEL", "gpt-4o-mini")
        raw = call_ai_provider_longform(provider, model, system_msg, user_msg, max_tokens=800)
        raw = (raw or "").strip()
        if not raw:
            return jsonify({"error": "AI returned empty payload"}), 502
        parsed = None
        try:
            parsed = json.loads(raw)
        except Exception:
            # Attempt to salvage JSON from surrounding text.
            try:
                start = raw.find("{")
                end = raw.rfind("}")
                if start != -1 and end != -1 and end > start:
                    parsed = json.loads(raw[start : end + 1])
            except Exception:
                parsed = None
        if not isinstance(parsed, dict):
            return jsonify({"error": "AI returned invalid JSON"}), 502
        facts = parsed.get("facts") if isinstance(parsed.get("facts"), dict) else {}
        evidence = parsed.get("evidence") if isinstance(parsed.get("evidence"), list) else []

        facts_json = json.dumps(facts or {}, ensure_ascii=False)
        evidence_json = json.dumps(evidence or [], ensure_ascii=False)

        # For UI: keep a human-friendly "source" label (avoid exposing AI/provider internals).
        sources_used: list[str] = []
        try:
            for dt, src, _title, _url, _content in rows:
                dt_s = str(dt or "").strip().lower()
                src_s = str(src or "").strip()
                if dt_s == "artist_external_wikipedia_intro":
                    sources_used.append("wikipedia")
                elif dt_s == "artist_external_web_snippets":
                    sources_used.append("web")
                elif src_s:
                    sources_used.append(src_s)
        except Exception:
            sources_used = []
        sources_used = list(dict.fromkeys([s for s in sources_used if (s or "").strip()]))[:8]
        source_label = ", ".join(sources_used) if sources_used else "web"

        with conn.transaction():
            with conn.cursor() as cur:
                cur.execute(
                    """
                    INSERT INTO assistant_entity_facts(entity_type, entity_id, facts_json, evidence_json, source, provider, model, updated_at)
                    VALUES ('artist', %s, %s, %s, %s, %s, %s, NOW())
                    ON CONFLICT (entity_type, entity_id) DO UPDATE SET
                        facts_json = EXCLUDED.facts_json,
                        evidence_json = EXCLUDED.evidence_json,
                        source = EXCLUDED.source,
                        provider = EXCLUDED.provider,
                        model = EXCLUDED.model,
                        updated_at = NOW()
                    """,
                    (int(artist_id), facts_json, evidence_json, source_label, str(provider), str(model)),
                )

        return jsonify(
            {
                "artist_id": int(artist_id),
                "artist_name": artist_name,
                "facts": facts,
                "evidence": evidence,
                "source": source_label,
                "provider": str(provider),
                "model": str(model),
                "updated_at": int(time.time()),
            }
        )
    finally:
        conn.close()


@app.get("/api/library/missing-tags")
def api_library_missing_tags():
    """Return albums in selected sections that have missing MusicBrainz or required tags.
    Prefer scan_editions from last completed scan when available."""
    if _get_library_mode() == "files":
        ok, err = _ensure_files_index_ready()
        if not ok:
            return jsonify({"albums": [], "error": err or "Files index unavailable"}), 503
        conn = _files_pg_connect()
        if conn is None:
            return jsonify({"albums": [], "error": "PostgreSQL unavailable"}), 503
        try:
            with conn.cursor() as cur:
                cur.execute(
                    """
                    SELECT a.name, alb.id, alb.title, alb.missing_required_tags_json
                    FROM files_albums alb
                    JOIN files_artists a ON a.id = alb.artist_id
                    WHERE alb.missing_required_tags_json IS NOT NULL
                      AND alb.missing_required_tags_json <> ''
                      AND alb.missing_required_tags_json <> '[]'
                    ORDER BY a.name, alb.title
                    """
                )
                rows = cur.fetchall()
            out = []
            for artist_name, album_id, album_title, missing_json in rows:
                try:
                    missing_tags = json.loads(missing_json) if missing_json else []
                except (TypeError, ValueError):
                    missing_tags = []
                if not missing_tags:
                    continue
                out.append({
                    "artist_name": artist_name or "",
                    "album_id": int(album_id),
                    "album_title": album_title or "",
                    "missing_tags": missing_tags,
                })
            return jsonify({"albums": out})
        finally:
            conn.close()

    _reload_section_ids_from_db()
    if not PLEX_CONFIGURED:
        return jsonify({"albums": []})
    if not SECTION_IDS:
        return jsonify({"albums": []})

    scan_id = get_last_completed_scan_id()
    if scan_id:
        con = sqlite3.connect(str(STATE_DB_FILE))
        cur = con.cursor()
        cur.execute("""
            SELECT artist, album_id, title_raw, missing_required_tags
            FROM scan_editions
            WHERE scan_id = ? AND missing_required_tags IS NOT NULL AND missing_required_tags != '' AND missing_required_tags != '[]'
            ORDER BY artist, title_raw
        """, (scan_id,))
        rows = cur.fetchall()
        con.close()
        if rows:
            results = []
            for artist_name, album_id, title_raw, missing_required_tags in rows:
                try:
                    missing_tags = json.loads(missing_required_tags) if isinstance(missing_required_tags, str) else (missing_required_tags or [])
                except (json.JSONDecodeError, TypeError):
                    missing_tags = []
                if missing_tags:
                    results.append({
                        "artist_name": artist_name or "",
                        "album_id": album_id,
                        "album_title": (title_raw or "").strip() or "",
                        "missing_tags": missing_tags,
                    })
            return jsonify({"albums": results})
        return jsonify({"albums": []})

    # No completed scan (e.g. after Clear results): show nothing until next scan
    return jsonify({"albums": []})


@app.get("/api/library/album/<int:album_id>/tracks")
def api_library_album_tracks(album_id):
    """Return track list for an album for playback (track_id, title, duration, file_url)."""
    if _get_library_mode() == "files":
        ok, err = _ensure_files_index_ready()
        if not ok:
            return jsonify({"error": err or "Files index unavailable"}), 503
        conn = _files_pg_connect()
        if conn is None:
            return jsonify({"error": "PostgreSQL unavailable"}), 503
        try:
            with conn.cursor() as cur:
                cur.execute(
                    """
                    SELECT alb.title, art.name, alb.has_cover
                    FROM files_albums alb
                    JOIN files_artists art ON art.id = alb.artist_id
                    WHERE alb.id = %s
                    """,
                    (album_id,),
                )
                album_row = cur.fetchone()
                if not album_row:
                    return jsonify({"error": "Album not found"}), 404
                album_title = album_row[0] or ""
                artist_name = album_row[1] or ""
                has_cover = bool(album_row[2])
                cur.execute(
                    """
                    SELECT id, title, duration_sec, track_num, file_path
                    FROM files_tracks
                    WHERE album_id = %s
                    ORDER BY disc_num ASC, track_num ASC, id ASC
                    """,
                    (album_id,),
                )
                rows = cur.fetchall()
            # If durations are missing (0), compute them on-demand via ffprobe and persist.
            # This keeps the Files index lightweight while ensuring the player UI shows real durations.
            duration_overrides = _files_fix_missing_album_track_durations(
                conn,
                album_id=int(album_id),
                rows=[(int(r[0] or 0), int(r[2] or 0), str(r[4] or "")) for r in rows],
            )
            base_url = request.url_root.rstrip("/")
            tracks = [
                {
                    "track_id": int(r[0]),
                    "title": r[1] or "",
                    "artist": artist_name,
                    "album": album_title,
                    "duration": int(duration_overrides.get(int(r[0] or 0)) or (r[2] or 0) or 0),
                    "index": int(r[3] or 0),
                    "file_url": f"{base_url}/api/library/track/{int(r[0])}/stream",
                }
                for r in rows
            ]
            album_thumb = f"{base_url}/api/library/files/album/{album_id}/cover?size=320" if has_cover else None
            return jsonify({"tracks": tracks, "album_thumb": album_thumb})
        finally:
            conn.close()

    if not PLEX_CONFIGURED:
        return jsonify({"error": "Plex not configured"}), 503
    db_conn = plex_connect()
    try:
        album_row = db_conn.execute(
            "SELECT title, parent_id FROM metadata_items WHERE id = ? AND metadata_type = 9",
            (album_id,),
        ).fetchone()
        if not album_row:
            return jsonify({"error": "Album not found"}), 404
        album_title, artist_parent_id = album_row[0], album_row[1]
        artist_name = ""
        if artist_parent_id:
            artist_row = db_conn.execute(
                "SELECT title FROM metadata_items WHERE id = ? AND metadata_type = 8",
                (artist_parent_id,),
            ).fetchone()
            if artist_row:
                artist_name = artist_row[0] or ""
        raw = get_tracks_with_ids(db_conn, album_id)
        base_url = request.url_root.rstrip("/")
        tracks = [
            {
                "track_id": t["id"],
                "title": t["title"],
                "artist": artist_name,
                "album": album_title or "",
                "duration": (t["duration_ms"] or 0) // 1000,
                "index": t["index"],
                "file_url": f"{base_url}/api/library/track/{t['id']}/stream",
            }
            for t in raw
        ]
        album_thumb = thumb_url(album_id)
        return jsonify({"tracks": tracks, "album_thumb": album_thumb})
    finally:
        db_conn.close()


def _files_fix_missing_album_track_durations(conn, *, album_id: int, rows: list[tuple[int, int, str]]) -> dict[int, int]:
    """
    Best-effort: ensure duration_sec is populated for the given album's tracks.
    Input rows: (track_id, duration_sec, file_path).
    Returns: {track_id: computed_duration_sec} overrides for tracks that were fixed in this request.
    """
    album_id = int(album_id or 0)
    if album_id <= 0:
        return {}
    duration_overrides: dict[int, int] = {}
    try:
        missing: list[tuple[int, Path]] = []
        for tid, dur, fpath in (rows or []):
            tid = int(tid or 0)
            if tid <= 0:
                continue
            if int(dur or 0) > 0:
                continue
            raw_path = str(fpath or "").strip()
            if not raw_path:
                continue
            p = path_for_fs_access(Path(raw_path))
            if not p.exists() or not p.is_file():
                continue
            missing.append((tid, p))
        if not missing:
            return {}

        pool = get_ffprobe_pool()
        futures = {pool.submit(_run_ffprobe_duration_sec, str(p)): tid for tid, p in missing[:96]}
        updates: list[tuple[int, int]] = []
        for fut in as_completed(futures):
            tid = int(futures.get(fut) or 0)
            try:
                dur_sec = int(fut.result() or 0)
            except Exception:
                dur_sec = 0
            if tid > 0 and dur_sec > 0:
                duration_overrides[tid] = dur_sec
                updates.append((dur_sec, tid))
        if not updates:
            return duration_overrides

        with conn.transaction():
            with conn.cursor() as cur:
                cur.executemany(
                    "UPDATE files_tracks SET duration_sec = %s, updated_at = NOW() WHERE id = %s",
                    updates,
                )
                # Keep album total_duration_sec consistent (best-effort).
                cur.execute(
                    """
                    UPDATE files_albums
                    SET total_duration_sec = COALESCE((
                        SELECT SUM(t.duration_sec)
                        FROM files_tracks t
                        WHERE t.album_id = %s
                    ), 0)
                    WHERE id = %s
                    """,
                    (int(album_id), int(album_id)),
                )
        _files_cache_invalidate_all()
        return duration_overrides
    except Exception:
        return {}


@app.get("/api/library/album/<int:album_id>")
def api_library_album_detail(album_id: int):
    """Return album details + tracklist (for album page). Files mode only."""
    if _get_library_mode() != "files":
        return jsonify({"error": "Files mode required"}), 400
    ok, err = _ensure_files_index_ready()
    if not ok:
        return jsonify({"error": err or "Files index unavailable"}), 503
    album_id = int(album_id or 0)
    if album_id <= 0:
        return jsonify({"error": "Invalid album id"}), 400

    cache_key = f"library:album:{album_id}"
    cached = _files_cache_get_json(cache_key)
    if cached is not None:
        return jsonify(cached)

    conn = _files_pg_connect()
    if conn is None:
        return jsonify({"error": "PostgreSQL unavailable"}), 503
    try:
        with conn.cursor() as cur:
            cur.execute(
                """
                SELECT
                    alb.id,
                    alb.title,
                    alb.title_norm,
                    COALESCE(alb.year, 0) AS year,
                    COALESCE(alb.date_text, '') AS date_text,
                    COALESCE(alb.genre, '') AS genre,
                    COALESCE(alb.label, '') AS label,
                    COALESCE(alb.format, '') AS format,
                    alb.is_lossless,
                    COALESCE(alb.track_count, 0) AS track_count,
                    COALESCE(alb.total_duration_sec, 0) AS total_duration_sec,
                    alb.has_cover,
                    COALESCE(alb.bandcamp_album_url, '') AS bandcamp_album_url,
                    COALESCE(alb.metadata_source, '') AS metadata_source,
                    art.id AS artist_id,
                    COALESCE(art.name, '') AS artist_name
                FROM files_albums alb
                JOIN files_artists art ON art.id = alb.artist_id
                WHERE alb.id = %s
                """,
                (album_id,),
            )
            row = cur.fetchone()
            if not row:
                return jsonify({"error": "Album not found"}), 404

            (
                _aid,
                album_title,
                title_norm,
                year,
                date_text,
                genre,
                label,
                fmt,
                is_lossless,
                track_count,
                total_duration_sec,
                has_cover,
                bandcamp_album_url,
                metadata_source,
                artist_id,
                artist_name,
            ) = row

            # Album profile (review/description) is keyed by (artist_norm, title_norm).
            artist_norm = " ".join((artist_name or "").split()).lower()
            title_norm = str(title_norm or "").strip()
            prof = {}
            if artist_norm and title_norm:
                try:
                    cur.execute(
                        """
                        SELECT description, short_description, source, updated_at
                        FROM files_album_profiles
                        WHERE artist_norm = %s AND title_norm = %s
                        """,
                        (artist_norm, title_norm),
                    )
                    prow = cur.fetchone()
                    if prow:
                        prof = {
                            "description": (prow[0] or "").strip(),
                            "short_description": (prow[1] or "").strip(),
                            "source": (prow[2] or "").strip(),
                            "updated_at": int(_dt_to_epoch(prow[3])) if prow[3] else 0,
                        }
                except Exception:
                    prof = {}

            # Tracks (detailed)
            cur.execute(
                """
                SELECT
                    id,
                    COALESCE(title, '') AS title,
                    COALESCE(disc_num, 0) AS disc_num,
                    COALESCE(track_num, 0) AS track_num,
                    COALESCE(duration_sec, 0) AS duration_sec,
                    COALESCE(format, '') AS format,
                    COALESCE(bitrate, 0) AS bitrate,
                    COALESCE(sample_rate, 0) AS sample_rate,
                    COALESCE(bit_depth, 0) AS bit_depth,
                    COALESCE(file_size_bytes, 0) AS file_size_bytes,
                    COALESCE(file_path, '') AS file_path
                FROM files_tracks
                WHERE album_id = %s
                ORDER BY disc_num ASC, track_num ASC, id ASC
                """,
                (album_id,),
            )
            track_rows = cur.fetchall()

        duration_overrides = _files_fix_missing_album_track_durations(
            conn,
            album_id=int(album_id),
            rows=[(int(r[0] or 0), int(r[4] or 0), str(r[10] or "")) for r in track_rows],
        )

        base_url = request.url_root.rstrip("/")
        tracks = []
        for r in track_rows:
            tid = int(r[0] or 0)
            title = str(r[1] or "").strip()
            disc_num = int(r[2] or 0)
            track_num = int(r[3] or 0)
            dur = int(duration_overrides.get(tid) or (r[4] or 0) or 0)
            t_fmt = (r[5] or "").strip()
            bitrate = int(r[6] or 0)
            sample_rate = int(r[7] or 0)
            bit_depth = int(r[8] or 0)
            size_bytes = int(r[9] or 0)
            file_path = str(r[10] or "").strip()

            # Heuristic: extract featured artists from title for nicer UI ("collab").
            feat = ""
            try:
                m = re.search(r"\\b(?:feat\\.?|ft\\.?|featuring)\\s+([^\\)\\]\\-]+)", title, flags=re.IGNORECASE)
                if m:
                    feat = str(m.group(1) or "").strip()
            except Exception:
                feat = ""

            tracks.append(
                {
                    "track_id": tid,
                    "title": title,
                    "disc_num": disc_num,
                    "track_num": track_num,
                    "duration_sec": dur,
                    "format": t_fmt,
                    "bitrate": bitrate,
                    "sample_rate": sample_rate,
                    "bit_depth": bit_depth,
                    "file_size_bytes": size_bytes,
                    "file_path": file_path,
                    "featured": feat,
                    "file_url": f"{base_url}/api/library/track/{tid}/stream" if tid > 0 else "",
                }
            )

        # Best-effort total duration (keep existing album value when present).
        try:
            total_duration_sec = int(total_duration_sec or 0)
        except Exception:
            total_duration_sec = 0
        if total_duration_sec <= 0 and tracks:
            total_duration_sec = sum(int(t.get("duration_sec") or 0) for t in tracks)

        cover_url = f"{base_url}/api/library/files/album/{album_id}/cover?size=640" if bool(has_cover) else None

        payload = {
            "album_id": int(album_id),
            "title": (album_title or "").strip(),
            "year": int(year or 0) if int(year or 0) > 0 else None,
            "date_text": (date_text or "").strip(),
            "genre": (genre or "").strip(),
            "label": (label or "").strip(),
            "format": (fmt or "").strip(),
            "is_lossless": bool(is_lossless),
            "track_count": int(track_count or 0),
            "total_duration_sec": int(total_duration_sec or 0),
            "has_cover": bool(has_cover),
            "cover_url": cover_url,
            "bandcamp_album_url": (bandcamp_album_url or "").strip() or None,
            "metadata_source": (metadata_source or "").strip() or None,
            "artist_id": int(artist_id or 0),
            "artist_name": (artist_name or "").strip(),
            "review": {
                "description": str(prof.get("description") or "").strip(),
                "short_description": str(prof.get("short_description") or "").strip(),
                "source": str(prof.get("source") or "").strip(),
                "updated_at": int(prof.get("updated_at") or 0),
            },
            "tracks": tracks,
        }
        _files_cache_set_json(cache_key, payload, ttl=30)
        return jsonify(payload)
    finally:
        conn.close()


def _track_file_path(db_conn, track_id: int) -> Optional[Path]:
    """Return the filesystem path for a track (metadata_item id), or None if not found."""
    row = db_conn.execute(
        """
        SELECT mp.file
        FROM metadata_items tr
        JOIN media_items mi ON mi.metadata_item_id = tr.id
        JOIN media_parts mp ON mp.media_item_id = mi.id
        WHERE tr.id = ? AND tr.metadata_type = 10
        LIMIT 1
        """,
        (track_id,),
    ).fetchone()
    if not row or not (raw := (row[0] or "").strip()):
        return None
    p = Path(raw)
    return path_for_fs_access(p)


@app.get("/api/library/track/<int:track_id>/stream")
def api_library_track_stream(track_id):
    """Stream a track from local file when possible, else proxy from Plex (avoids 502 when Plex URL unreachable from container)."""
    if _get_library_mode() == "files":
        ok, err = _ensure_files_index_ready()
        if not ok:
            return jsonify({"error": err or "Files index unavailable"}), 503
        conn = _files_pg_connect()
        if conn is None:
            return jsonify({"error": "PostgreSQL unavailable"}), 503
        try:
            with conn.cursor() as cur:
                cur.execute("SELECT file_path FROM files_tracks WHERE id = %s", (track_id,))
                row = cur.fetchone()
            if not row or not (row[0] or "").strip():
                return jsonify({"error": "Track not found"}), 404
            local_path = path_for_fs_access(Path(row[0]))
            if not local_path.exists() or not local_path.is_file():
                return jsonify({"error": "Track file missing"}), 404
            return send_file(str(local_path), as_attachment=False, conditional=True)
        finally:
            conn.close()

    if not PLEX_CONFIGURED:
        return jsonify({"error": "Plex not configured"}), 503
    db_conn = plex_connect()
    try:
        local_path = _track_file_path(db_conn, track_id)
    finally:
        db_conn.close()
    if local_path and local_path.exists() and local_path.is_file():
        try:
            return send_file(str(local_path), as_attachment=False, conditional=True)
        except Exception as e:
            logging.warning("track stream send_file failed for track %s: %s", track_id, e)
    url = f"{PLEX_HOST.rstrip('/')}/library/metadata/{track_id}/file?X-Plex-Token={PLEX_TOKEN}"
    try:
        r = requests.get(url, stream=True, timeout=60)
        r.raise_for_status()
        headers = {}
        if r.headers.get("Content-Type"):
            headers["Content-Type"] = r.headers["Content-Type"]
        if r.headers.get("Content-Length"):
            headers["Content-Length"] = r.headers["Content-Length"]
        return Response(
            r.iter_content(chunk_size=65536),
            status=r.status_code,
            headers=headers,
            direct_passthrough=True,
        )
    except requests.RequestException as e:
        logging.warning("track stream proxy failed for track %s: %s", track_id, e)
        return jsonify({"error": "Stream failed"}), 502


@app.get("/api/library/files/album/<int:album_id>/cover")
def api_library_files_album_cover(album_id):
    """Serve album cover from files-library index (files mode)."""
    if _get_library_mode() != "files":
        return jsonify({"error": "Files mode required"}), 400
    size = max(64, min(2048, _parse_int_loose(request.args.get("size"), 320)))
    ok, err = _ensure_files_index_ready()
    if not ok:
        return jsonify({"error": err or "Files index unavailable"}), 503
    conn = _files_pg_connect()
    if conn is None:
        return jsonify({"error": "PostgreSQL unavailable"}), 503
    try:
        with conn.cursor() as cur:
            cur.execute("SELECT cover_path FROM files_albums WHERE id = %s", (album_id,))
            row = cur.fetchone()
            cover_raw = (row[0] or "").strip() if row else ""
            if cover_raw:
                cover_path = path_for_fs_access(Path(cover_raw))
                if cover_path.exists() and cover_path.is_file():
                    cached = _ensure_cached_image_for_path(cover_path, kind="album", max_px=size)
                    to_send = cached or cover_path
                    return send_file(str(to_send), as_attachment=False, conditional=True)
            # Fallback to embedded cover from first track
            cur.execute(
                """
                SELECT file_path
                FROM files_tracks
                WHERE album_id = %s
                ORDER BY disc_num ASC, track_num ASC, id ASC
                LIMIT 1
                """,
                (album_id,),
            )
            tr_row = cur.fetchone()
        if tr_row and (tr_row[0] or "").strip():
            first_track = path_for_fs_access(Path(tr_row[0]))
            embedded = _extract_embedded_cover_from_audio(first_track)
            if embedded:
                raw, mime = embedded
                cached = _ensure_cached_image_from_bytes(
                    raw,
                    mime,
                    kind="embedded",
                    cache_key_hint=f"album-{album_id}",
                    max_px=size,
                )
                if cached and cached.exists():
                    return send_file(str(cached), as_attachment=False, conditional=True)
                return Response(raw, headers={"Content-Type": mime})
        # Avoid noisy 404s in the browser console: return a tiny transparent placeholder.
        # The UI still shows a "no cover" state (based on has_cover), but the image request won't error.
        try:
            transparent = base64.b64decode(
                "iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR4nGNgYAAAAAMAASsJTYQAAAAASUVORK5CYII="
            )
        except Exception:
            transparent = b""
        return Response(transparent or b"", headers={"Content-Type": "image/png"})
    finally:
        conn.close()


@app.get("/api/library/files/artist/<int:artist_id>/image")
def api_library_files_artist_image(artist_id):
    """Serve artist image from files-library index (files mode)."""
    if _get_library_mode() != "files":
        return jsonify({"error": "Files mode required"}), 400
    size = max(64, min(2048, _parse_int_loose(request.args.get("size"), 320)))
    ok, err = _ensure_files_index_ready()
    if not ok:
        return jsonify({"error": err or "Files index unavailable"}), 503
    conn = _files_pg_connect()
    if conn is None:
        return jsonify({"error": "PostgreSQL unavailable"}), 503
    try:
        name_norm = ""
        img_raw = ""
        with conn.cursor() as cur:
            cur.execute("SELECT name_norm, COALESCE(image_path, '') FROM files_artists WHERE id = %s", (artist_id,))
            row = cur.fetchone()
        if row:
            name_norm = str(row[0] or "").strip()
            img_raw = str(row[1] or "").strip()

        # Primary: local/external path stored on the artist row.
        if img_raw:
            img_path = path_for_fs_access(Path(img_raw))
            if img_path.exists() and img_path.is_file():
                cached = _ensure_cached_image_for_path(img_path, kind="artist", max_px=size)
                to_send = cached or img_path
                return send_file(str(to_send), as_attachment=False, conditional=True)

        # Fallback: external cache table (keeps UI working even if files_artists wasn't updated yet).
        if name_norm:
            with conn.cursor() as cur:
                cur.execute(
                    "SELECT COALESCE(image_path, '') FROM files_external_artist_images WHERE name_norm = %s",
                    (name_norm,),
                )
                erow = cur.fetchone()
            ext_raw = str((erow[0] if erow else "") or "").strip()
            if ext_raw:
                ext_path = path_for_fs_access(Path(ext_raw))
                if ext_path.exists() and ext_path.is_file():
                    cached = _ensure_cached_image_for_path(ext_path, kind="artist", max_px=size)
                    to_send = cached or ext_path
                    return send_file(str(to_send), as_attachment=False, conditional=True)

        return jsonify({"error": "Artist image not found"}), 404
    finally:
        conn.close()


@app.get("/api/library/external/artist-image/<path:name_norm>")
def api_library_external_artist_image(name_norm: str):
    """Serve cached external artist images (not necessarily in local library)."""
    if _get_library_mode() != "files":
        return jsonify({"error": "Files mode required"}), 400
    size = max(64, min(2048, _parse_int_loose(request.args.get("size"), 320)))
    ok, err = _ensure_files_index_ready()
    if not ok:
        return jsonify({"error": err or "Files index unavailable"}), 503
    key = _norm_artist_key(str(name_norm or ""))
    if not key:
        return jsonify({"error": "Invalid name"}), 400
    conn = _files_pg_connect()
    if conn is None:
        return jsonify({"error": "PostgreSQL unavailable"}), 503
    try:
        with conn.cursor() as cur:
            cur.execute(
                "SELECT COALESCE(image_path, '') FROM files_external_artist_images WHERE name_norm = %s",
                (key,),
            )
            row = cur.fetchone()
        img_raw = (row[0] or "").strip() if row else ""
        if not img_raw:
            return jsonify({"error": "External artist image not found"}), 404
        p = Path(img_raw)
        if not p.exists() or not p.is_file():
            return jsonify({"error": "External artist image missing"}), 404
        cached = _ensure_cached_image_for_path(p, kind="artist", max_px=size)
        to_send = cached or p
        return send_file(str(to_send), as_attachment=False, conditional=True)
    finally:
        conn.close()


@app.post("/api/lidarr/add-album")
def api_lidarr_add_album():
    """Add a broken album to Lidarr for re-download."""
    data = request.get_json() or {}
    artist_name = data.get("artist_name")
    album_id = data.get("album_id")
    musicbrainz_release_group_id = data.get("musicbrainz_release_group_id")
    album_title = data.get("album_title", "")
    
    if not artist_name or not musicbrainz_release_group_id:
        return jsonify({"error": "Missing required fields: artist_name, musicbrainz_release_group_id"}), 400
    
    success = add_broken_album_to_lidarr(artist_name, album_id or 0, musicbrainz_release_group_id, album_title)
    
    if success:
        # Update database
        import sqlite3
        con = sqlite3.connect(str(STATE_DB_FILE), timeout=30)
        cur = con.cursor()
        cur.execute("""
            UPDATE broken_albums SET sent_to_lidarr = 1 
            WHERE artist = ? AND album_id = ?
        """, (artist_name, album_id))
        con.commit()
        con.close()
        
        return jsonify({"success": True, "message": f"Album '{album_title}' added to Lidarr"})
    else:
        return jsonify({"success": False, "message": "Failed to add album to Lidarr"}), 500


def _run_lidarr_add_incomplete_albums(rows: List[tuple]):
    """Background worker: add each incomplete (broken) album to Lidarr. rows: (artist, album_id, musicbrainz_release_group_id, album_title)."""
    total = len(rows)
    added = 0
    failed = 0
    with lock:
        state["lidarr_add_incomplete"] = {
            "running": True,
            "current": 0,
            "total": total,
            "current_album": None,
            "current_artist": None,
            "added": 0,
            "failed": 0,
            "result": None,
        }
    con = sqlite3.connect(str(STATE_DB_FILE), timeout=30)
    try:
        for i, (artist_name, album_id, mbid, album_title) in enumerate(rows):
            with lock:
                if state.get("lidarr_add_incomplete") and state["lidarr_add_incomplete"].get("running"):
                    state["lidarr_add_incomplete"]["current"] = i
                    state["lidarr_add_incomplete"]["current_album"] = album_title
                    state["lidarr_add_incomplete"]["current_artist"] = artist_name
            success = add_broken_album_to_lidarr(artist_name, album_id, mbid or "", album_title)
            if success:
                added += 1
                cur = con.cursor()
                cur.execute("UPDATE broken_albums SET sent_to_lidarr = 1 WHERE artist = ? AND album_id = ?", (artist_name, album_id))
                con.commit()
            else:
                failed += 1
            with lock:
                if state.get("lidarr_add_incomplete") and state["lidarr_add_incomplete"].get("running"):
                    state["lidarr_add_incomplete"]["current"] = i + 1
                    state["lidarr_add_incomplete"]["added"] = added
                    state["lidarr_add_incomplete"]["failed"] = failed
        with lock:
            if state.get("lidarr_add_incomplete"):
                state["lidarr_add_incomplete"]["running"] = False
                state["lidarr_add_incomplete"]["result"] = {
                    "added": added,
                    "failed": failed,
                    "total": total,
                    "skipped": total - added - failed,
                }
            state["last_lidarr_add_added"] = added
            state["last_lidarr_add_failed"] = failed
    except Exception as e:
        logging.exception("lidarr add-incomplete-albums failed: %s", e)
        with lock:
            if state.get("lidarr_add_incomplete"):
                state["lidarr_add_incomplete"]["running"] = False
                state["lidarr_add_incomplete"]["result"] = {"error": str(e)}
    finally:
        con.close()


@app.post("/api/lidarr/add-incomplete-albums")
def api_lidarr_add_incomplete_albums():
    """Start adding all incomplete (broken) albums that are not yet sent to Lidarr."""
    if not LIDARR_URL or not LIDARR_API_KEY:
        return jsonify({"error": "Lidarr not configured (URL and API Key required)", "started": False}), 503
    with lock:
        if state.get("lidarr_add_incomplete") and state["lidarr_add_incomplete"].get("running"):
            return jsonify({"error": "Add incomplete albums already running", "started": False}), 409
    con = sqlite3.connect(str(STATE_DB_FILE), timeout=30)
    try:
        cur = con.cursor()
        cur.execute(
            "SELECT artist, album_id, musicbrainz_release_group_id FROM broken_albums WHERE sent_to_lidarr = 0"
        )
        raw_rows = cur.fetchall()
    finally:
        con.close()
    # Resolve album titles from Plex for display
    rows = []
    db_conn = None
    try:
        db_conn = plex_connect()
        for (artist_name, album_id, mbid) in raw_rows:
            title = album_title(db_conn, album_id) if db_conn else f"Album {album_id}"
            rows.append((artist_name, album_id, mbid, title))
    except Exception:
        rows = [(r[0], r[1], r[2], f"Album {r[1]}") for r in raw_rows]
    finally:
        if db_conn:
            db_conn.close()
    if not rows:
        return jsonify({"error": "No incomplete albums to add (or all already sent to Lidarr)", "started": False}), 404
    thread = threading.Thread(target=_run_lidarr_add_incomplete_albums, args=(rows,), daemon=True)
    thread.start()
    return jsonify({"started": True, "total": len(rows)})


@app.get("/api/lidarr/add-incomplete-albums/progress")
def api_lidarr_add_incomplete_albums_progress():
    """Return current add-incomplete-albums-to-Lidarr job progress."""
    with lock:
        prog = state.get("lidarr_add_incomplete")
    if prog is None:
        return jsonify({"running": False, "finished": False})
    return jsonify({
        "running": prog.get("running", False),
        "current": prog.get("current", 0),
        "total": prog.get("total", 0),
        "current_album": prog.get("current_album"),
        "current_artist": prog.get("current_artist"),
        "added": prog.get("added", 0),
        "failed": prog.get("failed", 0),
        "finished": not prog.get("running", True) and prog.get("result") is not None,
        "result": prog.get("result"),
    })


def get_artist_albums(db_conn, artist_id: int) -> List[dict]:
    """Get all albums for an artist from Plex DB (selected sections only ‚Äî SECTION_IDS)."""
    if not SECTION_IDS:
        return []
    placeholders = ",".join("?" for _ in SECTION_IDS)
    section_filter = f"AND library_section_id IN ({placeholders})"
    section_args = [artist_id] + list(SECTION_IDS)
    cursor = db_conn.execute(
        "SELECT id, title FROM metadata_items WHERE parent_id = ? AND metadata_type = 9 " + section_filter,
        section_args,
    )
    return [{"album_id": row[0], "title": row[1]} for row in cursor.fetchall()]

def add_artist_to_lidarr(artist_id: int, artist_name: str, artist_mbid: str | None = None) -> bool:
    """
    Add an artist to Lidarr with monitoring of missing albums.
    Returns True if successful, False otherwise.
    """
    if not LIDARR_URL or not LIDARR_API_KEY:
        logging.warning("Lidarr not configured (LIDARR_URL or LIDARR_API_KEY missing)")
        return False
    
    try:
        headers = {"X-Api-Key": LIDARR_API_KEY}
        
        # Get existing albums from Plex
        db_conn = plex_connect()
        existing_albums = get_artist_albums(db_conn, artist_id)
        db_conn.close()
        
        # Search for artist in Lidarr
        search_url = f"{LIDARR_URL.rstrip('/')}/api/v1/artist/lookup"
        search_term = f"mbid:{artist_mbid}" if artist_mbid else artist_name
        search_params = {"term": search_term}
        
        response = requests.get(search_url, headers=headers, params=search_params, timeout=10)
        
        if response.status_code != 200:
            logging.error("Lidarr artist search failed: %s", response.text)
            return False
        
        artists = response.json()
        if not artists:
            logging.warning("Artist '%s' not found in Lidarr", artist_name)
            return False
        
        # Use first matching artist
        lidarr_artist = artists[0]
        lidarr_artist_id = lidarr_artist.get('id')
        
        if not lidarr_artist_id:
            logging.warning("Lidarr artist '%s' has no ID", artist_name)
            return False
        
        # Check if artist already exists in Lidarr
        existing_url = f"{LIDARR_URL.rstrip('/')}/api/v1/artist/{lidarr_artist_id}"
        existing_response = requests.get(existing_url, headers=headers, timeout=10)
        
        if existing_response.status_code == 200:
            # Artist exists, update monitoring
            artist_data = existing_response.json()
            artist_data["monitored"] = True
            artist_data["monitor"] = "missing"  # Monitor missing albums
            
            update_url = f"{LIDARR_URL.rstrip('/')}/api/v1/artist"
            update_response = requests.put(update_url, headers=headers, json=artist_data, timeout=10)
            
            if update_response.status_code in (200, 202):
                logging.info("Successfully updated artist '%s' monitoring in Lidarr", artist_name)
                return True
            else:
                logging.error("Failed to update artist in Lidarr: %s", update_response.text)
                return False
        else:
            # Artist doesn't exist, add it
            add_url = f"{LIDARR_URL.rstrip('/')}/api/v1/artist"
            add_payload = {
                **lidarr_artist,
                "monitored": True,
                "monitor": "missing",
                "addOptions": {
                    "monitor": "missing",
                    "searchForMissingAlbums": True
                }
            }
            
            add_response = requests.post(add_url, headers=headers, json=add_payload, timeout=10)
            
            if add_response.status_code in (200, 201):
                logging.info("Successfully added artist '%s' to Lidarr", artist_name)
                return True
            else:
                logging.error("Failed to add artist to Lidarr: %s", add_response.text)
                return False
        
    except requests.exceptions.RequestException as e:
        logging.error("Lidarr API request failed: %s", e)
        return False
    except Exception as e:
        logging.error("Unexpected error adding artist to Lidarr: %s", e, exc_info=True)
        return False

@app.post("/api/lidarr/add-artist")
def api_lidarr_add_artist():
    """Add artist to Lidarr with monitoring of missing albums."""
    data = request.get_json() or {}
    artist_id = data.get("artist_id")
    artist_name = data.get("artist_name")
    artist_mbid = data.get("musicbrainz_artist_id")
    
    if not artist_id or not artist_name:
        return jsonify({"error": "Missing required fields: artist_id, artist_name"}), 400
    
    success = add_artist_to_lidarr(artist_id, artist_name, artist_mbid)
    
    if success:
        # Update database
        import sqlite3
        con = sqlite3.connect(str(STATE_DB_FILE), timeout=30)
        cur = con.cursor()
        cur.execute("""
            INSERT OR REPLACE INTO monitored_artists 
            (artist_id, artist_name, musicbrainz_artist_id, lidarr_artist_id, monitored_at)
            VALUES (?, ?, ?, ?, ?)
        """, (artist_id, artist_name, artist_mbid, None, time.time()))
        con.commit()
        con.close()
        
        return jsonify({"success": True, "message": f"Artist '{artist_name}' added to Lidarr"})
    else:
        return jsonify({"success": False, "message": "Failed to add artist to Lidarr"}), 500

def create_autobrr_filter(artist_names: List[str], quality_preferences: dict | None = None) -> bool:
    """
    Create an Autobrr filter for monitoring artists.
    Returns True if successful, False otherwise.
    """
    if not AUTOBRR_URL or not AUTOBRR_API_KEY:
        logging.warning("Autobrr not configured (AUTOBRR_URL or AUTOBRR_API_KEY missing)")
        return False
    
    try:
        headers = {
            "X-API-Token": AUTOBRR_API_KEY,
            "Content-Type": "application/json"
        }
        
        # Build filter data
        filter_name = f"PMDA - {', '.join(artist_names[:3])}" + ("..." if len(artist_names) > 3 else "")
        artists_str = ",".join(artist_names)
        
        filter_data = {
            "name": filter_name,
            "indexers": [],  # User should configure indexers in Autobrr UI
            "artists": artists_str,
            "match_releases": "",
            "except_releases": "",
            "max_size": "",
            "min_size": "",
            "delay": 0,
            "priority": 0,
            "max_downloads": 0,
            "max_downloads_unit": "HOUR",
            "match_hooks": [],
            "except_hooks": [],
            "actions": []  # User should configure actions in Autobrr UI
        }
        
        # Add quality preferences if provided
        if quality_preferences:
            filter_data.update(quality_preferences)
        
        # Create filter
        create_url = f"{AUTOBRR_URL.rstrip('/')}/api/filters"
        response = requests.post(create_url, headers=headers, json=filter_data, timeout=10)
        
        if response.status_code in (200, 201):
            logging.info("Successfully created Autobrr filter '%s' for %d artists", filter_name, len(artist_names))
            return True
        else:
            logging.error("Failed to create Autobrr filter: %s", response.text)
            return False
            
    except requests.exceptions.RequestException as e:
        logging.error("Autobrr API request failed: %s", e)
        return False
    except Exception as e:
        logging.error("Unexpected error creating Autobrr filter: %s", e, exc_info=True)
        return False

@app.post("/api/autobrr/create-filter")
def api_autobrr_create_filter():
    """Create an Autobrr filter for monitoring artists."""
    data = request.get_json() or {}
    artist_names = data.get("artist_names", [])
    quality_preferences = data.get("quality_preferences")
    
    if not artist_names:
        return jsonify({"error": "Missing required field: artist_names"}), 400
    
    success = create_autobrr_filter(artist_names, quality_preferences)
    
    if success:
        return jsonify({"success": True, "message": f"Autobrr filter created for {len(artist_names)} artist(s)"})
    else:
        return jsonify({"success": False, "message": "Failed to create Autobrr filter"}), 500

@app.post("/api/lidarr/test")
def api_lidarr_test():
    """Test Lidarr connection."""
    data = request.get_json() or {}
    url = data.get("url", "").strip().rstrip("/")
    api_key = data.get("api_key", "").strip()
    
    if not url or not api_key:
        return jsonify({"success": False, "message": "Lidarr URL and API Key are required"}), 400
    
    try:
        headers = {"X-Api-Key": api_key}
        # Test with system status endpoint
        test_url = f"{url}/api/v1/system/status"
        response = requests.get(test_url, headers=headers, timeout=10)
        
        if response.status_code == 200:
            status_data = response.json()
            version = status_data.get("version", "unknown")
            return jsonify({
                "success": True,
                "message": f"Lidarr connection successful (version {version})"
            })
        elif response.status_code == 401:
            return jsonify({
                "success": False,
                "message": "Authentication failed. Please check your API key."
            }), 401
        else:
            return jsonify({
                "success": False,
                "message": f"Lidarr returned status {response.status_code}: {response.text[:200]}"
            }), response.status_code
    except requests.exceptions.ConnectionError:
        return jsonify({
            "success": False,
            "message": f"Could not connect to Lidarr at {url}. Check if Lidarr is running and the URL is correct."
        }), 503
    except requests.exceptions.Timeout:
        return jsonify({
            "success": False,
            "message": "Connection to Lidarr timed out. Check your network connection."
        }), 504
    except Exception as e:
        logging.exception("Lidarr test failed")
        return jsonify({
            "success": False,
            "message": f"Unexpected error: {str(e)}"
        }), 500

@app.post("/api/autobrr/test")
def api_autobrr_test():
    """Test Autobrr connection."""
    data = request.get_json() or {}
    url = data.get("url", "").strip().rstrip("/")
    api_key = data.get("api_key", "").strip()
    
    if not url or not api_key:
        return jsonify({"success": False, "message": "Autobrr URL and API Key are required"}), 400
    
    try:
        headers = {"X-API-Token": api_key}
        # Test with health check endpoint
        test_url = f"{url}/api/healthz/liveness"
        response = requests.get(test_url, headers=headers, timeout=10)
        
        if response.status_code == 200:
            return jsonify({
                "success": True,
                "message": "Autobrr connection successful"
            })
        elif response.status_code == 401:
            return jsonify({
                "success": False,
                "message": "Authentication failed. Please check your API key."
            }), 401
        else:
            # Try alternative endpoint (config)
            config_url = f"{url}/api/config"
            config_response = requests.get(config_url, headers=headers, timeout=10)
            if config_response.status_code == 200:
                return jsonify({
                    "success": True,
                    "message": "Autobrr connection successful"
                })
            else:
                return jsonify({
                    "success": False,
                    "message": f"Autobrr returned status {config_response.status_code}: {config_response.text[:200]}"
                }), config_response.status_code
    except requests.exceptions.ConnectionError:
        return jsonify({
            "success": False,
            "message": f"Could not connect to Autobrr at {url}. Check if Autobrr is running and the URL is correct."
        }), 503
    except requests.exceptions.Timeout:
        return jsonify({
            "success": False,
            "message": "Connection to Autobrr timed out. Check your network connection."
        }), 504
    except Exception as e:
        logging.exception("Autobrr test failed")
        return jsonify({
            "success": False,
            "message": f"Unexpected error: {str(e)}"
        }), 500

def get_similar_artists_mb(artist_mbid: str) -> List[dict]:
    """Get similar artists from MusicBrainz using relations and tags."""
    if not USE_MUSICBRAINZ:
        return []
    
    similar = []
    
    try:
        # Get artist relations
        result = musicbrainzngs.get_artist_by_id(
            artist_mbid,
            includes=["artist-rels", "tags"]
        )
        artist_data = result.get("artist", {})
        relations = artist_data.get("artist-relation-list", [])
        
        # Add artists from relations
        for rel in relations:
            rel_type = rel.get("type", "")
            if rel_type in ["similar to", "influenced by", "collaboration", "member of", "founded"]:
                target_artist = rel.get("artist", {})
                if target_artist:
                    similar.append({
                        "name": target_artist.get("name", ""),
                        "mbid": target_artist.get("id", ""),
                        "type": rel_type
                    })
        
        # Also search by tags/genres for additional similar artists
        tags = artist_data.get("tag-list", [])
        if tags:
            # Get top tags
            top_tags = sorted(tags, key=lambda t: int(t.get("count", 0)), reverse=True)[:3]
            for tag_info in top_tags:
                tag_name = tag_info.get("name", "")
                if tag_name:
                    try:
                        # Search for artists with similar tags
                        search_result = musicbrainzngs.search_artists(tag=tag_name, limit=10)
                        artist_list = search_result.get("artist-list", [])
                        for artist in artist_list:
                            if artist.get("id") != artist_mbid:  # Don't include self
                                # Check if not already in similar list
                                if not any(s.get("mbid") == artist.get("id") for s in similar):
                                    similar.append({
                                        "name": artist.get("name", ""),
                                        "mbid": artist.get("id", ""),
                                        "type": f"tag: {tag_name}"
                                    })
                                if len(similar) >= 20:  # Limit total results
                                    break
                        if len(similar) >= 20:
                            break
                    except Exception:
                        continue
        
        # Remove duplicates and limit
        seen = set()
        unique_similar = []
        for s in similar:
            if s["mbid"] not in seen:
                seen.add(s["mbid"])
                unique_similar.append(s)
                if len(unique_similar) >= 15:
                    break
        
        return unique_similar
    except Exception as e:
        logging.error("Failed to get similar artists for MBID %s: %s", artist_mbid, e)
        return []

@app.get("/api/library/artist/<int:artist_id>/similar")
def api_library_artist_similar(artist_id):
    """Get similar artists for a given artist (providers + local genre fallback)."""
    if _get_library_mode() == "files":
        ok, err = _ensure_files_index_ready()
        if not ok:
            return jsonify({"error": err or "Files index unavailable"}), 503
        conn = _files_pg_connect()
        if conn is None:
            return jsonify({"error": "PostgreSQL unavailable"}), 503
        try:
            with conn.cursor() as cur:
                cur.execute("SELECT id, name, name_norm FROM files_artists WHERE id = %s", (int(artist_id),))
                row = cur.fetchone()
                if not row:
                    return jsonify({"error": "Artist not found"}), 404
                artist_name = (row[1] or "").strip()
                artist_norm = (row[2] or "").strip() or _norm_artist_key(artist_name)

                cur.execute(
                    """
                    SELECT COALESCE(similar_json, ''), updated_at
                    FROM files_artist_profiles
                    WHERE name_norm = %s
                    """,
                    (artist_norm,),
                )
                prof_row = cur.fetchone()

            base_url = request.url_root.rstrip("/")

            # 1) Prefer already-enriched similar artists from cached profiles.
            if prof_row and str(prof_row[0] or "").strip():
                try:
                    stored = json.loads(prof_row[0] or "[]")
                except Exception:
                    stored = []
                if isinstance(stored, list) and stored:
                    patched = _files_attach_similar_artist_refs(conn, stored, base_url)
                    # Warm missing images asynchronously (keeps API fast; UI can refetch).
                    try:
                        missing = [
                            str(it.get("name") or "").strip()
                            for it in (patched or [])
                            if isinstance(it, dict)
                            and str(it.get("name") or "").strip()
                            and not str(it.get("image_url") or "").strip()
                        ]
                        if missing:
                            _enqueue_files_similar_images_warm(artist_norm, missing[:12])
                    except Exception:
                        pass
                    return jsonify(
                        {
                            "artist_id": int(artist_id),
                            "source": "profile",
                            "similar_artists": patched[:20],
                        }
                    )

            # 2) Try Last.fm (best UX: includes similar artists + images).
            lastfm_info = _fetch_lastfm_artist_info(artist_name) or {}
            lf_sim = lastfm_info.get("similar") or []
            if isinstance(lf_sim, list) and lf_sim:
                # Best-effort: persist similar list (do not overwrite a good bio).
                try:
                    existing = _files_get_artist_profile_cached(artist_name, artist_norm) or {}
                    next_profile = dict(existing) if isinstance(existing, dict) else {}
                    if not next_profile.get("bio") and str(lastfm_info.get("bio") or "").strip():
                        next_profile["bio"] = str(lastfm_info.get("bio") or "").strip()
                    if not next_profile.get("short_bio") and str(lastfm_info.get("short_bio") or "").strip():
                        next_profile["short_bio"] = str(lastfm_info.get("short_bio") or "").strip()
                    if lastfm_info.get("tags"):
                        next_profile["tags"] = lastfm_info.get("tags") or []
                    next_profile["similar"] = lf_sim
                    if not (next_profile.get("source") or "").strip():
                        next_profile["source"] = "lastfm"
                    with conn.transaction():
                        _files_upsert_artist_profile(conn, artist_norm, artist_name, next_profile)
                except Exception:
                    pass
                # Best-effort: cache similar artists images so UI stays pretty offline.
                try:
                    for sim in lf_sim[:12]:
                        if not isinstance(sim, dict):
                            continue
                        sname = (sim.get("name") or "").strip()
                        surl = (sim.get("image_url") or "").strip()
                        if not sname or not surl:
                            continue
                        if _is_probably_placeholder_artist_image_url(surl):
                            continue
                        with conn.transaction():
                            _files_cache_external_artist_image(conn, artist_name=sname, provider="lastfm", image_url=surl, max_px=640)
                except Exception:
                    pass
                patched = _files_attach_similar_artist_refs(conn, lf_sim, base_url)
                # Warm missing images asynchronously (Last.fm similar sometimes lacks usable images).
                try:
                    missing = [
                        str(it.get("name") or "").strip()
                        for it in (patched or [])
                        if isinstance(it, dict)
                        and str(it.get("name") or "").strip()
                        and not str(it.get("image_url") or "").strip()
                    ]
                    if missing:
                        _enqueue_files_similar_images_warm(artist_norm, missing[:12])
                except Exception:
                    pass
                return jsonify(
                    {
                        "artist_id": int(artist_id),
                        "source": "lastfm",
                        "similar_artists": patched[:20],
                    }
                )

            # 3) Try MusicBrainz, but only when identity match is sane (avoid nonsense).
            mbid = ""
            mb_sim: list[dict] = []
            if USE_MUSICBRAINZ:
                try:
                    search_result = musicbrainzngs.search_artists(artist=artist_name, limit=3)
                    artist_list = search_result.get("artist-list") or []
                    for cand in artist_list[:3]:
                        if not isinstance(cand, dict):
                            continue
                        cand_name = (cand.get("name") or "").strip()
                        if cand_name and _provider_identity_text_score(artist_name, cand_name) >= 0.78:
                            mbid = (cand.get("id") or "").strip()
                            break
                except Exception:
                    mbid = ""
                if mbid:
                    try:
                        mb_sim = get_similar_artists_mb(mbid) or []
                    except Exception:
                        mb_sim = []
            if isinstance(mb_sim, list) and mb_sim:
                patched = _files_attach_similar_artist_refs(conn, mb_sim, base_url)
                return jsonify(
                    {
                        "artist_id": int(artist_id),
                        "artist_mbid": mbid,
                        "source": "musicbrainz",
                        "similar_artists": patched[:20],
                    }
                )

            # 4) Local fallback: overlap inferred genres in the local DB.
            genre_sim = _files_similar_artists_by_genre(conn, int(artist_id), limit=20) or []
            if genre_sim:
                genre_sim = _files_attach_similar_artist_refs(conn, genre_sim, base_url)
            return jsonify(
                {
                    "artist_id": int(artist_id),
                    "source": "genre" if genre_sim else "none",
                    "similar_artists": (genre_sim[:20] if isinstance(genre_sim, list) else []),
                }
            )
        finally:
            conn.close()

    if not PLEX_CONFIGURED:
        return jsonify({"error": "Plex not configured"}), 503
    
    if not USE_MUSICBRAINZ:
        return jsonify({"error": "MusicBrainz not enabled"}), 400
    
    db_conn = plex_connect()
    
    # Get artist name
    artist_row = db_conn.execute(
        "SELECT title FROM metadata_items WHERE id = ? AND metadata_type = 8",
        (artist_id,)
    ).fetchone()
    
    if not artist_row:
        db_conn.close()
        return jsonify({"error": "Artist not found"}), 404
    
    artist_name = artist_row[0]
    
    # Try to find MusicBrainz ID from artist's albums
    # Look for musicbrainz_albumartistid in any album's first track
    mbid = None
    album_rows = db_conn.execute("""
        SELECT alb.id
        FROM metadata_items alb
        WHERE alb.parent_id = ? AND alb.metadata_type = 9
        LIMIT 1
    """, (artist_id,)).fetchall()
    
    if album_rows:
        album_id = album_rows[0][0]
        # Get first track
        track_rows = db_conn.execute("""
            SELECT tr.id
            FROM metadata_items tr
            WHERE tr.parent_id = ? AND tr.metadata_type = 10
            LIMIT 1
        """, (album_id,)).fetchall()
        
        if track_rows:
            track_id = track_rows[0][0]
            # Get file path for this track
            file_rows = db_conn.execute("""
                SELECT mp.file
                FROM media_items mi
                JOIN media_parts mp ON mp.media_item_id = mi.id
                WHERE mi.metadata_item_id = ?
                LIMIT 1
            """, (track_id,)).fetchall()
            
            if file_rows:
                file_path = file_rows[0][0]
                # Try to extract MBID from file
                try:
                    folder = first_part_path(db_conn, album_id)
                    if folder:
                        first_audio = next((p for p in folder.rglob("*") if AUDIO_RE.search(p.name)), None)
                        if first_audio:
                            meta = extract_tags(first_audio)
                            mbid = meta.get('musicbrainz_albumartistid') or meta.get('musicbrainz_artistid')
                except Exception:
                    pass
    
    db_conn.close()
    
    if not mbid:
        # Try to search MusicBrainz by artist name
        try:
            search_result = musicbrainzngs.search_artists(artist=artist_name, limit=1)
            if search_result.get("artist-list"):
                mbid = search_result["artist-list"][0]["id"]
        except Exception as e:
            logging.warning("Failed to search MusicBrainz for artist '%s': %s", artist_name, e)
            return jsonify({"error": "Could not find MusicBrainz ID for artist"}), 404
    
    if not mbid:
        return jsonify({"error": "Could not find MusicBrainz ID for artist"}), 404
    
    similar = get_similar_artists_mb(mbid)
    # Partie 4.2: Last.fm fallback for similar artists
    if USE_LASTFM and LASTFM_API_KEY.strip():
        try:
            params = {"method": "artist.getSimilar", "artist": artist_name, "api_key": LASTFM_API_KEY, "format": "json"}
            resp = requests.get("https://ws.audioscrobbler.com/2.0/", params=params, timeout=10)
            if resp.status_code == 200:
                data = resp.json()
                lf_artists = data.get("similarartists", {}).get("artist") or []
                if isinstance(lf_artists, dict):
                    lf_artists = [lf_artists]
                seen_mbids = {s.get("mbid") for s in similar if s.get("mbid")}
                seen_names = {s.get("name", "").lower() for s in similar}
                for a in lf_artists[:15]:
                    name = (a.get("name") or "").strip()
                    lf_mbid = (a.get("mbid") or "").strip()
                    if not name:
                        continue
                    if lf_mbid and lf_mbid in seen_mbids:
                        continue
                    if name.lower() in seen_names:
                        continue
                    similar.append({"name": name, "mbid": lf_mbid or "", "type": "Last.fm"})
                    if lf_mbid:
                        seen_mbids.add(lf_mbid)
                    seen_names.add(name.lower())
                    if len(similar) >= 20:
                        break
        except Exception as e:
            logging.debug("[Similar artists] Last.fm fallback failed: %s", e)
    return jsonify({
        "artist_mbid": mbid,
        "similar_artists": similar[:20]
    })

@app.get("/api/library/release-group/<mbid>/labels")
def api_library_release_group_labels(mbid):
    """Get labels for a MusicBrainz release-group (from first release)."""
    if not PLEX_CONFIGURED:
        return jsonify({"error": "Plex not configured"}), 503
    if not USE_MUSICBRAINZ:
        return jsonify({"error": "MusicBrainz not enabled"}), 400
    mbid = (mbid or "").strip()
    if not mbid or len(mbid) != 36:
        return jsonify({"error": "Invalid release-group MBID"}), 400
    try:
        if MB_QUEUE_ENABLED and USE_MUSICBRAINZ:
            rg_data = get_mb_queue().submit(
                f"rg_labels_{mbid}",
                lambda: musicbrainzngs.get_release_group_by_id(mbid, includes=["releases", "artist-credits"]),
            )
        else:
            rg_data = musicbrainzngs.get_release_group_by_id(mbid, includes=["releases", "artist-credits"])
        rg = rg_data.get("release-group", {})
        releases = rg.get("release-list") or []
        if not releases:
            return jsonify({"labels": []})
        first_release_id = releases[0].get("id")
        if not first_release_id:
            return jsonify({"labels": []})
        if MB_QUEUE_ENABLED and USE_MUSICBRAINZ:
            rel_data = get_mb_queue().submit(f"rel_labels_{first_release_id}", lambda: musicbrainzngs.get_release_by_id(first_release_id, includes=["labels"]))
        else:
            rel_data = musicbrainzngs.get_release_by_id(first_release_id, includes=["labels"])
        release = rel_data.get("release", {})
        label_list = release.get("label-list") or []
        labels = []
        for lb in label_list:
            label = lb.get("label", {}) if isinstance(lb.get("label"), dict) else {}
            name = label.get("name") or lb.get("name") or ""
            lid = label.get("id") or lb.get("id") or ""
            if name or lid:
                labels.append({"name": name, "id": lid})
        return jsonify({"labels": labels})
    except musicbrainzngs.WebServiceError as e:
        if "404" in str(e):
            return jsonify({"error": "Release group not found"}), 404
        raise
    except Exception as e:
        logging.exception("Failed to get labels for release-group %s", mbid)
        return jsonify({"error": str(e)}), 500

@app.get("/api/library/artist/<int:artist_id>/monitored")
def api_library_artist_monitored(artist_id):
    """Check if an artist is monitored in Lidarr."""
    if _get_library_mode() == "files":
        # files-library artist IDs are internal to the index and may change after rebuilds;
        # keep this endpoint deterministic in files mode.
        return jsonify({"monitored": False})
    import sqlite3
    con = sqlite3.connect(str(STATE_DB_FILE), timeout=30)
    cur = con.cursor()
    cur.execute("SELECT 1 FROM monitored_artists WHERE artist_id = ?", (artist_id,))
    is_monitored = cur.fetchone() is not None
    con.close()
    return jsonify({"monitored": is_monitored})

def get_artist_images_mb(artist_mbid: str) -> List[str]:
    """Get artist images from MusicBrainz/Wikimedia."""
    if not USE_MUSICBRAINZ:
        return []
    
    try:
        result = musicbrainzngs.get_artist_by_id(
            artist_mbid,
            includes=["url-rels"]
        )
        image_urls = []
        artist_data = result.get("artist", {})
        url_relations = artist_data.get("url-relation-list", [])
        
        for url_rel in url_relations:
            target = url_rel.get("target", "")
            if "wikimedia" in target.lower() or "commons.wikimedia" in target.lower():
                image_urls.append(target)
        
        return image_urls
    except Exception as e:
        logging.error("Failed to get artist images for MBID %s: %s", artist_mbid, e)
        return []


def _artist_folder_has_image(artist_folder: Path) -> bool:
    """Return True if the artist folder already has a dedicated artist image file."""
    if not artist_folder or not artist_folder.is_dir():
        return False
    return any((artist_folder / n).is_file() for n in _ARTIST_IMAGE_NAMES)


def _is_probably_placeholder_artist_image_url(url: str) -> bool:
    low = (url or "").strip().lower()
    if not low:
        return True
    # Known Last.fm "missing artist image" hashes frequently returned for unknown artists.
    placeholder_tokens = (
        "2a96cbd8b46e442fc41c2b86b821562f",
        "4128a6eb29f94943c9d206c08e625904",
        "c6f59c1e5e7240a4c0d427abd71f3dbb",
        # Generic placeholder patterns from various CDNs/search results.
        "placeholder-artist",
        "/placeholders/",
        "default_avatar",
        "default-avatar",
        "noimage",
        "no-image",
        "blank.jpg",
        "spacer.gif",
        "transparent.png",
    )
    if any(tok in low for tok in placeholder_tokens):
        return True
    if "default" in low and ("last.fm" in low or "lastfm" in low):
        return True
    # Common known placeholder endpoints.
    if "static-images.merchbar.com" in low and "/placeholders/" in low:
        return True
    if "s0.wp.com/i/blank.jpg" in low:
        return True
    return False


def _is_usable_artist_image_bytes(raw: bytes, *, min_dim: int = 220, min_bytes: int = 8192) -> bool:
    if not raw or len(raw) < min_bytes:
        return False
    try:
        from PIL import Image
        from io import BytesIO

        with Image.open(BytesIO(raw)) as img:
            w, h = img.size
            if w < min_dim or h < min_dim:
                return False
            # Reject quasi-flat placeholders (very low color diversity).
            thumb = img.convert("RGB").resize((32, 32), Image.Resampling.BILINEAR)
            colors = thumb.getcolors(maxcolors=4096)
            if colors is not None and len(colors) < 20:
                return False
    except Exception:
        # If we cannot inspect image content, keep previous permissive behavior.
        return True
    return True


def _is_usable_artist_image_path(path: Path, *, min_dim: int = 220, min_bytes: int = 8192) -> bool:
    """
    Validate an on-disk cached artist image without loading it through the "download" path.
    Used to decide whether an existing cache entry is good enough to keep.
    """
    try:
        if not path or not path.is_file():
            return False
        st = path.stat()
        if int(st.st_size or 0) < int(min_bytes):
            return False
    except Exception:
        return False
    try:
        from PIL import Image

        with Image.open(path) as img:
            w, h = img.size
            if w < min_dim or h < min_dim:
                return False
            thumb = img.convert("RGB").resize((32, 32), Image.Resampling.BILINEAR)
            colors = thumb.getcolors(maxcolors=4096)
            if colors is not None and len(colors) < 20:
                return False
    except Exception:
        # If we cannot inspect image content, keep permissive behavior.
        return True
    return True


def _image_ahash_hex(raw: bytes, size: int = 8) -> str | None:
    try:
        from PIL import Image
        from io import BytesIO
    except Exception:
        return None
    try:
        with Image.open(BytesIO(raw)) as img:
            gray = img.convert("L").resize((size, size), Image.Resampling.BILINEAR)
            pixels = list(gray.getdata())
        if not pixels:
            return None
        avg = sum(pixels) / len(pixels)
        bits = "".join("1" if px >= avg else "0" for px in pixels)
        return f"{int(bits, 2):0{size * size // 4}x}"
    except Exception:
        return None


def _hamming_hex(a: str, b: str) -> int:
    try:
        return bin(int(a, 16) ^ int(b, 16)).count("1")
    except Exception:
        return 999


def _is_artist_image_distinct_from_local_covers(artist_folder: Path, candidate_raw: bytes) -> bool:
    """
    Reject candidate artist images that are near-identical to local album covers for the same artist.
    """
    if not artist_folder or not artist_folder.is_dir():
        return True
    cand_hash = _image_ahash_hex(candidate_raw)
    if not cand_hash:
        return True
    compared = 0
    try:
        for child in sorted(artist_folder.iterdir(), key=lambda p: str(p)):
            if not child.is_dir():
                continue
            for pat in ("cover.*", "folder.*", "album.*", "artwork.*", "front.*"):
                for cover in child.glob(pat):
                    if cover.suffix.lower() not in {".jpg", ".jpeg", ".png", ".webp"}:
                        continue
                    try:
                        raw = cover.read_bytes()
                    except Exception:
                        continue
                    cover_hash = _image_ahash_hex(raw)
                    if not cover_hash:
                        continue
                    dist = _hamming_hex(cand_hash, cover_hash)
                    compared += 1
                    if dist <= 4:
                        logging.debug(
                            "Artist image candidate rejected: too similar to album cover (%s, distance=%s)",
                            cover,
                            dist,
                        )
                        return False
                    if compared >= 12:
                        return True
    except Exception:
        return True
    return True


def _fetch_and_save_artist_image_mb(artist_mbid: str, artist_folder: Path) -> bool:
    """Fetch first artist image from MusicBrainz/Wikimedia and save to artist_folder/artist.jpg. Returns True if saved."""
    urls = get_artist_images_mb(artist_mbid)
    for url in urls:
        try:
            resp = requests.get(url, timeout=10, allow_redirects=True)
            if resp.status_code != 200:
                continue
            ct = resp.headers.get("content-type", "").lower()
            if "image/" in ct:
                if not _is_usable_artist_image_bytes(resp.content):
                    continue
                if not _is_artist_image_distinct_from_local_covers(artist_folder, resp.content):
                    continue
                ext = ".jpg"
                if "png" in ct:
                    ext = ".png"
                out = artist_folder / f"artist{ext}"
                out.write_bytes(resp.content)
                logging.info("Saved artist image from MusicBrainz to %s", out)
                return True
            # If HTML (e.g. Commons page), try og:image
            if "text/html" in ct:
                m = re.search(r'<meta\s+property="og:image"\s+content="([^"]+)"', resp.text)
                if m:
                    img_url = m.group(1).strip()
                    img_resp = requests.get(img_url, timeout=10, allow_redirects=True)
                    if img_resp.status_code == 200 and "image/" in img_resp.headers.get("content-type", "").lower():
                        if not _is_usable_artist_image_bytes(img_resp.content):
                            continue
                        if not _is_artist_image_distinct_from_local_covers(artist_folder, img_resp.content):
                            continue
                        (artist_folder / "artist.jpg").write_bytes(img_resp.content)
                        logging.info("Saved artist image from Wikimedia to %s", artist_folder / "artist.jpg")
                        return True
        except Exception as e:
            logging.warning("Artist image fetch (MB) failed for %s: %s", url, e)
    return False


def _fetch_artist_image_lastfm(artist_name: str) -> Optional[str]:
    """Get largest artist image URL from Last.fm artist.getInfo. Returns URL or None."""
    if not USE_LASTFM:
        return None
    api_key = (getattr(sys.modules[__name__], "LASTFM_API_KEY", "") or "").strip()
    if not api_key:
        return None
    try:
        resp = requests.get(
            "https://ws.audioscrobbler.com/2.0/",
            params={"method": "artist.getInfo", "artist": artist_name, "api_key": api_key, "format": "json"},
            timeout=10,
        )
        if resp.status_code != 200:
            return None
        data = resp.json()
        if data.get("error"):
            return None
        artist = data.get("artist", {})
        images = artist.get("image") or []
        url = None
        for img in images:
            if isinstance(img, dict) and img.get("#text"):
                cand = (img.get("#text") or "").strip()
                if not cand or _is_probably_placeholder_artist_image_url(cand):
                    continue
                url = cand
                if img.get("size") == "extralarge":
                    return url
        return url
    except Exception as e:
        logging.warning("Last.fm artist.getInfo failed for %s: %s", artist_name, e)
        return None


def _fetch_artist_image_discogs(artist_name: str) -> Optional[str]:
    """Get artist image URL from Discogs (search artist, first result). Returns URL or None."""
    if not USE_DISCOGS:
        return None
    token = (getattr(sys.modules[__name__], "DISCOGS_USER_TOKEN", "") or "").strip()
    if not token:
        return None
    try:
        import discogs_client
        d = discogs_client.Client("PMDA/0.6.6", user_token=token)
        results = d.search(artist_name, type="artist")
        page = results.page(1)
        if not page:
            return None
        artist = page[0]
        artist_id = getattr(artist, "id", None) or (artist.data.get("id") if getattr(artist, "data", None) else None)
        if not artist_id:
            return None
        full_artist = d.artist(artist_id)
        images = getattr(full_artist, "images", None)
        if images and len(images) > 0:
            img = images[0]
            return img.get("uri") or img.get("resource_url")
        return None
    except Exception as e:
        logging.warning("Discogs artist image fetch failed for %s: %s", artist_name, e)
        return None


def _fetch_artist_image_fanart(artist_mbid: str) -> Optional[str]:
    """
    Fetch an artist image from fanart.tv using a MusicBrainz artist MBID.
    Returns a direct image URL (usually `artistthumb`) or None.
    """
    api_key = (getattr(sys.modules[__name__], "FANART_API_KEY", "") or "").strip()
    mbid = str(artist_mbid or "").strip()
    if not api_key or not mbid or len(mbid) != 36:
        return None
    try:
        resp = requests.get(
            f"https://webservice.fanart.tv/v3/music/{mbid}",
            params={"api_key": api_key},
            timeout=10,
            allow_redirects=True,
        )
        if resp.status_code != 200:
            return None
        data = resp.json() if resp.content else {}
        if not isinstance(data, dict):
            return None

        def _pick_best(list_val: object) -> str:
            if not isinstance(list_val, list):
                return ""
            best_url = ""
            best_likes = -1
            for it in list_val:
                if not isinstance(it, dict):
                    continue
                url = str(it.get("url") or "").strip()
                if not url:
                    continue
                try:
                    if _is_probably_placeholder_artist_image_url(url):
                        continue
                except Exception:
                    pass
                likes = _parse_int_loose(it.get("likes"), 0)
                if likes > best_likes:
                    best_likes = likes
                    best_url = url
            return best_url

        # Prefer an artist thumb (portrait/headshot), fall back to background art when needed.
        url = _pick_best(data.get("artistthumb"))
        if url:
            return url
        url = _pick_best(data.get("artistbackground"))
        if url:
            return url
        return None
    except Exception as e:
        logging.debug("Fanart.tv artist image fetch failed for MBID %s: %s", mbid, e)
        return None


def _fetch_artist_image_audiodb(artist_name: str) -> Optional[str]:
    """
    Fetch an artist image from TheAudioDB by name search.
    Returns a direct image URL (`strArtistThumb` preferred) or None.
    """
    api_key = (getattr(sys.modules[__name__], "THEAUDIODB_API_KEY", "") or "").strip()
    name = str(artist_name or "").strip()
    if not api_key or not name:
        return None
    try:
        resp = requests.get(
            f"https://www.theaudiodb.com/api/v1/json/{api_key}/search.php",
            params={"s": name},
            timeout=10,
            allow_redirects=True,
        )
        if resp.status_code != 200:
            return None
        data = resp.json() if resp.content else {}
        if not isinstance(data, dict):
            return None
        artists = data.get("artists") or []
        if not isinstance(artists, list) or not artists:
            return None
        best_url = ""
        best_score = 0.0
        for a in artists[:10]:
            if not isinstance(a, dict):
                continue
            cand_name = str(a.get("strArtist") or "").strip()
            if not cand_name:
                continue
            score = _provider_identity_text_score(name, cand_name)
            if score < 0.78:
                continue
            url = (
                str(a.get("strArtistThumb") or "").strip()
                or str(a.get("strArtistFanart") or "").strip()
                or str(a.get("strArtistFanart2") or "").strip()
            )
            if not url:
                continue
            try:
                if _is_probably_placeholder_artist_image_url(url):
                    continue
            except Exception:
                pass
            if score > best_score:
                best_score = score
                best_url = url
        return best_url or None
    except Exception as e:
        logging.debug("TheAudioDB artist image fetch failed for %s: %s", name, e)
        return None


def _fetch_artist_image_web(artist_name: str) -> Optional[str]:
    """
    Fallback: try to find an artist image via web search (Serper + OpenGraph image).
    Returns image URL or None.
    """
    key = (getattr(sys.modules[__name__], "SERPER_API_KEY", "") or "").strip()
    if not key:
        return None
    query = f"{artist_name} musician photo"
    results = _serper_web_search(query, num=5)
    if not results:
        return None
    for item in results:
        link = item.get("link") or ""
        if not link:
            continue
        try:
            resp = requests.get(link, timeout=8, allow_redirects=True)
        except Exception as e:
            logging.debug("[ArtistWebImage] Failed to fetch %s: %s", link, e)
            continue
        if resp.status_code != 200:
            continue
        ct = (resp.headers.get("content-type") or "").split(";")[0].strip().lower()
        # Direct image URL
        if ct.startswith("image/"):
            return link
        # HTML page ‚Äì try og:image
        if "text/html" in ct and resp.text:
            try:
                m = re.search(
                    r'<meta\s+property=["\']og:image["\']\s+content=["\']([^"\']+)["\']',
                    resp.text,
                    re.IGNORECASE,
                )
                if not m:
                    m = re.search(
                        r'<meta\s+content=["\']([^"\']+)["\']\s+property=["\']og:image["\']',
                        resp.text,
                        re.IGNORECASE,
                    )
                if not m:
                    continue
                img_url = m.group(1).strip()
                if img_url:
                    return img_url
            except Exception as e:
                logging.debug("[ArtistWebImage] Failed to extract og:image from %s: %s", link, e)
                continue
    return None


def _fetch_and_save_artist_image(artist_name: str, artist_folder: Path, artist_mbid: Optional[str] = None) -> Optional[str]:
    """
    Fetch artist image from MB, then Last.fm, then Discogs and save to artist_folder (artist.jpg).
    Skips if artist_folder already has a dedicated artist image.
    Returns provider string on success ("musicbrainz", "lastfm", "discogs"), or None on failure.
    """
    if not artist_folder or not artist_folder.is_dir():
        return None
    if _artist_folder_has_image(artist_folder):
        return None
    if not USE_MUSICBRAINZ and not USE_LASTFM and not USE_DISCOGS:
        return None
    # Try MusicBrainz first (if we have MBID)
    if artist_mbid and USE_MUSICBRAINZ:
        if _fetch_and_save_artist_image_mb(artist_mbid, artist_folder):
            return "musicbrainz"
    # Fanart.tv (MBID-based) ‚Äì optional but often high-quality.
    if artist_mbid:
        url = _fetch_artist_image_fanart(artist_mbid)
        if url:
            try:
                resp = requests.get(url, timeout=10, allow_redirects=True)
                if resp.status_code == 200 and resp.content and _is_usable_artist_image_bytes(resp.content):
                    if not _is_artist_image_distinct_from_local_covers(artist_folder, resp.content):
                        logging.debug("Skipped Fanart.tv artist image for '%s': too similar to local cover", artist_name)
                    else:
                        (artist_folder / "artist.jpg").write_bytes(resp.content)
                        logging.info("Saved artist image from Fanart.tv to %s", artist_folder / "artist.jpg")
                        return "fanart"
            except Exception as e:
                logging.warning("Artist image download (Fanart.tv) failed: %s", e)
    # Last.fm
    if USE_LASTFM:
        url = _fetch_artist_image_lastfm(artist_name)
        if url and not _is_probably_placeholder_artist_image_url(url):
            try:
                resp = requests.get(url, timeout=10, allow_redirects=True)
                if resp.status_code == 200 and resp.content and _is_usable_artist_image_bytes(resp.content):
                    if not _is_artist_image_distinct_from_local_covers(artist_folder, resp.content):
                        logging.debug("Skipped Last.fm artist image for '%s': too similar to local cover", artist_name)
                    else:
                        (artist_folder / "artist.jpg").write_bytes(resp.content)
                        logging.info("Saved artist image from Last.fm to %s", artist_folder / "artist.jpg")
                        return "lastfm"
            except Exception as e:
                logging.warning("Artist image download (Last.fm) failed: %s", e)
    # TheAudioDB (name-based) ‚Äì optional.
    url = _fetch_artist_image_audiodb(artist_name)
    if url:
        try:
            resp = requests.get(url, timeout=10, allow_redirects=True)
            if resp.status_code == 200 and resp.content and _is_usable_artist_image_bytes(resp.content):
                if not _is_artist_image_distinct_from_local_covers(artist_folder, resp.content):
                    logging.debug("Skipped TheAudioDB artist image for '%s': too similar to local cover", artist_name)
                else:
                    (artist_folder / "artist.jpg").write_bytes(resp.content)
                    logging.info("Saved artist image from TheAudioDB to %s", artist_folder / "artist.jpg")
                    return "audiodb"
        except Exception as e:
            logging.warning("Artist image download (TheAudioDB) failed: %s", e)
    # Discogs
    if USE_DISCOGS:
        url = _fetch_artist_image_discogs(artist_name)
        if url:
            try:
                resp = requests.get(url, timeout=10, allow_redirects=True)
                if resp.status_code == 200 and resp.content and _is_usable_artist_image_bytes(resp.content):
                    if not _is_artist_image_distinct_from_local_covers(artist_folder, resp.content):
                        logging.debug("Skipped Discogs artist image for '%s': too similar to local cover", artist_name)
                    else:
                        (artist_folder / "artist.jpg").write_bytes(resp.content)
                        logging.info("Saved artist image from Discogs to %s", artist_folder / "artist.jpg")
                        return "discogs"
            except Exception as e:
                logging.warning("Artist image download (Discogs) failed: %s", e)
    # Web search fallback (Serper + OpenGraph) when other providers did not yield an image
    web_url = _fetch_artist_image_web(artist_name)
    if web_url and not _is_probably_placeholder_artist_image_url(web_url):
        try:
            resp = requests.get(web_url, timeout=10, allow_redirects=True)
            if resp.status_code == 200 and resp.content and _is_usable_artist_image_bytes(resp.content):
                if not _is_artist_image_distinct_from_local_covers(artist_folder, resp.content):
                    return None
                (artist_folder / "artist.jpg").write_bytes(resp.content)
                logging.info("Saved artist image from web search to %s", artist_folder / "artist.jpg")
                return "web"
        except Exception as e:
            logging.warning("Artist image download (web search) failed: %s", e)
    return None

@app.get("/api/library/artist/<int:artist_id>/images")
def api_library_artist_images(artist_id):
    """Get artist images from MusicBrainz/Wikimedia."""
    if _get_library_mode() == "files":
        if not USE_MUSICBRAINZ:
            return jsonify({"error": "MusicBrainz not enabled"}), 400
        ok, err = _ensure_files_index_ready()
        if not ok:
            return jsonify({"error": err or "Files index unavailable"}), 503
        conn = _files_pg_connect()
        if conn is None:
            return jsonify({"error": "PostgreSQL unavailable"}), 503
        try:
            with conn.cursor() as cur:
                cur.execute("SELECT name FROM files_artists WHERE id = %s", (artist_id,))
                row = cur.fetchone()
            if not row:
                return jsonify({"error": "Artist not found"}), 404
            artist_name = row[0] or ""
        finally:
            conn.close()
        mbid = None
        try:
            search_result = musicbrainzngs.search_artists(artist=artist_name, limit=1)
            if search_result.get("artist-list"):
                mbid = search_result["artist-list"][0]["id"]
        except Exception as e:
            logging.warning("Failed to search MusicBrainz for artist '%s': %s", artist_name, e)
            return jsonify({"error": "Could not find MusicBrainz ID for artist"}), 404
        if not mbid:
            return jsonify({"error": "Could not find MusicBrainz ID for artist"}), 404
        image_urls = get_artist_images_mb(mbid)
        return jsonify({"artist_mbid": mbid, "images": image_urls})

    if not PLEX_CONFIGURED:
        return jsonify({"error": "Plex not configured"}), 503
    
    if not USE_MUSICBRAINZ:
        return jsonify({"error": "MusicBrainz not enabled"}), 400
    
    db_conn = plex_connect()
    
    # Get artist name
    artist_row = db_conn.execute(
        "SELECT title FROM metadata_items WHERE id = ? AND metadata_type = 8",
        (artist_id,)
    ).fetchone()
    
    if not artist_row:
        db_conn.close()
        return jsonify({"error": "Artist not found"}), 404
    
    artist_name = artist_row[0]
    
    # Try to find MusicBrainz ID (same logic as similar artists)
    mbid = None
    album_rows = db_conn.execute("""
        SELECT alb.id
        FROM metadata_items alb
        WHERE alb.parent_id = ? AND alb.metadata_type = 9
        LIMIT 1
    """, (artist_id,)).fetchall()
    
    if album_rows:
        album_id = album_rows[0][0]
        folder = first_part_path(db_conn, album_id)
        if folder:
            first_audio = next((p for p in folder.rglob("*") if AUDIO_RE.search(p.name)), None)
            if first_audio:
                meta = extract_tags(first_audio)
                mbid = meta.get('musicbrainz_albumartistid') or meta.get('musicbrainz_artistid')
    
    db_conn.close()
    
    if not mbid:
        # Try to search MusicBrainz by artist name
        try:
            search_result = musicbrainzngs.search_artists(artist=artist_name, limit=1)
            if search_result.get("artist-list"):
                mbid = search_result["artist-list"][0]["id"]
        except Exception as e:
            logging.warning("Failed to search MusicBrainz for artist '%s': %s", artist_name, e)
            return jsonify({"error": "Could not find MusicBrainz ID for artist"}), 404
    
    if not mbid:
        return jsonify({"error": "Could not find MusicBrainz ID for artist"}), 404
    
    image_urls = get_artist_images_mb(mbid)
    return jsonify({
        "artist_mbid": mbid,
        "images": image_urls
    })

@app.get("/api/library/album/<int:album_id>/tags")
def api_library_album_tags(album_id):
    """Get current tags and MusicBrainz info for an album."""
    if _get_library_mode() == "files":
        ok, err = _ensure_files_index_ready()
        if not ok:
            return jsonify({"error": err or "Files index unavailable"}), 503
        conn = _files_pg_connect()
        if conn is None:
            return jsonify({"error": "PostgreSQL unavailable"}), 503
        try:
            with conn.cursor() as cur:
                cur.execute(
                    """
                    SELECT
                        alb.id,
                        alb.title,
                        art.name,
                        alb.folder_path,
                        alb.primary_tags_json,
                        alb.musicbrainz_release_group_id,
                        alb.has_cover
                    FROM files_albums alb
                    JOIN files_artists art ON art.id = alb.artist_id
                    WHERE alb.id = %s
                    """,
                    (album_id,),
                )
                row = cur.fetchone()
            if not row:
                return jsonify({"error": "Album not found"}), 404
            folder_path = path_for_fs_access(Path(row[3]))
            current_tags = {}
            try:
                current_tags = json.loads(row[4]) if row[4] else {}
            except (TypeError, ValueError):
                current_tags = {}
            if not current_tags:
                first_audio = next((p for p in folder_path.rglob("*") if AUDIO_RE.search(p.name)), None)
                current_tags = extract_tags(first_audio) if first_audio else {}
            thumb_url_files = (
                f"{request.url_root.rstrip('/')}/api/library/files/album/{album_id}/cover"
                if bool(row[6]) else None
            )
            return jsonify({
                "album_id": int(row[0]),
                "album_title": row[1] or "",
                "artist_name": row[2] or "",
                "folder": str(folder_path),
                "current_tags": current_tags or {},
                "musicbrainz_id": (row[5] or "").strip(),
                "mb_info": None,
                "thumb_url": thumb_url_files,
            })
        finally:
            conn.close()

    if not PLEX_CONFIGURED:
        return jsonify({"error": "Plex not configured"}), 503
    
    db_conn = plex_connect()
    
    # Get album info
    album_row = db_conn.execute(
        "SELECT id, title, parent_id FROM metadata_items WHERE id = ? AND metadata_type = 9",
        (album_id,)
    ).fetchone()
    
    if not album_row:
        db_conn.close()
        return jsonify({"error": "Album not found"}), 404
    
    album_title_str = album_row[1]
    artist_id = album_row[2]
    
    # Get artist name
    artist_row = db_conn.execute(
        "SELECT title FROM metadata_items WHERE id = ? AND metadata_type = 8",
        (artist_id,)
    ).fetchone()
    artist_name = artist_row[0] if artist_row else "Unknown"
    
    # Get folder path
    folder = first_part_path(db_conn, album_id)
    if not folder:
        db_conn.close()
        return jsonify({"error": "Album folder not found"}), 404
    
    # Get tags from first audio file
    first_audio = next((p for p in folder.rglob("*") if AUDIO_RE.search(p.name)), None)
    current_tags = extract_tags(first_audio) if first_audio else {}
    
    # Try to find MusicBrainz release-group info
    mb_info = None
    mbid = current_tags.get('musicbrainz_releasegroupid') or current_tags.get('musicbrainz_releaseid')
    
    if mbid and USE_MUSICBRAINZ:
        try:
            if current_tags.get('musicbrainz_releasegroupid'):
                mb_info, _ = fetch_mb_release_group_info(mbid)
            else:
                # Get release-group from release
                def _fetch_release():
                    return musicbrainzngs.get_release_by_id(mbid, includes=["release-groups"])["release"]
                
                if MB_QUEUE_ENABLED and USE_MUSICBRAINZ:
                    rel = get_mb_queue().submit(f"rel_{mbid}", _fetch_release)
                else:
                    rel = _fetch_release()
                rgid = rel['release-group']['id']
                mb_info, _ = fetch_mb_release_group_info(rgid)
                mbid = rgid
        except Exception as e:
            logging.warning("Failed to fetch MB info for album %s: %s", album_id, e)
    
    db_conn.close()
    
    return jsonify({
        "album_id": album_id,
        "album_title": album_title_str,
        "artist_name": artist_name,
        "current_tags": current_tags,
        "musicbrainz_id": mbid,
        "musicbrainz_info": mb_info,
        "folder": str(folder)
    })


def _vision_verify_cover_before_inject(
    image_bytes: bytes,
    mime: str,
    artist: str,
    album_title: str,
    source: str = "CAA",
    *,
    fail_open: bool = True,
) -> bool:
    """
    Ask vision AI whether the proposed cover image matches the album (artist/title).
    Returns True only if the AI answers Yes. Logs verdict and adds to detailed log.
    """
    # Cost + false-negative control:
    # Only run vision gating for inherently risky "Web" cover pulls. For trusted providers
    # (MusicBrainz/CAA, Discogs, Last.fm, Bandcamp), vision is flaky and can reject correct covers.
    # Those providers are already guarded by strict identity checks upstream.
    if str(source or "").strip().lower() != "web":
        return True
    if not image_bytes or not getattr(sys.modules[__name__], "USE_AI_VISION_BEFORE_COVER_INJECT", False):
        return True
    if not getattr(sys.modules[__name__], "ai_provider_ready", False):
        return True
    try:
        if len(image_bytes) > _MAX_COVER_SIZE_BYTES or (mime and mime != "image/jpeg"):
            image_bytes, mime = _resize_cover_for_vision(image_bytes, mime or "image/jpeg")
        if not image_bytes:
            logging.warning("[Vision before inject] Resize failed for %s / %s", artist, album_title)
            return True if fail_open else False
        b64 = base64.b64encode(image_bytes).decode("ascii")
        data_uri = f"data:{mime or 'image/jpeg'};base64,{b64}"
        system_msg = "Answer only Yes or No. Optionally end with (confidence: N) where N is 0-100."
        user_msg = (
            f"This image is the proposed album cover for: Artist = {artist!r}, Album = {album_title!r}. "
            "Does it look like a legitimate album cover for this album (e.g. title or artist visible, not random artwork)? "
            "Answer only Yes or No. Optionally end with (confidence: N)."
        )
        provider = getattr(sys.modules[__name__], "AI_PROVIDER", "openai")
        # Prefer an explicit vision model when configured; fall back to the main model.
        vision_model = (
            (getattr(sys.modules[__name__], "OPENAI_VISION_MODEL", None) or "").strip()
            or getattr(sys.modules[__name__], "RESOLVED_MODEL", None)
            or getattr(sys.modules[__name__], "OPENAI_MODEL", "gpt-4o-mini")
        )
        resp = call_ai_provider_vision(
            provider,
            vision_model,
            system_msg,
            user_msg,
            image_base64=[{"type": "image_url", "image_url": {"url": data_uri}}],
            max_tokens=20,
        )
        resp_txt = (resp or "").strip()
        verdict_clean, vision_confidence = parse_ai_confidence(resp_txt)
        verdict = (verdict_clean or "").strip().upper()
        if not verdict:
            # If the model returns an empty/invalid verdict, never block cover injection.
            logging.info(
                "[Vision before inject] artist=%r album=%r source=%s verdict=(empty) -> %s",
                artist,
                album_title,
                source,
                "accepted (fail-open)" if fail_open else "rejected (fail-closed)",
            )
            return True if fail_open else False
        if vision_confidence is not None:
            logging.info("[Vision before inject] confidence: %d", vision_confidence)
        ok = verdict.startswith("YES")
        # If the model says NO but with high confidence and the source is a trusted
        # provider (MusicBrainz/CAA, Discogs, Last.fm, Bandcamp or generic Web search
        # built from exact artist+album), we prefer to trust the provider match over
        # the vision heuristic and accept the cover anyway.
        if not ok and vision_confidence is not None:
            # We only override a NO verdict for non-MusicBrainz providers where we have
            # a strong text match (Discogs/Last.fm/Bandcamp/Web). For CAA/MusicBrainz,
            # we trust Vision when it says NO to avoid wrong covers coming from MB.
            trusted_sources = {"Discogs", "Last.fm", "Bandcamp", "Web"}
            ai_conf_min = getattr(sys.modules[__name__], "AI_CONFIDENCE_MIN", 50)
            if source in trusted_sources and vision_confidence >= ai_conf_min:
                logging.info(
                    "[Vision before inject] overriding NO verdict for trusted source %s "
                    "(confidence=%d >= %d) ‚Äì accepting cover.",
                    source,
                    vision_confidence,
                    ai_conf_min,
                )
                ok = True
        logging.info(
            "[Vision before inject] artist=%r album=%r source=%s verdict=%s -> %s",
            artist,
            album_title,
            source,
            verdict,
            "accepted" if ok else "rejected (cover not saved)",
        )
        if not ok:
            logging.info(
                "[Vision before inject] Cover rejected: not saving or embedding for %s / %s",
                artist,
                album_title,
            )
        return ok
    except Exception as e:
        logging.warning("[Vision before inject] Verification failed for %s / %s: %s", artist, album_title, e)
        return True if fail_open else False


def _embed_cover_in_audio_files(cover_path: Path, audio_files: list) -> None:
    """
    Embed the cover image from cover_path into all audio files (MP3, FLAC, MP4).
    Logs errors per file but does not raise.
    """
    if not cover_path or not cover_path.is_file():
        return
    try:
        from mutagen import File as MutagenFile
        from mutagen.id3 import ID3, APIC
        from mutagen.mp3 import MP3
        from mutagen.flac import FLAC, Picture
        from mutagen.mp4 import MP4, MP4Cover
    except ImportError:
        logging.warning("improve-album: mutagen not available for embedding cover")
        return
    data = cover_path.read_bytes()
    suffix = cover_path.suffix.lower()
    if suffix in (".png",):
        mime = "image/png"
        mp4_fmt = MP4Cover.FORMAT_PNG
    else:
        mime = "image/jpeg"
        mp4_fmt = MP4Cover.FORMAT_JPEG
    for audio_file in audio_files:
        try:
            audio = MutagenFile(str(audio_file))
            if audio is None:
                continue
            if isinstance(audio, MP3):
                if audio.tags is None:
                    audio.add_tags(ID3())
                audio.tags.add(APIC(encoding=3, mime=mime, type=3, desc="Cover", data=data))
            elif isinstance(audio, FLAC):
                pic = Picture()
                pic.type = 3
                pic.mime = mime
                pic.desc = "front cover"
                pic.data = data
                audio.add_picture(pic)
            elif isinstance(audio, MP4):
                if audio.tags is None:
                    from mutagen.mp4 import MP4Tags
                    audio.add_tags(MP4Tags())
                audio.tags["covr"] = [MP4Cover(data, mp4_fmt)]
            else:
                continue
            audio.save()
        except Exception as e:
            logging.error("improve-album: embed cover failed for %s: %s", audio_file, e)


def _normalize_artist_credit_mode(mode: str | None) -> str:
    """Return a safe, normalized artist credit mode."""
    m = (mode or "").strip().lower()
    if m in ("album_artist_strict", "musicbrainz_full_credit", "picard_like_default"):
        return m
    return "album_artist_strict"


def _split_main_and_featuring(artist_str: str | None, album_artist: str | None) -> tuple[str, list[str]]:
    """
    Very small heuristic to split an artist credit into main + featuring artists.
    Does NOT try to be perfect ‚Äì just enough to avoid losing obvious \"feat.\" guests.
    """
    base_album_artist = (album_artist or "").strip()
    if not artist_str:
        return (base_album_artist, [])
    s = str(artist_str).strip()
    if not s:
        return (base_album_artist or s, [])
    lower = s.lower()
    # Common separators for featuring-style credits
    seps = [" feat. ", " featuring ", " ft. ", " feat ", " vs ", " with "]
    cut_pos = -1
    sep_len = 0
    for sep in seps:
        idx = lower.find(sep)
        if idx != -1:
            cut_pos = idx
            sep_len = len(sep)
            break
    if cut_pos != -1:
        main = s[:cut_pos].strip()
        rest = s[cut_pos + sep_len :].strip()
    else:
        main = s
        rest = ""
    # If album artist appears in the main credit, prefer it as canonical main artist
    main_effective = base_album_artist or main
    if base_album_artist and base_album_artist.lower() not in main.lower():
        # Keep existing main when album artist is unknown / compilation
        main_effective = main or base_album_artist
    featuring: list[str] = []
    if rest:
        # Split on common separators between multiple guests
        for part in re.split(r"[,&/]| and ", rest):
            name = part.strip(" -")
            if name:
                featuring.append(name)
    return (main_effective, featuring)


# PMDA album-level tags (stored in audio files; keys are lowercased by extract_tags()).
PMDA_ID_TAG = "pmda_id"
PMDA_MATCHED_TAG = "pmda_matched"
PMDA_MATCH_PROVIDER_TAG = "pmda_match_provider"
PMDA_COVER_TAG = "pmda_cover"
PMDA_COVER_PROVIDER_TAG = "pmda_cover_provider"
PMDA_ARTIST_IMAGE_TAG = "pmda_artist_image"
PMDA_ARTIST_PROVIDER_TAG = "pmda_artist_provider"
PMDA_COMPLETE_TAG = "pmda_complete"


def _pmda_bool_from_str(val: str) -> bool:
    v = (val or "").strip().lower()
    return v in ("1", "true", "yes", "on")


def _set_pmda_tag(audio, key: str, value: str) -> None:
    """
    Write a PMDA_* tag to the given Mutagen audio object.
    - key: canonical tag name, e.g. 'PMDA_ID', 'PMDA_COMPLETE'.
    - value: string value to store (UUID, 'true'/'false', provider name, etc.).
    """
    if not value or not isinstance(value, str):
        return
    value = value.strip()
    if not value:
        return
    key_up = key.strip().upper()
    # FLAC / VorbisComment
    try:
        from mutagen.flac import FLAC  # type: ignore
    except Exception:  # pragma: no cover
        FLAC = None  # type: ignore[assignment]
    if FLAC is not None and isinstance(audio, FLAC):
        audio[key_up] = value
        return
    # ID3 / MP3 (TXXX frame)
    try:
        from mutagen.id3 import ID3, TXXX  # type: ignore
        from mutagen.mp3 import MP3  # type: ignore
    except Exception:  # pragma: no cover
        ID3 = TXXX = MP3 = None  # type: ignore[assignment]
    if ID3 is not None and isinstance(audio, (MP3, ID3)):
        if audio.tags is None:
            audio.add_tags()
        frame = TXXX(encoding=3, desc=key_up, text=[value])
        audio["TXXX:" + key_up] = frame
        return
    # MP4 / M4A (freeform atom)
    try:
        from mutagen.mp4 import MP4  # type: ignore
    except Exception:  # pragma: no cover
        MP4 = None  # type: ignore[assignment]
    if MP4 is not None and isinstance(audio, MP4):
        audio["----:com.pmda:" + key_up] = [value.encode("utf-8")]
        return


def _write_pmda_album_tags(
    folder: Path,
    audio_files: list[Path],
    *,
    pmda_id: Optional[str],
    match_provider: Optional[str],
    cover_provider: Optional[str],
    artist_provider: Optional[str],
    matched: bool,
    cover: bool,
    artist_image: bool,
    complete: bool,
) -> None:
    """
    Write PMDA_* album-level tags to all audio files in an album folder.
    Called at the end of improve-album once we know what was actually applied.
    """
    if not audio_files:
        return
    # Writing tags triggers filesystem events; suppress watcher-triggered rescans for a short period.
    try:
        _files_watcher_suppress_folder(folder, seconds=120.0, reason="pmda_tag_write")
    except Exception:
        pass
    try:
        from mutagen import File as MutagenFile  # type: ignore
    except Exception:
        return
    pmda_id_val = (pmda_id or "").strip() or str(uuid.uuid4())
    matched_str = "true" if matched else "false"
    cover_str = "true" if cover else "false"
    artist_str = "true" if artist_image else "false"
    complete_str = "true" if complete else "false"
    match_provider_val = (match_provider or "").strip()
    cover_provider_val = (cover_provider or "").strip()
    artist_provider_val = (artist_provider or "").strip()
    for p in audio_files:
        try:
            audio = MutagenFile(str(p))
            if audio is None:
                continue
            _set_pmda_tag(audio, "PMDA_ID", pmda_id_val)
            _set_pmda_tag(audio, "PMDA_MATCHED", matched_str)
            if match_provider_val:
                _set_pmda_tag(audio, "PMDA_MATCH_PROVIDER", match_provider_val)
            _set_pmda_tag(audio, "PMDA_COVER", cover_str)
            if cover_provider_val:
                _set_pmda_tag(audio, "PMDA_COVER_PROVIDER", cover_provider_val)
            _set_pmda_tag(audio, "PMDA_ARTIST_IMAGE", artist_str)
            if artist_provider_val:
                _set_pmda_tag(audio, "PMDA_ARTIST_PROVIDER", artist_provider_val)
            _set_pmda_tag(audio, "PMDA_COMPLETE", complete_str)
            audio.save()
        except Exception as e:
            logging.warning("PMDA tag write failed for %s: %s", p, e)


def _apply_artist_album_tags_to_audio(
    audio,
    *,
    album_artist: str,
    track_artist: str,
    album_title: str,
    year_str: str | None,
    genre_str: str | None = None,
) -> None:
    """
    Central helper to write basic ARTIST/ALBUM (and album artist when supported) on a Mutagen audio object.
    This keeps album artist consistent across all tracks so PMDA does not accidentally split albums.
    """
    mode = _normalize_artist_credit_mode(globals().get("ARTIST_CREDIT_MODE", ARTIST_CREDIT_MODE))
    # For now, track_artist is already chosen by the caller based on mode.
    aa = (album_artist or track_artist or "").strip()
    ta = (track_artist or album_artist or "").strip()
    title = (album_title or "").strip()
    year = (year_str or "").strip() if year_str else ""
    genre = (genre_str or "").strip() if genre_str else ""

    # ID3 / MP3
    try:
        from mutagen.id3 import ID3, TPE1, TPE2, TALB, TDRC, TCON, TXXX  # type: ignore
        from mutagen.mp3 import MP3  # type: ignore
    except Exception:  # pragma: no cover - optional dependency
        ID3 = MP3 = None  # type: ignore[assignment]

    if ID3 is not None and isinstance(audio, (MP3, ID3)):
        if audio.tags is None:
            audio.add_tags()
        # Track artist
        if ta:
            audio.tags.add(TPE1(encoding=3, text=ta))
        # Album artist (TPE2)
        if aa:
            audio.tags.add(TPE2(encoding=3, text=aa))
        # Album title / year / genre
        if title:
            audio.tags.add(TALB(encoding=3, text=title))
        if year:
            audio.tags.add(TDRC(encoding=3, text=year))
        if genre:
            audio.tags.add(TCON(encoding=3, text=genre))
        # Simple featuring storage in a TXXX frame when mode is not album_artist_strict
        if mode != "album_artist_strict":
            # Caller can optionally add a TXXX later using _split_main_and_featuring; we keep helper minimal for now.
            pass
        return

    # FLAC / VorbisComment
    try:
        from mutagen.flac import FLAC  # type: ignore
    except Exception:  # pragma: no cover
        FLAC = None  # type: ignore[assignment]

    if FLAC is not None and isinstance(audio, FLAC):
        if ta:
            audio["ARTIST"] = ta
        if title:
            audio["ALBUM"] = title
        if aa:
            audio["ALBUMARTIST"] = aa
        if year:
            audio["DATE"] = year
        if genre:
            audio["GENRE"] = genre
        return

    # MP4 / M4A
    try:
        from mutagen.mp4 import MP4  # type: ignore
    except Exception:  # pragma: no cover
        MP4 = None  # type: ignore[assignment]

    if MP4 is not None and isinstance(audio, MP4):
        if ta:
            audio["\xa9ART"] = [ta]
        if title:
            audio["\xa9alb"] = [title]
        if year:
            audio["\xa9day"] = [year]
        if genre:
            audio["\xa9gen"] = [genre]
        # Some players use aART for album artist; set it when available
        if aa:
            try:
                audio["aART"] = [aa]
            except Exception:
                # aART may not be supported by all taggers; ignore silently
                pass
        return

def _improve_single_album(album_id: int, db_conn, known_release_group_id: Optional[str] = None) -> dict:
    """
    Improve one album: resolve MusicBrainz ID (from known_release_group_id, tags, or search), update tags on all audio files, fetch cover if missing.
    known_release_group_id: optional MBID from last scan (scan_editions); when set, used instead of tags/search.
    Fallback: Discogs then Last.fm when MB does not provide tags or cover.
    Returns dict with:
      - steps (list of str)
      - summary (str)
      - tags_updated (bool)
      - cover_saved (bool)
      - provider_used (str|None)
      - pmda_matched / pmda_cover / pmda_artist_image / pmda_complete (bool) for PMDA stats
      - pmda_match_provider / pmda_cover_provider / pmda_artist_provider (str|None)
    """
    steps: List[str] = []
    tags_updated = False
    cover_saved = False
    provider_used: Optional[str] = None
    discogs_release_id = ""
    lastfm_album_mbid = ""
    bandcamp_album_url = ""

    album_row = db_conn.execute(
        "SELECT id, title, parent_id FROM metadata_items WHERE id = ? AND metadata_type = 9",
        (album_id,)
    ).fetchone()
    if not album_row:
        return {
            "steps": ["Album not found"],
            "summary": "Album not found.",
            "tags_updated": False,
            "cover_saved": False,
            "provider_used": None,
        }

    album_title_str = album_row[1]
    artist_id = album_row[2]
    artist_row = db_conn.execute(
        "SELECT title FROM metadata_items WHERE id = ? AND metadata_type = 8",
        (artist_id,)
    ).fetchone()
    artist_name = artist_row[0] if artist_row else "Unknown"

    folder = first_part_path(db_conn, album_id)
    if not folder:
        return {
            "steps": ["Album folder not found"],
            "summary": "Album folder not found.",
            "tags_updated": False,
            "cover_saved": False,
            "provider_used": None,
        }
    skip_mb_for_live = False
    if getattr(sys.modules[__name__], "SKIP_MB_FOR_LIVE_ALBUMS", True) and _is_likely_live_album(folder, album_title_str):
        skip_mb_for_live = True
        steps.append("Skipped MusicBrainz for live album; trying Discogs/Bandcamp for tags and cover")

    audio_files = [p for p in folder.rglob("*") if AUDIO_RE.search(p.name)]
    if not audio_files:
        return {"steps": ["No audio files in album folder"], "summary": "No audio files found.", "tags_updated": False, "cover_saved": False, "provider_used": None}

    first_audio = audio_files[0]
    current_tags = extract_tags(first_audio)
    # PMDA reprocess policy: use PMDA_* tags, not raw MusicBrainz IDs.
    # - PMDA_COMPLETE=true AND no required tags missing -> album 100% fixed: always skip in improve-all.
    # - pmda_id present but not complete and REPROCESS_INCOMPLETE_ALBUMS=false -> skip (user disabled reprocess).
    # - No PMDA tags yet or incomplete (including missing REQUIRED_TAGS) + REPROCESS_INCOMPLETE_ALBUMS=true -> process.
    pmda_id_existing = (current_tags.get(PMDA_ID_TAG) or "").strip() or None
    pmda_complete_existing = _pmda_bool_from_str(current_tags.get(PMDA_COMPLETE_TAG, ""))
    reprocess_incomplete = getattr(sys.modules[__name__], "REPROCESS_INCOMPLETE_ALBUMS", True)
    # If album was previously marked complete but now misses some REQUIRED_TAGS (e.g. new "genre" requirement),
    # treat it as incomplete so we can re-fill tags from fallbacks.
    missing_required_now: List[str] = []
    try:
        missing_required_now = _check_required_tags(current_tags, REQUIRED_TAGS, edition=None)
    except Exception:
        missing_required_now = []
    if pmda_complete_existing and not missing_required_now:
        return {
            "steps": ["Skipped (album already fully processed by PMDA)"],
            "summary": "Album already fully processed by PMDA; skipped.",
            "tags_updated": False,
            "cover_saved": False,
            "provider_used": None,
        }
    if not reprocess_incomplete and pmda_id_existing:
        return {
            "steps": ["Skipped (album previously processed by PMDA; reprocess of incomplete albums disabled)"],
            "summary": "Album previously processed by PMDA; reprocess of incomplete albums is disabled.",
            "tags_updated": False,
            "cover_saved": False,
            "provider_used": None,
        }
    release_mbid = known_release_group_id or current_tags.get("musicbrainz_releasegroupid") or current_tags.get("musicbrainz_releaseid")
    if skip_mb_for_live:
        release_mbid = None

    if not release_mbid and USE_MUSICBRAINZ and not skip_mb_for_live:
        album_norm = norm_album(album_title_str)
        tracks = set()
        try:
            for p in audio_files[:20]:
                meta = extract_tags(p)
                t = (meta.get("title") or meta.get("TIT2") or "").strip()
                if t:
                    tracks.add(t)
        except Exception:
            pass
        rg_info, _verified_by_ai = search_mb_release_group_by_metadata(artist_name, album_norm, tracks, title_raw=album_title_str)
        if rg_info and isinstance(rg_info.get("id"), str):
            release_mbid = rg_info["id"]
            steps.append("Found MusicBrainz release group via search")
    if release_mbid:
        if known_release_group_id:
            steps.append("Using MusicBrainz ID from scan")
        elif not steps:
            steps.append("Using existing MusicBrainz ID")

    try:
        from mutagen import File as MutagenFile
        HAS_MUTAGEN = True
    except ImportError:
        HAS_MUTAGEN = False

    if not HAS_MUTAGEN:
        return {
            "steps": steps + ["Mutagen not installed"],
            "summary": "Cannot update tags: mutagen library not installed.",
            "tags_updated": False,
            "cover_saved": cover_saved,
            "provider_used": None,
        }

    # At this point we know the album is eligible for processing (not already
    # fully handled by PMDA, or reprocess of incomplete albums explicitly allowed),
    # and Mutagen is available. Only now do we create a backup so that albums
    # which are already 100% fixed are not backed up again on subsequent runs.
    if BACKUP_BEFORE_FIX:
        backup_dst = _backup_album_folder_before_fix(folder, artist_name, album_title_str)
        if backup_dst:
            steps.append(f"Backed up to {backup_dst}")
        else:
            steps.append("Backup skipped: copy failed")

    mb_release_info = None
    pmda_match_provider: Optional[str] = None
    pmda_cover_provider: Optional[str] = None
    pmda_artist_provider: Optional[str] = None
    if release_mbid:
        release_group_id = release_mbid
        tag_src = "musicbrainz_releasegroupid" if current_tags.get("musicbrainz_releasegroupid") else ("musicbrainz_releaseid" if current_tags.get("musicbrainz_releaseid") else "")
        if tag_src:
            resolved = resolve_mbid_to_release_group(release_mbid, tag_src)
            if resolved:
                release_group_id = resolved
        try:
            result = musicbrainzngs.get_release_group_by_id(release_group_id, includes=["releases", "artist-credits"])
            mb_release_info = result.get("release-group", {})
            if mb_release_info:
                strict_ok, strict_reason = _strict_identity_match_details(
                    local_artist=artist_name,
                    local_title=album_title_str,
                    candidate_artist=_extract_mb_artist_names(mb_release_info),
                    candidate_title=mb_release_info.get("title") or "",
                )
                if not strict_ok:
                    logging.warning(
                        "improve-album: rejected MBID %s for %s / %s (%s)",
                        release_group_id,
                        artist_name,
                        album_title_str,
                        strict_reason,
                    )
                    steps.append(f"MusicBrainz rejected by strict identity: {strict_reason}")
                    mb_release_info = None
                    release_mbid = None
        except Exception as e:
            logging.warning("improve-album: failed to fetch release group %s: %s", release_group_id, e)
            steps.append(f"MusicBrainz lookup failed: {e}")

    artist_mbid = current_tags.get("musicbrainz_albumartistid") or current_tags.get("musicbrainz_artistid")
    if not artist_mbid and USE_MUSICBRAINZ:
        try:
            search_result = musicbrainzngs.search_artists(artist=artist_name, limit=1)
            if search_result.get("artist-list"):
                artist_mbid = search_result["artist-list"][0]["id"]
        except Exception as e:
            logging.warning("improve-album: artist search failed for '%s': %s", artist_name, e)

    # Compute which required tags are currently missing on the first audio file.
    # This lets us decide when to call Last.fm/Bandcamp even if MusicBrainz already provided basic tags.
    try:
        missing_required = _check_required_tags(current_tags, REQUIRED_TAGS, edition=None)
    except Exception:
        missing_required = []

    # Log what we retrieved and from where (for fix-all visibility)
    if mb_release_info and artist_mbid:
        date_str = mb_release_info.get("first-release-date", "")
        year_str = date_str.split("-")[0] if (date_str and "-" in date_str) else (date_str or "")
        log_tag(
            "Fix-all [album_id=%s] source=MusicBrainz tags: ARTIST=%s ALBUM=%s DATE=%s MUSICBRAINZ_ARTISTID=%s MUSICBRAINZ_RELEASEGROUPID=%s -> injecting into %d file(s) in %s",
            album_id,
            artist_name,
            mb_release_info.get("title", album_title_str),
            year_str or "(none)",
            artist_mbid,
            release_mbid or "(none)",
            len(audio_files),
            folder,
        )

    files_updated = 0
    pmda_processed_id = str(uuid.uuid4()) if (mb_release_info and artist_mbid) else None
    for audio_file in audio_files:
        try:
            audio = MutagenFile(str(audio_file))
            if audio is None:
                continue
            if mb_release_info and artist_mbid:
                if pmda_match_provider is None:
                    pmda_match_provider = "musicbrainz"
                # Derive album / year once per file
                mb_title = mb_release_info.get("title", album_title_str)
                date_str = mb_release_info.get("first-release-date", "")
                year = date_str.split("-")[0] if (date_str and "-" in date_str) else (date_str or "")
                # For now, keep track artist aligned with album artist to avoid splitting albums by track artist
                _apply_artist_album_tags_to_audio(
                    audio,
                    album_artist=artist_name,
                    track_artist=artist_name,
                    album_title=mb_title,
                    year_str=year,
                )
                # Preserve MusicBrainz identifiers on formats that support them
                try:
                    from mutagen.flac import FLAC  # type: ignore
                    from mutagen.mp4 import MP4  # type: ignore
                except Exception:
                    FLAC = MP4 = None  # type: ignore[assignment]
                if FLAC is not None and isinstance(audio, FLAC):
                    audio["MUSICBRAINZ_ARTISTID"] = artist_mbid
                    audio["MUSICBRAINZ_ALBUMARTISTID"] = artist_mbid
                    if release_mbid:
                        audio["MUSICBRAINZ_RELEASEGROUPID"] = release_mbid
                elif MP4 is not None and isinstance(audio, MP4):
                    audio["----:com.apple.iTunes:MusicBrainz Artist Id"] = [artist_mbid.encode("utf-8")]
                    audio["----:com.apple.iTunes:MusicBrainz Album Artist Id"] = [artist_mbid.encode("utf-8")]
                    if release_mbid:
                        audio["----:com.apple.iTunes:MusicBrainz Release Group Id"] = [release_mbid.encode("utf-8")]
                audio.save()
                files_updated += 1
                log_tag("Fix-all: injected MusicBrainz tags into %s", audio_file)
        except Exception as e:
            logging.error("improve-album: error updating %s: %s", audio_file, e)
            steps.append(f"Error updating {audio_file.name}: {e}")

    if files_updated > 0:
        tags_updated = True
        steps.append(f"Updated tags on {files_updated} file(s)")

    has_cover = any(
        f.name.lower().startswith(("cover", "folder", "album", "artwork", "front"))
        and f.suffix.lower() in [".jpg", ".jpeg", ".png", ".webp"]
        for f in folder.iterdir() if f.is_file()
    )
    cover_outcome: Optional[str] = "already_present" if has_cover else None
    artist_image_outcome: Optional[str] = None
    if not has_cover and release_mbid:
        try:
            cover_url = f"http://coverartarchive.org/release-group/{release_mbid}/front"
            cover_resp = requests.get(cover_url, timeout=5, allow_redirects=True)
            if cover_resp.status_code == 200:
                content = cover_resp.content
                mime = (cover_resp.headers.get("content-type") or "").split(";")[0].strip() or "image/jpeg"
                if not mime.startswith("image/"):
                    mime = "image/jpeg"
                # Use vision only when identity is ambiguous. When we have an MB release-group id,
                # Cover Art Archive is considered safe and we avoid costly + flaky vision gating.
                use_vision = bool(USE_AI_VISION_BEFORE_COVER_INJECT) and False
                if use_vision:
                    if not _vision_verify_cover_before_inject(content, mime, artist_name, album_title_str, "CAA"):
                        steps.append("Vision: cover rejected (not matching album)")
                        cover_outcome = "CAA_vision_rejected"
                        logging.info("[improve-album] Cover from CAA rejected by vision for %s / %s", artist_name, album_title_str)
                    else:
                        steps.append("Vision: cover accepted")
                        cover_outcome = "CAA_saved_vision_ok"
                        cover_path = folder / "cover.jpg"
                        with open(cover_path, "wb") as f:
                            f.write(content)
                        cover_saved = True
                        try:
                            _files_watcher_suppress_folder(folder, seconds=120.0, reason="cover_write")
                        except Exception:
                            pass
                        log_cov("Fix-all [album_id=%s] source=Cover Art Archive saved cover to %s", album_id, cover_path)
                        steps.append("Fetched and saved cover art")
                        _embed_cover_in_audio_files(cover_path, audio_files)
                        try:
                            with lock:
                                state["scan_cover_from_mb"] = state.get("scan_cover_from_mb", 0) + 1
                        except Exception:
                            pass
                else:
                    cover_outcome = "CAA_saved"
                    cover_path = folder / "cover.jpg"
                    with open(cover_path, "wb") as f:
                        f.write(content)
                    cover_saved = True
                    try:
                        _files_watcher_suppress_folder(folder, seconds=120.0, reason="cover_write")
                    except Exception:
                        pass
                    log_cov("Fix-all [album_id=%s] source=Cover Art Archive saved cover to %s", album_id, cover_path)
                    steps.append("Fetched and saved cover art")
                    _embed_cover_in_audio_files(cover_path, audio_files)
                    try:
                        with lock:
                            state["scan_cover_from_mb"] = state.get("scan_cover_from_mb", 0) + 1
                    except Exception:
                        pass
        except Exception as e:
            logging.warning("improve-album: cover fetch failed: %s", e)
            steps.append("Cover fetch failed")

    if tags_updated or cover_saved:
        provider_used = "musicbrainz"

    def _apply_fallback_tags(artist_str: str, album_str: str, year_str: str, source: str = "fallback", genre: str | None = None) -> int:
        """Write artist, album, year (and optional genre) to all audio files. Returns number of files updated."""
        log_tag(
            "Fix-all [album_id=%s] source=%s tags: ARTIST=%s ALBUM=%s YEAR=%s GENRE=%s -> injecting into %d file(s) in %s",
            album_id,
            source,
            artist_str,
            album_str,
            year_str or "(none)",
            (genre or "(none)"),
            len(audio_files),
            folder,
        )
        count = 0
        for audio_file in audio_files:
            try:
                audio = MutagenFile(str(audio_file))
                if audio is None:
                    continue
                _apply_artist_album_tags_to_audio(
                    audio,
                    album_artist=artist_str,
                    track_artist=artist_str,
                    album_title=album_str,
                    year_str=year_str,
                    genre_str=genre,
                )
                audio.save()
                count += 1
                log_tag("Fix-all: injected %s tags into %s", source, audio_file)
            except Exception as e:
                logging.error("improve-album: fallback tag error %s: %s", audio_file, e)
        return count

    # Fallback: Discogs then Last.fm/Bandcamp to fill in what MusicBrainz did not provide
    has_cover_now = has_cover or cover_saved
    if USE_DISCOGS and (not tags_updated or not has_cover_now):
        discogs_info = _fetch_discogs_release(artist_name, album_title_str)
        discogs_strict_ok = False
        if discogs_info:
            strict_ok, strict_reason = _strict_identity_match_details(
                local_artist=artist_name,
                local_title=album_title_str,
                candidate_artist=discogs_info.get("artist_name") or "",
                candidate_title=discogs_info.get("title") or "",
            )
            if not strict_ok:
                logging.info(
                    "Fix-all [album_id=%s] Discogs candidate rejected for %s / %s (%s)",
                    album_id,
                    artist_name,
                    album_title_str,
                    strict_reason,
                )
                steps.append(f"Discogs rejected by strict identity: {strict_reason}")
                discogs_info = None
            else:
                discogs_strict_ok = True
        if discogs_info:
            discogs_release_id = str(discogs_info.get("release_id") or "").strip() or discogs_release_id
            artist_str = discogs_info.get("artist_name") or artist_name
            album_str = discogs_info.get("title") or album_title_str
            year_str = (discogs_info.get("year") or "").strip()
            if not tags_updated and (album_str or artist_str):
                n = _apply_fallback_tags(artist_str, album_str, year_str, "Discogs")
                if n > 0:
                    tags_updated = True
                    files_updated = n
                    steps.append(f"Updated tags from Discogs on {n} file(s)")
            if not has_cover_now and discogs_info.get("cover_url"):
                try:
                    best_cover = _download_best_cover_image("Discogs", discogs_info.get("cover_url"))
                    if best_cover:
                        content, mime, _url_used = best_cover
                        use_vision = bool(USE_AI_VISION_BEFORE_COVER_INJECT) and (not discogs_strict_ok)
                        if use_vision:
                            if not _vision_verify_cover_before_inject(content, mime, artist_name, album_title_str, "Discogs"):
                                steps.append("Vision: cover rejected (not matching album)")
                            else:
                                steps.append("Vision: cover accepted")
                                cover_path = folder / "cover.jpg"
                                with open(cover_path, "wb") as f:
                                    f.write(content)
                                cover_saved = True
                                has_cover_now = True
                                cover_outcome = "Discogs_saved"
                                try:
                                    _files_watcher_suppress_folder(folder, seconds=120.0, reason="cover_write")
                                except Exception:
                                    pass
                                log_cov("Fix-all [album_id=%s] source=Discogs saved cover to %s", album_id, cover_path)
                                steps.append("Fetched and saved cover art from Discogs")
                                _embed_cover_in_audio_files(cover_path, audio_files)
                                try:
                                    with lock:
                                        state["scan_cover_from_discogs"] = state.get("scan_cover_from_discogs", 0) + 1
                                except Exception:
                                    pass
                        else:
                            cover_path = folder / "cover.jpg"
                            with open(cover_path, "wb") as f:
                                f.write(content)
                            cover_saved = True
                            has_cover_now = True
                            try:
                                _files_watcher_suppress_folder(folder, seconds=120.0, reason="cover_write")
                            except Exception:
                                pass
                            log_cov("Fix-all [album_id=%s] source=Discogs saved cover to %s", album_id, cover_path)
                            steps.append("Fetched and saved cover art from Discogs")
                            _embed_cover_in_audio_files(cover_path, audio_files)
                            try:
                                with lock:
                                    state["scan_cover_from_discogs"] = state.get("scan_cover_from_discogs", 0) + 1
                            except Exception:
                                pass
                except Exception as e:
                    logging.warning("improve-album: Discogs cover fetch failed: %s", e)
            if tags_updated or cover_saved:
                provider_used = "discogs"

    # Always allow Last.fm when some required tags (e.g. genre) are still missing,
    # even if MusicBrainz already provided basic tags.
    lastfm_used_for_tags = False
    if USE_LASTFM and ((not tags_updated or not has_cover_now) or ("genre" in missing_required)):
        lastfm_info = _fetch_lastfm_album_info(artist_name, album_title_str, release_mbid)
        lastfm_strict_ok = False
        if lastfm_info:
            strict_ok, strict_reason = _strict_identity_match_details(
                local_artist=artist_name,
                local_title=album_title_str,
                candidate_artist=lastfm_info.get("artist") or lastfm_info.get("artist_name") or "",
                candidate_title=lastfm_info.get("title") or lastfm_info.get("album") or "",
            )
            if not strict_ok:
                logging.info(
                    "Fix-all [album_id=%s] Last.fm candidate rejected for %s / %s (%s)",
                    album_id,
                    artist_name,
                    album_title_str,
                    strict_reason,
                )
                steps.append(f"Last.fm rejected by strict identity: {strict_reason}")
                lastfm_info = None
            else:
                lastfm_strict_ok = True
        if lastfm_info:
            lfm_mbid_val = str(lastfm_info.get("mbid") or "").strip()
            if lfm_mbid_val:
                lastfm_album_mbid = lfm_mbid_val
            artist_str = lastfm_info.get("artist") or artist_name
            album_str = lastfm_info.get("title") or album_title_str
            year_str = ""
            # Use primary Last.fm toptag as genre when available
            toptags = lastfm_info.get("toptags") or []
            primary_genre = ""
            if toptags:
                primary = toptags[0]
                primary_genre = (primary if isinstance(primary, str) else str(primary)).strip()
            tags_before_lastfm = tags_updated
            if (album_str or artist_str) and (not tags_updated or "genre" in missing_required):
                n = _apply_fallback_tags(artist_str, album_str, year_str, "Last.fm", genre=primary_genre or None)
                if n > 0:
                    tags_updated = True
                    files_updated = n
                    steps.append(f"Updated tags from Last.fm on {n} file(s)")
                    lastfm_used_for_tags = True
                    if primary_genre:
                        # Keep current_tags in sync for downstream fallback decisions (Bandcamp genre).
                        current_tags["genre"] = primary_genre
            if not has_cover_now and lastfm_info.get("cover_url"):
                try:
                    best_cover = _download_best_cover_image("Last.fm", lastfm_info.get("cover_url"))
                    if best_cover:
                        content, mime, _url_used = best_cover
                        # Use vision only when identity is ambiguous; strict provider identity is considered safe.
                        use_vision = bool(USE_AI_VISION_BEFORE_COVER_INJECT) and (not lastfm_strict_ok)
                        if use_vision:
                            if not _vision_verify_cover_before_inject(content, mime, artist_name, album_title_str, "Last.fm"):
                                steps.append("Vision: cover rejected (not matching album)")
                            else:
                                steps.append("Vision: cover accepted")
                                cover_path = folder / "cover.jpg"
                                with open(cover_path, "wb") as f:
                                    f.write(content)
                                cover_saved = True
                                has_cover_now = True
                                try:
                                    _files_watcher_suppress_folder(folder, seconds=120.0, reason="cover_write")
                                except Exception:
                                    pass
                                log_cov("Fix-all [album_id=%s] source=Last.fm saved cover to %s", album_id, cover_path)
                                steps.append("Fetched and saved cover art from Last.fm")
                                _embed_cover_in_audio_files(cover_path, audio_files)
                                try:
                                    with lock:
                                        state["scan_cover_from_lastfm"] = state.get("scan_cover_from_lastfm", 0) + 1
                                except Exception:
                                    pass
                        else:
                            cover_path = folder / "cover.jpg"
                            with open(cover_path, "wb") as f:
                                f.write(content)
                            cover_saved = True
                            has_cover_now = True
                            try:
                                _files_watcher_suppress_folder(folder, seconds=120.0, reason="cover_write")
                            except Exception:
                                pass
                            log_cov("Fix-all [album_id=%s] source=Last.fm saved cover to %s", album_id, cover_path)
                            steps.append("Fetched and saved cover art from Last.fm")
                            _embed_cover_in_audio_files(cover_path, audio_files)
                            try:
                                with lock:
                                    state["scan_cover_from_lastfm"] = state.get("scan_cover_from_lastfm", 0) + 1
                            except Exception:
                                pass
                except Exception as e:
                    logging.warning("improve-album: Last.fm cover fetch failed: %s", e)
            # Only mark Last.fm as match provider if it actually supplied tags
            if tags_updated and not tags_before_lastfm:
                provider_used = "lastfm"

    # Bandcamp: ultimate fallback when still missing tags or cover.
    # Also used to fill in missing genre when Bandcamp exposes useful tags.
    genre_missing_now = not str((current_tags or {}).get("genre") or "").strip()
    # Special case: if Last.fm was the identity provider, prefer Bandcamp tags for richer genres
    # when available (Bandcamp often exposes multiple useful genre tags).
    mb_identity_used = bool(mb_release_info and artist_mbid)
    want_bandcamp_genre = bool(lastfm_used_for_tags and not mb_identity_used and ("genre" in missing_required))
    if USE_BANDCAMP and ((not tags_updated or not has_cover_now) or genre_missing_now or want_bandcamp_genre):
        bandcamp_info = _fetch_bandcamp_album_info(artist_name, album_title_str)
        bandcamp_strict_ok = False
        if bandcamp_info:
            strict_ok, strict_reason = _strict_identity_match_details(
                local_artist=artist_name,
                local_title=album_title_str,
                candidate_artist=bandcamp_info.get("artist_name") or "",
                candidate_title=bandcamp_info.get("title") or "",
            )
            if not strict_ok:
                logging.info(
                    "Fix-all [album_id=%s] Bandcamp candidate rejected for %s / %s (%s)",
                    album_id,
                    artist_name,
                    album_title_str,
                    strict_reason,
                )
                steps.append(f"Bandcamp rejected by strict identity: {strict_reason}")
                bandcamp_info = None
            else:
                bandcamp_strict_ok = True
        if bandcamp_info:
            bandcamp_album_url = str(bandcamp_info.get("album_url") or "").strip() or bandcamp_album_url
            artist_str = bandcamp_info.get("artist_name") or artist_name
            album_str = bandcamp_info.get("title") or album_title_str
            year_raw = (bandcamp_info.get("year") or "").strip()
            # Extract 4-digit year from Bandcamp's "released ..." string when possible
            year_match = re.search(r"\b(\d{4})\b", year_raw) if year_raw else None
            year_str = year_match.group(1) if year_match else year_raw
            # Infer a genre string from Bandcamp tags when available
            bandcamp_tags = bandcamp_info.get("tags") or []
            inferred_genre = _infer_genre_from_bandcamp_tags(bandcamp_tags) if bandcamp_tags else None
            tags_before_bandcamp = tags_updated
            if (album_str or artist_str) and (not tags_updated or "genre" in missing_required or want_bandcamp_genre):
                n = _apply_fallback_tags(artist_str, album_str, year_str, "Bandcamp", genre=inferred_genre)
                if n > 0:
                    tags_updated = True
                    files_updated = n
                    steps.append(f"Updated tags from Bandcamp on {n} file(s)")
                    if inferred_genre:
                        current_tags["genre"] = inferred_genre
            if not has_cover_now and bandcamp_info.get("cover_url"):
                try:
                    best_cover = _download_best_cover_image(
                        "Bandcamp",
                        bandcamp_info.get("cover_url"),
                        cover_candidates=bandcamp_info.get("cover_candidates") or [],
                        headers={"User-Agent": "PMDA/1.0 (metadata fallback)"},
                    )
                    if best_cover:
                        content, mime, _url_used = best_cover
                        # Use vision only when identity is ambiguous; strict provider identity is considered safe.
                        use_vision = bool(USE_AI_VISION_BEFORE_COVER_INJECT) and (not bandcamp_strict_ok)
                        if use_vision:
                            if not _vision_verify_cover_before_inject(content, mime, artist_name, album_title_str, "Bandcamp"):
                                steps.append("Vision: cover rejected (not matching album)")
                            else:
                                steps.append("Vision: cover accepted")
                                cover_path = folder / "cover.jpg"
                                with open(cover_path, "wb") as f:
                                    f.write(content)
                                cover_saved = True
                                has_cover_now = True
                                try:
                                    _files_watcher_suppress_folder(folder, seconds=120.0, reason="cover_write")
                                except Exception:
                                    pass
                                log_cov("Fix-all [album_id=%s] source=Bandcamp saved cover to %s", album_id, cover_path)
                                steps.append("Fetched and saved cover art from Bandcamp")
                                _embed_cover_in_audio_files(cover_path, audio_files)
                                try:
                                    with lock:
                                        state["scan_cover_from_bandcamp"] = state.get("scan_cover_from_bandcamp", 0) + 1
                                except Exception:
                                    pass
                        else:
                            cover_path = folder / "cover.jpg"
                            with open(cover_path, "wb") as f:
                                f.write(content)
                            cover_saved = True
                            has_cover_now = True
                            try:
                                _files_watcher_suppress_folder(folder, seconds=120.0, reason="cover_write")
                            except Exception:
                                pass
                            log_cov("Fix-all [album_id=%s] source=Bandcamp saved cover to %s", album_id, cover_path)
                            steps.append("Fetched and saved cover art from Bandcamp")
                            _embed_cover_in_audio_files(cover_path, audio_files)
                            try:
                                with lock:
                                    state["scan_cover_from_bandcamp"] = state.get("scan_cover_from_bandcamp", 0) + 1
                            except Exception:
                                pass
                except Exception as e:
                    logging.warning("improve-album: Bandcamp cover fetch failed: %s", e)
            # Only mark Bandcamp as match provider if it actually supplied tags
            if tags_updated and not tags_before_bandcamp:
                provider_used = "bandcamp"

    # Web+AI+vision fallback for cover when all providers failed but cover likely exists on the web.
    # We trigger this whenever there is no saved/embedded cover and a Serper API key is configured.
    if (
        not has_cover_now
        and not cover_saved
        and (getattr(sys.modules[__name__], "SERPER_API_KEY", "") or "").strip()
    ):
        try:
            web_result = _fetch_cover_from_web(artist_name, album_title_str)
        except Exception as e:
            logging.warning("improve-album: web cover search failed: %s", e)
            web_result = None
        if web_result is not None:
            content, mime = web_result
            mime = (mime or "").split(";")[0].strip() or "image/jpeg"
            if not mime.startswith("image/"):
                mime = "image/jpeg"
            accepted = True
            if USE_AI_VISION_BEFORE_COVER_INJECT:
                # Web covers are inherently risky: if vision fails, fail-closed and do not save.
                accepted = _vision_verify_cover_before_inject(
                    content,
                    mime,
                    artist_name,
                    album_title_str,
                    "Web",
                    fail_open=False,
                )
                if not accepted:
                    steps.append("Vision: web cover rejected (not matching album)")
                    logging.info(
                        "[improve-album] Cover from web rejected by vision for %s / %s",
                        artist_name,
                        album_title_str,
                    )
            if accepted:
                steps.append("Vision: cover accepted" if USE_AI_VISION_BEFORE_COVER_INJECT else "Vision: not used")
                cover_path = folder / "cover.jpg"
                with open(cover_path, "wb") as f:
                    f.write(content)
                cover_saved = True
                has_cover_now = True
                cover_outcome = "Web_saved"
                pmda_cover_provider = "web_ai"
                log_cov("Fix-all [album_id=%s] source=Web search saved cover to %s", album_id, cover_path)
                steps.append("Fetched and saved cover art from web search")
                _embed_cover_in_audio_files(cover_path, audio_files)
                try:
                    with lock:
                        state["scan_cover_from_web"] = state.get("scan_cover_from_web", 0) + 1
                except Exception:
                    pass

    # Fetch and save artist image to artist folder if missing (MB, Last.fm, Discogs)
    if tags_updated or cover_saved:
        artist_folder = folder.parent
        had_artist_image = (artist_folder / "artist.jpg").exists()
        artist_provider = _fetch_and_save_artist_image(artist_name, artist_folder, artist_mbid)
        if artist_provider:
            pmda_artist_provider = artist_provider
            steps.append("Fetched and saved artist image")
            artist_image_outcome = "already_present" if had_artist_image else "fetched"
        else:
            artist_image_outcome = "already_present" if had_artist_image else "skipped"
    if artist_image_outcome is None:
        artist_image_outcome = "skipped"
    if cover_saved and cover_outcome is None:
        cover_outcome = "saved"
    cover_outcome = cover_outcome or ("already_present" if has_cover else "not_found")
    tags_outcome = f"{provider_used or 'none'} ({files_updated} files)" if tags_updated else "none"
    log_tag(
        "Fix-all [album_id=%s] summary: tags=%s, cover=%s, artist_image=%s",
        album_id, tags_outcome, cover_outcome, artist_image_outcome,
    )

    # Derive PMDA album-level flags for tagging and reprocess logic
    pmda_matched = bool(tags_updated)
    pmda_cover = bool(cover_saved or has_cover)
    pmda_artist_image = artist_image_outcome in ("fetched", "already_present")
    pmda_complete = pmda_matched and pmda_cover and pmda_artist_image
    # Infer cover provider from outcome if not already set
    if pmda_cover_provider is None:
        if cover_outcome and "Discogs" in cover_outcome:
            pmda_cover_provider = "discogs"
        elif cover_outcome and "Last.fm" in cover_outcome:
            pmda_cover_provider = "lastfm"
        elif cover_outcome and "Bandcamp" in cover_outcome:
            pmda_cover_provider = "bandcamp"
        elif cover_outcome and cover_outcome.startswith("CAA"):
            pmda_cover_provider = "musicbrainz"
        elif pmda_cover:
            pmda_cover_provider = "local"
    # Determine match provider if still unknown
    if pmda_matched and not pmda_match_provider:
        pmda_match_provider = provider_used or ("musicbrainz" if release_mbid else None)
    # Determine PMDA_ID (reuse existing if present on first file)
    pmda_id = None
    existing_pmda_id = (current_tags.get(PMDA_ID_TAG) or "").strip()
    if existing_pmda_id:
        pmda_id = existing_pmda_id

    if pmda_matched or pmda_cover or pmda_artist_image or pmda_complete:
        _write_pmda_album_tags(
            folder,
            audio_files,
            pmda_id=pmda_id,
            match_provider=pmda_match_provider,
            cover_provider=pmda_cover_provider,
            artist_provider=pmda_artist_provider,
            matched=pmda_matched,
            cover=pmda_cover,
            artist_image=pmda_artist_image,
            complete=pmda_complete,
        )

    summary = f"Updated tags on {files_updated} file(s)." + (" Fetched cover art." if cover_saved else "")
    return {
        "steps": steps,
        "summary": summary,
        "tags_updated": tags_updated,
        "cover_saved": cover_saved,
        "provider_used": provider_used,
        "pmda_matched": pmda_matched,
        "pmda_cover": pmda_cover,
        "pmda_artist_image": pmda_artist_image,
        "pmda_complete": pmda_complete,
        "pmda_match_provider": pmda_match_provider,
        "pmda_cover_provider": pmda_cover_provider,
        "pmda_artist_provider": pmda_artist_provider,
        "discogs_release_id": discogs_release_id,
        "lastfm_album_mbid": lastfm_album_mbid,
        "bandcamp_album_url": bandcamp_album_url,
    }


def _track_index_from_file(audio_path: Path, tags: Optional[dict] = None) -> Optional[int]:
    """Derive track position from tags (TRCK) or filename (e.g. 01, 02). Returns 1-based index or None."""
    if tags is None:
        tags = extract_tags(audio_path)
    trck = (tags.get("trck") or tags.get("track") or "").strip()
    if trck:
        part = trck.split("/")[0].strip()
        try:
            return int(part)
        except ValueError:
            pass
    name = audio_path.stem
    match = re.search(r"\b(0*)(\d{1,3})\b", name)
    if match:
        try:
            return int(match.group(2))
        except ValueError:
            pass
    return None


def _infer_artist_album_from_folder(folder_path: Path, audio_files: List[Path]) -> Tuple[str, str]:
    """Infer artist and album from first file tags, then folder name, then first filename."""
    artist_name = "Unknown"
    album_title_str = folder_path.name or "Unknown Album"
    if not audio_files:
        return (artist_name, album_title_str)
    first_tags = extract_tags(audio_files[0])
    artist_name = (
        (first_tags.get("artist") or first_tags.get("albumartist") or "").strip()
        or artist_name
    )
    album_title_str = (first_tags.get("album") or "").strip() or album_title_str
    # Folder-based fallback is more reliable than filename parsing for album identity.
    # For untagged albums, parent folder is almost always the artist.
    if (artist_name or "").strip().lower() in {"unknown", "unknown artist", "various", "various artists"}:
        try:
            parent_name = (folder_path.parent.name or "").replace("_", " ").strip()
        except Exception:
            parent_name = ""
        if parent_name:
            artist_name = parent_name
    if artist_name == "Unknown" or album_title_str == folder_path.name:
        parts = audio_files[0].stem.split(" - ")
        if len(parts) >= 2:
            artist_name = artist_name if artist_name != "Unknown" else parts[0].strip()
            album_title_str = album_title_str if album_title_str != folder_path.name else parts[1].strip()
    album_title_str = _sanitize_album_title_display(album_title_str)
    return (artist_name, album_title_str)


def _improve_folder_by_path(folder_path: Path) -> dict:
    """
    Improve one album by folder path (no Plex). Infers artist/album from tags or filename,
    runs MusicBrainz search + tag/cover pipeline, detects dupes within folder.
    Returns same shape as _improve_single_album plus dupes_in_folder and files_updated.
    """
    steps: List[str] = []
    tags_updated = False
    cover_saved = False
    provider_used: Optional[str] = None
    files_updated = 0
    dupes_in_folder: List[dict] = []
    discogs_release_id = ""
    lastfm_album_mbid = ""
    bandcamp_album_url = ""

    if not folder_path.is_dir():
        return {"steps": ["Not a directory"], "summary": "Invalid path.", "tags_updated": False, "cover_saved": False, "provider_used": None, "dupes_in_folder": [], "files_updated": 0}

    audio_files = sorted([p for p in folder_path.rglob("*") if AUDIO_RE.search(p.name)])
    if not audio_files:
        return {"steps": ["No audio files"], "summary": "No audio files found.", "tags_updated": False, "cover_saved": False, "provider_used": None, "dupes_in_folder": [], "files_updated": 0}

    artist_name, album_title_str = _infer_artist_album_from_folder(folder_path, audio_files)

    # Dupe detection: group by track index
    by_track: Dict[int, List[Path]] = defaultdict(list)
    for p in audio_files:
        idx = _track_index_from_file(p)
        if idx is not None:
            by_track[idx].append(p)
    for idx, paths in by_track.items():
        if len(paths) > 1:
            dupes_in_folder.append({"track": idx, "paths": [str(p) for p in paths]})
    if dupes_in_folder:
        steps.append(f"Duplicate track positions detected: {len(dupes_in_folder)} group(s)")

    if BACKUP_BEFORE_FIX:
        backup_dst = _backup_album_folder_before_fix(folder_path, artist_name, album_title_str)
        if backup_dst:
            steps.append(f"Backed up to {backup_dst}")
        else:
            steps.append("Backup skipped: copy failed")

    first_audio = audio_files[0]
    current_tags = extract_tags(first_audio)
    release_mbid = current_tags.get("musicbrainz_releasegroupid") or current_tags.get("musicbrainz_releaseid")
    # Compute which required tags are currently missing on the first audio file.
    # This lets us decide when to call Last.fm/Bandcamp even if MusicBrainz already provided basic tags.
    try:
        missing_required = _check_required_tags(current_tags, REQUIRED_TAGS, edition=None)
    except Exception:
        missing_required = []

    if not release_mbid and USE_MUSICBRAINZ:
        album_norm = norm_album(album_title_str)
        tracks = set()
        try:
            for p in audio_files[:20]:
                meta = extract_tags(p)
                t = (meta.get("title") or meta.get("tit2") or "").strip()
                if t:
                    tracks.add(t)
        except Exception:
            pass
        rg_info, _ = search_mb_release_group_by_metadata(
            artist_name, album_norm, tracks, title_raw=album_title_str, album_folder=folder_path
        )
        if rg_info and isinstance(rg_info.get("id"), str):
            release_mbid = rg_info["id"]
            steps.append("Found MusicBrainz release group via search")
    if release_mbid and not steps:
        steps.append("Using existing MusicBrainz ID")

    try:
        from mutagen import File as MutagenFile
        HAS_MUTAGEN = True
    except ImportError:
        HAS_MUTAGEN = False

    if not HAS_MUTAGEN:
        return {"steps": steps + ["Mutagen not installed"], "summary": "Cannot update tags: mutagen not installed.", "tags_updated": False, "cover_saved": False, "provider_used": None, "dupes_in_folder": dupes_in_folder, "files_updated": 0}

    mb_release_info = None
    pmda_match_provider: Optional[str] = None
    pmda_cover_provider: Optional[str] = None
    pmda_artist_provider: Optional[str] = None
    if release_mbid:
        release_group_id = release_mbid
        tag_src = "musicbrainz_releasegroupid" if current_tags.get("musicbrainz_releasegroupid") else ("musicbrainz_releaseid" if current_tags.get("musicbrainz_releaseid") else "")
        if tag_src:
            resolved = resolve_mbid_to_release_group(release_mbid, tag_src)
            if resolved:
                release_group_id = resolved
        try:
            result = musicbrainzngs.get_release_group_by_id(release_group_id, includes=["releases", "artist-credits"])
            mb_release_info = result.get("release-group", {})
            if mb_release_info:
                strict_ok, strict_reason = _strict_identity_match_details(
                    local_artist=artist_name,
                    local_title=album_title_str,
                    candidate_artist=_extract_mb_artist_names(mb_release_info),
                    candidate_title=mb_release_info.get("title") or "",
                )
                if not strict_ok:
                    logging.warning(
                        "improve-folder: rejected MBID %s for %s / %s (%s)",
                        release_group_id,
                        artist_name,
                        album_title_str,
                        strict_reason,
                    )
                    steps.append(f"MusicBrainz rejected by strict identity: {strict_reason}")
                    mb_release_info = None
                    release_mbid = None
        except Exception as e:
            logging.warning("improve-folder: failed to fetch release group %s: %s", release_group_id, e)
            steps.append(f"MusicBrainz lookup failed: {e}")

    artist_mbid = current_tags.get("musicbrainz_albumartistid") or current_tags.get("musicbrainz_artistid")
    if not artist_mbid and USE_MUSICBRAINZ:
        try:
            search_result = musicbrainzngs.search_artists(artist=artist_name, limit=1)
            if search_result.get("artist-list"):
                artist_mbid = search_result["artist-list"][0]["id"]
        except Exception as e:
            logging.warning("improve-folder: artist search failed for '%s': %s", artist_name, e)

    def _apply_fallback_tags_folder(artist_str: str, album_str: str, year_str: str, source: str, genre: str | None = None) -> int:
        count = 0
        for audio_file in audio_files:
            try:
                audio = MutagenFile(str(audio_file))
                if audio is None:
                    continue
                _apply_artist_album_tags_to_audio(
                    audio,
                    album_artist=artist_str,
                    track_artist=artist_str,
                    album_title=album_str,
                    year_str=year_str,
                    genre_str=genre,
                )
                audio.save()
                count += 1
            except Exception as e:
                logging.error("improve-folder: fallback tag error %s: %s", audio_file, e)
        return count

    for audio_file in audio_files:
        try:
            audio = MutagenFile(str(audio_file))
            if audio is None:
                continue
            if mb_release_info and artist_mbid:
                if pmda_match_provider is None:
                    pmda_match_provider = "musicbrainz"
                mb_title = mb_release_info.get("title", album_title_str)
                date_str = mb_release_info.get("first-release-date", "")
                year = date_str.split("-")[0] if (date_str and "-" in date_str) else (date_str or "")
                _apply_artist_album_tags_to_audio(
                    audio,
                    album_artist=artist_name,
                    track_artist=artist_name,
                    album_title=mb_title,
                    year_str=year,
                )
                try:
                    from mutagen.flac import FLAC  # type: ignore
                    from mutagen.mp4 import MP4  # type: ignore
                except Exception:
                    FLAC = MP4 = None  # type: ignore[assignment]
                if FLAC is not None and isinstance(audio, FLAC):
                    audio["MUSICBRAINZ_ARTISTID"] = artist_mbid
                    audio["MUSICBRAINZ_ALBUMARTISTID"] = artist_mbid
                    if release_mbid:
                        audio["MUSICBRAINZ_RELEASEGROUPID"] = release_mbid
                elif MP4 is not None and isinstance(audio, MP4):
                    audio["----:com.apple.iTunes:MusicBrainz Artist Id"] = [artist_mbid.encode("utf-8")]
                    audio["----:com.apple.iTunes:MusicBrainz Album Artist Id"] = [artist_mbid.encode("utf-8")]
                    if release_mbid:
                        audio["----:com.apple.iTunes:MusicBrainz Release Group Id"] = [release_mbid.encode("utf-8")]
                audio.save()
                files_updated += 1
        except Exception as e:
            logging.error("improve-folder: error updating %s: %s", audio_file, e)

    if files_updated > 0:
        tags_updated = True
        steps.append(f"Updated tags on {files_updated} file(s)")

    has_cover = any(
        f.name.lower().startswith(("cover", "folder", "album", "artwork", "front"))
        and f.suffix.lower() in [".jpg", ".jpeg", ".png", ".webp"]
        for f in folder_path.iterdir() if f.is_file()
    )
    if not has_cover and release_mbid:
        try:
            cover_url = f"http://coverartarchive.org/release-group/{release_mbid}/front"
            cover_resp = requests.get(cover_url, timeout=5, allow_redirects=True)
            if cover_resp.status_code == 200:
                content = cover_resp.content
                mime = (cover_resp.headers.get("content-type") or "").split(";")[0].strip() or "image/jpeg"
                if not mime.startswith("image/"):
                    mime = "image/jpeg"
                # With a validated MB release-group id, CAA is considered trusted.
                # Do not gate it behind vision (costly + can reject correct covers).
                use_vision = bool(USE_AI_VISION_BEFORE_COVER_INJECT) and False
                if use_vision:
                    if not _vision_verify_cover_before_inject(content, mime, artist_name, album_title_str, "CAA"):
                        steps.append("Vision: cover rejected (not matching album)")
                    else:
                        steps.append("Vision: cover accepted")
                        cover_path = folder_path / "cover.jpg"
                        with open(cover_path, "wb") as f:
                            f.write(content)
                        cover_saved = True
                        pmda_cover_provider = "musicbrainz"
                        steps.append("Fetched and saved cover art")
                        _embed_cover_in_audio_files(cover_path, audio_files)
                else:
                    cover_path = folder_path / "cover.jpg"
                    with open(cover_path, "wb") as f:
                        f.write(content)
                    cover_saved = True
                    pmda_cover_provider = "musicbrainz"
                    steps.append("Fetched and saved cover art")
                    _embed_cover_in_audio_files(cover_path, audio_files)
        except Exception as e:
            logging.warning("improve-folder: cover fetch failed: %s", e)
    if tags_updated or cover_saved:
        provider_used = "musicbrainz"

    has_cover_now = has_cover or cover_saved
    if USE_DISCOGS and (not tags_updated or not has_cover_now):
        discogs_info = _fetch_discogs_release(artist_name, album_title_str)
        discogs_strict_ok = False
        if discogs_info:
            strict_ok, strict_reason = _strict_identity_match_details(
                local_artist=artist_name,
                local_title=album_title_str,
                candidate_artist=discogs_info.get("artist_name") or "",
                candidate_title=discogs_info.get("title") or "",
            )
            if not strict_ok:
                logging.info(
                    "improve-folder: Discogs candidate rejected for %s / %s (%s)",
                    artist_name,
                    album_title_str,
                    strict_reason,
                )
                steps.append(f"Discogs rejected by strict identity: {strict_reason}")
                discogs_info = None
            else:
                discogs_strict_ok = True
        if discogs_info:
            discogs_release_id = str(discogs_info.get("release_id") or "").strip() or discogs_release_id
            artist_str = discogs_info.get("artist_name") or artist_name
            album_str = discogs_info.get("title") or album_title_str
            year_str = (discogs_info.get("year") or "").strip()
            if not tags_updated and (album_str or artist_str):
                n = _apply_fallback_tags_folder(artist_str, album_str, year_str, "Discogs")
                if n > 0:
                    tags_updated = True
                    files_updated = n
                    steps.append(f"Updated tags from Discogs on {n} file(s)")
            if not has_cover_now and discogs_info.get("cover_url"):
                try:
                    best_cover = _download_best_cover_image("Discogs", discogs_info.get("cover_url"))
                    if best_cover:
                        content, mime, _url_used = best_cover
                        # Use vision only when identity is ambiguous; strict provider identity is considered safe.
                        use_vision = bool(USE_AI_VISION_BEFORE_COVER_INJECT) and (not discogs_strict_ok)
                        if use_vision:
                            if not _vision_verify_cover_before_inject(content, mime, artist_name, album_title_str, "Discogs"):
                                steps.append("Vision: cover rejected (not matching album)")
                            else:
                                steps.append("Vision: cover accepted")
                                cover_path = folder_path / "cover.jpg"
                                with open(cover_path, "wb") as f:
                                    f.write(content)
                                cover_saved = True
                                has_cover_now = True
                                pmda_cover_provider = "discogs"
                                try:
                                    _files_watcher_suppress_folder(folder_path, seconds=120.0, reason="cover_write")
                                except Exception:
                                    pass
                                _embed_cover_in_audio_files(cover_path, audio_files)
                                steps.append("Fetched and saved cover art from Discogs")
                        else:
                            cover_path = folder_path / "cover.jpg"
                            with open(cover_path, "wb") as f:
                                f.write(content)
                            cover_saved = True
                            has_cover_now = True
                            pmda_cover_provider = "discogs"
                            try:
                                _files_watcher_suppress_folder(folder_path, seconds=120.0, reason="cover_write")
                            except Exception:
                                pass
                            _embed_cover_in_audio_files(cover_path, audio_files)
                            steps.append("Fetched and saved cover art from Discogs")
                except Exception as e:
                    logging.warning("improve-folder: Discogs cover fetch failed: %s", e)
            if tags_updated or cover_saved:
                provider_used = "discogs"

    # Always allow Last.fm when some required tags (e.g. genre) are still missing,
    # even if MusicBrainz already provided basic tags.
    lastfm_used_for_tags = False
    if USE_LASTFM and ((not tags_updated or not has_cover_now) or ("genre" in missing_required)):
        lastfm_info = _fetch_lastfm_album_info(artist_name, album_title_str, release_mbid)
        lastfm_strict_ok = False
        if lastfm_info:
            strict_ok, strict_reason = _strict_identity_match_details(
                local_artist=artist_name,
                local_title=album_title_str,
                candidate_artist=lastfm_info.get("artist") or lastfm_info.get("artist_name") or "",
                candidate_title=lastfm_info.get("title") or lastfm_info.get("album") or "",
            )
            if not strict_ok:
                logging.info(
                    "improve-folder: Last.fm candidate rejected for %s / %s (%s)",
                    artist_name,
                    album_title_str,
                    strict_reason,
                )
                steps.append(f"Last.fm rejected by strict identity: {strict_reason}")
                lastfm_info = None
            else:
                lastfm_strict_ok = True
        if lastfm_info:
            lfm_mbid_val = str(lastfm_info.get("mbid") or "").strip()
            if lfm_mbid_val:
                lastfm_album_mbid = lfm_mbid_val
            artist_str = lastfm_info.get("artist") or artist_name
            album_str = lastfm_info.get("title") or album_title_str
            year_str = ""
            # Use primary Last.fm toptag as genre when available
            toptags = lastfm_info.get("toptags") or []
            primary_genre = ""
            if toptags:
                primary = toptags[0]
                primary_genre = (primary if isinstance(primary, str) else str(primary)).strip()
            if (album_str or artist_str) and (not tags_updated or "genre" in missing_required):
                n = _apply_fallback_tags_folder(artist_str, album_str, year_str, "Last.fm", genre=primary_genre or None)
                if n > 0:
                    tags_updated = True
                    files_updated = n
                    steps.append(f"Updated tags from Last.fm on {n} file(s)")
                    lastfm_used_for_tags = True
                    if primary_genre:
                        current_tags["genre"] = primary_genre
            if not has_cover_now and lastfm_info.get("cover_url"):
                try:
                    best_cover = _download_best_cover_image("Last.fm", lastfm_info.get("cover_url"))
                    if best_cover:
                        content, mime, _url_used = best_cover
                        # Use vision only when identity is ambiguous; strict provider identity is considered safe.
                        use_vision = bool(USE_AI_VISION_BEFORE_COVER_INJECT) and (not lastfm_strict_ok)
                        if use_vision:
                            if not _vision_verify_cover_before_inject(content, mime, artist_name, album_title_str, "Last.fm"):
                                steps.append("Vision: cover rejected (not matching album)")
                            else:
                                steps.append("Vision: cover accepted")
                                cover_path = folder_path / "cover.jpg"
                                with open(cover_path, "wb") as f:
                                    f.write(content)
                                cover_saved = True
                                has_cover_now = True
                                pmda_cover_provider = "lastfm"
                                try:
                                    _files_watcher_suppress_folder(folder_path, seconds=120.0, reason="cover_write")
                                except Exception:
                                    pass
                                _embed_cover_in_audio_files(cover_path, audio_files)
                                steps.append("Fetched and saved cover art from Last.fm")
                        else:
                            cover_path = folder_path / "cover.jpg"
                            with open(cover_path, "wb") as f:
                                f.write(content)
                            cover_saved = True
                            has_cover_now = True
                            pmda_cover_provider = "lastfm"
                            try:
                                _files_watcher_suppress_folder(folder_path, seconds=120.0, reason="cover_write")
                            except Exception:
                                pass
                            _embed_cover_in_audio_files(cover_path, audio_files)
                            steps.append("Fetched and saved cover art from Last.fm")
                except Exception as e:
                    logging.warning("improve-folder: Last.fm cover fetch failed: %s", e)
            if tags_updated or cover_saved:
                provider_used = "lastfm"

    genre_missing_now = not str((current_tags or {}).get("genre") or "").strip()
    # Special case: if Last.fm was the identity provider, prefer Bandcamp tags for richer genres
    # when available (Bandcamp often exposes multiple useful genre tags).
    mb_identity_used = bool(mb_release_info and artist_mbid)
    want_bandcamp_genre = bool(lastfm_used_for_tags and not mb_identity_used and ("genre" in missing_required))
    if USE_BANDCAMP and ((not tags_updated or not has_cover_now) or genre_missing_now or want_bandcamp_genre):
        bandcamp_info = _fetch_bandcamp_album_info(artist_name, album_title_str)
        bandcamp_strict_ok = False
        if bandcamp_info:
            strict_ok, strict_reason = _strict_identity_match_details(
                local_artist=artist_name,
                local_title=album_title_str,
                candidate_artist=bandcamp_info.get("artist_name") or "",
                candidate_title=bandcamp_info.get("title") or "",
            )
            if not strict_ok:
                logging.info(
                    "improve-folder: Bandcamp candidate rejected for %s / %s (%s)",
                    artist_name,
                    album_title_str,
                    strict_reason,
                )
                steps.append(f"Bandcamp rejected by strict identity: {strict_reason}")
                bandcamp_info = None
            else:
                bandcamp_strict_ok = True
        if bandcamp_info:
            bandcamp_album_url = str(bandcamp_info.get("album_url") or "").strip() or bandcamp_album_url
            artist_str = bandcamp_info.get("artist_name") or artist_name
            album_str = bandcamp_info.get("title") or album_title_str
            year_raw = (bandcamp_info.get("year") or "").strip()
            year_match = re.search(r"\b(\d{4})\b", year_raw) if year_raw else None
            year_str = year_match.group(1) if year_match else year_raw
            bandcamp_tags = bandcamp_info.get("tags") or []
            inferred_genre = _infer_genre_from_bandcamp_tags(bandcamp_tags) if bandcamp_tags else None
            if (album_str or artist_str) and (not tags_updated or "genre" in missing_required or want_bandcamp_genre):
                n = _apply_fallback_tags_folder(artist_str, album_str, year_str, "Bandcamp", genre=inferred_genre)
                if n > 0:
                    tags_updated = True
                    files_updated = n
                    steps.append(f"Updated tags from Bandcamp on {n} file(s)")
                    if inferred_genre:
                        current_tags["genre"] = inferred_genre
            if not has_cover_now and bandcamp_info.get("cover_url"):
                try:
                    best_cover = _download_best_cover_image(
                        "Bandcamp",
                        bandcamp_info.get("cover_url"),
                        cover_candidates=bandcamp_info.get("cover_candidates") or [],
                        headers={"User-Agent": "PMDA/1.0 (metadata fallback)"},
                    )
                    if best_cover:
                        content, mime, _url_used = best_cover
                        # Use vision only when identity is ambiguous; strict provider identity is considered safe.
                        use_vision = bool(USE_AI_VISION_BEFORE_COVER_INJECT) and (not bandcamp_strict_ok)
                        if use_vision:
                            if not _vision_verify_cover_before_inject(content, mime, artist_name, album_title_str, "Bandcamp"):
                                steps.append("Vision: cover rejected (not matching album)")
                            else:
                                steps.append("Vision: cover accepted")
                                cover_path = folder_path / "cover.jpg"
                                with open(cover_path, "wb") as f:
                                    f.write(content)
                                cover_saved = True
                                has_cover_now = True
                                pmda_cover_provider = "bandcamp"
                                try:
                                    _files_watcher_suppress_folder(folder_path, seconds=120.0, reason="cover_write")
                                except Exception:
                                    pass
                                _embed_cover_in_audio_files(cover_path, audio_files)
                                steps.append("Fetched and saved cover art from Bandcamp")
                        else:
                            cover_path = folder_path / "cover.jpg"
                            with open(cover_path, "wb") as f:
                                f.write(content)
                            cover_saved = True
                            has_cover_now = True
                            pmda_cover_provider = "bandcamp"
                            try:
                                _files_watcher_suppress_folder(folder_path, seconds=120.0, reason="cover_write")
                            except Exception:
                                pass
                            _embed_cover_in_audio_files(cover_path, audio_files)
                            steps.append("Fetched and saved cover art from Bandcamp")
                except Exception as e:
                    logging.warning("improve-folder: Bandcamp cover fetch failed: %s", e)
            if tags_updated or cover_saved:
                provider_used = "bandcamp"

    root_dirs = _files_root_dir_strings()
    artist_folder = _files_guess_artist_folder(folder_path, artist_name, root_dirs=root_dirs)
    artist_provider = None
    had_artist_image = bool(_first_artist_image_path(artist_folder)) if artist_folder else False
    external_cached_path: Optional[str] = None
    if tags_updated or cover_saved:
        if artist_folder:
            artist_provider = _fetch_and_save_artist_image(artist_name, artist_folder, artist_mbid)
            if artist_provider:
                pmda_artist_provider = artist_provider
                steps.append("Fetched and saved artist image")
        else:
            # Flat libraries (albums directly under FILES_ROOTS) have no reliable per-artist folder.
            # In that case, cache an external image instead of writing artist.jpg into the library root.
            try:
                conn = _files_pg_connect()
            except Exception:
                conn = None
            if conn is not None:
                try:
                    # If an external image is already cached, count it immediately (even if we don't fetch a new one).
                    try:
                        key = _norm_artist_key(artist_name)
                        ext = _files_get_external_artist_images(conn, [key]).get(key) or {}
                        ext_path = str(ext.get("image_path") or "").strip()
                        if ext_path:
                            external_cached_path = ext_path
                    except Exception:
                        pass
                    url = ""
                    prov = ""
                    try:
                        if artist_mbid:
                            url = (_fetch_artist_image_fanart(artist_mbid) or "").strip()
                            if url:
                                prov = "fanart"
                    except Exception:
                        url = ""
                        prov = ""
                    if not url:
                        try:
                            url = (_fetch_artist_image_lastfm(artist_name) or "").strip()
                            if url:
                                prov = "lastfm"
                        except Exception:
                            url = ""
                            prov = ""
                    if not url:
                        try:
                            w = _fetch_wikipedia_artist_bio(artist_name, lang="en") or _fetch_wikipedia_artist_bio(artist_name, lang="fr") or {}
                            if isinstance(w, dict):
                                url = str(w.get("image_url") or "").strip()
                                if url:
                                    prov = "wikipedia"
                        except Exception:
                            url = ""
                            prov = ""
                    if not url:
                        try:
                            url = (_fetch_artist_image_audiodb(artist_name) or "").strip()
                            if url:
                                prov = "audiodb"
                        except Exception:
                            url = ""
                            prov = ""
                    if url and prov:
                        with conn.transaction():
                            external_cached_path = _files_cache_external_artist_image(
                                conn,
                                artist_name=artist_name,
                                provider=prov,
                                image_url=url,
                                max_px=640,
                            )
                        if external_cached_path:
                            pmda_artist_provider = prov
                            steps.append("Cached external artist image")
                except Exception:
                    pass
                finally:
                    try:
                        conn.close()
                    except Exception:
                        pass
    has_artist_image_now = bool(_first_artist_image_path(artist_folder)) if artist_folder else bool(external_cached_path)
    pmda_matched = bool(tags_updated)
    pmda_cover = bool(cover_saved or has_cover_now)
    pmda_artist_image = has_artist_image_now
    pmda_complete = pmda_matched and pmda_cover and pmda_artist_image

    if pmda_matched and not pmda_match_provider:
        pmda_match_provider = provider_used or ("musicbrainz" if release_mbid else None)
    if pmda_cover and not pmda_cover_provider:
        if provider_used in {"musicbrainz", "discogs", "lastfm", "bandcamp"}:
            pmda_cover_provider = provider_used
        elif has_cover_now:
            pmda_cover_provider = "local"
    if pmda_artist_image and not pmda_artist_provider:
        pmda_artist_provider = artist_provider or ("local" if had_artist_image else None)
    pmda_id = (current_tags.get(PMDA_ID_TAG) or "").strip() or None
    if pmda_matched or pmda_cover or pmda_artist_image or pmda_complete:
        _write_pmda_album_tags(
            folder_path,
            audio_files,
            pmda_id=pmda_id,
            match_provider=pmda_match_provider,
            cover_provider=pmda_cover_provider,
            artist_provider=pmda_artist_provider,
            matched=pmda_matched,
            cover=pmda_cover,
            artist_image=pmda_artist_image,
            complete=pmda_complete,
        )

    summary = f"Updated tags on {files_updated} file(s)." + (" Fetched cover art." if cover_saved else "")
    return {
        "steps": steps,
        "summary": summary,
        "tags_updated": tags_updated,
        "cover_saved": cover_saved,
        "provider_used": provider_used,
        "dupes_in_folder": dupes_in_folder,
        "files_updated": files_updated,
        "pmda_matched": pmda_matched,
        "pmda_cover": pmda_cover,
        "pmda_artist_image": pmda_artist_image,
        "pmda_complete": pmda_complete,
        "pmda_match_provider": pmda_match_provider,
        "pmda_cover_provider": pmda_cover_provider,
        "pmda_artist_provider": pmda_artist_provider,
        "discogs_release_id": discogs_release_id,
        "lastfm_album_mbid": lastfm_album_mbid,
        "bandcamp_album_url": bandcamp_album_url,
    }


@app.post("/api/library/improve-album")
def api_library_improve_album():
    """Improve a single album: query MusicBrainz for tags, update files, fetch cover if missing. Used by Fix column."""
    if _get_library_mode() == "files":
        data = request.get_json() or {}
        album_id = data.get("album_id")
        if not album_id:
            return jsonify({"error": "Missing album_id"}), 400
        try:
            album_id = int(album_id)
        except (TypeError, ValueError):
            return jsonify({"error": "Invalid album_id"}), 400
        ok, err = _ensure_files_index_ready()
        if not ok:
            return jsonify({"error": err or "Files index unavailable"}), 503
        conn = _files_pg_connect()
        if conn is None:
            return jsonify({"error": "PostgreSQL unavailable"}), 503
        try:
            with conn.cursor() as cur:
                cur.execute("SELECT folder_path FROM files_albums WHERE id = %s", (album_id,))
                row = cur.fetchone()
            if not row or not (row[0] or "").strip():
                return jsonify({"error": "Album not found"}), 404
            folder_path = path_for_fs_access(Path(row[0]))
        finally:
            conn.close()
        if not folder_path.exists() or not folder_path.is_dir():
            return jsonify({"error": "Album folder not found on disk"}), 404
        result = _improve_folder_by_path(folder_path)
        _trigger_files_index_rebuild_async(reason="improve_album")
        return jsonify(result)

    if not PLEX_CONFIGURED:
        return jsonify({"error": "Plex not configured"}), 503
    data = request.get_json() or {}
    album_id = data.get("album_id")
    if not album_id:
        return jsonify({"error": "Missing album_id"}), 400
    try:
        album_id = int(album_id)
    except (TypeError, ValueError):
        return jsonify({"error": "Invalid album_id"}), 400
    db_conn = plex_connect()
    try:
        result = _improve_single_album(album_id, db_conn)
        return jsonify(result)
    finally:
        db_conn.close()


@app.post("/api/drop/improve")
def api_drop_improve():
    """
    Accept multipart upload of audio files (one album). Save to temp dir, run improve-by-path,
    return result. Limits: max 50 files, 500 MB total. Temp dir deleted after success.
    """
    if "files" not in request.files and not request.files:
        return jsonify({"error": "No files uploaded"}), 400
    file_list = request.files.getlist("files") if request.files.get("files") else list(request.files.values())
    if not file_list or not any(f and f.filename for f in file_list):
        return jsonify({"error": "No files uploaded"}), 400
    allowed_ext = {".flac", ".mp3", ".m4a", ".aac", ".ogg", ".opus", ".wav", ".alac", ".ape", ".dsf", ".aif", ".aiff", ".wma", ".m4b", ".mp4"}
    total_size = 0
    files_to_save = []
    for f in file_list:
        if not f or not f.filename:
            continue
        base = os.path.basename(f.filename).strip()
        if not base or ".." in base or base.startswith("."):
            continue
        ext = Path(base).suffix.lower()
        if ext not in allowed_ext:
            continue
        try:
            f.stream.seek(0, 2)
            size = f.stream.tell()
            f.stream.seek(0)
        except Exception:
            size = 0
        total_size += size
        files_to_save.append((f, base))
    if len(files_to_save) > DROP_MAX_FILES:
        return jsonify({"error": f"Too many files (max {DROP_MAX_FILES})"}), 400
    if total_size > DROP_MAX_BYTES:
        return jsonify({"error": f"Total size too large (max {DROP_MAX_BYTES // (1024*1024)} MB)"}), 400
    if not files_to_save:
        return jsonify({"error": "No valid audio files"}), 400

    DROP_ALBUMS_BASE.mkdir(parents=True, exist_ok=True)
    temp_id = str(uuid.uuid4())[:8]
    temp_dir = DROP_ALBUMS_BASE / temp_id
    temp_dir.mkdir(parents=True, exist_ok=True)
    try:
        for f, base in files_to_save:
            safe_name = re.sub(r'[^\w\s\-\.]', "_", base)[:200]
            dest = temp_dir / safe_name
            f.save(str(dest))
        result = _improve_folder_by_path(temp_dir)
        return jsonify(result)
    except Exception as e:
        logging.exception("drop/improve failed: %s", e)
        return jsonify({"error": str(e), "steps": [], "summary": str(e), "tags_updated": False, "cover_saved": False, "provider_used": None, "dupes_in_folder": [], "files_updated": 0}), 500
    finally:
        try:
            if temp_dir.exists():
                shutil.rmtree(temp_dir, ignore_errors=True)
        except Exception as e:
            logging.debug("drop/improve cleanup %s: %s", temp_dir, e)


def _improve_one_album_item(item: dict) -> tuple:
    """Run improve on one album (own DB connection). Returns (index, item, result, steps) for state update."""
    idx = item.get("_idx", 0)
    album_id = item.get("album_id")
    artist_name = item.get("artist", "Unknown")
    album_title = item.get("album_title", f"Album {album_id}")
    known_mbid = item.get("musicbrainz_id") if isinstance(item.get("musicbrainz_id"), str) and (item.get("musicbrainz_id") or "").strip() else None
    folder_path_raw = (item.get("folder") or "").strip() if isinstance(item.get("folder"), str) else ""
    # Files mode: improve by folder path from scan_editions (album_id is not a Plex metadata_items id).
    if _get_library_mode() == "files" and folder_path_raw:
        result = _improve_folder_by_path(Path(folder_path_raw))
        steps_raw = result.get("steps", [])
        steps = [{"label": s if isinstance(s, str) else s.get("label", str(s)), "success": True} for s in steps_raw]
        return (idx, album_id, album_title, artist_name, result, steps)
    db_conn = plex_connect()
    try:
        result = _improve_single_album(album_id, db_conn, known_release_group_id=known_mbid)
        steps_raw = result.get("steps", [])
        steps = [{"label": s if isinstance(s, str) else s.get("label", str(s)), "success": True} for s in steps_raw]
        return (idx, album_id, album_title, artist_name, result, steps)
    finally:
        db_conn.close()


def _build_improve_items_from_editions(artist_name: str, editions: list[dict]) -> list[dict]:
    """
    Convert scan editions for one artist into improve-album items, deduplicated by album_id.
    Used to stream post-processing artist-by-artist in Files mode.
    """
    items: list[dict] = []
    root_dirs = _files_root_dir_strings()
    seen_album_ids: set[int] = set()
    for e in editions or []:
        try:
            album_id = int(e.get("album_id") or 0)
        except Exception:
            album_id = 0
        if album_id <= 0 or album_id in seen_album_ids:
            continue
        seen_album_ids.add(album_id)
        folder_raw = e.get("folder")
        folder_str = str(folder_raw).strip() if folder_raw is not None else ""
        mbid = (
            (e.get("musicbrainz_id") or "")
            or ((e.get("meta") or {}).get("musicbrainz_releasegroupid") or "")
            or ((e.get("meta") or {}).get("musicbrainz_id") or "")
        )
        mbid = str(mbid or "").strip()
        title = (e.get("title_raw") or e.get("album_norm") or f"Album {album_id}")
        # Snapshot pre-fix health so post-processing can update live counters by delta.
        folder_path: Optional[Path] = None
        if folder_str:
            try:
                folder_path = Path(folder_str)
            except Exception:
                folder_path = None
        meta_snapshot = dict(e.get("meta") or {})
        # Required tags check needs tracks; derive them if absent (so "tracks" requirement is meaningful).
        edition_for_required = dict(e)
        if not (edition_for_required.get("tracks") or []):
            ordered_paths = [Path(p) for p in (e.get("ordered_paths") or []) if str(p).strip()]
            if folder_path is not None and not ordered_paths:
                try:
                    ordered_paths = _files_collect_ordered_audio_paths(folder_path, [])
                except Exception:
                    ordered_paths = []
            derived_tracks = [
                {"title": p.stem or f"Track {i + 1}", "idx": i + 1}
                for i, p in enumerate(ordered_paths)
            ]
            edition_for_required["tracks"] = derived_tracks
        try:
            pre_missing_required = _check_required_tags(meta_snapshot, REQUIRED_TAGS, edition=edition_for_required)
        except Exception:
            pre_missing_required = []
        pre_has_cover = False
        pre_has_artist_image = False
        if folder_path is not None:
            try:
                pre_has_cover = bool(album_folder_has_cover(folder_path))
            except Exception:
                pre_has_cover = False
            try:
                artist_folder = _files_guess_artist_folder(folder_path, artist_name, root_dirs=root_dirs)
                pre_has_artist_image = bool(_artist_folder_has_image(artist_folder)) if artist_folder else False
            except Exception:
                pre_has_artist_image = False
        pre_has_mb_id = bool(mbid)
        pre_has_artist_mb_id = bool(
            meta_snapshot.get("musicbrainz_albumartistid")
            or meta_snapshot.get("musicbrainz_artistid")
            or meta_snapshot.get("musicbrainz_albumartist_id")
            or meta_snapshot.get("musicbrainz_artist_id")
        )
        items.append(
            {
                "artist": (artist_name or "").strip() or "Unknown Artist",
                "album_id": album_id,
                "album_title": str(title or "").strip() or f"Album {album_id}",
                "musicbrainz_id": mbid,
                "folder": folder_str,
                # Keep scan edition context so post-process cache refresh can preserve
                # required-tags health (notably "tracks") for incremental changed-only scans.
                "tracks": list(e.get("tracks") or []),
                "meta": dict(e.get("meta") or {}),
                "ordered_paths": list(e.get("ordered_paths") or []),
                "fingerprint": e.get("fingerprint") or "",
                "primary_metadata_source": e.get("primary_metadata_source") or e.get("metadata_source") or "",
                "metadata_source": e.get("metadata_source") or e.get("primary_metadata_source") or "",
                "discogs_release_id": e.get("discogs_release_id") or "",
                "lastfm_album_mbid": e.get("lastfm_album_mbid") or "",
                "bandcamp_album_url": e.get("bandcamp_album_url") or "",
                "br": e.get("br") or 0,
                "sr": e.get("sr") or 0,
                "bd": e.get("bd") or 0,
                "pre_missing_required_tags": list(pre_missing_required or []),
                "pre_has_cover": bool(pre_has_cover),
                "pre_has_artist_image": bool(pre_has_artist_image),
                "pre_has_mb_id": bool(pre_has_mb_id),
                "pre_has_artist_mb_id": bool(pre_has_artist_mb_id),
            }
        )
    return items


def _run_improve_all_albums_global(best_albums_list: List[dict]):
    """Background worker: improve each 'best' album from duplicate groups (global fix-all). Saves last_fix_all_by_provider for summary/chart."""
    total = len(best_albums_list)
    workers = max(1, min(8, getattr(sys.modules[__name__], "IMPROVE_ALL_WORKERS", 1)))
    logging.info("Improve-all (Magic): started for %d album(s) ‚Äì tags, covers, artist images (workers=%d)", total, workers)
    albums_improved = 0
    tags_updated_count = 0
    covers_downloaded = 0
    # PMDA-level stats for this improve-all run (used later in summary_json)
    pmda_albums_processed = 0
    pmda_albums_complete = 0
    pmda_albums_with_cover = 0
    pmda_albums_with_artist_image = 0
    album_log = []
    providers = ["musicbrainz", "discogs", "lastfm", "bandcamp"]
    by_provider = {p: {"identified": 0, "covers": 0, "tags": 0} for p in providers}
    # Inject index for ordering
    items_with_idx = [{**item, "_idx": i} for i, item in enumerate(best_albums_list)]
    with lock:
        state["improve_all"] = {
            "running": True,
            "global": True,
            "artist_id": None,
            "current": 0,
            "total": total,
            "current_album_id": None,
            "current_album": None,
            "current_artist": None,
            "current_provider": "musicbrainz",
            "provider_status": {p: "pending" for p in providers},
            "log": [],
            "result": None,
            "error": None,
        }
    try:
        if workers <= 1:
            # Sequential (original flow)
            for i, item in enumerate(items_with_idx):
                idx, album_id, album_title, artist_name, result, steps = _improve_one_album_item(item)
                prov = result.get("provider_used") or "musicbrainz"
                if prov in by_provider:
                    if result.get("tags_updated") or result.get("cover_saved"):
                        by_provider[prov]["identified"] += 1
                    if result.get("cover_saved"):
                        by_provider[prov]["covers"] += 1
                    if result.get("tags_updated"):
                        by_provider[prov]["tags"] += 1
                if result.get("tags_updated"):
                    tags_updated_count += 1
                if result.get("cover_saved"):
                    covers_downloaded += 1
                if result.get("tags_updated") or result.get("cover_saved"):
                    albums_improved += 1
                # PMDA stats from this album (only when we have PMDA flags)
                if result.get("pmda_matched") or result.get("pmda_cover") or result.get("pmda_artist_image"):
                    pmda_albums_processed += 1
                if result.get("pmda_cover"):
                    pmda_albums_with_cover += 1
                if result.get("pmda_artist_image"):
                    pmda_albums_with_artist_image += 1
                if result.get("pmda_complete"):
                    pmda_albums_complete += 1
                album_log.append({
                    "album_id": album_id,
                    "title": album_title,
                    "artist": artist_name,
                    "summary": result.get("summary", ""),
                    "steps": steps,
                })
                with lock:
                    if state.get("improve_all") and state["improve_all"].get("running"):
                        state["improve_all"]["current"] = idx + 1
                        state["improve_all"]["current_album_id"] = album_id
                        state["improve_all"]["current_album"] = album_title
                        state["improve_all"]["current_artist"] = artist_name
                        state["improve_all"]["log"] = list(album_log)
                        state["improve_all"]["current_steps"] = steps
                        state["improve_all"]["provider_status"] = {p: "ok" for p in providers}
        else:
            # Parallel: run up to `workers` albums at a time (MB queue still serializes MB calls)
            completed_with_idx: List[tuple] = []  # (idx, album_id, album_title, artist_name, result, steps)
            with ThreadPoolExecutor(max_workers=workers) as executor:
                future_to_item = {executor.submit(_improve_one_album_item, it): it for it in items_with_idx}
                for future in as_completed(future_to_item):
                    try:
                        idx, album_id, album_title, artist_name, result, steps = future.result()
                        completed_with_idx.append((idx, album_id, album_title, artist_name, result, steps))
                        prov = result.get("provider_used") or "musicbrainz"
                        if prov in by_provider:
                            if result.get("tags_updated") or result.get("cover_saved"):
                                by_provider[prov]["identified"] += 1
                            if result.get("cover_saved"):
                                by_provider[prov]["covers"] += 1
                            if result.get("tags_updated"):
                                by_provider[prov]["tags"] += 1
                        if result.get("tags_updated"):
                            tags_updated_count += 1
                        if result.get("cover_saved"):
                            covers_downloaded += 1
                        if result.get("tags_updated") or result.get("cover_saved"):
                            albums_improved += 1
                        if result.get("pmda_matched") or result.get("pmda_cover") or result.get("pmda_artist_image"):
                            pmda_albums_processed += 1
                        if result.get("pmda_cover"):
                            pmda_albums_with_cover += 1
                        if result.get("pmda_artist_image"):
                            pmda_albums_with_artist_image += 1
                        if result.get("pmda_complete"):
                            pmda_albums_complete += 1
                        album_log.append({
                            "album_id": album_id,
                            "title": album_title,
                            "artist": artist_name,
                            "summary": result.get("summary", ""),
                            "steps": steps,
                        })
                        with lock:
                            if state.get("improve_all") and state["improve_all"].get("running"):
                                state["improve_all"]["current"] = len(album_log)
                                state["improve_all"]["current_album_id"] = album_id
                                state["improve_all"]["current_album"] = album_title
                                state["improve_all"]["current_artist"] = artist_name
                                state["improve_all"]["log"] = list(album_log)
                                state["improve_all"]["current_steps"] = steps
                                state["improve_all"]["provider_status"] = {p: "ok" for p in providers}
                    except Exception as e:
                        logging.exception("improve-all worker failed: %s", e)
            # Sort album_log by original index for consistent final report
            idx_to_entry = {t[0]: {"album_id": t[1], "title": t[2], "artist": t[3], "summary": t[4].get("summary", ""), "steps": t[5]} for t in completed_with_idx}
            album_log = [idx_to_entry[i] for i in range(total) if i in idx_to_entry]
        with lock:
            if state.get("improve_all"):
                state["improve_all"]["running"] = False
                state["improve_all"]["result"] = {
                    "message": f"Processed {total} album(s). Tags updated on {tags_updated_count} album(s), {covers_downloaded} cover(s) saved.",
                    "albums_processed": total,
                    "albums_improved": albums_improved,
                    "covers_downloaded": covers_downloaded,
                    "tags_updated": tags_updated_count,
                    "by_provider": by_provider,
                    "album_log": album_log,
                    "pmda_albums_processed": pmda_albums_processed,
                    "pmda_albums_complete": pmda_albums_complete,
                    "pmda_albums_with_cover": pmda_albums_with_cover,
                    "pmda_albums_with_artist_image": pmda_albums_with_artist_image,
                }
                # Also expose PMDA stats at scan level so they can be included in summary_json
                state["scan_pmda_albums_processed"] = pmda_albums_processed
                state["scan_pmda_albums_complete"] = pmda_albums_complete
                state["scan_pmda_albums_with_cover"] = pmda_albums_with_cover
                state["scan_pmda_albums_with_artist_image"] = pmda_albums_with_artist_image
            state["last_fix_all_by_provider"] = by_provider
            state["last_fix_all_total_albums"] = total
            msg = state["improve_all"]["result"].get("message", "")
            logging.info("Improve-all (Magic): finished. %s", msg)
        # Auto-export: rebuild Files export library after Magic when enabled
        try:
            if _get_library_mode() == "files" and getattr(sys.modules[__name__], "AUTO_EXPORT_LIBRARY", False):
                logging.info("Auto-export: rebuilding Files library after Magic run (AUTO_EXPORT_LIBRARY=True).")
                _run_export_library()
        except Exception as e:
            logging.exception("Auto-export library after Magic failed: %s", e)
        if _get_library_mode() == "files":
            _trigger_files_index_rebuild_async(reason="improve_all_completed")
    except Exception as e:
        logging.exception("improve-all (global) failed: %s", e)
        with lock:
            if state.get("improve_all"):
                state["improve_all"]["running"] = False
                state["improve_all"]["error"] = str(e)


def _run_improve_all_albums(artist_id: int, album_ids: list, album_titles: dict):
    """Background worker: improve each album for an artist and update state."""
    total = len(album_ids)
    albums_improved = 0
    tags_updated_count = 0
    covers_downloaded = 0
    album_log = []
    providers = ["musicbrainz", "discogs", "lastfm", "bandcamp"]
    by_provider = {p: {"identified": 0, "covers": 0, "tags": 0} for p in providers}
    with lock:
        state["improve_all"] = {
            "running": True,
            "global": False,
            "artist_id": artist_id,
            "current": 0,
            "total": total,
            "current_album_id": None,
            "current_album": None,
            "current_artist": None,
            "current_provider": "musicbrainz",
            "provider_status": {p: "pending" for p in providers},
            "log": [],
            "result": None,
            "error": None,
        }
    try:
        for i, album_id in enumerate(album_ids):
            title = album_titles.get(album_id, f"Album {album_id}")
            with lock:
                if state.get("improve_all") and state["improve_all"].get("running"):
                    state["improve_all"]["current_album_id"] = album_id
                    state["improve_all"]["current_album"] = title
                    state["improve_all"]["current_provider"] = "musicbrainz"
                    state["improve_all"]["provider_status"] = {p: ("ok" if p == "musicbrainz" else "pending") for p in providers}
            db_conn = plex_connect()
            try:
                result = _improve_single_album(album_id, db_conn)
                prov = result.get("provider_used") or "musicbrainz"
                if prov in by_provider:
                    if result.get("tags_updated") or result.get("cover_saved"):
                        by_provider[prov]["identified"] += 1
                    if result.get("cover_saved"):
                        by_provider[prov]["covers"] += 1
                    if result.get("tags_updated"):
                        by_provider[prov]["tags"] += 1
                if result.get("tags_updated"):
                    tags_updated_count += 1
                if result.get("cover_saved"):
                    covers_downloaded += 1
                if result.get("tags_updated") or result.get("cover_saved"):
                    albums_improved += 1
                steps_raw = result.get("steps", [])
                steps = [{"label": s if isinstance(s, str) else s.get("label", str(s)), "success": True} for s in steps_raw]
                with lock:
                    if state.get("improve_all"):
                        state["improve_all"]["provider_status"] = {p: "ok" for p in providers}
                album_log.append({
                    "album_id": album_id,
                    "title": title,
                    "summary": result.get("summary", ""),
                    "steps": steps,
                })
            finally:
                db_conn.close()
            with lock:
                if state.get("improve_all") and state["improve_all"].get("running"):
                    state["improve_all"]["current"] = i + 1
                    state["improve_all"]["log"] = list(album_log)
                    state["improve_all"]["current_steps"] = steps
        with lock:
            if state.get("improve_all"):
                state["improve_all"]["running"] = False
                state["improve_all"]["result"] = {
                    "message": f"Processed {total} album(s). Tags updated on {tags_updated_count} album(s), {covers_downloaded} cover(s) saved.",
                    "albums_processed": total,
                    "albums_improved": albums_improved,
                    "covers_downloaded": covers_downloaded,
                    "tags_updated": tags_updated_count,
                    "by_provider": by_provider,
                    "album_log": album_log,
                }
    except Exception as e:
        logging.exception("improve-all-albums failed: %s", e)
        with lock:
            if state.get("improve_all"):
                state["improve_all"]["running"] = False
                state["improve_all"]["error"] = str(e)


@app.post("/api/library/improve-all-albums")
def api_library_improve_all_albums():
    """Start improving all albums for an artist (MusicBrainz tags + cover)."""
    if _get_library_mode() == "plex" and not PLEX_CONFIGURED:
        return jsonify({"error": "Plex not configured"}), 503
    data = request.get_json() or {}
    artist_id = data.get("artist_id")
    if artist_id is None:
        return jsonify({"error": "Missing artist_id"}), 400
    try:
        artist_id = int(artist_id)
    except (TypeError, ValueError):
        return jsonify({"error": "Invalid artist_id"}), 400
    with lock:
        if state.get("improve_all") and state["improve_all"].get("running"):
            return jsonify({"error": "Improve-all already running", "started": False}), 409
    if _get_library_mode() == "files":
        ok, err = _ensure_files_index_ready()
        if not ok:
            return jsonify({"error": err or "Files index unavailable", "started": False}), 503
        conn = _files_pg_connect()
        if conn is None:
            return jsonify({"error": "PostgreSQL unavailable", "started": False}), 503
        try:
            with conn.cursor() as cur:
                cur.execute("SELECT name FROM files_artists WHERE id = %s", (artist_id,))
                arow = cur.fetchone()
                if not arow:
                    return jsonify({"error": "Artist not found", "started": False}), 404
                artist_name = arow[0] or ""
                cur.execute(
                    """
                    SELECT id, title, folder_path, musicbrainz_release_group_id
                    FROM files_albums
                    WHERE artist_id = %s
                    ORDER BY title ASC
                    """,
                    (artist_id,),
                )
                rows = cur.fetchall()
        finally:
            conn.close()
        if not rows:
            return jsonify({"error": "No albums found for this artist", "started": False}), 404
        items = [
            {
                "artist": artist_name,
                "album_id": int(r[0]),
                "album_title": (r[1] or "").strip() or f"Album {int(r[0])}",
                "folder": (r[2] or "").strip(),
                "musicbrainz_id": (r[3] or "").strip(),
            }
            for r in rows
        ]
        thread = threading.Thread(
            target=_run_improve_all_albums_global,
            args=(items,),
            daemon=True,
        )
        thread.start()
        return jsonify({"started": True, "total": len(items)})

    db_conn = plex_connect()
    try:
        if not SECTION_IDS:
            album_ids, album_titles = [], {}
        else:
            placeholders = ",".join("?" for _ in SECTION_IDS)
            section_filter = f"AND library_section_id IN ({placeholders})"
            section_args = [artist_id] + list(SECTION_IDS)
            rows = db_conn.execute(
                f"SELECT id, title FROM metadata_items WHERE parent_id = ? AND metadata_type = 9 {section_filter}",
                section_args,
            ).fetchall()
            album_ids = [r[0] for r in rows]
            album_titles = {r[0]: r[1] for r in rows}
    finally:
        db_conn.close()
    if not album_ids:
        return jsonify({"error": "No albums found for this artist (in selected libraries)", "started": False}), 404
    thread = threading.Thread(
        target=_run_improve_all_albums,
        args=(artist_id, album_ids, album_titles),
        daemon=True,
    )
    thread.start()
    return jsonify({"started": True, "total": len(album_ids)})


@app.post("/api/library/improve-all")
def api_library_improve_all():
    """Start global 'Fix all albums': improve each 'best' edition from duplicate groups + all albums from last scan that have a MusicBrainz match (tags + cover + artist image from MB ‚Üí Discogs ‚Üí Last.fm ‚Üí Bandcamp)."""
    if _get_library_mode() == "plex" and not PLEX_CONFIGURED:
        return jsonify({"error": "Plex not configured"}), 503
    with lock:
        if state.get("improve_all") and state["improve_all"].get("running"):
            return jsonify({"error": "Improve-all already running", "started": False}), 409
        if not state["duplicates"]:
            state["duplicates"] = load_scan_from_db()
        best_albums = []
        seen_ids = set()
        for artist_name, groups in state["duplicates"].items():
            for g in groups:
                best = g.get("best")
                if not best:
                    continue
                album_id = best.get("album_id")
                if album_id is None or album_id in seen_ids:
                    continue
                seen_ids.add(album_id)
                best_albums.append({
                    "artist": artist_name,
                    "album_id": album_id,
                    "album_title": best.get("title_raw") or best.get("album_norm") or f"Album {album_id}",
                    "musicbrainz_id": best.get("musicbrainz_id"),
                    "folder": (best.get("folder") or "").strip(),
                })
        # Also include all albums from last scan so improve-all can enrich tags/covers
        # even when there is no MusicBrainz ID yet (e.g. Bandcamp/Last.fm-only matches, or new REQUIRED_TAGS like "genre").
        scan_id = get_last_completed_scan_id()
        if scan_id is not None:
            con = sqlite3.connect(str(STATE_DB_FILE))
            cur = con.cursor()
            try:
                # 1) Albums avec MBID (comportement historique)
                cur.execute(
                    "SELECT artist, album_id, title_raw, musicbrainz_id, folder FROM scan_editions WHERE scan_id = ?",
                    (scan_id,),
                )
                rows = cur.fetchall()
                for row in rows:
                    artist_name, album_id, title_raw, mbid, folder = row[0], row[1], row[2] or "", (row[3] or "").strip(), row[4] or ""
                    if album_id in seen_ids:
                        continue
                    # Inclure tous les albums du dernier scan, m√™me sans MBID, pour permettre l'enrichissement via Discogs/Last.fm/Bandcamp
                    seen_ids.add(album_id)
                    best_albums.append({
                        "artist": artist_name,
                        "album_id": album_id,
                        "album_title": (title_raw or "").strip() or f"Album {album_id}",
                        "musicbrainz_id": mbid or "",
                        "folder": (folder or "").strip(),
                    })
            except Exception as e:
                logging.debug("Fix-all: could not load scan_editions for extra albums: %s", e)
            finally:
                con.close()
    if not best_albums:
        return jsonify({"error": "No albums to fix (no duplicate groups and no scan with MusicBrainz matches). Run a scan first.", "started": False}), 404
    thread = threading.Thread(target=_run_improve_all_albums_global, args=(best_albums,), daemon=True)
    thread.start()
    return jsonify({"started": True, "total": len(best_albums)})


@app.get("/api/library/improve-all-albums/progress")
@app.get("/api/library/improve-all/progress")
def api_library_improve_all_progress():
    """Return current improve-all job progress (per-artist or global: running, current, total, result, error)."""
    with lock:
        prog = state.get("improve_all")
    if prog is None:
        return jsonify({"running": False, "finished": False})
    out = {
        "running": prog.get("running", False),
        "global": prog.get("global", False),
        "current": prog.get("current", 0),
        "total": prog.get("total", 0),
        "albums_processed": prog.get("current", 0),
        "total_albums": prog.get("total", 0),
        "current_album_id": prog.get("current_album_id"),
        "current_album": prog.get("current_album"),
        "current_artist": prog.get("current_artist"),
        "current_provider": prog.get("current_provider"),
        "provider_status": prog.get("provider_status", {}),
        "current_steps": prog.get("current_steps", []),
        "album_log": prog.get("log", []),
        "finished": not prog.get("running", True) and (prog.get("result") is not None or prog.get("error") is not None),
        "result": prog.get("result"),
        "error": prog.get("error"),
    }
    return jsonify(out)


@app.post("/api/musicbrainz/fix-artist-tags")
def api_musicbrainz_fix_artist_tags():
    """Fix tags for an artist and all their albums using MusicBrainz data. Also fetches missing images."""
    if not PLEX_CONFIGURED:
        return jsonify({"error": "Plex not configured"}), 503
    
    if not USE_MUSICBRAINZ:
        return jsonify({"error": "MusicBrainz not enabled"}), 400
    
    try:
        from mutagen import File as MutagenFile
        from mutagen.id3 import ID3, TIT2, TPE1, TALB, TDRC, TCON, APIC, TXXX
        from mutagen.mp3 import MP3
        from mutagen.flac import FLAC
        from mutagen.mp4 import MP4
        HAS_MUTAGEN = True
    except ImportError:
        HAS_MUTAGEN = False
    
    if not HAS_MUTAGEN:
        return jsonify({"error": "mutagen library not installed. Please install it to fix tags."}), 500
    
    data = request.get_json() or {}
    artist_id = data.get("artist_id")
    
    if not artist_id:
        return jsonify({"error": "Missing artist_id"}), 400
    
    import sqlite3
    db_conn = plex_connect()
    
    # Get artist info
    artist_row = db_conn.execute(
        "SELECT id, title FROM metadata_items WHERE id = ? AND metadata_type = 8",
        (artist_id,)
    ).fetchone()
    
    if not artist_row:
        db_conn.close()
        return jsonify({"error": "Artist not found"}), 404
    
    artist_name = artist_row[1]
    
    # Find MusicBrainz ID for artist
    mbid = None
    album_rows = db_conn.execute("""
        SELECT alb.id
        FROM metadata_items alb
        WHERE alb.parent_id = ? AND alb.metadata_type = 9
        LIMIT 1
    """, (artist_id,)).fetchall()
    
    if album_rows:
        album_id = album_rows[0][0]
        folder = first_part_path(db_conn, album_id)
        if folder:
            first_audio = next((p for p in folder.rglob("*") if AUDIO_RE.search(p.name)), None)
            if first_audio:
                meta = extract_tags(first_audio)
                mbid = meta.get('musicbrainz_albumartistid') or meta.get('musicbrainz_artistid')
    
    if not mbid:
        # Search MusicBrainz
        try:
            search_result = musicbrainzngs.search_artists(artist=artist_name, limit=1)
            if search_result.get("artist-list"):
                mbid = search_result["artist-list"][0]["id"]
        except Exception as e:
            logging.warning("Failed to search MusicBrainz for artist '%s': %s", artist_name, e)
    
    if not mbid:
        db_conn.close()
        return jsonify({"error": "Could not find MusicBrainz ID for artist"}), 404
    
    # Get all albums for this artist (selected sections only)
    if not SECTION_IDS:
        album_rows = []
    else:
        placeholders = ",".join("?" for _ in SECTION_IDS)
        section_filter = f"AND library_section_id IN ({placeholders})"
        section_args = [artist_id] + list(SECTION_IDS)
        album_rows = db_conn.execute(
            f"SELECT id, title FROM metadata_items WHERE parent_id = ? AND metadata_type = 9 {section_filter}",
            section_args,
        ).fetchall()
    
    albums_updated = 0
    albums_with_images = 0
    errors = []
    
    # Process each album
    for album_id, album_title in album_rows:
        try:
            folder = first_part_path(db_conn, album_id)
            if not folder:
                continue
            
            # Get all audio files in album
            audio_files = [p for p in folder.rglob("*") if AUDIO_RE.search(p.name)]
            if not audio_files:
                continue
            
            # Get MusicBrainz release info for this album
            first_audio = audio_files[0]
            current_tags = extract_tags(first_audio)
            release_mbid = current_tags.get('musicbrainz_releasegroupid') or current_tags.get('musicbrainz_releaseid')
            release_group_id = None
            if release_mbid:
                tag_src = "musicbrainz_releasegroupid" if current_tags.get("musicbrainz_releasegroupid") else "musicbrainz_releaseid"
                release_group_id = resolve_mbid_to_release_group(release_mbid, tag_src) or release_mbid
            
            mb_release_info = None
            if release_group_id:
                try:
                    result = musicbrainzngs.get_release_group_by_id(release_group_id, includes=["releases", "artist-credits"])
                    mb_release_info = result.get("release-group", {})
                except Exception:
                    pass
            
            # Update tags for all audio files
            for audio_file in audio_files:
                try:
                    audio = MutagenFile(str(audio_file))
                    if audio is None:
                        continue
                    
                    # Update basic tags from MusicBrainz
                    if mb_release_info:
                        # Artist
                        if isinstance(audio, (MP3, ID3)):
                            audio.tags.add(TPE1(encoding=3, text=artist_name))
                        elif isinstance(audio, FLAC):
                            audio["ARTIST"] = artist_name
                        elif isinstance(audio, MP4):
                            audio["\xa9ART"] = [artist_name]
                        
                        # Album
                        album_title_mb = mb_release_info.get("title", album_title)
                        if isinstance(audio, (MP3, ID3)):
                            audio.tags.add(TALB(encoding=3, text=album_title_mb))
                        elif isinstance(audio, FLAC):
                            audio["ALBUM"] = album_title_mb
                        elif isinstance(audio, MP4):
                            audio["\xa9alb"] = [album_title_mb]
                        
                        # Date
                        date_str = mb_release_info.get("first-release-date", "")
                        if date_str:
                            year = date_str.split("-")[0] if "-" in date_str else date_str
                            if isinstance(audio, (MP3, ID3)):
                                audio.tags.add(TDRC(encoding=3, text=year))
                            elif isinstance(audio, FLAC):
                                audio["DATE"] = year
                            elif isinstance(audio, MP4):
                                audio["\xa9day"] = [year]
                        
                        # MusicBrainz IDs
                        if isinstance(audio, FLAC):
                            audio["MUSICBRAINZ_ARTISTID"] = mbid
                            audio["MUSICBRAINZ_ALBUMARTISTID"] = mbid
                            if release_group_id:
                                audio["MUSICBRAINZ_RELEASEGROUPID"] = release_group_id
                        elif isinstance(audio, MP4):
                            audio["----:com.apple.iTunes:MusicBrainz Artist Id"] = [mbid.encode('utf-8')]
                            audio["----:com.apple.iTunes:MusicBrainz Album Artist Id"] = [mbid.encode('utf-8')]
                            if release_group_id:
                                audio["----:com.apple.iTunes:MusicBrainz Release Group Id"] = [release_group_id.encode('utf-8')]
                    
                    audio.save()
                    albums_updated += 1
                    
                    # Try to fetch and save album cover if missing
                    if not any(f.name.lower().startswith(('cover', 'folder', 'album', 'artwork', 'front')) 
                              and f.suffix.lower() in ['.jpg', '.jpeg', '.png', '.webp'] 
                              for f in folder.iterdir() if f.is_file()):
                        # Try to get cover from MusicBrainz
                        if release_group_id:
                            try:
                                # Get cover art from MusicBrainz Cover Art Archive
                                cover_url = f"http://coverartarchive.org/release-group/{release_group_id}/front"
                                cover_resp = requests.get(cover_url, timeout=5, allow_redirects=True)
                                if cover_resp.status_code == 200:
                                    cover_path = folder / "cover.jpg"
                                    with open(cover_path, 'wb') as f:
                                        f.write(cover_resp.content)
                                    albums_with_images += 1
                            except Exception:
                                pass
                
                except Exception as e:
                    errors.append(f"Error updating {audio_file.name}: {str(e)}")
                    logging.error("Error updating tags for %s: %s", audio_file, e)
        
        except Exception as e:
            errors.append(f"Error processing album {album_title}: {str(e)}")
            logging.error("Error processing album %s: %s", album_title, e)
    
    db_conn.close()
    
    return jsonify({
        "success": True,
        "message": f"Updated tags for {albums_updated} file(s) across {len(album_rows)} album(s). Fetched {albums_with_images} cover image(s).",
        "albums_processed": len(album_rows),
        "files_updated": albums_updated,
        "images_fetched": albums_with_images,
        "errors": errors[:10]  # Limit error messages
    })

@app.post("/api/musicbrainz/fix-album-tags")
def api_musicbrainz_fix_album_tags():
    """Fix tags for a single album using MusicBrainz data."""
    if _get_library_mode() == "files":
        data = request.get_json() or {}
        album_id = data.get("album_id")
        tags_to_apply = data.get("tags", {}) or {}
        if not album_id:
            return jsonify({"error": "Missing album_id"}), 400
        try:
            album_id = int(album_id)
        except (TypeError, ValueError):
            return jsonify({"error": "Invalid album_id"}), 400
        ok, err = _ensure_files_index_ready()
        if not ok:
            return jsonify({"error": err or "Files index unavailable"}), 503
        conn = _files_pg_connect()
        if conn is None:
            return jsonify({"error": "PostgreSQL unavailable"}), 503
        try:
            with conn.cursor() as cur:
                cur.execute("SELECT folder_path FROM files_albums WHERE id = %s", (album_id,))
                row = cur.fetchone()
            if not row or not (row[0] or "").strip():
                return jsonify({"error": "Album not found"}), 404
            folder_path = path_for_fs_access(Path(row[0]))
        finally:
            conn.close()
        if not folder_path.exists() or not folder_path.is_dir():
            return jsonify({"error": "Album folder not found"}), 404

        album_artist = (tags_to_apply.get("albumartist") or tags_to_apply.get("artist") or "").strip()
        track_artist = (tags_to_apply.get("artist") or tags_to_apply.get("albumartist") or "").strip()
        album_title = (tags_to_apply.get("album") or "").strip()
        year_val = (tags_to_apply.get("year") or tags_to_apply.get("date") or "").strip()
        if year_val and len(year_val) >= 4:
            year_val = year_val[:4]
        genre_val = (tags_to_apply.get("genre") or "").strip()
        from mutagen import File as MutagenFile

        audio_files = [p for p in folder_path.rglob("*") if AUDIO_RE.search(p.name)]
        updated = 0
        errors = []
        for p in audio_files:
            try:
                audio = MutagenFile(str(p))
                if audio is None:
                    continue
                _apply_artist_album_tags_to_audio(
                    audio,
                    album_artist=album_artist,
                    track_artist=track_artist or album_artist,
                    album_title=album_title,
                    year_str=year_val,
                    genre_str=genre_val or None,
                )
                audio.save()
                updated += 1
            except Exception as e:
                errors.append(f"{p.name}: {e}")
        _trigger_files_index_rebuild_async(reason="manual_tag_fix")
        return jsonify({
            "success": True,
            "message": f"Updated tags on {updated} file(s).",
            "files_updated": updated,
            "errors": errors[:20],
        })

    if not PLEX_CONFIGURED:
        return jsonify({"error": "Plex not configured"}), 503
    
    data = request.get_json() or {}
    album_id = data.get("album_id")
    tags_to_apply = data.get("tags", {})
    
    if not album_id:
        return jsonify({"error": "Missing album_id"}), 400
    
    # For now, return success but don't actually write tags
    # Tag writing requires mutagen or similar library
    # This is a placeholder for future implementation
    return jsonify({
        "success": True,
        "message": "Tag fixing not yet implemented. This will require mutagen library for tag writing.",
        "tags_to_apply": tags_to_apply
    })

@app.get("/api/scan-history/<int:scan_id>")
def api_scan_history_detail(scan_id):
    """Return details of a specific scan or dedupe entry."""
    import sqlite3
    con = sqlite3.connect(str(STATE_DB_FILE))
    cur = con.cursor()
    cur.execute("PRAGMA table_info(scan_history)")
    cols_info = [r[1] for r in cur.fetchall()]
    has_entry_type = "entry_type" in cols_info
    has_summary_json = "summary_json" in cols_info
    if has_entry_type and has_summary_json:
        cur.execute("""
            SELECT scan_id, start_time, end_time, duration_seconds, albums_scanned,
                   duplicates_found, artists_processed, artists_total, ai_used_count,
                   mb_used_count, ai_enabled, mb_enabled, auto_move_enabled,
                   space_saved_mb, albums_moved, status,
                   duplicate_groups_count, total_duplicates_count, broken_albums_count,
                   missing_albums_count, albums_without_artist_image, albums_without_album_image,
                   albums_without_complete_tags, albums_without_mb_id, albums_without_artist_mb_id,
                   entry_type, summary_json
            FROM scan_history
            WHERE scan_id = ?
        """, (scan_id,))
    elif has_entry_type:
        cur.execute("""
            SELECT scan_id, start_time, end_time, duration_seconds, albums_scanned,
                   duplicates_found, artists_processed, artists_total, ai_used_count,
                   mb_used_count, ai_enabled, mb_enabled, auto_move_enabled,
                   space_saved_mb, albums_moved, status,
                   duplicate_groups_count, total_duplicates_count, broken_albums_count,
                   missing_albums_count, albums_without_artist_image, albums_without_album_image,
                   albums_without_complete_tags, albums_without_mb_id, albums_without_artist_mb_id,
                   entry_type
            FROM scan_history
            WHERE scan_id = ?
        """, (scan_id,))
    else:
        cur.execute("""
            SELECT scan_id, start_time, end_time, duration_seconds, albums_scanned,
                   duplicates_found, artists_processed, artists_total, ai_used_count,
                   mb_used_count, ai_enabled, mb_enabled, auto_move_enabled,
                   space_saved_mb, albums_moved, status,
                   duplicate_groups_count, total_duplicates_count, broken_albums_count,
                   missing_albums_count, albums_without_artist_image, albums_without_album_image,
                   albums_without_complete_tags, albums_without_mb_id, albums_without_artist_mb_id
            FROM scan_history
            WHERE scan_id = ?
        """, (scan_id,))
    row = cur.fetchone()
    con.close()

    if not row:
        return jsonify({"error": "Scan not found"}), 404

    out = {
        "scan_id": row[0],
        "start_time": row[1],
        "end_time": row[2],
        "duration_seconds": row[3],
        "albums_scanned": row[4] or 0,
        "duplicates_found": row[5] or 0,
        "artists_processed": row[6] or 0,
        "artists_total": row[7] or 0,
        "ai_used_count": row[8] or 0,
        "mb_used_count": row[9] or 0,
        "ai_enabled": bool(row[10]),
        "mb_enabled": bool(row[11]),
        "auto_move_enabled": bool(row[12]),
        "space_saved_mb": row[13] or 0,
        "albums_moved": row[14] or 0,
        "status": row[15] or "completed",
        "duplicate_groups_count": row[16] or 0 if len(row) > 16 else 0,
        "total_duplicates_count": row[17] or 0 if len(row) > 17 else 0,
        "broken_albums_count": row[18] or 0 if len(row) > 18 else 0,
        "missing_albums_count": row[19] or 0 if len(row) > 19 else 0,
        "albums_without_artist_image": row[20] or 0 if len(row) > 20 else 0,
        "albums_without_album_image": row[21] or 0 if len(row) > 21 else 0,
        "albums_without_complete_tags": row[22] or 0 if len(row) > 22 else 0,
        "albums_without_mb_id": row[23] or 0 if len(row) > 23 else 0,
        "albums_without_artist_mb_id": row[24] or 0 if len(row) > 24 else 0,
    }
    if has_entry_type and len(row) > 25:
        out["entry_type"] = row[25] or "scan"
    else:
        out["entry_type"] = "scan"
    if has_summary_json and len(row) > 26 and row[26]:
        try:
            summary = json.loads(row[26])
            out["summary_json"] = summary
            if isinstance(summary, dict) and "steps_executed" in summary:
                out["steps_executed"] = summary["steps_executed"]
        except (TypeError, ValueError):
            out["summary_json"] = None
    else:
        out["summary_json"] = None
    return jsonify(out)

@app.get("/api/scan-history/<int:scan_id>/moves")
def api_scan_history_moves(scan_id):
    """Return all moves for a specific scan."""
    import sqlite3
    con = sqlite3.connect(str(STATE_DB_FILE))
    cur = con.cursor()
    cur.execute("PRAGMA table_info(scan_moves)")
    move_cols = [r[1] for r in cur.fetchall()]
    has_extra = "album_title" in move_cols and "fmt_text" in move_cols
    has_reason = "move_reason" in move_cols
    if has_extra and has_reason:
        cur.execute("""
            SELECT move_id, scan_id, artist, album_id, original_path, moved_to_path,
                   size_mb, moved_at, restored, album_title, fmt_text, move_reason
            FROM scan_moves
            WHERE scan_id = ?
            ORDER BY moved_at DESC
        """, (scan_id,))
    elif has_extra:
        cur.execute("""
            SELECT move_id, scan_id, artist, album_id, original_path, moved_to_path,
                   size_mb, moved_at, restored, album_title, fmt_text
            FROM scan_moves
            WHERE scan_id = ?
            ORDER BY moved_at DESC
        """, (scan_id,))
    else:
        cur.execute("""
            SELECT move_id, scan_id, artist, album_id, original_path, moved_to_path,
                   size_mb, moved_at, restored
            FROM scan_moves
            WHERE scan_id = ?
            ORDER BY moved_at DESC
        """, (scan_id,))
    rows = cur.fetchall()
    con.close()
    
    moves = []
    for row in rows:
        m = {
            "move_id": row[0],
            "scan_id": row[1],
            "artist": row[2],
            "album_id": row[3],
            "original_path": row[4],
            "moved_to_path": row[5],
            "size_mb": row[6] or 0,
            "moved_at": row[7],
            "restored": bool(row[8]),
        }
        if has_extra and len(row) >= 11:
            m["album_title"] = row[9] or ""
            m["fmt_text"] = row[10] or ""
        if has_reason and len(row) >= 12:
            m["move_reason"] = (row[11] or "").strip().lower() or "dedupe"
        moves.append(m)
    
    return jsonify(moves)

@app.post("/api/scan-history/<int:scan_id>/restore")
def api_scan_history_restore(scan_id):
    """Restore moved files to their original location."""
    data = request.get_json() or {}
    move_ids = data.get("move_ids", [])
    restore_all = data.get("all", False)
    
    import sqlite3
    con = sqlite3.connect(str(STATE_DB_FILE))
    cur = con.cursor()
    
    if restore_all:
        cur.execute("""
            SELECT move_id, original_path, moved_to_path, artist
            FROM scan_moves
            WHERE scan_id = ? AND restored = 0
        """, (scan_id,))
    else:
        if not move_ids:
            return jsonify({"error": "No move_ids provided"}), 400
        placeholders = ",".join("?" * len(move_ids))
        cur.execute(f"""
            SELECT move_id, original_path, moved_to_path, artist
            FROM scan_moves
            WHERE scan_id = ? AND move_id IN ({placeholders}) AND restored = 0
        """, (scan_id, *move_ids))
    
    rows = cur.fetchall()
    if not rows:
        con.close()
        return jsonify({"error": "No moves found to restore"}), 404
    
    artists_to_refresh = set()
    restored_count = 0
    restored_paths: List[dict] = []
    
    for move_id, original_path, moved_to_path, artist in rows:
        src = Path(moved_to_path)
        dst = Path(original_path)
        
        if not src.exists():
            logging.warning(f"Restore: source {src} does not exist, skipping")
            continue
        
        try:
            dst.parent.mkdir(parents=True, exist_ok=True)
            safe_move(str(src), str(dst))
            artists_to_refresh.add(artist)
            restored_count += 1
            restored_paths.append({"from": moved_to_path, "to": original_path})
            
            # Mark as restored
            cur.execute("UPDATE scan_moves SET restored = 1 WHERE move_id = ?", (move_id,))
        except Exception as e:
            logging.error(f"Restore: failed to restore {src} ‚Üí {dst}: {e}")
            continue
    
    con.commit()
    con.close()
    
    # Refresh Plex for affected artists
    for artist in artists_to_refresh:
        letter = quote_plus(artist[0].upper())
        art_enc = quote_plus(artist)
        try:
            plex_api(f"/library/sections/{SECTION_ID}/refresh?path=/music/matched/{letter}/{art_enc}", method="GET")
        except Exception as e:
            logging.warning(f"Restore: plex refresh failed for {artist}: {e}")
    
    return jsonify({
        "restored": restored_count,
        "artists_refreshed": len(artists_to_refresh),
        "restored_paths": restored_paths,
    })

@app.post("/api/scan-history/<int:scan_id>/dedupe")
def api_scan_history_dedupe(scan_id):
    """Manually dedupe albums from a previous scan."""
    # Load scan results from DB (dict artist -> list of groups)
    scan_results = load_scan_from_db()
    if not scan_results:
        return jsonify({"error": "No scan results found for this scan"}), 404
    
    flat_groups = [g for groups in scan_results.values() for g in groups]
    if not flat_groups:
        return jsonify({"error": "No duplicate groups to dedupe"}), 404
    
    # Start deduplication
    background_dedupe(flat_groups)
    return jsonify({"status": "ok", "message": "Deduplication started"})

@app.get("/api/dedupe")
def api_dedupe():
    with lock:
        deduping = state["deduping"]
        progress = state["dedupe_progress"]
        total = state["dedupe_total"]
        start_time = state.get("dedupe_start_time")
        saved_this_run = state.get("dedupe_saved_this_run", 0)
        current_group = state.get("dedupe_current_group")
        last_write = state.get("dedupe_last_write")

    percent = round(100 * progress / total, 1) if total else 0
    eta_seconds = None
    if start_time and total and progress > 0:
        elapsed = time.time() - start_time
        avg_per_group = elapsed / progress
        remaining = total - progress
        eta_seconds = max(0, int(remaining * avg_per_group))

    return jsonify(
        deduping=deduping,
        progress=progress,
        total=total,
        saved=get_stat("space_saved"),
        saved_this_run=saved_this_run,
        moved=get_stat("removed_dupes"),
        percent=percent,
        eta_seconds=eta_seconds,
        current_group=current_group,
        last_write=last_write,
    )

@app.get("/details/<artist>/<int:album_id>")
def details(artist, album_id):
    if not PLEX_CONFIGURED:
        return jsonify({"error": "Plex not configured", "requiresConfig": True}), 503
    art = artist.replace("_", " ")
    with lock:
        groups = state["duplicates"].get(art)
    if groups is None:
        groups = load_scan_from_db().get(art, [])
    for g in groups:
        best_album_id_in_g = g.get("album_id") or (g.get("best", {}).get("album_id") if g.get("best") else None)
        if best_album_id_in_g == album_id:
            editions = [g["best"]] + g["losers"]
            best_album_id = best_album_id_in_g
            artist_rating_key = None
            try:
                db_conn = plex_connect()
                best_track_titles = {(t.title or "").strip().lower() for t in get_tracks(db_conn, best_album_id)}
                # Plex Web: artist page = /library/metadata/{artist_id}; album's parent is the artist (metadata_type 9)
                row = db_conn.execute(
                    "SELECT parent_id FROM metadata_items WHERE id = ? AND metadata_type = 9",
                    (best_album_id,),
                ).fetchone()
                if row and row[0]:
                    artist_rating_key = int(row[0])
            except Exception:
                best_track_titles = set()
                db_conn = None

            out = []
            rationale = g["best"].get("rationale", "")
            for i, e in enumerate(editions):
                folder_path = path_for_fs_access(Path(e["folder"])) if e.get("folder") else None
                is_best = i == 0
                # Size: losers have size_mb in DB; best we compute (frontend expects bytes)
                if is_best:
                    size_mb = safe_folder_size(folder_path) // (1024 * 1024) if folder_path else 0
                else:
                    size_mb = e.get("size", 0) or (safe_folder_size(folder_path) // (1024 * 1024) if folder_path else 0)
                size_bytes = size_mb * (1024 * 1024)

                track_list = []
                if db_conn:
                    try:
                        for t in get_tracks_for_details(db_conn, e["album_id"]):
                            title_norm = (t.get("title") or t.get("name") or "").strip().lower()
                            is_bonus = not is_best and title_norm not in best_track_titles
                            raw_path = t.get("path")
                            track_path = str(path_for_fs_access(Path(raw_path))) if raw_path else None
                            track_list.append({
                                "idx": t.get("idx", 0),
                                "title": t.get("title") or t.get("name"),
                                "name": t.get("name") or t.get("title"),
                                "dur": t.get("dur", 0),
                                "duration": t.get("duration"),
                                "format": t.get("format"),
                                "bitrate": t.get("bitrate"),
                                "is_bonus": is_bonus,
                                "path": track_path,
                            })
                    except Exception as track_err:
                        logging.warning(
                            "details: tracks failed for edition album_id=%s: %s",
                            e["album_id"], track_err
                        )

                # Edition-level br/sr/bd: use stored values, or derive from tracks / folder so we never send 0 for real files
                br_out = (e.get("br", 0) // 1000) if isinstance(e.get("br"), int) else (e.get("br") or 0)
                sr_out = e.get("sr", 0) or 0
                bd_out = e.get("bd", 0) or 0
                if (br_out == 0 or sr_out == 0 or bd_out == 0) and track_list:
                    # Derive bitrate from first track that has it (tracks have bitrate in kbps)
                    br_from_tracks = next((t.get("bitrate") for t in track_list if t.get("bitrate")), None)
                    if br_from_tracks is not None and br_out == 0:
                        br_out = br_from_tracks if br_from_tracks < 100000 else br_from_tracks // 1000
                if (br_out == 0 or sr_out == 0 or bd_out == 0) and folder_path:
                    try:
                        _fmt_score, br_bps, sr_hz, bd_val, _ = analyse_format(folder_path)
                        if br_out == 0 and br_bps:
                            br_out = br_bps // 1000 if br_bps >= 1000 else br_bps
                        if sr_out == 0 and sr_hz:
                            sr_out = sr_hz
                        if bd_out == 0 and bd_val:
                            bd_out = bd_val
                    except Exception:
                        pass

                thumb_data = fetch_cover_as_base64(e["album_id"])
                path_str = str(folder_path) if folder_path is not None else ""
                out.append({
                    "thumb_data": thumb_data,
                    "title_raw": e.get("title_raw") or "",
                    "size": size_bytes,
                    "fmt": e.get("fmt_text", e.get("fmt", "")),
                    "br": br_out,
                    "sr": sr_out,
                    "bd": bd_out,
                    "path": path_str,
                    "folder": path_str,
                    "album_id": e["album_id"],
                    "track_count": len(track_list),
                    "tracks": track_list,
                    "musicbrainz_id": e.get("musicbrainz_id"),  # Include MusicBrainz ID if available
                    "match_verified_by_ai": bool(e.get("match_verified_by_ai", False)),
                })
            if db_conn:
                try:
                    db_conn.close()
                except Exception:
                    pass
            return jsonify(
                artist=art,
                album=g["best"]["title_raw"],
                artist_id=artist_rating_key,
                editions=out,
                rationale=rationale,
                merge_list=g["best"].get("merge_list", []),
            )
    return jsonify({}), 404

def _normalize_edition_as_best(edition: dict, artist: str) -> dict:
    """Ensure an edition has the keys expected for group['best']."""
    e = dict(edition)
    if "title_raw" not in e or e["title_raw"] is None:
        try:
            db = plex_connect()
            e["title_raw"] = album_title(db, e["album_id"])
            db.close()
        except Exception:
            e["title_raw"] = ""
    e.setdefault("album_norm", (e.get("title_raw") or "").lower())
    e.setdefault("fmt_text", e.get("fmt", ""))
    e.setdefault("br", 0)
    e.setdefault("sr", 0)
    e.setdefault("bd", 0)
    e.setdefault("rationale", "")
    e.setdefault("merge_list", [])
    e.setdefault("used_ai", False)
    e.setdefault("meta", {})
    e.setdefault("dur", 0)
    e.setdefault("discs", 1)
    return e


def _run_dedupe_artist_one(art: str, album_id: int, keep_edition_album_id: Optional[int], group_copy: dict) -> None:
    """Run dedupe for one group in a background thread. Updates state and DB."""
    with lock:
        state["deduping"] = True
        state["dedupe_progress"] = 0
        state["dedupe_total"] = 1
    try:
        moved_list = perform_dedupe(group_copy)
        removed_count = len(moved_list)
        total_mb = sum(item["size"] for item in moved_list)
        increment_stat("removed_dupes", removed_count)
        increment_stat("space_saved", total_mb)
        logging.debug(f"dedupe_artist(): removed {removed_count} dupes, freed {total_mb} MB")

        letter = quote_plus(art[0].upper())
        art_enc = quote_plus(art)
        try:
            plex_api(f"/library/sections/{SECTION_ID}/refresh?path=/music/matched/{letter}/{art_enc}", method="GET")
            plex_api(f"/library/sections/{SECTION_ID}/emptyTrash", method="PUT")
        except Exception as e:
            logging.warning(f"dedupe_artist(): plex refresh/emptyTrash failed: {e}")

        with lock:
            groups = state["duplicates"].get(art, [])
            groups[:] = [gr for gr in groups if gr["album_id"] != album_id]
            if not groups:
                state["duplicates"].pop(art, None)
            con = sqlite3.connect(str(STATE_DB_FILE))
            cur = con.cursor()
            cur.execute("DELETE FROM duplicates_best WHERE artist = ? AND album_id = ?", (art, album_id))
            cur.execute("DELETE FROM duplicates_loser WHERE artist = ? AND album_id = ?", (art, album_id))
            con.commit()
            con.close()
            sid = state.get("scan_id")
            state["dedupe_progress"] = 1
            state["deduping"] = False
        if sid is not None:
            update_dedupe_scan_summary(sid, total_mb, removed_count)
    except Exception as e:
        logging.exception("dedupe_artist background: %s", e)
        with lock:
            state["deduping"] = False
            state["dedupe_progress"] = 0
            state["dedupe_total"] = 0


@app.post("/dedupe/artist/<artist>")
def dedupe_artist(artist):
    r = _requires_config()
    if r is not None:
        return r
    ensure_dedupe_scan_id()
    art = artist.replace("_", " ")
    data = request.get_json() or {}
    raw = data.get("album_id")
    album_id = int(raw) if raw is not None else None
    keep_edition_album_id = data.get("keep_edition_album_id")
    if keep_edition_album_id is not None:
        keep_edition_album_id = int(keep_edition_album_id)

    group_copy = None
    with lock:
        groups = state["duplicates"].get(art, [])
        for g in list(groups):
            if g["album_id"] != album_id:
                continue
            if keep_edition_album_id is not None:
                editions = [g["best"]] + g["losers"]
                kept = None
                losers = []
                for e in editions:
                    aid = e.get("album_id")
                    if aid == keep_edition_album_id:
                        kept = e
                    else:
                        losers.append(e)
                if kept is None or not losers:
                    return jsonify({"error": "Invalid keep_edition_album_id or no editions to remove"}), 400
                group_copy = {
                    "artist": art,
                    "album_id": album_id,
                    "best": _normalize_edition_as_best(kept, art),
                    "losers": losers,
                }
            else:
                import copy
                group_copy = copy.deepcopy(g)
            break

    if group_copy is None:
        return jsonify({"error": "Group not found"}), 404

    threading.Thread(target=_run_dedupe_artist_one, args=(art, album_id, keep_edition_album_id, group_copy), daemon=True).start()
    return jsonify(status="started", message="Deduplication started", moved=[]), 202


# Allowed extensions for bonus track move (security)
_MOVE_TRACK_EXTENSIONS = frozenset(
    ".flac .wav .m4a .mp3 .ogg .opus .aac .ape .alac .dsf .aif .aiff .wma .mp4 .m4b .m4p .aifc".split()
)


def _merge_bonus_tracks_for_group(g: dict) -> None:
    """
    For one duplicate group, move bonus tracks (names in merge_list) from loser
    editions into the best edition folder. Idempotent per track.
    """
    merge_list = g["best"].get("merge_list") or []
    if not merge_list:
        return
    merge_set = {(t.strip().lower()): t.strip() for t in merge_list}
    best_folder = path_for_fs_access(Path(g["best"]["folder"]))
    db_conn = None
    try:
        db_conn = plex_connect()
        for loser in g["losers"]:
            source_folder = path_for_fs_access(Path(loser["folder"]))
            for t in get_tracks_for_details(db_conn, loser["album_id"]):
                title = (t.get("title") or t.get("name") or "").strip()
                if not title or title.lower() not in merge_set:
                    continue
                raw_path = t.get("path")
                if not raw_path:
                    continue
                track_path = Path(raw_path)
                try:
                    src_resolved = path_for_fs_access(track_path).resolve()
                    base_resolved = source_folder.resolve()
                except Exception:
                    continue
                if not src_resolved.is_file():
                    continue
                if src_resolved.suffix.lower() not in _MOVE_TRACK_EXTENSIONS:
                    continue
                try:
                    if not src_resolved.is_relative_to(base_resolved):
                        continue
                except AttributeError:
                    if not str(src_resolved).startswith(str(base_resolved)):
                        continue
                dest_file = best_folder / src_resolved.name
                if dest_file.exists():
                    stem, suf = dest_file.stem, dest_file.suffix
                    n = 1
                    while dest_file.exists():
                        dest_file = best_folder / f"{stem} ({n}){suf}"
                        n += 1
                try:
                    safe_move(str(src_resolved), str(dest_file))
                    logging.info("merge_bonus: moved %s ‚Üí %s", src_resolved.name, best_folder)
                except Exception as e:
                    logging.warning("merge_bonus: failed %s ‚Üí %s: %s", src_resolved, dest_file, e)
        try:
            plex_path = g["best"]["folder"]
            plex_api(f"/library/sections/{SECTION_ID}/refresh?path={plex_path}", method="GET")
        except Exception as e:
            logging.warning("merge_bonus: Plex refresh failed: %s", e)
    finally:
        if db_conn:
            try:
                db_conn.close()
            except Exception:
                pass


@app.post("/dedupe/move-track/<artist>")
def dedupe_move_track(artist):
    """
    Move a single bonus track file from one edition folder to the kept edition folder.
    Body: { "album_id": "<group id>", "source_index": int, "track_path": str, "target_index": int }.
    """
    r = _requires_config()
    if r is not None:
        return r
    art = artist.replace("_", " ")
    data = request.get_json() or {}
    raw_album_id = data.get("album_id")
    album_id = int(raw_album_id) if raw_album_id is not None else None
    source_index = data.get("source_index")
    target_index = data.get("target_index")
    track_path_raw = data.get("track_path")

    if album_id is None or source_index is None or target_index is None or not track_path_raw:
        return jsonify(success=False, message="Missing album_id, source_index, target_index or track_path"), 400

    try:
        source_index = int(source_index)
        target_index = int(target_index)
    except (TypeError, ValueError):
        return jsonify(success=False, message="source_index and target_index must be integers"), 400

    with lock:
        groups = state["duplicates"].get(art)
        if groups is None:
            groups = load_scan_from_db().get(art, [])
        g = next((gr for gr in groups if gr["album_id"] == album_id), None)

    if g is None:
        return jsonify(success=False, message="Duplicate group not found"), 404

    editions = [g["best"]] + g["losers"]
    if source_index < 0 or source_index >= len(editions) or target_index < 0 or target_index >= len(editions):
        return jsonify(success=False, message="Invalid source_index or target_index"), 400
    if source_index == target_index:
        return jsonify(success=False, message="Source and target editions must differ"), 400

    source_folder = path_for_fs_access(Path(editions[source_index]["folder"]))
    target_folder = path_for_fs_access(Path(editions[target_index]["folder"]))

    track_path = Path(track_path_raw)
    try:
        src_resolved = track_path.resolve()
        base_resolved = source_folder.resolve()
    except Exception as e:
        return jsonify(success=False, message=f"Invalid path: {e}"), 400

    if not src_resolved.is_file():
        return jsonify(success=False, message="track_path is not a file"), 400
    if src_resolved.suffix.lower() not in _MOVE_TRACK_EXTENSIONS:
        return jsonify(success=False, message="File type not allowed for move"), 400
    try:
        if not src_resolved.is_relative_to(base_resolved):
            return jsonify(success=False, message="Track must be inside the source edition folder"), 400
    except AttributeError:
        if not str(src_resolved).startswith(str(base_resolved)):
            return jsonify(success=False, message="Track must be inside the source edition folder"), 400

    dest_file = target_folder / src_resolved.name
    if dest_file.exists():
        stem, suf = dest_file.stem, dest_file.suffix
        n = 1
        while dest_file.exists():
            dest_file = target_folder / f"{stem} ({n}){suf}"
            n += 1

    try:
        safe_move(str(src_resolved), str(dest_file))
    except Exception as e:
        logging.exception("move-track: move failed %s ‚Üí %s", src_resolved, dest_file)
        return jsonify(success=False, message=str(e)), 500

    # Ask Plex to rescan so the kept album sees the new file
    try:
        plex_path = editions[target_index]["folder"]
        plex_api(f"/library/sections/{SECTION_ID}/refresh?path={plex_path}", method="GET")
    except Exception as e:
        logging.warning("move-track: Plex refresh failed: %s", e)

    return jsonify(success=True, message="Track moved to kept edition", dest=str(dest_file)), 200


def _dedupe_all_impl():
    """Shared logic for POST /dedupe/all and POST /api/dedupe/all."""
    r = _requires_config()
    if r is not None:
        return r
    with lock:
        all_groups = [g for lst in state["duplicates"].values() for g in lst]
        if not state["duplicates"]:
            state["duplicates"] = load_scan_from_db()
        all_groups = [g for lst in state["duplicates"].values() for g in lst]
    if not all_groups:
        return "", 204
    threading.Thread(target=background_dedupe, args=(all_groups,), daemon=True).start()
    return "", 204


@app.post("/api/dedupe/all")
def api_dedupe_all():
    return _dedupe_all_impl()


@app.post("/dedupe/all")
def dedupe_all():
    return _dedupe_all_impl()


@app.post("/dedupe/merge-and-dedupe")
def dedupe_merge_and_dedupe():
    """
    First merge bonus tracks (from merge_list) into the kept edition for every group
    that has extra tracks, then run full dedupe (move loser folders, update Plex).
    """
    r = _requires_config()
    if r is not None:
        return r
    with lock:
        if not state["duplicates"]:
            state["duplicates"] = load_scan_from_db()
        all_groups = [g for lst in state["duplicates"].values() for g in lst]

    for g in all_groups:
        if g["best"].get("merge_list"):
            try:
                _merge_bonus_tracks_for_group(g)
            except Exception as e:
                logging.warning("merge_and_dedupe: merge_bonus failed for %s: %s", g.get("artist"), e)

    threading.Thread(target=background_dedupe, args=(all_groups,), daemon=True).start()
    return "", 204

@app.post("/dedupe/selected")
def dedupe_selected():
    r = _requires_config()
    if r is not None:
        return r
    ensure_dedupe_scan_id()
    data = request.get_json() or {}
    selected = data.get("selected", [])
    moved_list: List[Dict] = []
    total_moved = 0
    removed_count = 0
    artists_to_refresh = set()

    for sel in selected:
        art_key, aid_str = sel.split("||", 1)
        art = art_key.replace("_", " ")
        album_id = int(aid_str)
        with lock:
            groups = state["duplicates"].get(art, [])
            for g in list(groups):
                if g["album_id"] == album_id:
                    logging.debug(f"dedupe_selected(): removing selected group for artist '{art}', album_id={album_id}")
                    moved = perform_dedupe(g)
                    moved_list.extend(moved)
                    total_moved += sum(item["size"] for item in moved)
                    removed_count += len(g["losers"])
                    artists_to_refresh.add(art)
                    groups.remove(g)
                    if not groups:
                        del state["duplicates"][art]
                    con = sqlite3.connect(str(STATE_DB_FILE))
                    cur = con.cursor()
                    cur.execute("DELETE FROM duplicates_best WHERE artist = ? AND album_id = ?", (art, album_id))
                    cur.execute("DELETE FROM duplicates_loser WHERE artist = ? AND album_id = ?", (art, album_id))
                    con.commit()
                    con.close()
                    break

    for art in artists_to_refresh:
        letter  = quote_plus(art[0].upper())
        art_enc = quote_plus(art)
        try:
            plex_api(f"/library/sections/{SECTION_ID}/refresh?path=/music/matched/{letter}/{art_enc}", method="GET")
            plex_api(f"/library/sections/{SECTION_ID}/emptyTrash", method="PUT")
        except Exception as e:
            logging.warning(f"dedupe_selected(): plex refresh/emptyTrash failed for {art}: {e}")

    increment_stat("removed_dupes", removed_count)
    increment_stat("space_saved", total_moved)
    logging.debug(f"dedupe_selected(): removed {removed_count} dupes, freed {total_moved} MB")

    with lock:
        sid = state.get("scan_id")
    if sid is not None:
        update_dedupe_scan_summary(sid, total_moved, removed_count)

    return jsonify(moved=moved_list), 200


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Assistant API ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
@app.get("/api/assistant/status")
def api_assistant_status():
    """Lightweight health/status endpoint for the in-UI assistant."""
    payload = {
        "library_mode": _get_library_mode(),
        "ai_provider": getattr(sys.modules[__name__], "AI_PROVIDER", "openai"),
        "ai_model": getattr(sys.modules[__name__], "RESOLVED_MODEL", None) or getattr(sys.modules[__name__], "OPENAI_MODEL", ""),
        "ai_ready": bool(getattr(sys.modules[__name__], "ai_provider_ready", False)),
        "ai_error": getattr(sys.modules[__name__], "AI_FUNCTIONAL_ERROR_MSG", None) if not bool(getattr(sys.modules[__name__], "ai_provider_ready", False)) else None,
        "postgres_ready": False,
        "pg_host": PMDA_PG_HOST,
        "pg_port": PMDA_PG_PORT,
        "pg_db": PMDA_PG_DB,
        "pg_user": PMDA_PG_USER,
        "config_dir": str(CONFIG_DIR),
        "config_sources": {k: ENV_SOURCES.get(k, "") for k in ("AI_PROVIDER", "OPENAI_API_KEY", "OPENAI_MODEL", "FILES_ROOTS", "LIBRARY_MODE")},
    }
    if _get_library_mode() == "files":
        payload["postgres_ready"] = bool(_files_pg_init_schema())
    return jsonify(payload)


@app.get("/api/assistant/session/<session_id>")
def api_assistant_get_session(session_id: str):
    if _get_library_mode() != "files":
        return jsonify({"error": "Assistant is available in Files mode only"}), 400
    ok, err = _ensure_files_index_ready()
    if not ok:
        return jsonify({"error": err or "Files index unavailable"}), 503
    conn = _files_pg_connect()
    if conn is None:
        return jsonify({"error": "PostgreSQL unavailable"}), 503
    try:
        limit = max(1, min(400, _parse_int_loose(request.args.get("limit"), 120)))
        with conn.cursor() as cur:
            cur.execute("SELECT session_id FROM assistant_sessions WHERE session_id = %s", ((session_id or "").strip(),))
            if not cur.fetchone():
                return jsonify({"session_id": (session_id or "").strip(), "messages": []})
            cur.execute(
                """
                SELECT id, role, content, context_json, metadata_json, EXTRACT(EPOCH FROM created_at)::BIGINT
                FROM assistant_messages
                WHERE session_id = %s
                ORDER BY created_at ASC
                LIMIT %s
                """,
                ((session_id or "").strip(), int(limit)),
            )
            rows = cur.fetchall()
        messages = []
        for mid, role, content, ctx_json, meta_json, created_at in rows:
            try:
                ctx = json.loads(ctx_json or "{}") if ctx_json else {}
            except Exception:
                ctx = {}
            try:
                meta = json.loads(meta_json or "{}") if meta_json else {}
            except Exception:
                meta = {}
            messages.append(
                {
                    "id": int(mid or 0),
                    "role": (role or "").strip().lower(),
                    "content": (content or "").strip(),
                    "created_at": int(created_at or 0),
                    "context": ctx if isinstance(ctx, dict) else {},
                    "metadata": meta if isinstance(meta, dict) else {},
                }
            )
        return jsonify({"session_id": (session_id or "").strip(), "messages": messages})
    finally:
        conn.close()


@app.post("/api/assistant/chat")
def api_assistant_chat():
    """Chat endpoint: uses Postgres-backed RAG (artist profiles + local library snapshot)."""
    if _get_library_mode() != "files":
        return jsonify({"error": "Assistant is available in Files mode only"}), 400
    if not bool(getattr(sys.modules[__name__], "ai_provider_ready", False)):
        msg = getattr(sys.modules[__name__], "AI_FUNCTIONAL_ERROR_MSG", None) or "AI is not configured"
        return jsonify({"error": msg}), 503
    data = request.get_json() or {}
    if not isinstance(data, dict):
        return jsonify({"error": "Invalid JSON body"}), 400
    user_message = str(data.get("message") or "").strip()
    if not user_message:
        return jsonify({"error": "message is required"}), 400
    session_id = str(data.get("session_id") or "").strip()
    ctx = data.get("context") or {}
    if not isinstance(ctx, dict):
        ctx = {}

    ok, err = _ensure_files_index_ready()
    if not ok:
        return jsonify({"error": err or "Files index unavailable"}), 503
    conn = _files_pg_connect()
    if conn is None:
        return jsonify({"error": "PostgreSQL unavailable"}), 503

    try:
        _assistant_maybe_gc(conn)
        session_id = _assistant_ensure_session(conn, session_id)

        # Conversation history before we insert current user message.
        history = _assistant_fetch_session_messages(conn, session_id, limit=10)

        # Ensure a library snapshot doc exists so collection-wide questions have grounding.
        try:
            _assistant_ingest_library_rag(conn)
        except Exception:
            pass

        context_artist_id = _parse_int_loose(ctx.get("artist_id"), 0)
        context_inferred = False
        context_info: dict = {}

        if context_artist_id and int(context_artist_id) > 0:
            context_info = _assistant_ingest_artist_rag(conn, int(context_artist_id))
        else:
            # Best-effort inference for "Ask PMDA" from anywhere.
            inferred_ids = _assistant_find_artist_ids_for_query(conn, user_message, limit=1)
            if inferred_ids:
                context_inferred = True
                context_artist_id = inferred_ids[0]
                ctx = dict(ctx)
                ctx["artist_id"] = int(context_artist_id)
                ctx["context_inferred"] = True
                context_info = _assistant_ingest_artist_rag(conn, int(context_artist_id))

        # Persist the user message.
        user_msg_row = _assistant_insert_message(
            conn,
            session_id=session_id,
            role="user",
            content=user_message,
            context=ctx,
            metadata={},
        )

        # Fast path: answer common DB-grounded questions without paying LLM tokens.
        try:
            tool = _assistant_try_handle_tool_query(
                conn,
                user_message=user_message,
                context_artist_id=int(context_artist_id or 0),
                base_url=request.url_root.rstrip("/"),
            )
        except Exception:
            tool = {"handled": False}

        if tool.get("handled"):
            assistant_text = str(tool.get("assistant_text") or "").strip()
            citations = tool.get("citations") or []
            links = tool.get("links") or []
            assistant_meta = {
                "provider": "pmda_db",
                "model": str(tool.get("tool") or "sql_tool_v1"),
                "context_inferred": bool(context_inferred),
                "citations": citations,
                "links": links,
                "tool": str(tool.get("tool") or ""),
            }
            assistant_msg_row = _assistant_insert_message(
                conn,
                session_id=session_id,
                role="assistant",
                content=assistant_text,
                context=ctx,
                metadata=assistant_meta,
            )
            return jsonify(
                {
                    "session_id": session_id,
                    "user_message": user_msg_row,
                    "assistant_message": assistant_msg_row,
                    "citations": citations,
                }
            )

        # Slow path: LLM-assisted SQL query over the library DB (read-only).
        try:
            sql_tool = _assistant_try_handle_sql_agent_query(
                conn,
                user_message=user_message,
                context_artist_id=int(context_artist_id or 0),
                base_url=request.url_root.rstrip("/"),
            )
        except Exception:
            sql_tool = {"handled": False}

        if sql_tool.get("handled"):
            assistant_text = str(sql_tool.get("assistant_text") or "").strip()
            citations = sql_tool.get("citations") or []
            links = sql_tool.get("links") or []
            assistant_meta = {
                "provider": "pmda_db",
                "model": str(sql_tool.get("tool") or "sql_agent_v1"),
                "context_inferred": bool(context_inferred),
                "citations": citations,
                "links": links,
                "tool": str(sql_tool.get("tool") or ""),
            }
            assistant_msg_row = _assistant_insert_message(
                conn,
                session_id=session_id,
                role="assistant",
                content=assistant_text,
                context=ctx,
                metadata=assistant_meta,
            )
            return jsonify(
                {
                    "session_id": session_id,
                    "user_message": user_msg_row,
                    "assistant_message": assistant_msg_row,
                    "citations": citations,
                }
            )

        retrieved = _assistant_retrieve_chunks(conn, user_message, artist_id=int(context_artist_id or 0), k=8)
        system_msg, prompt = _assistant_build_prompt(
            user_message=user_message,
            retrieved=retrieved,
            history=history,
            context_info=context_info,
        )

        provider = getattr(sys.modules[__name__], "AI_PROVIDER", "openai")
        model = getattr(sys.modules[__name__], "RESOLVED_MODEL", None) or getattr(sys.modules[__name__], "OPENAI_MODEL", "gpt-4o-mini")
        assistant_text = call_ai_provider_longform(provider, model, system_msg, prompt, max_tokens=900)

        assistant_meta = {
            "provider": provider,
            "model": model,
            "context_inferred": bool(context_inferred),
            "citations": retrieved.get("citations") or [],
            "links": [],
        }
        assistant_msg_row = _assistant_insert_message(
            conn,
            session_id=session_id,
            role="assistant",
            content=assistant_text,
            context=ctx,
            metadata=assistant_meta,
        )

        return jsonify(
            {
                "session_id": session_id,
                "user_message": user_msg_row,
                "assistant_message": assistant_msg_row,
                "citations": retrieved.get("citations") or [],
            }
        )
    finally:
        conn.close()


# ‚îÄ‚îÄ‚îÄ Integrated frontend (self-hosted: serve SPA from same container) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
if _HAS_STATIC_UI:
    def _ui_build_payload():
        """Return the current Vite asset paths from dist/index.html.

        This is used by the frontend to detect stale SPA tabs after a deploy and prompt reload.
        """
        import re

        index_path = os.path.join(_FRONTEND_DIST, "index.html")
        try:
            with open(index_path, "r", encoding="utf-8") as f:
                html = f.read()
        except Exception as e:
            return {"ok": False, "error": f"Failed to read index.html: {e}"}

        js_matches = re.findall(r'src="(/assets/[^"]+\.js)"', html)
        css_matches = re.findall(r'href="(/assets/[^"]+\.css)"', html)

        # Prefer the main Vite entry (usually /assets/index-<hash>.js)
        asset_js = next((p for p in js_matches if "/assets/index-" in p), js_matches[0] if js_matches else None)
        asset_css = next((p for p in css_matches if "/assets/index-" in p), css_matches[0] if css_matches else None)

        try:
            index_mtime = os.path.getmtime(index_path)
        except Exception:
            index_mtime = None

        return {
            "ok": True,
            "index_mtime": index_mtime,
            "asset_js": asset_js,
            "asset_css": asset_css,
        }

    @app.get("/api/ui/build")
    def api_ui_build():
        payload = _ui_build_payload()
        resp = jsonify(payload)
        resp.headers["Cache-Control"] = "no-store, no-cache, must-revalidate, max-age=0"
        resp.headers["Pragma"] = "no-cache"
        resp.headers["Expires"] = "0"
        return resp

    def _send_index_no_cache():
        resp = send_from_directory(_FRONTEND_DIST, "index.html")
        # Prevent stale SPA shell after deploys; assets remain hash-versioned.
        resp.headers["Cache-Control"] = "no-store, no-cache, must-revalidate, max-age=0"
        resp.headers["Pragma"] = "no-cache"
        resp.headers["Expires"] = "0"
        return resp

    @app.get("/")
    def serve_index():
        return _send_index_no_cache()

    @app.get("/assets/<path:path>")
    def serve_assets(path):
        return send_from_directory(os.path.join(_FRONTEND_DIST, "assets"), path)

    @app.get("/<path:path>")
    def serve_spa_fallback(path):
        """SPA: serve static file from dist if present, else index.html for client-side routing."""
        if request.path.startswith(("/api/", "/scan/", "/dedupe/", "/details/")):
            return jsonify(error="Not found"), 404
        path_obj = os.path.join(_FRONTEND_DIST, path)
        if os.path.isfile(path_obj):
            return send_from_directory(_FRONTEND_DIST, path)
        return _send_index_no_cache()


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ MAIN ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
import os

if __name__ == "__main__":
    # Web UI only: start server first so UI is available immediately, then run startup checks (cross-check in background).
    def run_server():
        app.run(host="0.0.0.0", port=WEBUI_PORT, threaded=True, use_reloader=False)

    # Fast checks before server (can SystemExit)
    if PLEX_CONFIGURED:
        _validate_plex_connection()
        if not _self_diag():
            raise SystemExit("Self‚Äëdiagnostic failed ‚Äì please fix the issues above and restart PMDA.")
    else:
        logging.info("Skipping startup checks (Plex not configured ‚Äì use Settings to configure).")

    server_thread = threading.Thread(target=run_server, daemon=False)
    server_thread.start()
    logging.info("Web UI listening on http://0.0.0.0:%s", WEBUI_PORT)

    def run_cross_check_background():
        if _get_library_mode() == "files":
            logging.info("PATH cross-check skipped at startup (LIBRARY_MODE=files).")
        elif PLEX_CONFIGURED and not DISABLE_PATH_CROSSCHECK:
            _cross_check_bindings(raise_on_abort=False)
        elif DISABLE_PATH_CROSSCHECK:
            logging.info("PATH cross-check skipped (DISABLE_PATH_CROSSCHECK=true).")

    cross_check_thread = threading.Thread(target=run_cross_check_background, daemon=True)
    cross_check_thread.start()

    if _get_library_mode() == "files":
        _restart_files_watcher_if_needed()
        def run_files_index_bootstrap():
            ok, err = _ensure_files_index_ready()
            if ok:
                logging.info("Files library index is ready.")
            else:
                logging.warning("Files library index bootstrap failed: %s", err)
        threading.Thread(target=run_files_index_bootstrap, daemon=True, name="files-index-bootstrap").start()

    server_thread.join()  # block forever (app.run never returns)
